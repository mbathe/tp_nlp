{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "class Graphe:\n",
    "    def __init__(self, documents, custom_stop_words=None):\n",
    "        self.documents = documents\n",
    "        self.terms = []\n",
    "        self.co_occurrence = None\n",
    "        self.distance_matrix = None\n",
    "        self.graph = None\n",
    "        # Liste des mots personnalisés à exclure\n",
    "        self.custom_stop_words = custom_stop_words if custom_stop_words else list(stopwords.words(\n",
    "            'english'))\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.documents = [re.sub(\n",
    "            r'(#\\S+|@\\S+|\\S*@\\S*\\s?|http\\S+|[^A-Za-z0-9]\\'\\'|\\d+|<[^>]*>|[^A-Za-z0-9\\'\\- ]+)', \"\", doc) for doc in self.documents]\n",
    "        vectorizer = CountVectorizer(ngram_range=(\n",
    "            1, 2), stop_words=self.custom_stop_words)\n",
    "        X = vectorizer.fit_transform(self.documents)\n",
    "        self.terms = vectorizer.get_feature_names_out()\n",
    "        self.co_occurrence = (X.T @ X).toarray()\n",
    "        np.fill_diagonal(self.co_occurrence, 0)\n",
    "        print(X.shape, self.terms.shape, self.co_occurrence.shape)\n",
    "\n",
    "    def compute_distance_matrix(self):\n",
    "        self.distance_matrix = np.zeros(self.co_occurrence.shape)\n",
    "        for i in tqdm(range(self.co_occurrence.shape[0])):\n",
    "            for j in range(self.co_occurrence.shape[1]):\n",
    "                if self.co_occurrence[i, j] > 0:\n",
    "                    self.distance_matrix[i, j] = 1 / self.co_occurrence[i, j]\n",
    "                else:\n",
    "                    self.distance_matrix[i, j] = np.inf\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.graph = nx.Graph()\n",
    "        for i in tqdm(range(len(self.terms))):\n",
    "            for j in range(len(self.terms)):\n",
    "                if i != j:\n",
    "                    self.graph.add_edge(\n",
    "                        self.terms[i], self.terms[j], weight=self.distance_matrix[i, j])\n",
    "\n",
    "    def detect_keywords(self):\n",
    "        keyword_scores = {}\n",
    "        for i, term in enumerate(self.terms):\n",
    "            neighbors = list(self.graph.neighbors(term))\n",
    "            if neighbors:\n",
    "                sum_distance = np.sum(\n",
    "                    [self.distance_matrix[i, self.terms.tolist().index(n)] for n in neighbors])\n",
    "                keyword_scores[term] = sum_distance\n",
    "\n",
    "        # Trier par moyenne des distances\n",
    "        keywords = sorted(keyword_scores.items(), key=lambda item: item[1])\n",
    "        return keywords\n",
    "\n",
    "    def summarize(self, num_sentences=2):\n",
    "        sentences = []\n",
    "        for doc in self.documents:\n",
    "            sentences.extend(sent_tokenize(doc, language='english'))\n",
    "\n",
    "        vectorizer = CountVectorizer(stop_words='english')\n",
    "        X_sentences = vectorizer.fit_transform(sentences)\n",
    "        co_occurrence_sent = (X_sentences.T @ X_sentences).toarray()\n",
    "        np.fill_diagonal(co_occurrence_sent, 0)\n",
    "\n",
    "        graph_sent = nx.Graph()\n",
    "        for i in tqdm(range(len(sentences))):\n",
    "            for j in range(len(sentences)):\n",
    "                if i != j and co_occurrence_sent[i, j] > 0:\n",
    "                    graph_sent.add_edge(\n",
    "                        sentences[i], sentences[j], weight=1/co_occurrence_sent[i, j])\n",
    "\n",
    "        centrality_sent = nx.degree_centrality(graph_sent)\n",
    "        ranked_sentences = sorted(\n",
    "            centrality_sent.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        summary = [sentence for sentence,\n",
    "                   _ in ranked_sentences[:num_sentences]]\n",
    "        return summary\n",
    "\n",
    "    def analyze(self):\n",
    "        self.preprocess()\n",
    "        self.compute_distance_matrix()\n",
    "        self.build_graph()\n",
    "        return self.detect_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 370.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./src/docfile/txts2/719.txt\n",
      "(1, 9506) (9506,) (9506, 9506)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9506/9506 [00:20<00:00, 473.25it/s]\n",
      "100%|██████████| 9506/9506 [04:21<00:00, 36.30it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "documents = []\n",
    "for file in tqdm(glob.glob(os.path.join(os.getenv('TXT_FOLDER2'), \"*.txt\"))[0:1]):\n",
    "    print(file)\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read().strip().lower()\n",
    "        if len(doc) > 10:\n",
    "            documents.append(doc)\n",
    "# print(documents[0])\n",
    "graphe = Graphe(documents)\n",
    "keywords = graphe.analyze()\n",
    "\n",
    "print(\"Mots-clés détectés :\")\n",
    "\"\"\" for word, score in keywords:\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "summary = graphe.summarize(num_sentences=1) \"\"\"\n",
    "print(\"\\nRésumé des documents :\")\n",
    "\"\"\" for sentence in summary:\n",
    "    print(f\"- {sentence}\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp-nlp-R8Mk3SIc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
