{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible\n"
     ]
    }
   ],
   "source": [
    "from src.code.tools.preprocessing import TextPreprocessor\n",
    "from src.code.tools.metrics import get_bert_score\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "import os\n",
    "from src.code.TF_IDF_Fasttext.dbscan import DBSCAN\n",
    "from src.code.TFIDF.pre_processing import Processing\n",
    "from src.code.bert.bert import Bert\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from nltk.tokenize import sent_tokenize\n",
    "load_dotenv()\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Méthode utilisant les mots-clés pour obtenir le résumé du document** : nous déterminons les poids tf-idf de chaque mot dans une phrase du document, puis nous trouvons le poids définitif de chaque mot dans le document en faisant la somme des poids des mots dans chaque document où il intervient. Ensuite, nous trions ces mots-clés et prenons les top_n mots qui ont les meilleurs scores. Le document résumé est celui dont la somme des scores des mots parmi les n_top mots-clés est maximale.\n",
    "2. **Méthode utilisant BERT** : ici, nous utilisons le modèle BERT pour obtenir un embedding de chaque phrase du document. Ensuite, nous faisons la moyenne des embeddings pour obtenir l'embedding du document. Le mot qui résume le mieux le document est celui dont le vecteur est le plus proche, en utilisant la similarité cosinus du vecteur du document.\n",
    "3. **Méthode utilisant TF-IDF et la sémantique avec FastText** : Dans cette méthode, nous utilisons FastText pour réaliser un embedding sémantique des mots, puis nous regroupons sémantiquement les mots en clusters en nous basant sur leur signification. Pour le regroupement, nous appliquons un seuil avec l'algorithme DBSCAN. Une fois que les clusters ont été construits, nous calculons les scores TF-IDF de chaque cluster dans un document, en considérant la somme des scores TF-IDF de chaque mot appartenant au cluster. Une fois les scores TF-IDF de chaque cluster dans les documents calculés, nous identifions les clusters les plus pertinents en cherchant ceux dont la somme des poids dans tous les documents est maximale. Une fois ces n_top_clusters sélectionnés, les meilleurs documents sont ceux dont la somme des scores TF-IDF des mots présents dans le cluster est maximale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "methodes = {\n",
    "    \"TF_IDF_KEY_WORD\": 1,\n",
    "    # \"BERT\": 2,\n",
    "    \"TF_IDF_FASTTEXT\": 3\n",
    "}\n",
    "\n",
    "processor = TextPreprocessor()\n",
    "tf_idf_keyword_process = Processing()\n",
    "tf_idf_fasttext_process = DBSCAN(eps=0.4, model=ft)\n",
    "\n",
    "\n",
    "def get_resume_by_method(documents, methodes, top_n, tokens, tf_idf_dict):\n",
    "    best_resumes_method = {}\n",
    "    for k, v in methodes.items():\n",
    "        if k == \"TF_IDF_KEY_WORD\":\n",
    "            top_doc_indices_keywords = tf_idf_keyword_process.get_resume_docs_index(\n",
    "                top_n=top_n, number_key_words=30, tokens=tokens, tf_idf_dict=tf_idf_dict)\n",
    "            best_resumes_method[k] = \"\\n\".join(\n",
    "                documents[top_doc_indices_keywords[0:top_n]])\n",
    "            \"\"\"  elif k == \"BERT\":\n",
    "            bert = Bert(documents=documents)\n",
    "            bert.processing()\n",
    "            top_doc_indices_bert, resume_doc_bert = bert.get_resume(top_n=1)\n",
    "            best_resumes_method[k] = \"\\n\".join(resume_doc_bert[0:top_n]) \"\"\"\n",
    "\n",
    "        elif k == \"TF_IDF_FASTTEXT\":\n",
    "            best_resumes_index = tf_idf_fasttext_process.get_resumes_doc(\n",
    "                top_n=2, n_clusters=3, tokens=tokens, tf_idf_dict=tf_idf_dict)\n",
    "            best_resumes_method[k] = \"\\n\".join(\n",
    "                documents[best_resumes_index[0:top_n]])\n",
    "\n",
    "    return list(best_resumes_method.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (407600892.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    documents, methodes=methodes, top_n=top_n, tokens, tf_idf_dict)\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def get_best_method(methodes, n_documents, top_n):\n",
    "    scores = np.zeros((len(methodes.keys()), n_documents), dtype=int)\n",
    "    for idx, file in enumerate(tqdm(glob.glob(os.path.join(os.getenv('TXT_FOLDER2'), \"*.txt\"))[0:n_documents])):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            doc = f.read().strip().lower()\n",
    "            if len(doc) > 10:\n",
    "                documents = np.array(sent_tokenize(doc))\n",
    "                # debut = time.time()\n",
    "                tokens, tf_idf_dict = processor.preprocess(documents)\n",
    "                # fin = time.time()\n",
    "\n",
    "                # temps_execution = fin - debut\n",
    "                # #print(f\"Temps d'exécution : {temps_execution:.6f} secondes\")\n",
    "\n",
    "                documents_resume = get_resume_by_method(\n",
    "                    documents, methodes=methodes, top_n=top_n, tokens=tokens, tf_idf_dict=tf_idf_dict)\n",
    "\n",
    "                P, R, F1 = get_bert_score(\n",
    "                    documents_resume, [doc]*len(methodes.keys()))\n",
    "                F1 = F1.numpy()\n",
    "                # #print(F1)\n",
    "                scores[:, idx] += (np.array(np.argsort(F1)[::-1])+1)\n",
    "    return scores, scores.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_resume_by_method() missing 2 required positional arguments: 'tokens' and 'tf_idf_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mget_best_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m scores\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mget_best_method\u001b[0;34m(methodes, n_documents, top_n)\u001b[0m\n\u001b[1;32m     12\u001b[0m tokens, tf_idf_dict \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mpreprocess(documents)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# fin = time.time()\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# temps_execution = fin - debut\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# #print(f\"Temps d'exécution : {temps_execution:.6f} secondes\")\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m documents_resume \u001b[38;5;241m=\u001b[39m \u001b[43mget_resume_by_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m P, R, F1 \u001b[38;5;241m=\u001b[39m get_bert_score(\n\u001b[1;32m     22\u001b[0m     documents_resume, [doc]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(methodes\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     23\u001b[0m F1 \u001b[38;5;241m=\u001b[39m F1\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mTypeError\u001b[0m: get_resume_by_method() missing 2 required positional arguments: 'tokens' and 'tf_idf_dict'"
     ]
    }
   ],
   "source": [
    "scores = get_best_method(methodes, 5, 2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
