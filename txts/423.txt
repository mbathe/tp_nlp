Trust in  Artificial  Intelligence  A five country study March 2021 KPMG .com.au
2 Trust in artificial intelligence University of Queensland Researchers  Professor Nicole Gillespie, Dr Steve  Lockey and Dr Caitlin Curtis KPMG Advisors James Mabbott, Ali Akbari, Rossana  Bianchi and Rita Fentener van Vlissingen  Acknowledgements We are grateful for the insightful input,  expertise and feedback provided by  members of the Trust, Ethics and  Governance Alliance at the University   of Queensland. Citation Gillespie, N., Lockey, S., & Curtis, C.  (2021). Trust in Artificial Intelligence:   A Five Country Study. The University   of Queensland and KPMG Australia.   doi: 10.14264/e34bfa3 Contents Executive summary  1 Introduction  5 How we conducted the research  6 Do citizens trust AI?  8 Who do citizens trust to develop   and regulate AI?  16 What expectations do people have   about AI regulation?  24 What principles are important for   people to trust AI systems?  32 How do citizens feel about AI at work?  36 How do citizens view key AI challenges?  40 How well do citizens understand AI?  44 What are the key drivers of trust   and acceptance of AI?  50 Conclusion and implications  54 Appendix 1  57 Appendix 2  58 Endnotes  60 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
1 Trust in artificial intelligence Executive  summary Artificial Intelligence (AI) is enabling  rapid innovation with many potential  benefits for society and business. The  COVID-19 pandemic has accelerated  the uptake of advanced technology,   and investment in AI continues to   grow exponentially1.  Realising the benefits AI offers requires building and  maintaining the public’s trust: citizens need to be  confident AI is being developed and used in an ethical  and trustworthy manner2. AI poses considerable risks and  challenges to society which have raised concerns about  whether AI systems are worthy of trust. These concerns  have been fuelled by high profile cases of AI use that were  biased, discriminatory, manipulative, unlawful, or violated  human rights.  This survey is the first to take a deep dive into  understanding citizen trust and expectations of AI use  across multiple countries. To do this, we surveyed a  nationally representative sample of 1,200+ citizens from  the United States, Canada, Germany, the United Kingdom  and Australia, respectively (total sample 6,054). We asked  about AI systems in general, as well as AI systems in  two domains – healthcare and human resources – where  AI is rapidly being deployed and is likely to impact large  numbers of citizens. Our findings provide important and timely research  insights into citizens’ trust and attitudes towards AI. We  draw on these insights to lay out an evidence-based  pathway for strengthening trust and acceptance of AI  systems, and discuss the implications for government,  business and non-government organisations (NGOs). Below, we summarise the key findings. Most of  these findings hold across all countries and therefore  are reported in aggregate form. Significant country  differences in the findings are highlighted.  ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
2 Trust in artificial intelligence T rust is central to the acceptance   of AI, and is influenced by four   key drivers Our results confirm that trust strongly  influences AI acceptance, and hence  is important for the societal uptake of  AI and realising its benefits. There are  four key drivers that influence citizens’  trust in AI systems: 1) beliefs about  the adequacy of current regulations  and laws to make AI use safe, 2) the  perceived impact of AI on jobs, 3)  familiarity and understanding of AI,  and 4) the perceived uncertain impact  of AI on society. These drivers are  important across all five countries. Of these drivers, the perceived  adequacy of current regulations   and laws is clearly the strongest.   This highlights the importance  of ensuring adequate regulatory  and legal mechanisms are in place  to protect people from the risks  associated with AI use. Such  regulation in turn supports citizen  uptake and adoption.Citizens have low trust in AI  systems but generally ‘accept’   or ‘tolerate’ AI Trust in AI is low across the five  countries, with citizens generally  wary or ambivalent about trusting  AI systems. Only about a quarter  (28%) of citizens are willing to trust  AI systems in general. Two out of  five citizens are unwilling to share  their information or data with an AI  system and a third are unwilling to  trust the output of AI systems (e.g. a  recommendation or decision). While  many citizens are hesitant to trust AI  systems, they generally accept (42%)  or tolerate (28%) AI, but few approve  (15%) or embrace (6%) it, and some  outright reject AI (9%).  Citizens’ trust and support of AI  depends on the purpose of the AI  system: the public is more trusting  and supportive of AI use in healthcare  (i.e. for aiding medical diagnosis  and treatment), and less trusting  and supportive of AI use in human  resources (i.e. for aiding hiring and  promotion decisions). Citizens also  view the benefits of AI in healthcare  as greater than the risks, whereas  they view the risks of AI in human  resources as greater than the  benefits. However, regardless of the  application, citizens are still wary with  the majority unwilling or ambivalent  about trusting AI in both healthcare  (63%) and human resources (77%). Younger generations, notably Gen Z  and Millennials, are generally more  trusting and accepting of AI systems  than older generations. In Germany  and Australia, those with a university  education are also more accepting of  AI than those without a degree.Confidence in entities to   develop, use and regulate   AI varies across countries Citizens have the most confidence  in their national universities and  research institutions, as well as their  defence organisations, to develop  and use (71-77%) and regulate  and govern AI (67-73%) in the best  interest of the public. In contrast,  citizens have less confidence in  governments and commercial  organisations to do this. 58% – 62%  have confidence in commercial  organisations and government to  develop and use AI, and 54 – 58%  have confidence in these entities  to regulate and govern AI. This may  be because most citizens believe  commercial organisations (62%) and  government (52%) innovate with  AI for financial gain, rather than for  societal benefit.  Countries differ in their confidence  of entities to use and govern AI.  Americans are less confident  in a broad range of entities to  regulate and govern AI, compared  to citizens in other countries. US  and UK respondents are also less  confident in their governments to  develop and use AI in the public’s  best interest compared to other  countries, a finding that mirrors  the lower trust these countries  have in their governments more  generally. In contrast, Australians  are more confident in their research  institutions and defence forces to  develop, use and regulate AI.  ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
3 Trust in artificial intelligence Citizens expect AI to be   regulated with external,  independent oversight  The large majority of citizens (81%)  expect AI to be regulated. While  there are small country differences,  there is general agreement (61-62%)  that there should be a new, dedicated  independent AI regulator, and that  government and existing regulators  should play a role in the regulation  of AI systems. Co-regulation and  involvement of industry that develop  or use AI is also seen as desirable by  the majority (54-59%). US respondents are less likely than  respondents in other countries to  report that government and existing  regulators should regulate AI, and  more likely to believe AI regulation  is not required. In contrast, UK  respondents are more likely to expect  a new, dedicated, independent AI  regulator than other countries.Current safeguards are insufficient  given the uncertainty around AI Despite the strong expectations  of AI regulation, most citizens  (67%) across the five countries  either disagree or are ambivalent  that current regulations and laws  are sufficient to make the use of  AI safe. This powerfully highlights  the importance of strengthening  and communicating the regulatory  and legal framework governing AI  (including data privacy laws) across  all surveyed countries.  Most citizens (66-79%) in each  country believe the impact of  AI on society is uncertain and  unpredictable. It is therefore not  surprising that the large majority  (96%) expect AI governance  challenges to be carefully managed.  The public view data challenges such  as surveillance (61%), fake online  content (60%), cyber-attacks (60%),  and data privacy (59%) to be the  most likely to impact large numbers  of citizens within their country in  the next 10 years. Half also viewed  disease misdiagnosis as likely to  impact society. Citizens expect organisations  to uphold the principles of  trustworthy AI  Citizens in each country have very  clear expectations of the principles  and related practices they expect  organisations deploying AI systems  to uphold in order to be trusted. These  principles mirror those proposed by  the European Commission’s High  Level Expert Group on AI. Almost all  citizens (95%) expect AI systems to  meet high standards of:    –performance and accuracy    –data privacy    –security and governance    –transparency and explainability    –accountability    –risk and impact mitigation    –fairness    –human oversight Most citizens (more than 57%)  would be more willing to use AI  systems if assurance mechanisms  were in place, such as independent  AI ethics reviews, AI ethics  certifications, national standards  for transparency, and AI codes of  conduct. Organisations can directly  build trust and consumer willingness  to use AI systems by supporting and  implementing these mechanisms. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
4 Trust in artificial intelligence Citizens feel comfortable with  some but not all uses of AI at work Only about one in five citizens (22%)  believe AI will create more jobs than  it will eliminate. Despite this, 45%  of employed citizens report using  AI in their work, but most use AI  rarely or occasionally. Most citizens  (70-76%) are comfortable with the  use of AI at work for the purposes of  task automation and augmentation.  However, they are less comfortable  with the use of AI for employeefocused activities, such as monitoring  and evaluating employees, or in  recruitment and selection. Citizens want to know more  about AI but currently have low  awareness and understanding   of AI and its uses  Most citizens (62%) have heard  about AI. However, three out of  every five citizens report a low  understanding of AI, including how  and when it is used in everyday  applications. For example, even  though 76% of citizens use social  media, only 41% are aware social  media uses AI. Men and the  university-educated are more likely  to be aware of AI and understand  when it is being used. The good  news is that most citizens across all  countries (83%) want to know more  about AI. Considered together, the  results suggest there is a strong  need and appetite for a public AI  literacy program. A pathway to strengthen   public trust in AI  Collectively these survey insights  provide an evidence-based pathway  for building and maintaining the trust  and acceptance of AI systems by  citizens of western nations. As we  discuss in detail in the concluding  section, this pathway requires  government and business to take  action by: 1) living up to citizens’  expectations of trustworthy AI,  2) strengthening the regulatory  framework for governing AI, and 3)  enhancing AI literacy of the public   and employees.  The survey insights are relevant  for informing AI policy and practice  within business, government, and  NGOs at the national level, as well as  multinational and pan-governmental  AI policy and practice (e.g. the Global  Partnership on AI). Resources are  available to support organisations to  embed the principles and practices  of trustworthy AI into their everyday  operations, and put in place  mechanisms that support stakeholder  trust in their use of AI3. Given the rapid investment and  deployment of AI, it will be important  to regularly re-examine public trust  and expectations of AI systems as  they evolve over time, to ensure  AI use is aligned with and meeting  societal expectations. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
5 Trust in artificial intelligence Introduction Artificial Intelligence (AI) is  an increasingly ubiquitous  part of our everyday lives  and is transforming the way  we live and work4. AI is used in a range of applications, such  as calculating the best travel route to take  in real-time, predicting what customers  will buy, identifying credit card fraud,  helping diagnose disease, identifying  people from photos, and enabling selfdriving vehicles. All sectors of the global  economy are embracing AI. In the words  of Klaus Schwab, Chairman of the World  Economic Forum, we are entering a  fourth industrial revolution characterised  ‘by a fusion of technologies that is  blurring the lines between the physical,  digital, and biological spheres’5.  What is AI?  Artificial Intelligence (AI) refers  to computer systems that can  perform tasks or make predictions,  recommendations or decisions that  usually require human intelligence.  AI systems can perform these tasks  and make these decisions based  on objectives set by humans but  without explicit human instructions  (OECD, 2019). The benefits and promise of AI for society and business  are undeniable. AI is helping people make better  predictions and informed decisions, enabling innovation,  productivity gains and improved efficiency, and lowering  costs. It is helping protect physical and financial security  and facilitating the global fight against COVID-19, to name  just a few of its beneficial applications.  The risks and challenges that AI poses for society are  equally undeniable. These include the risk of codifying and  reinforcing unfair biases, infringing on human rights such  as privacy, spreading fake online content, technological  unemployment and the dangers stemming from  mass surveillance technologies, critical AI failures and  autonomous weapons. These issues are causing public  concern and raising questions about the trustworthiness  and regulation of AI systems6.  The public’s trust in AI technologies is vital for continual  acceptance. If AI systems do not prove to be worthy of  trust, their widespread acceptance and adoption will be  hindered, and the potentially vast societal and economic  benefits will not be fully realised7.  Despite the central importance of trust, to date little is  known about citizens’ trust in AI or what influences it  across countries. Prior public attitude surveys8 have instead  examined general acceptance and support. In 2020, we  conducted the first deep dive survey examining Australians’  trust in AI systems (Lockey, Gillespie & Curtis, 2020). This  report extends this deep dive on trust in AI by examining  citizen perspectives across five nation states: the United  States, Canada, Germany, the United Kingdom and Australia.  This multi-country survey is designed to understand and  quantify citizens’ trust in and support of AI, to benchmark  these attitudes over time, as well as explore similarities and  differences in trust and expectations across five western  countries. Understanding similarities and differences  across countries is important given AI systems are not  bounded by physical borders and are rapidly being deployed  and used across the globe. By taking this deep dive into the  question of trust, this research provides a comprehensive  and nuanced understanding of US, Canadian, German, UK  and Australian citizens’ trust in AI systems. The research  provides insights into the key drivers of trust, community  expectations and confidence in the regulation of AI and  management of societal challenges associated with AI.  It also sheds light on citizens’ current understanding and  awareness of AI, and the practices and principles citizens  expect organisations to use to responsibly design, develop,  deploy and govern AI in society and the workplace.  ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
6 Trust in artificial intelligence Age Groups 6% 30%32% 4%28% Generation Z   (18 – 23) Baby   Boomer   (56 – 74)Millennial (24 – 39) Silent   Generation (75 – 89)Generation X (40 – 55)Gender 51% 49% Female MaleHow we conducted   the research We collected data in each country  using research panels. This approach  is commonly used in survey research  to recruit groups of people who are  representative of a national population.  Our total sample included 6,054 respondents. The  sample size across countries ranged from 1,200  to 1,229 respondents, with each sample nationally  representative of the United States (USA), Canadian,  German, United Kingdom (UK), and Australian  populations on gender9, age and location matched  against each country’s census data.  All data was collected in 2020, with data from the  USA, Canada, Germany and the UK collected from  mid-November to mid-December, and Australian10  data from late June to late July 2020. Surveys in  Germany were administered in German, and Canadian  respondents could opt to complete in English or  French. To ensure survey equivalence, surveys were  translated and back translated into German and French.  We conducted statistical analyses to examine  differences between countries. Where findings are  common across countries, we report aggregated  results. Where significant and meaningful differences  were found between countries, we report countrylevel data. We also report country-level data on citizen  trust. Further details of the methodology and statistical  procedures are detailed in Appendix 1 . Who completed the survey? Demographic details for the total sample are  summarised here. The demographic profile for   each country is reported in Appendix 2.57% 43%  Metropolitan Regional or RuralArea of Residence 42% 58% University  educationNo university educationEducationCountry Sample 1,223 1,202 1,2001,229 1,200USA Germany AustraliaCanada UK ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
7 Trust in artificial intelligence ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
8 Trust in artificial intelligence Do citizens  trust AI? ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
9 Trust in artificial intelligence T o answer this question, we asked  citizens how much they trust, accept  and support AI in general, as well as  two specific applications – Human  Resource AI used to inform decisions  about hiring and promotion, and  Healthcare AI used to inform  decisions about how to diagnose and  treat patients (see Appendix 1). ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
10 Trust in artificial intelligence Most people are unwilling  or ambivalent about trusting  AI systems Most citizens across the five  countries are wary about trusting  in AI system. As shown in Figure 1  (top chart), over a third indicate they  are unwilling to trust AI systems  in general and about a third report  ambivalence. Only about a quarter  report they are willing to trust AI  systems in general. As shown in Figure 1, citizen trust  is influenced by the specific AI  application. Citizens are more trusting  of the use of AI in healthcare and  less trusting of AI use in human  resources11. Overall, most citizens  report being unwilling or ambivalent  about trusting AI in healthcare (63%)  and HR (77%). There are no significant differences  between countries in willingness to  trust AI systems in general. However,  Australian citizens are less trusting of  Human Resource AI than US citizens  (mean 3.3/7 vs 3.7), and Healthcare   AI than Canadian citizens (mean 3.8   vs 4.1).Figure 1.  Willingness to trust AI systems Healthcare AI% Unwilling % Neutral % Willing Whole sample USA Canada Germany UK Australia 31 32 37 32 34 34 27 35 38 30 32 38 31 30 39 38 27 35 Unwilling = 'Completely unwilling', 'Unwilling', 'Somewhat unwilling' Neutral = 'Neither willing nor unwilling' Willing = 'Somewhat willing', 'Willing' or 'Completely willing'HR AI% Unwilling % Neutral % Willing Whole sample USA Canada Germany UK Australia 45 32 23 37 35 28 46 33 21 45 33 22 44 32 24 52 28 20AI'How willing ar e you to: r ely on information pr ovided by an AI system / shar e information with an AI system' [8 questions] % Unwilling % Neutral % Willing Whole sample USA Canada Germany UK Australia 37 35 28 33 39 28 38 35 27 39 35 26 35 39 26 41 27 32 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
11 Trust in artificial intelligence We drilled down to examine two key  ways that trust manifests: reliance  and information sharing. Citizens are more willing to rely on   output and share information with   AI systems used in healthcare, than AI  systems used in human resources  This revealed a similar pattern of trust across the three AI  applications (see Figure 2). People are more willing to rely  on Healthcare AI (35%, mean 3.9/7) and AI systems in  general (30%, mean 3.8/7) than Human Resource AI (25%,  3.5/7). Citizens are also more willing to share information  with Healthcare AI (40%, mean 4.0) than either AI systems  in general (26%, mean 3.6/7) or Human Resource AI (24%,  mean 3.4/7).  There are no significant differences between countries  in willingness to trust AI systems in general. However,  Australian citizens are less willing to rely on (mean 3.4 vs  3.7) and share information (mean 3.4 vs 3.6) with Human  Resource AI than US citizens, and less willing to rely on  Healthcare AI than Canadian (mean 3.6 vs 4.0) and   German (mean 3.6 vs 4.0) citizens.Reliance  Assesses people’s  willingness to rely on an AI  system’s output, such as a  recommendation or decision  (i.e. to trust that it is accurate).  If people are not willing to  rely on AI system output, the  system will not be used.Information sharing  Relates to the willingness to  share information or data with  an AI system (i.e. to provide  data to enable the system  to work or perform a service  for you). All AI systems are  trained on large databases, but  only some require the specific  user to share information as  input to function.  Figure 2.  Willingness to rely on and share  information with AI systems Healthcare AI % Unwilling % Neutral % Willing Rely on AI ouput Share information with AI system34 31 35 29 31 40% Unwilling % Neutral % WillingRely on AI output Share information with AI system34 36 30 39 35 26 HR AI Rely on AI output Share information with AI system43 32 25 44 24 32% Unwilling % Neutral % WillingAI  ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
12 Trust in artificial intelligence In order to trust an AI system,  citizens need to believe it is  trustworthy. We assessed two key  components of trustworthiness.Ability Relates to the perceived reliance, performance and accuracy  of AI output.  Integrity and humanity  Relates to perceptions that the AI is developed based on  sound ethical principles (e.g. fairness), is transparent about  the data it collects and how it is used, and upholds the rights  of users and societal interests.  Most citizens do not view AI systems as  trustworthy. However, they are more likely  to perceive AI systems in general as capable  than designed to operate with integrity   and humanity.  As shown at the top of Figure 3, more citizens agree AI  systems in general are capable (46%, mean 4.3/7) than  designed to operate with integrity and humanity (30%,  mean 3.9/7). This distinction between the two aspects of  trustworthiness was less evident for Healthcare AI and HR AI. More citizens believe Healthcare AI would operate with  integrity and humanity (45% agree; mean 4.3/7) than  AI systems in general (30% agree; mean 3.9/7) or HR  AI systems (34% agree; mean 3.9/7). This finding likely  reflects that citizens generally have higher trust in healthcare  institutions (mean 4.7/7) than other institutions (mean  business 3.7/7; mean government 3.6/7 , mean media 3.4/7). There is one notable country difference: Australians perceive  AI systems in general to be more capable (56% agree, mean  4.5/7) than Germans (39% agree, mean 4.2/7). Figure 3.  Perceptions of the ability, integrity   and humanity of AI systems % Disagr ee % Neutral % Agr ee Ability Integrity & Humanity25 38 37 32 34 34% Disagr ee % Neutral % Agr ee Ability Integrity & Humanity16 38 46 31 39 30 % Disagr ee % Neutral % Agr ee Ability Integrity & Humanity19 36 45 21 34 45 Ability sample item: I believe [AI application] pr oduce output that is accurate. Integrity & Humanity sample item: I believe [AI application] ar e developed based on sound ethical principles (e.g. fair ness).Healthcare AI AI  HR AI ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
13 Trust in artificial intelligence Figure 4.  Support for AI systemsMore people support than oppose the development and use of AI,   but some applications are less supported than others As shown in Figure 4, more citizens support AI development than oppose it. However, in line with the  pattern of findings for trust, significantly more citizens support AI systems in general (47%, mean 3.3/5)  and AI use in healthcare (46%, mean 3.3/5), than AI use in human resources (34%, mean 3.0/5).  It is notable that, regardless of the AI application, a significant proportion of citizens are ambivalent about  AI development and use (34-36%), or oppose its development and use (18-30%). There are no meaningful  differences in support of AI systems across countries.  'How much do you support or oppose the development and use of AI?' 7 Strongly oppose12 9 11 Somewhat oppose18 11 35 Neither support nor oppose3634 37 Somewhat support2835 10 Strongly support611 % AI % HR AI % Healthcar e AI ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
14 Trust in artificial intelligence The young and the university-educated are  more trusting and approving of AI systems As shown in Figure 6, younger people, notably Generation  Z and Millennials, are more likely to trust and perceive AI  systems as trustworthy, and approve or embrace AI, than  older generations [Trust mean 4.0 vs 3.5-3.6/7; Trustworthy  mean 4.3 vs 4.0/7; Accept mean 3.0 vs 2.6-2.8/5]. This  generational difference held across all countries, however  only in relation to AI systems in general and AI use in human  resources: there are no generational differences in relation  to AI in healthcare. In Germany and Australia, people with a university  education are more likely to approve or embrace AI than  those without a university degree (mean 3.0 vs 2.7/5): 30%  of German citizens and 29% of Australian citizens with a  university degree approve or embrace AI, compared to  19% of Germans and 17% of Australians without a degree. Figure 5.  AI Acceptance Figure 6.  AI acceptance and trust by generationCitizens generally accept  or tolerate AI, but few  approve or embrace it As shown in Figure 5, about two  out of every five citizens ‘accept’  AI. However, only about one in five  ‘approve’ of AI or ‘embrace’ it, and  over a third of citizens (37%) report  they either ‘tolerate’ or ‘reject’ AI.  Only a small proportion of citizens  position themselves on the extreme  poles of either outright ‘rejecting’   or ‘embracing’ AI.  There are no significant differences   in acceptance across countries.  When people ‘approve’ or ‘embrace’  AI, they are more likely to engage  with it and be supportive of its  development and use. This is  highlighted in the strong relationship  between AI acceptance and support  for the development and use of AI  in general (r = .73). It is at these  higher levels of acceptance that the  benefits of AI and its widespread  adoption are likely to be realised.'In thinking about AI, which of the following best r epresents your view?' 9% I reject AI28% I tolerate AI15% I appr ove AI6% I embrace AI42% I accept AI % Approve/Embrace % Trust Gen Z & Millennials (18 - 39) 72 34 Gen X (40 - 55) 62 29 Boomers & Silent (56 - 89)2427 19 16 24 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
15 Trust in artificial intelligence ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
16 Trust in artificial intelligence Who do   citizens trust  to develop and  regulate AI? ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
17 Trust in artificial intelligence T o answer this question, we asked  respondents how much confidence they  have in different entities to develop and use  AI, as well as regulate and govern AI. We first  explore the insights for the total sample,  and then examine country differences. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
18 Trust in artificial intelligence Citizens are most  confident in university  and research institutions,  as well as defence  organisations, to develop  and use AI and regulate  and govern AI.As shown in Figure 7 , the majority of  citizens have moderate to complete  confidence in their national universities  and research institutions (77%, mean  3.4/5) and national defence forces  (71%, mean 3.2/5) to develop and use  AI in the best interests of the public.  In comparison, about two thirds of  citizens have moderate to complete  confidence in technology and  commercial organisations (62-64%,  means 2.9/5). On average, citizens  have the least confidence in federal  and state government to develop   and use AI (58%, mean 2.8/5).It is noteworthy that around a third of  citizens report no or low confidence in  government, technology companies  and commercial organisations to  develop and use AI. The lack of  confidence in technology companies  and commercial organisations is  striking given that most citizens’  experience of AI is with applications  developed and used by such  organisations. A solution may be  for commercial and technology  companies and government to  collaborate in AI development  with more trusted entities, such as  universities and research institutions. Figure 7 .  Confidence in entities to develop and use AI (total sample) 'How much conﬁdence do you have in each of the following entities  to develop and use AI in the best inte rests of the public?’ Universities and r esear ch institutions 6 17 32 45 Defence for ces7 22 31 40 Intergover nmental r esear ch organisations 13 23 32 32 Independent r esear ch organisations13 23 33 31 Technology companies 5 31 33 31 Commer cial or ganisations 6 32 37 25 Gover nmentMy State/Pr ovinicial6 36 34 24 Federal/National Gover nment 6 36 34 24% No or low confidence % Moderate confidence % High or complete confidence % Don't know or missing ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
19 Trust in artificial intelligence Figure 8.  Confidence in entities to regulate and govern AI (total sample)Citizens show a similar pattern regarding confidence in entities to regulate and govern AI  in the best interest of the public (Figure 8)12. Citizens report higher confidence in national  university and research institutions (73% moderate to complete confidence, mean 3.2), as  well as security and defence agencies (67%, mean 3.1) to regulate and govern AI than other  entities. Citizens reported the least confidence in governments (federal and state, mean 2.8),  commercial organisations (mean 2.8) and technology companies (mean 2.7). Over a third of  citizens report no or low confidence in these entities to develop and regulate AI (see Figure 8). Overall, citizens generally show less confidence in all institutions to regulate and govern   AI than to develop and use it.  'How much conﬁdence do you have in each of the following to regulate and govern AI in the best interests of the public?' % Don't know or missing% No or low confidence % Moderate confidence % High or complete confidence Universities and resear ch institutions 7 20 34 39 Security and defence agencies 8 25 31 36 Intergover nmental and non-gover nmental resear ch or ganisations12 27 34 27 National AI governance body 11 27 36 26 Regulatory agencies 7 30 37 26 International or ganisations 9 32 34 25 Federal and State Gover nment 7 35 33 25 Commer cial or ganisations 7 37 34 22 Technology companies 6 40 30 24 'Existing r egulatory agencies' includes independent, gover nment-funded bodies.  ‘National AI governance body’ refers to an AI partnership or an association of tech companies, academics, and civil society groups.  ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
20 Trust in artificial intelligence Countries vary in their confidence in entities to develop, use and regulate AI  Figures 9 and 10 highlight important  differences between countries in  their confidence to develop, use and  regulate AI13. Americans are less  confident in their state and federal  governments, research institutions,  and intergovernmental and nongovernmental research organisations  (e.g. AAAI14), to develop and use  AI compared to all other countries  (see Figure 9). Americans also have  lower confidence in a broad range  of entities to regulate and govern AI,  compared to other countries, except  for entities that involve industry  (such as commercial and tech  organisations, see Figure 10).The British are also less confident in  their government to develop, use and  regulate AI compared to other countries  (except the USA, see Figures 9 and 10).  This lower confidence in governments  in the US and UK likely reflects the  significantly lower trust these two  countries have in their governments  (means 3.2 and 3.3/7 respectively),  compared to the other countries  surveyed (means range 3.7 to 4.0/7).  This view is supported by   the finding that general trust in  government is strongly correlated  with confidence in government  to develop and use (r = .57), and  regulate and govern AI (r = .62).In contrast, Australians are more  confident in their security and defence  forces to develop, use and regulate  AI than all other countries, and  more confident in their universities  and research organisations than  most countries (see Figures 9 and  10). This likely reflects Australians’  significantly higher general trust in  their government (mean 4.0 vs 3.6/7)  and in their universities and research  institutions (mean 4.7 vs 4.4/7), than  the other countries surveyed.  Finally, German respondents are less  confident in their defence forces and in  technology companies to develop and  use AI compared to other countries. Figure 9.  Confidence in entities to develop and use AI (reported by country) 'How much conﬁdence do you have in each of the following entities  to develop and use AI in the best inter ests of the public?' % Don't know or missing% No or low confidence % Moderate confidence% High or complete confidence  Universities and research institutions Defence ForcesUSA 9 26 30 35 Canada 7 17 35 41 Germany 6 15 28 51 UK 7 15 33 45 Australia 11 32 54 3 USA 9 26 27 38 Canada 8 22 32 38 Germany 8 28 34 30 UK 7 19 32 42 Australia 16 30 51 3Intergovernmental research organisations Technology companiesUSA 15 32 28 25 Canada 14 23 34 29 Germany 11 21 32 36 UK 13 20 32 35 Australia 10 19 35 36 USA 7 30 30 33 Canada 6 33 33 28 Germany 6 35 36 23 UK 5 27 35 33 Australia 30 33 35 2 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
21 Trust in artificial intelligence Independent research organisations Federal/National GovernmentUSA 13 28 28 31 Canada 14 25 33 28 Germany 13 20 33 34 UK 14 18 38 30 Australia 9 23 34 34 *UK has no comparative State GovernementUSA 8 45 30 17 Canada 7 30 38 25 Germany 6 30 32 32 UK 6 43 33 18 Australia 32 37 28 3State/Provincial Government * USA 8 45 30 17 Canada 7 33 36 24 Germany 7 30 33 30 Australia 34 37 26 3 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
22 Trust in artificial intelligence Figure 10.  Confidence in entities to regulate and govern AI (reported by country) 'How much conﬁdence do you have in each of the following to r egulate  and gover n AI in the best interests of the public?' % Don't know or missing% No or low confidence % Moderate confidence% High or complete confidence  Universities and research institutes Security and defence agencies Existing regulatory agencies Intergovernmental and non-governmental  research organisationsFederal / State / Provincial Government Technology companies Commercial organisationsUSA 9 30 Canada 9 20 Germany 9 16 UK 8 18 Australia 16 USA 9 34 Canada 9 22 Germany 9 26 UK 7 23 Australia 19 USA 9 40 Canada 9 28 Germany 8 27 UK 6 2929 Australia 25 USA 12 34 Canada 12 28 Germany 13 22 UK 13 25 Australia 9 28USA 8 Canada 8 31 Germany 9 29 UK 6 41 Australia USA 8 40 Canada 8 41 Germany 9 38 32 21 UK 5 38 Australia 42 USA 11 38 Canada 8 36 Germany 9 32 UK 7 32 Australia 4432 35 30 3035 35 29 34 30 31 33 32 37 37 39 31 32 35 33 34 383845 29 36 34 31 26 31 31 30 30 35 36 37 3429 36 45 39 46 28 35 35 39 45 19 26 28 26 41 22 25 32 28 2518 25 28 22 26 20 26 25 21 21 23 24 19 'Existing r egulatory agencies' includes independent, gover nment-funded bodies3 3 333 3 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
23 Trust in artificial intelligence Figure 11.  Motivation to innovate with AIPeople believe organisations innovate with AI mostly for financial reasons One reason for the low confidence  in commercial organisations to  develop and govern AI may be that  people think such organisations  are motivated to innovate with AI  primarily to cut labour costs and  increase revenue (financial motivation)  rather than to help solve societal  problems and enhance societal  wellbeing (societal motivation).  As shown in Figure 11, 62% (mean  5.1) of the public believe commercial  organisations innovate with AI for  financial gain, whereas only a third  (32%, mean 4.0) agree they innovate  for societal benefit.  This pattern was replicated for  government and even non-profit  organisations, although the difference  between financial and societal motivation to innovate was less  pronounced. About half of citizens  agree government (52%, mean 4.8)  and non-profits (48%, mean 4.7)  innovate with AI for financial reasons,  whereas over a third (37-38%, mean  4.3-4.2) believe these organisations  innovate with AI for societal benefit  (see Figure 11). This pattern was shared across each  of the five countries. It is noteworthy  that compared to citizens in other  countries, Australians were more likely  to view commercial organisations  and government organisations as  deploying AI for financial reasons. A significant proportion of the  public (43-46%) are unsure whether  commercial, government and  non-profit organisations innovate for societal good. This represents  an opportunity for organisations  to strengthen communication and  public understanding of when AI is  being deployed to deliver societal  benefits or shared benefits. The  societal motivation to innovate with  AI was significantly correlated with  both trust (r=.57) and acceptance  (r=.49) of AI, suggesting using AI  for societal benefit is one pathway  to strengthen public trust and  acceptance of AI. % Disagr ee % Neutral % Agr ee 7 Financial Motivation3162 24 Societal Motivation44 32 Commercial Organisations10 Financial Motivation3852 20 Societal Motivation43 37 Government10 Financial Motivation4248 16 Societal Motivation46 38 Non-profits ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
24 Trust in artificial intelligence What  expectations   do people   have about   AI regulation? ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
25 Trust in artificial intelligence We asked several questions related to the  expectations the public have around AI  development and regulation, including  the extent to which they think regulation  is necessary, who should regulate, and  whether current regulations and institutional  safeguards are sufficient. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
26 Trust in artificial intelligence AI regulation is required,  and citizens expect external,  independent oversight.  Most citizens across countries  (81%) view AI regulation as required  [ranging from 79% (USA) to 83%  (Canada)]. This finding corroborates  those of previous surveys, indicating  strong citizen desire for regulation15.  As shown in Figure 12, the majority of  citizens expect a range of bodies to play  a role in regulating AI, including a new,  dedicated, independent AI regulator  (62%), as well as government and/or  existing regulators (61%). Co-regulation  by industry, government and existing  regulators is also desirable (59%),  and over half of citizens (54%) expect  industry that uses or develops AI to play  a role in regulation. The desire for a dedicated,  independent AI regulator may  reflect the fact that confidence in  the government to regulate AI is  not uniformly high. As discussed  in the previous section, a little over  a third of the public have no or low  confidence in the government to  regulate AI.  There are several significant  differences between countries in  their expectations of who should  regulate AI (see Figure 12).  Fewer Americans expect external  regulation by government and  existing regulators (49% vs 60%+ in  other countries). American citizens  are also more likely to believe AI  regulation is not required (21%)  compared to Canadian and British  citizens (17-19%). This finding aligns with the pattern  reported for the USA in the prior  section: Americans are less likely to  expect government regulation and less  confident in their ability to regulate.  In contrast, British citizens are more  likely to expect a new dedicated,  independent AI regulator (71%) than   all other countries (56-60%). The Australian sample was excluded  from these analyses due to use of  a different response format and  wording of questions that does not  allow direct comparisons to be made.  The pattern of results for Australians  was broadly similar: the vast majority  believe AI regulation is needed (96%),  and the majority expect government  and existing regulators to play a  role in regulation (63%), as well as  co-regulation by government and  industry (59%). Figure 12.  Citizen expectations of who should regulate AI 'AI should be r egulated by…' Disagr ee Neutral Agree A new independent AI regulator Whole Sample USA Canada Germany UK The Government and/or existing regulators Whole Sample USA Canada Germany UK Co-regulation Whole Sample USA Canada Germany UK Industry that uses/develops AI Whole Sample USA Canada Germany UK 11 27 62 13 31 56 12 28 60 12 28 60 7 22 71 15 24 61 21 30 49 11 23 66 15 25 60 12 21 6715 26 59 17 30 53 14 23 63 17 27 56 13 24 63 20 26 54 17 29 54 23 27 50 18 27 55 21 24 55 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
27 Trust in artificial intelligence Current safeguards are  insufficient given the  uncertainty around AI Most citizens (70%) believe the  impact of AI on society is uncertain  and unpredictable (see Figure  13). While citizens in all countries  perceive a great deal of uncertainty  around AI, Australians (79%, mean  5.5) perceive more uncertainty than  all other countries (66-70%, means  5.1-5.3).  There is also a difference in  perceived uncertainty across AI  applications: AI use in Healthcare   is perceived as less uncertain (64%,  mean 5.1) than AI use in both human  resources (73%, mean 5.4) and in  general (71%, mean 5.3).Figure 13.  Perceptions of AI uncertainty Citizens generally disagree (37-42%) or are ambivalent (2427%) that current safeguards around AI (rules, regulations  and laws) are sufficient to make the use of AI safe or protect  them from problems (see Figure 14). Similarly, the majority  either disagree (41%) or are ambivalent (24%) that the  government adequately regulates AI. This corroborates  previous European16 surveys reporting citizens do not   think current rules are effective in regulating AI. There was only one significant difference across   countries: Germans are more confident that their  government adequately regulates AI (35%, mean   3.9) compared to the USA, UK and Australia (26-30%,   mean 3.5-3.6).'To what extent do you agr ee with the following: There are many unknowns about AI [sample item]'% Disagr ee % Neutral % Agr ee USA 6 28 66 Canada 4 26 70 Germany 7 27 66 UK 5 27 68 Australia 4 17 79 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
28 Trust in artificial intelligence Figure 14.  Perception of current regulations, laws and rules to make AI use safe 'To what extent do you agr ee with the following…' % Don't know or missing % Disagr ee % Neutral % Agr ee There are enough current safeguards to make me feel comfortable with the use of AI Whole sample There are sufﬁcient regulatory processes in place to protect me from problems that may arise from the use of AI The current law helps me feel that the use of AI is safeI feel conﬁdent that the government adequately  regulates AI sample I feel the current rules and regulations are sufﬁcient  to control AI Whole sample USA Canada Germany UK Australia 6 37 24 33 USA 6 37 24 33 Canada 6 36 24 34 Germany 5 34 28 33 UK 7 36 24 33 Australia 7 45 19 29 Whole sample5 39 25 31 USA 4 39 26 31 Canada 6 27 36 31 Germany 4 35 28 33 UK 6 39 24 31 Australia 6 45 19 30 Whole sample6 38 27 29 USA 5 38 28 29 Canada 6 36 29 29 Germany 4 38 29 29 UK 7 36 28 29 Australia 8 44 21 27Whole6 41 24 29 USA 5 46 23 26 Canada 6 39 25 30 Germany 4 33 28 35 UK 7 41 24 28 Australia 6 46 19 29 6 42 25 27 5 42 25 28 6 39 27 28 4 43 26 27 7 39 27 27 8 48 19 25 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
29 Trust in artificial intelligence Figure 15.  Perceived balance of risks and benefits of AI applicationsThe risks and benefits of AI depend on their application We asked citizens about the balance between the risks and benefits of AI. As shown in Figure 15,  citizens’ perceptions of the balance of risks and benefits differ significantly across AI applications.  For Healthcare AI and AI systems in general, more citizens perceive the benefits to outweigh the risks  (Healthcare AI: 38% benefits>risks, 27% risks>benefits; AI in general: 35% benefits>risks, 28%  risks>benefits). In contrast, more citizens perceive the risks of Human Resource AI to outweigh the  benefits (37% risks>benefits; 25% benefits>risks).  It is notable that of those who believe the benefits outweigh the risks, more believe that the benefits  are slightly greater (20-28%) than much greater (7-10%). It is also notable that over a third of citizens  believe the benefits and risks of AI are about equal. 'Overall, which best r epresents your view on the beneﬁts and risk of AI systems?' % AI % HR AI % Healthcar e AI Risks ar e much greater than benefits16 1112 Risks ar e slightly greater than benefits21 16 16 Benefits and risks ar e about equal38 3537 Benefits ar e slightly gr eater than risks2028 28 Benefits ar e much gr eater than risks5107 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
30 Trust in artificial intelligence Assurance mechanisms enhance trust in AI systems Most citizens (57% to 66%) indicate they would be more willing to use an AI system if there are  assurance mechanisms in place to support ethical and trustworthy use. These mechanisms include  independent AI ethics reviews, AI codes of conduct, national standards on AI explainability and  transparency, and AI ethics certification (see Figure 16). These mechanisms increase perceptions   of current safeguards and reduce uncertainty.  More Australian citizens, and fewer German and American citizens, report that these assurance   mechanisms would increase their willingness to use AI systems, compared to citizens in other countries.  Figure 16.  AI Assurance mechanisms 'I would be mor e willing to use an AI system if…' % Disagr ee % Neutral  % Agr ee An independent body conducted regular reviews  of the ethics of AI systems Whole sample USA Canada Germany UK Australia It had been reviewed by an AI ethics board Whole sample USA Canada Germany UK Australia The organisation using the AI system had an AI ethics  code of conduct Whole sample USA Canada Germany UK Australia It adhered to national standards for AI explainability  and transparency Whole sample USA Canada Germany UK Australia It has an AI ethics certiﬁcation Whole sample USA Canada Germany UK Australia 12 22 66 13 27 60 12 21 67 15 27 58 11 22 67 910 81 12 23 65 13 29 58 12 23 65 16 27 57 11 22 67 911 80 13 23 64 15 29 56 12 22 66 16 29 55 12 23 65 912 7912 24 64 13 29 58 11 24 65 16 29 55 12 24 64 10 14 76 15 28 57 16 34 50 15 29 56 19 32 49 13 29 58 11 17 72 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
31 Trust in artificial intelligence ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
32 Trust in artificial intelligence What principles  are important for  people to trust  AI systems? ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
33 Trust in artificial intelligence Eight AI design and governance  principles and associated practices  are highly important for trust.  ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
34 Trust in artificial intelligence A proliferation of reports and guidance documents on the development and deployment of trustworthy   AI have been produced over the past few years17. One goal of this survey was to determine what practices and principles are important for citizens within  western nations to trust in AI. To answer this question, we asked about the importance of 33 practices  associated with the eight principles for trustworthy AI. These principles were adapted primarily from the 2019  European Union Principles for Trustworthy AI18. Specifically, we asked how important each   of these practices are for respondents’ trust in AI systems.  Principles and Practices for Trustworthy AI T echnical robustness  and safety The performance and accuracy  of AI system output is  assessed before and regularly  during deployment to ensure  it operates as intended. The  robustness of output is tested  in a range of situations, and  only data of appropriate quality  is used to develop AI. T ransparency   and explainability The purpose of the AI system,  how it functions and arrives  at its solutions, and how  data is used and managed is  transparently explained and  reasonably understandable  to a variety of stakeholders.  Developers keep an audit trail  of the method and datasets  used to develop AI. Data privacy, security  and governance Safety and privacy measures  are designed into the AI  system. Data used for AI is  kept secure, used only for the  specific purpose to which it  is agreed, and is not shared  with other apps or third parties  without permission. Robust  security measures are in  place to identify and prevent  adversarial attacks.  Fairness and   non-discrimination The outcomes of AI systems  are assessed regularly to  ensure they are fair, free of  unfair bias, and designed to  be inclusive to a diversity of  users. AI is developed with the  participation and input of   a diverse range of people.Human agency   and oversight  There is appropriate human  oversight and control of AI  systems and their impact  on stakeholders by people  with required expertise and  resources to do so. AI systems  are regularly reviewed to  ensure they are operating in a  trustworthy and ethical manner. Accountability   and contestability There is clear accountability  and responsibility if something  goes wrong with an AI  system. Any impacted user or  stakeholder is able to challenge  the outcomes of an AI system  via a fair and accessible human  review process. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
35 Trust in artificial intelligence Results indicate that the vast majority  of citizens (95%) view every one  of these eight principles, and the  practices that underlie them, as  moderately to extremely important  for trust in AI systems (see Figure 17).  This held across all three AI application  uses (AI systems in general, in  healthcare and human resources). This provides clear public  endorsement of these principles  and practices and a blueprint for  developing and using AI in a way   that supports trust. While the large majority of citizens  across all five countries viewed  these principles as important, there  are two clear patterns of differences  across countries (see Figure 18):  Australians rated these principles as  more important to trust in AI systems  (94%), than all other countries (7081%). In contrast, US citizens rated  these principles as less important  for trust in AI systems (70%) than  citizens in other countries (78-94%).AI literacy People are supported in understanding  AI systems, including when it is  appropriate to use them, and the ethical  considerations of their use. Risk and impact mitigation The risks, unintended consequences  and potential for harm from an AI  system are fully assessed and mitigated  prior to and during its deployment. Figure 17 .  Importance of the Principles for Trustworthy AI Figure 18.  Importance of the Principles for Trustworthy AI by Country 'How important ar e the following […] for you to trust AI systems?' % Low Importance % Moderate Importance % High Importance USA Canada Germany UK Australia8 22 70 4 15 81 6 15 79 4 18 78 5 94 Low importance = 'Not at all important', 'Very low importance', or 'Low importance' Moderate importance = 'Moderately important' High importance = 'High importance', 'Very high importance', or 'Extr emely important'1'How important ar e the following […] for you to trust AI systems?' % Low Importance % Moderate Importance % High Importance Data Privacy, Security & Governance Technical Robustness & Safety Transparency & Explainability Human Agency & Oversight Accountability & Contestability Fairness, Inclusion & Non-discrimination AI Literacy Risk & Impact Mitigation6 14 80 5 16 79 5 17 78 5 19 76 5 19 76 5 19 76 5 22 73 6 21 73 Low importance = 'Not at all important', 'Very low importance', or 'Low importance' Moderate importance = 'Moderately important' High importance = 'High importance', 'Very high importance', or 'Extr emely important' ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
36 Trust in artificial intelligence How do citizens  feel about AI   at work? ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
37 Trust in artificial intelligence T o understand how people feel about the  use of AI at work, we asked questions about  the impact of AI on jobs, citizens’ AI use  at work, and their comfort with AI use to  support different work functions. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
38 Trust in artificial intelligence Only one in five citizens  believe AI will create more  jobs than it will eliminate  Most citizens (78%) either disagree or  are unsure that AI will create more jobs  than it will eliminate (see Figure 19).   The concern that AI will eliminate jobs  is also expressed in prior national   and transnational surveys19. Australians are more likely to believe AI  will eliminate more jobs than it creates  than citizens in the other countries  (61% vs 41-48%, mean 3.1 vs 3.5/5). Most people ‘never’ or ‘rarely’ use AI in their work As shown in Figure 20, over 40% of employed citizens report that they never  use AI in their work (ranging from 42% in the USA to 45% in the UK)20. One  in five citizens (19%) report using AI in their work rarely (i.e. about 10%). In  contrast, about a quarter (26%) of citizens report using AI in their work about  30% of the time or more frequently.  Given many citizens report a low understanding and awareness of AI use,  these figures may partially reflect that people are not aware of AI use in their  work. 12% opted for the ‘don’t know’ option suggesting they did not have  sufficient understanding to gauge whether AI is being used in their work. There are no significant differences in AI use at work across countries. Figure 19.  Perceived impact of AI  on jobsFigure 20. Use of AI at work 'In general, to what extent do you agree that AI will cr eate mor e jobs than it will eliminate?' Neutral31%Disagree47%Agree22%'How often do you use AI in your work?' Don't know 12%Never 43% Rarely (about 10%)19% Occasionally (about 30%)11% About half of the time7% Often (about 70%)4% Almost always  (about 90%)2% Always 2% ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
39 Trust in artificial intelligence Most people are comfortable with AI   at work when it is not focused on them Most citizens are at least moderately comfortable with  AI use in task automation and augmentation at work (see  Figure 21). However, when AI is focused on them as  employees, they are much less comfortable. Most citizens (70-76%) are either highly or moderately  comfortable with AI use in task automation and  augmentation at work, such as monitoring the  organization’s digital and physical security, automating  physical tasks (e.g. robot on an assembly line), tailoring  marketing to customers, and assisting with customer  queries (e.g. chatbots).  However, citizens are considerably less comfortable with  AI use when it is focused on themselves as employees –  such as to monitor employees, evaluate performance and  support recruitment and selection decisions. Only 46-58%  of citizens feel comfortable with AI use in these employeefocused activities.  Examination of country differences revealed that   Germans are significantly less comfortable with AI   use to support recruitment and selection and to monitor  employees (compared to Americans), automate physical  tasks (compared to the British), and to monitor security  (compared to Australians). On average, people who report using AI in their work, also  report feeling more comfortable with the use of AI for the  various uses in Figure 21, than those who do not use AI in  their work. This most likely reflects their greater familiarity  with AI at work. Younger people and the university-educated are more comfortable   with AI at work Younger people, specifically Gen Z and Millennials, are more comfortable with AI use at work than older  respondents. 78% of Gen Z and Millennials are at least moderately comfortable with the use of AI   at work for the activities shown in Figure 21, compared to 62% of older respondents.  Similarly, the university-educated are more comfortable with AI use at work than those without a university  degree. 73% of the university-educated are at least moderately comfortable compared to 64% of those  without a degree.Figure 21. Comfort with the use of AI at work 'How comfortable ar e you with AI being used in the  following ways at work?' % Low Comfort % Moderate Comfort % High ComfortText Automation and Augmentation Employee-focused ActivityMonitoring digital and physical security24 32 44 Automating physical tasks 26 34 40 Automating tasks pattern  detection & interpretation27 35 38 Automating administration 28 37 35 Analysing and tailoring  marketing30 38 32 Assisting with queries 30 38 32 Supporting recruitment & selection42 34 24 Evaluating employee  performance46 32 22 Monitoring employees 54 25 21 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
40 Trust in artificial intelligence How do citizens  view key AI  challenges? ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
41 Trust in artificial intelligence The pervasive use of AI in society is leading to a  range of challenges. We asked respondents to rate  the extent to which a series of AI societal challenges  need to be carefully managed, and the likelihood of  these challenges affecting large numbers of people  in their country in the next ten years. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
42 Trust in artificial intelligence AI challenges need to be carefully managed All twelve of the AI challenges we presented need to be carefully managed by governments and  organisations. Figure 22 shows that most respondents (83% or more) rate the careful management of AI  challenges as very or extremely important.  While older generations are more likely to rate the careful management of the AI challenges as highly  important, the large majority of all generations rated these challenges as highly important (ranging from  79% for Gen Z and Millennials, 84% for Gen X, to 94% for Boomers and Silent Generation). These findings align with and update those found in a 2019 US survey which reported Americans regard  each of these AI challenges as needing careful management21. We further found that Australians rate the  careful management of these challenges as more important (95% high, mean 4.7/5) than respondents  from other countries (range from USA 79%, mean 4.2/5 to Canada 85%, mean 4.4/5)22.  Figure 22. Importance of careful management of AI challenges 'How important is it for companies and gove rnments to car efully manage this challenge?'% Low importance %Moderate importance % High importance Cyber Attack 3 9 88 Autonomous Vehicles 4 10 86 Misaligned with Human Values 4 10 86 Data Privacy 3 12 85 Disease Misdiagnosis 3 12 85 Fake Online Content 3 13 84 Critical AI Failur es 4 12 84 Surveillance 4 12 84 Criminal Justice Bias 4 12 84 Autonomous W eapons 5 12 83 HR Bias 5 12 83 Technological Unemployment 4 13 83 Low importance = 'Not at all important' or 'Slightly important' Moderate importance = 'Moderately important' High importance = 'Very important' or 'Extr emely important' ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
43 Trust in artificial intelligence Data challenges considered most likely to impact people in the near future Figure 23 indicates that most respondents (59-61%) think data challenges such as surveillance, fake  online content, cyber-attacks, and data privacy are most likely to impact large numbers of people in their  country over the next ten years. The only challenge which people perceive to be more unlikely (42%) than  likely (36%) to impact large numbers is the use of lethal autonomous weapons. Germans tend to believe these challenges are less likely to impact large numbers of people (41%  ‘likely’, mean 4.2/7), than respondents from other countries (ranging from Australia 45% ‘likely’, mean  4.5 to USA 53% ‘likely’, mean 4.6). In contrast, Americans are more likely to believe that autonomous  weapons will impact large numbers of people (46% likely, mean 4.3/7), than respondents from other  countries (34%, mean 3.7/7). Figure 23. Likelihood of AI challenges impacting large numbers of citizens 'In the next 10 years, how likely do you think it is that this challenge will impact la rge numbers of the people in your country?'% Unlikely % Equally likely as unlikely % Likely Surveillance 17 22 61 Fake Online Content 17 23 60 Cyber Attack 19 21 60 Data Privacy 19 22 59 Disease Misdiagnosis 25 25 50 HR Bias 26 25 49 Technological Unemployment30 25 45 Critical AI Failur es 29 26 45 Misaligned with Human Values30 25 45 Autonomous Vehicles 31 24 45 Criminal Justice Bias 29 28 43 Autonomous W eapons 42 22 36 Unlikely = 'Very unlikely (<5% chance'), 'Unlikely (5-20% chance)' or 'Somewhat unlikely (20-40% chance)' Equally likely as unlikely = 40-60% chance Likely = 'Somewhat likely (60-80% chance)', 'Likely (80-95% chance)' or 'Very likely (>95% chance)' ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
44 Trust in artificial intelligence How well  do citizens  understand AI? ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
45 Trust in artificial intelligence T o identify how well citizens  understand AI, we asked about  AI awareness, subjective and  objective knowledge of AI and  interest to learn more.  ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
46 Trust in artificial intelligence Three out of five citizens  are aware of AI  Only 62% of citizens had heard, read   or seen something about AI (see Figure  24). This is higher than a 2017 European  survey23 which found less than 50%  of respondents had heard of AI,  suggesting awareness is increasing. More UK respondents (70%) had   heard of AI than Canadian (62%)   or US respondents (62%)24. Most citizens do not feel they understand AI  On average, three out of five citizens (60%, see Figure 25) report low  subjective knowledge of AI, indicating that they feel they know little about  AI, or when and how it is being used25. Only a small proportion of citizens  (14%) report high subjective knowledge of AI. This aligns with findings from  a recent European survey reporting only 9% of European citizens feel well  informed about AI26. As shown in Figure 25, German citizens report higher subjective knowledge  than citizens from all other countries. Figure 24.  Awareness of AI Figure 25. Subjective knowledge of AI 'Have you hear d, read, or seen anything about AI?' Don't know7% No31% Yes62%'To what extent do you… a) feel you know a lot about AI? b) feel informed about how AI is used? c) think you understand when AI is being used?'% Low % Moderate % High USA 61 25 14 Canada 62 28 10 Germany 51 26 23 UK 61 26 13 Australia 62 26 12 Low = 'Not at all' or 'Slightly' Moderate = 'Moderately' High = 'Considerably' or 'A gr eat deal' ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
47 Trust in artificial intelligence Citizens have a low understanding   of when AI is used Given the low understanding of AI, it is not surprising  that citizens often don’t know that AI is used in common  everyday technologies. When asked if the common  technologies shown in Figure 26 use AI, overall, less than  50% correctly answered yes. That is, people could not  correctly identify if the technology used AI better than a  chance guess. In particular, the majority of citizens are unaware that AI is  used in applications such as accommodation sharing (76%  unaware) and ridesharing (69%) apps, email filters (62%),  social media (59%) and product recommenders (56%). In  contrast, there is more awareness of AI use in embodied  applications (e.g. with voice) such as chatbots (43%  unaware) and virtual assistants (38%). Surprisingly, use of a technology does not necessarily  translate into an increased understanding of whether AI  is part of the technology. As shown in Figure 26, this is  particularly the case with social media, email filters and  traffic navigation apps. For example, while 79% of citizens  report using traffic navigation apps, 49% are unaware this  technology uses AI.  There are several country differences. Australian citizens  are more aware, and Germans less aware, that AI is used  in virtual assistants (Germany 42% unaware vs Australia  29%) and facial recognition (Germany 50% unaware vs  Australia 35%). German citizens are also less aware that AI  is used in product recommendation systems (61% unaware)  compared to Canadian (53%) and British respondents  (54%). This lower awareness for Germans is surprising given  they report higher subjective knowledge of AI than all other  countries. Yet Germans use many of the AI systems shown  in Figure 26 less than respondents in other countries (49%  average use, compared to other countries ranging from 56%  to 59%). Figure 26.  Use of AI technologies and understanding  of these technologies use of AI 'For each technology below , please indicate if you have  used it and if it uses AI?' % Unawar e that technology uses AI % Who use this technology Accommodation Sharing Apps76 33 Ridesharing Apps69 36 Email Filters62 63 Social Media59 76 Product Recommendations56 58 Traffic Navigation Apps49 79 Text recognition47 69 Facial Recognition45 36 Chatbots43 37 Virtual Assistants38 55 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
48 Trust in artificial intelligence Most citizens want to  know more about AI While citizens generally lack  knowledge and awareness of AI, the  large majority (83%) are interested  in learning more about AI. Only  17% report no interest in learning  more about AI (ranging from 14% of  Australians to 23% of Americans).  Americans report less interest  in learning more about AI than  Australians and Germans.  Men and the university-educated, as well as younger people, also report higher subjective knowledge   of AI (see Figure 28). Over half of men (51%) report at least moderate subjective knowledge of AI,  compared to less than a third (31%) of women, indicating a significant gender gap. Similarly, half of people  with a university degree (49%) report at least moderate subjective knowledge compared to a third (34%)  of those without a degree. Finally, we see a similar pattern over generations: about half (49%) of Gen Z  and Millennials report at least moderate subjective knowledge compared to 38% of Gen X and a third  (33%) of Baby Boomers and Silent Generation. Some population segments have more awareness   and knowledge of AI Men and those with a university education are more aware of AI. As shown  in Figure 27 , 20% more men than women had heard of AI, and 20% more  respondents with a university degree than those without a degree. Men and those with university degrees are also more likely to understand  when AI is being used in common applications than women and people  without degrees. Two in five (40%) citizens without a university degree  were unable to correctly identify AI use in any of the common applications  presented to them (see Figure 26), compared to just 23% of those with   a degree. Similarly, 39% of women did not correctly identify AI use in any   of the applications, compared to 27% of men. Figure 27 . AI awareness by population segment 'Have you hear d, read, or seen anything about AI?' % Yes Male 72 Female 52University Education 74 No University Education54 * In Australia the question speciﬁed 'in the past 12 months' ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
49 Trust in artificial intelligence Figure 28. Subjective knowledge by population segment Low Moderate High Male Female Gen Z & Millennials (18 - 39) Gen X (40 - 55) Boomers & Silent (56 - 89) University Education No University Education69 21 10 51 28 21 62 25 13 67 25 8 51 30 19 66 23 1149 32 19'To what extent do you… a) feel you know a lot about AI? b) feel informed about how AI is used? c) think you understand when AI is being used?' ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
50 Trust in artificial intelligence What are the   key drivers  of trust and  acceptance   of AI? ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
51 Trust in artificial intelligence T o identify the most important  drivers of trust and acceptance of AI  systems examined in this report, we  used a statistical technique called  path analysis. We explain the path  model in Figure 29, together with  notes on interpreting the model. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
52 Trust in artificial intelligence Trust is central to AI  acceptance The path model shows that trust is a  central driver of AI acceptance (B =  .47). This finding empirically supports  why trustworthy AI matters: if people  perceive AI systems to be trustworthy  and are willing to trust them, this leads  to the acceptance necessary to realise  the benefits of AI.  Trust acts as the central vehicle  through which other drivers impact  AI acceptance. Each of the four  drivers on the left-hand side of the  model influences trust, which in turn  influences acceptance. Given the key  role of trust in driving acceptance, it is  important to understand what drives  trust in AI systems.  The strongest driver of  trust is believing current  regulations and laws are  sufficient to ensure AI use  is safe  As shown in the path model,  believing current safeguards are  sufficient is the strongest driver of  trust. The relative importance of  current safeguards (B = .55) is more  than twice that of the next strongest  driver, the perceived impact of AI on  jobs (B = .20).  This demonstrates the importance of  developing adequate regulatory and  legal systems that protect people  from problems that may arise from  AI use, and make them feel safe to  use AI systems. Given most people  either disagree or are ambivalent  that current AI safeguards are  adequate, ensuring AI is governed  by an appropriate regulatory and  legal framework is a critical first  step towards enhancing trust and  acceptance of AI. The perceived impact of AI  on jobs, and familiarity with  AI, influence trust  People’s beliefs about the impact of  AI on jobs is the second strongest  driver of trust (B = .20). People who  believe AI will create more jobs than  it will eliminate are more likely to  trust in AI systems. Familiarity with AI  was the third driver of trust (B =.12).  This shows that people who feel that  they understand how and when AI is  used and have knowledge of AI use in  common applications are more likely  to trust AI systems and accept AI.  The more people believe  the impact of AI is  uncertain, the less they  trust AI systems The model indicates that if people  believe the impact of AI on society  is uncertain and unpredictable, they  are less likely to trust in (B = -.08)  and accept AI. This is the fourth  driver of trust. AI context and education  influence trust and  acceptance We also found that two other factors  had a smaller impact on trust and  acceptance. People are generally  more trusting of AI in a healthcare  context than AI in general or AI in  Human Resources (B = .06). People  with a university degree also tend to  be more accepting of AI than those  without a degree (B = .06). How to read   the path model When reading the path model,  follow the arrows from left  to right. The values on the  arrows indicate their relative  importance in driving trust  and acceptance: the larger  the number, the stronger the  effect. The negative value of AI  uncertainty indicates that when  uncertainty increases, trust  and acceptance decrease. All  other relationships are positive,  which means, for example the  more people believe current  safeguards are sufficient,  the more they will trust AI  systems, and the more they  trust AI systems, the more  they accept AI. Only significant  relationships are shown27. The model is based on all   data (across countries and   AI applications). ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
53 Trust in artificial intelligence Figure 29.  A model of the key drivers of trust and acceptance of AI systems The belief that current  regulations, laws  and safeguards are  sufficient to protect  people and ensure AI  use is safe. This is the  strongest predictor of  trust in AI systems.The extent to which  people trust AI systems  and perceive them to   be trustworthy.The extent to which  people accept and  approve of AI.   The belief that AI will  create more jobs than  it will eliminate.  The extent to which  people feel they  understand AI, know  when AI is used in  common applications,  and have used common  AI applications. The belief that the  societal impact of  AI is unpredictable  and there are many  unknowns about AI.Other factors also had   a small impact on trust  and acceptance.   People are more trusting  of Healthcare AI than  other systems.  People with a university  degree are more  accepting of AI than  those without a degree. Trust is central to the acceptance of AI systems and is influenced by four key drivers.   This model lays out a pathway to building trust and acceptance of AI. Familiarity with AICurrent Safeguards AI UncertaintyAI AcceptanceHealthcare AI Education T rust in AI Systems.55 .20 .47.06 .06 .12 -.08Job Impact of AI ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
54 Trust in artificial intelligence The findings provide a clear overview of the current  and future challenges to building and preserving trust  and acceptance of AI systems. They also reveal more  commonalities in citizens’ views across western nations  than differences.  A key insight from the survey is that the public generally  has low trust towards AI systems. Given trust is a central  factor influencing the acceptance and adoption of AI, this  low trust is likely to impair societal uptake and the ability  of western nations to realise the societal and economic  benefits of AI, if left unaddressed. The following insights  lay out a roadmap for enhancing public trust in AI.Conclusion  and implications Together, the findings of this multi-country  nationally representative survey of US,  Canadian, German, UK and Australian  citizens highlight important insights on   the public’s trust and attitudes towards   AI and lays out a pathway for building   trust and acceptance of AI systems.  ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
55 Trust in artificial intelligence Live up to citizens’ expectations of trustworthy AI   –Our findings reveal that citizens across the five western  nations surveyed have very clear and consistent  expectations of the principles and practices they  expect AI systems to uphold in order to be trusted.  They expect organisations to maintain high standards  of AI systems in terms of:    –performance and accuracy   –data privacy   –security and governance   –transparency and explainability   –accountability   –risk and impact mitigation   –fairness   –human oversight   –These principles and practices reflect those identified in  numerous recent government reports on trustworthy,  ethical AI 28, and our findings provide clear public  endorsement for them, as well as underscoring their  importance for public trust.   –The public clearly expect AI systems will be monitored  and evaluated on an ongoing basis. Organisations should  undertake regular in-house and independent ethics reviews  of their AI systems to ensure they operate according to  these principles and practices.   –Our survey revealed that most people believe organisations  innovate with AI for financial reasons (e.g. cost saving or  profit maximisation) rather than to benefit society more  broadly. This imbalance is most pronounced for commercial  organisations, followed by government and then nonprofit organisations. This highlights the opportunity for  organisations to better engage AI systems for the benefit  of citizens, customers and employees, as well as better  demonstrate how their use of AI supports societal health   and wellbeing.   –It is also important to recognise that people’s trust in and  support for AI depends on its application. Citizens are  broadly more trusting and supportive of AI use in healthcare  than in human resources, and view the risks and benefits  differently across these applications. These findings  suggest citizens are more likely to approve of, and engage  with, AI systems for healthcare diagnosis and treatment,  than for HR hiring and promotion purposes. This highlights  the importance of taking a contextualised approach to the  development and deployment of AI systems.  –Many citizens believe AI will eliminate more jobs   than it creates, and this belief strongly influenced   trust of AI systems.    –Our findings further reveal that most citizens are  comfortable with AI use at work for the purposes of  task automation and augmentation. This suggests most  employees will be generally receptive to the use of AI for  these purposes. However, citizens are less comfortable  with AI use at work for employee-focused activities,  such as evaluating and monitoring performance, and  recruitment and selection.    –Taken together, these findings highlight that organisations  looking to accelerate the use and uptake of AI need to build  trust with customers, employees and the public more  broadly – it is not enough to focus on only one stakeholder  group. Experiences of AI both as an employee and more  broadly as a citizen, will influence trust and acceptance  of AI deployment. Governments and employers need to  consider and plan for how AI will influence technological  unemployment and how to accelerate investment in data  and technology literacy for the public and the workplace to  help ensure a responsible transition.   –The findings further serve as a warning to employers of  the dangers of using AI for purposes that will alienate  their people, and suggests employers who are active in  upskilling and educating their employees will be better  placed to engage them in the use of AI.    –Organisations also need to consider that different cohorts  in the workplace and community have different views  about AI, with younger people more trusting and accepting  of AI, and the university-educated more likely to accept AI.  A one-size-fits-all approach is therefore unlikely to work. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
56 Trust in artificial intelligence Strengthen the regulatory framework for governing AI   –Most citizens view the societal impacts of AI as  uncertain and unpredictable. Furthermore, most citizens  believe the challenges associated with AI such as fake  online content, surveillance, data privacy, cyber-attacks,  data privacy, and disease misdiagnosis, are likely to  impact a large number of fellow citizens in their country.  The public are near unanimous in their expectation that  governments and the companies deploying AI carefully  manage these challenges.    –It is understandable, therefore, that the large majority  (81%) of the citizens surveyed expect AI to be regulated.  However, many view the current regulatory and legal  framework as insufficient to make AI use safe and  protect people from the risks.    –Given this pattern and the finding from this survey that  the perceived adequacy of current regulations and laws  is the single most important driver of public trust in AI  systems, a clear pathway to enhancing trust in AI is to  strengthen the regulatory and legal framework governing  AI, and citizens’ understanding of these frameworks.   A strong governance environment reassures the public  that AI is being deployed in a trustworthy way and is safe,  while also providing guidance and confidence to business  to innovate and adopt AI29.    –The public clearly want appropriate regulation that is  fit-for-purpose to manage the risks and uncertainties  associated with AI. Our results further show that most  of the public expect an independent AI regulator, as  well as the government and existing regulators, to be  involved in regulating and governing AI systems, rather  than leaving it to industry only. Most of the public have  at least moderate confidence in the government to  do so in the public’s best interest. However, the US  and UK are trailing behind the other countries in their  confidence in government to regulate AI. Given these  countries also have lower trust more generally towards  their governments, strengthening general trust towards  government is likely to be an important building block to  enhancing confidence in AI regulation.    –Given the public has the most confidence in universities,  research and defence organisations to develop and use,  as well as regulate and govern AI systems, there is an  opportunity for business and government to partner with  these organisations around AI initiatives.   –Our findings further indicate that organisations can  directly build trust and willingness to use AI systems by  adopting assurance mechanisms that support the ethical  deployment of AI systems. These include actions such  as establishing independent AI ethics reviews, adopting  codes of conduct and national standards, and obtaining  AI ethics certification.Enhance Public AI literacy   –A key finding is that the public generally has low  awareness and understanding of AI and its use in  everyday life. While younger people, men, and the  university-educated tend to be more aware and  understand AI better, even these groups report low to  moderate AI understanding.    –At the same time, a large majority of the community  are interested in learning more about AI (83%), and  report that supporting people to understand AI, is  important for them to trust AI systems (95%). This last  insight is further supported by our path model, which  identified familiarity and understanding of AI as a key  driver of trust and acceptance of AI.   –Collectively, these insights paint a clear picture of the  need to increase the AI literacy of the public. Educating  the community about what AI is and when and how it  is being used is important for a range of reasons. First,  despite the current low awareness and understanding,  the community have strong views on the regulation,  use and design of AI. Increasing public literacy will  assist in ensuring these views are well informed.  Second, AI literacy empowers citizens, consumers  and employees to better seize the benefits and  opportunities from AI systems, while also identifying  and managing the potential risks (e.g. of data sharing  and privacy). Third, AI literacy is fundamental to the  public’s ability to effectively contribute to public policy  and debate on the stewardship of AI into society, and  facilitates meaningful public consultation on AI design  and use.   –Some countries have already invested in providing  free AI public literacy courses30 and the European  Commission recently developed a Digital Education  Action Plan to facilitate AI and digital education31. We  recommend that enhancing the public’s AI literacy  be a responsibility shared by government (e.g.  formal programs within schools and for citizens), and  organisations using or developing AI (e.g. by investing in  employee and customer AI literacy programs and tools). These three pathways are each important for the  responsible stewardship of AI into society and provide  complementary ways to build and maintain citizen trust in  AI systems. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
57 Trust in artificial intelligence Appendix 1 Methodological and Statistical Notes  In this section, we explain our methodological and statistical approach. Survey design Where possible, we used or adapted existing validated  measures from academic research or from previous  attitude surveys32.  We asked a subset of questions to explore whether  respondents’ trust and attitudes towards AI differ depending  on whether we asked about AI systems in general versus  two specific use cases of AI systems: Healthcare AI and  Human Resource AI. These two domains were chosen as  they represent domains where AI is rapidly being deployed  and is likely to impact a large numbers of citizens (healthcare  as an essential service, and HR as relevant to a broad range  of employment contexts).  Before answering questions, respondents were provided  with a brief description of what the system does, how  it works and how it is used (see shortened descriptions  below). The research team developed these descriptions  based on a range of in use systems and input from domain  experts working in healthcare and human resources.We extensively piloted and refined the survey before full  launch. To ensure survey equivalence across countries, we  conducted translation and back translation of the French and  German versions of the survey using separate professional  translators and piloted these surveys separately with French  and German speakers prior to use. Reporting differences between countries   and applications, and within people Our in-text reporting of between-country, betweenapplication, between-citizen and within-person differences  was informed by statistical analyses and adhered to wellestablished benchmarks for interpreting between- and withinsubject effect sizes (see Cohen, 1988; Lakens, 2013). We used  a stringent cut off of p<.01 to interpret statistical significance. We used one-way analysis of variance (ANOVA) to examine  differences between countries, between AI applications  and between citizens (e.g. generational differences). Where  there are statistically significant differences between groups  (p<.01), we examined the omega-squared effect size to  determine the magnitude of difference between the groups.  Differences with an effect size less than .01 were deemed  practically insignificant and are not reported. Meaningful  patterns of between country findings that exceeded the .01  effect size are reported33. We performed paired-sample t-tests to examine withinperson differences (for instance, the variability in one’s  willingness to rely on versus share information with an AI  system). We used a measure of effect size to determine the  magnitude of statistically significant effects. Specifically, we  used Hedges’ g with a cut-off of .30 to indicate a robust and  practically meaningful difference. Rounding When percentages did not add up to 100% due to rounding,  we distributed the remaining value based on decreasing  order of the values’ decimal part, as per the Largest  Remainder Method. Human Resource AI  An AI system used to improve  the prediction and evaluation of  performance by collecting and  comparing employee data and job  performance over time. Managers use  the system to inform decisions about  hiring and promotion.Healthcare AI  An AI system used to improve the  diagnosis and treatment of disease.  The system compares patient health  data to existing databases to produce  recommendations. Doctors use the  system to inform decisions about how  to diagnose and treat patients. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
58 Trust in artificial intelligence Appendix 2 Country Samples Country demographic profiles The demographic profile of each sample  was nationally represented on age,  gender and location based on official  national statistics within each country,  specifically: USA – United States Census  Bureau, Canada – Statistics Canada,  Germany – Federal Statistical Office, UK  – Office for National Statistics, Australia  – Australian Bureau of Statistics. Income  reflects household income for all  countries except Australia, which reports  personal income.  The gender balance was 51% female and  49% male for all countries. The mean age  across countries ranged from 46 years  (USA and Australia) to 49 years (Canada).  The countries vary in the proportion  of the sample that live in metropolitan  regions, with Canada having the highest  metropolitan sample (73%) followed by  Australia, the USA, then Germany and  the UK (46%). Country samples also  vary on education, with USA having the  highest percentage with a university  education (48%) and Germany the lowest  (34%). In forming income categories  for each country, we consulted data  from each country’s census or national  statistics body related to median  household income to ensure this figure  was within the relevant country’s middle  category. Descriptive analysis of our  income categories confirmed the median  response in each country was in the  middle category, as expected. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
59 Trust in artificial intelligence Area Non-metropolitan 48% Metropolitan 52% Area Non-metropolitan 27% Metropolitan 73%Area Non-metropolitan 58% Metropolitan 42% Area Non-metropolitan 54% Metropolitan 46%Area Non-metropolitan 31% Metropolitan 69% Age Generation Z (18-23) 8% Millennial (24-39) 34% Generation X (40-55) 27% Baby Boomer (56-74) 27% Silent Generation (75-89) 4% Age Generation Z (18-23) 3% Millennial (24-39) 29% Generation X (40-55) 33% Baby Boomer (56-74) 32% Silent Generation (75-89) 3%Age Generation Z (18-23) 5% Millennial (24-39) 30% Generation X (40-55) 29% Baby Boomer (56-74) 33% Silent Generation (75-89) 3% Age Generation Z (18-23) 3% Millennial (24-39) 37% Generation X (40-55) 27% Baby Boomer (56-74) 29% Silent Generation (75-89) 4%Age Generation Z (18-23) 13% Millennial (24-39) 29% Generation X (40-55) 24% Baby Boomer (56-74) 28% Silent Generation (75-89) 6% Education High school, GED or less 28% Vocational/trade/ technical qualification7% Associate degree 17% Undergraduate degree 29% Postgraduate degree 19% Education Secondary school or less 22% Vocational/trade/technical  qualification12% College/CEGEP or other nonuniversity certificate or diploma21% Undergraduate degree 34% Postgraduate degree 11%Education Secondary school or less 7% Vocational / apprenticeship/  professional41% Secondary school graduation/ university entrance qualification18% Undergraduate degree 17% Postgraduate degree 8% Education School to O Level, GCSE,   National 5 (or similar) or less19% Vocational/trade/technical  qualification22% School to A Level (or similar) 15% Undergraduate degree 32% Postgraduate degree 12%Education Year 11 or lower 11% Completed Year 12 21% Vocational/trade/ technical qualifications27% Undergraduate degree 31% Postgraduate degree 10% Income (Household) Less than $25,000 20% $25,000 – $49,999 22% $50,000 – $74,999 21% $75,000 – $99,999 13% $100,000 or more 24% Income (Household) Less than $25,000 12% $25,000 – $49,999 21% $50,000 – $74,999 22% $75,000 – $99,999 19% $100,000 or more 26%Income (Household) Less than €18,000 21% €18,000 – €30,999 24% €31,000 – €51,999 28% €52,000 – €99,999 22% €100,000 or more 5% Income (Household) Less than £18,000 21% £18,000 – £30,999 29% £31,000 – £51,999 29% £52,000 – £99,999 18% £100,000 or more 3%Income (Personal) Less than $25,000 27% $25,000 – $49,999 26% $50,000 – $74,999 20% $75,000 – $99,999 11% $100,000 or more 16%USA: 1,223 Canada: 1,229Germany: 1,202 UK: 1,200Australia: 1,200 ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
60 Trust in artificial intelligence Endnotes 1  International Data Corporation. (2021, February  23). IDC Forecasts Improved Growth for Global AI  Market in 2021 . Retrieved from www.idc.com 2  AI HLEG. (2019). Ethics Guidelines for Trustworthy  AI. European Commission. Retrieved from https:// ec.europa.eu/; 3  For example, see Gillespie, N., Curtis, C., Bianchi,  R., Akbari, A., and Fentener van Vlissingen, R.  (2020). Achieving Trustworthy AI: A Model for  Trustworthy Artificial Intelligence. KPMG and  The University of Queensland Report. doi. org/10.14264/ca0819d 4  Loureiro, S. M. C., Guerreiro, J., & Tussyadiah, I.  (In press). Artificial intelligence in business: State  of the art and future research agenda. Journal of  Business Research. 5  Schwab, K. (2016).The Fourth Industrial Revolution.  World Economic Forum. https://www.weforum. org/about/the-fourth-industrial-revolution-by-klausschwab Schwab, K. (2015, December 12). The Fourth  Industrial Revolution: What it means and how to  respond. Foreign Affairs. Retrieved from https:// www.foreignaffairs.com/articles/2015-12-12/ fourth-industrial-revolution. [Accessed October  6, 2020]. 6  OECD (2019), Artificial Intelligence in Society,  OECD Publishing, Paris. https://doi.org/10.1787/ eedfee77-en 7  Multiple international and pan governmental  organisations, including the OECD, The European  Commission, and the G7 Innovation Ministers,  note the importance of trust in AI and developing  ‘trustworthy’ AI systems, to support continual  AI adoption. This is also recognised in the AI  roadmaps and strategic plans of the five countries  examined in this report (see for example https:// assets.publishing.service.gov.uk/government/ uploads/system/uploads/attachment_data/ file/949539/AI_Council_AI_Roadmap.pdf and  https://www.nitrd.gov/news/National-AI-RDStrategy-2019.aspx 8  Eurobarometer. (2019). Europeans and Artificial  Intelligence. European Commission. Retrieved  from https://ec.europa.eu/commfrontoffice/ publicopinion/index.cfm/ResultDoc/download/ DocumentKy/89670; Eurobarometer. (2017).  Attitudes towards the impact of digitisation and  automation on daily life (Report no. 460). Retrieved  from https://ec.europa.eu/; BEUC (2020). Artificial  Intelligence: what consumers say. Findings and  policy recommendations of a multi-country survey  on AI. The European Consumer Organisation.  Retrieved from https://www.beuc.eu/publications/ beuc-x-2020-078_artificial_intelligence_what_ consumers_say_report.pdf; Pew Research  Centre. (2020). Science and Scientists Held in  High Esteem Across Global Publics: Yet there is  ambivalence in many publics over developments  in AI, workplace automation, food science.  Retrieved from https://www.pewresearch.org/science/2020/09/29/science-and-scientists-heldin-high-esteem-across-global-publics/; Zhang, B.,  & Dafoe, A. (2019). Artificial intelligence: American  attitudes and trends. Retrieved from SSRN  3312874. 9  Seven respondents identified as a gender ‘other’  than male or female. Due to the low proportion  of respondents, we were unable to run statistical  analyses for this group. In analyses reporting  gender differences (see Figures 27 and 28, related  to awareness of AI and subjective knowledge), we  note that 43% of ‘other’ respondents had heard  of AI (29% had not heard of AI, 28% don’t know),  and just 14% of these respondents reported high  subjective knowledge. 10  The Australian sample used in this report  was stratified from a larger sample of 2,575  Australians collected for an Australian-specific  report (Lockey, Gillespie, & Curtis, 2020). We  stratified the sample to achieve a similar size  to the four other country samples and avoid  disproportionate influence of aggregated country  data. There are no substantive demographic  differences between the full Australian sample  and the sub-set used in this report.  11  Healthcare AI is most trusted (M = 3.9), followed  by AI (M = 3.70), then Human Resource AI (M  = 3.5). These differences are all statistically  significant (all comparison p < .001, ANOVA effect  size = .02). 12  We asked respondents to rate their confidence  in twelve entities. Given similarities in the  data across some of these entities, for  simplicity we amalgamated responses to the  following six entities into three entities: ‘The  federal government’ and ‘My state/province  government’ to form ‘Federal/State/Provincial  Government’; ‘Independent regulatory bodies  funded by the government’ with ‘Existing  agencies that regulate and govern specific  sectors’ to form ‘Existing regulatory agencies’;  ‘Intergovernmental research organizations  (e.g. CERN)’ with ‘Non-government  scientific organizations (e.g. AAAI)’ to form  ‘Intergovernmental and non-governmental  research organizations’. 13  For ease of interpretation and display, we report  data for seven of the nine entities. Responses to  ‘National AI Governance Body’ and ‘International  Organisations (e.g. United Nations, European  Union)’ followed the same pattern of country  differences, with the US reporting less  confidence in these entities to regulate and  govern AI compared to other countries.14  AAAI refers to the Association for the  Advancement of Artificial Intelligence, an  international scientific society promoting  research in, and responsible use of, AI. 15  Eurobarometer. (2017). Attitudes towards the  impact of digitisation and automation on daily  life (Report no. 460). Retrieved from https:// ec.europa.eu/; Zhang, B., & Dafoe, A. (2019).  Artificial intelligence: American attitudes and  trends. Retrieved from SSRN 3312874. 16  Eurobarometer (2019). Europeans and Artificial  Intelligence. European Commission. Retrieved  from https://ec.europa.eu/commfrontoffice/ publicopinion/index.cfm/ResultDoc/download/ DocumentKy/89670; Eurobarometer. (2017).  Attitudes towards the impact of digitisation  and automation on daily life (Report no. 460).  Retrieved from https://ec.europa.eu/; BEUC  (2020). Artificial Intelligence: what consumers  say. Findings and policy recommendations of  a multi-country survey on AI. The European  Consumer Organisation. Retrieved from https:// www.beuc.eu/publications/beuc-x-2020-078_ artificial_intelligence_what_consumers_say_ report.pdf 17  Jobin, A., Ienca, M., & Vayena, E. (2019). The  global landscape of AI ethics guidelines. Nature  Machine Intelligence, 1(9), 389-399. 18  AI HLEG. (2019). Ethics Guidelines for  Trustworthy AI. European Commission. Retrieved  from https://ec.europa.eu/ 19  Edelman AI. (2019). 2019 Edelman AI Survey.  Retrieved from https://www.edelman.com/;  Eurobarometer. (2017). Attitudes towards the  impact of digitisation and automation on daily  life (Report no. 460). Retrieved from https:// ec.europa.eu/; Zhang, B., & Dafoe, A. (2019).  Artificial intelligence: American attitudes and  trends. Retrieved from SSRN 3312874. 20  This question was asked differently in the  Australian survey (“What percentage of your  work involves some form of AI?” compared to  “How often do you use AI in your work?”). As  such, responses cannot be directly compared,  and Australian data is excluded. Full reporting  of the Australian sample is available in Lockey,  Gillespie & Curtis (2020). 21  Zhang, B., & Dafoe, A. (2019). Artificial  intelligence: American attitudes and trends.  Retrieved from SSRN 3312874. 22  Exceptions were autonomous vehicles and  criminal justice bias, where Australia was higher  than only select countries, not all. ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
61 Trust in artificial intelligence 23  Eurobarometer. (2017). Attitudes towards the  impact of digitisation and automation on daily  life (Report no. 460). Retrieved from https:// ec.europa.eu/ 24  Australians were asked whether they had heard,  read, or seen anything about AI “in the past 12  months” , whereas this qualifier was removed  for other countries. Therefore, we exclude  Australians from the examination of country  differences. 25  Responses to the three items assessing  subjective knowledge were aggregated to  produce an overall score. 26  BEUC (2020). Artificial Intelligence: what  consumers say. Findings and policy  recommendations of a multi-country survey  on AI. The European Consumer Organisation.  Retrieved from https://www.beuc.eu/ publications/beuc-x-2020-078_artificial_ intelligence_what_consumers_say_report.pdf 27  For an accessible introductory text detailing the  path analytic approach, see Hayes, A. F . (2017).  Introduction to mediation, moderation, and  conditional process analysis A regression-based  approach (2nd ed.). Guilford Press: New York. 28  For example: AI HLEG. (2019). Ethics Guidelines  for Trustworthy AI. European Commission.  Retrieved from https://ec.europa.eu/ 29  The importance of effective governance and  legislation is noted in the AI roadmaps of several  countries (for example, see https://www. gov.uk/government/publications/ai-roadmap,  https://www.ki-strategie-deutschland.de/files/ downloads/Fortschreibung_KI-Strategie_engl.pdf,  and https://cifar.ca/wp-content/uploads/2020/05/ rebooting-regulation-exploring-the-future-of-aipolicy-in-canada.pdf )30  An example is Finland’s ‘Elements of AI’  course run by the University of Helsinki (see  https://www.elementsofai.com/). The UK  government has developed a free online  ‘Skills Toolkit’ that includes introductory  courses on AI (https://nationalcareers.service. gov.uk/find-a-course/the-skills-toolkit ) 31  European Commission. (2020). Digital  Education Action Plan (2021-2027): Resetting  Education for the Digital Age. Retrieved  from https://ec.europa.eu/education/sites/ default/files/document-library-docs/deapcommunication-sept2020_en.pdf 32  We draw from validated scaled published in  academic journals (e.g. Porting & Pidgeon,  2005; Pavlou et al., 2007), and in places,  used or adapted scales from prior attitude  surveys by Zhang and Dafoe (2019) and  Eurobarometer (2017) 33  See Field, A. (2013). Discovering statistics  using IBM SPSS statistics (4th ed.). Sage:  London. (see page 474 values for 2 of 0.01,  0.06, 0.14 indicate small, medium and large  effects respectively). ©2021 The University of Queensland  ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company  limited by guarantee. All rights reserved. The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.  Liability limited by a scheme approved under Professional Standards Legislation.
KPMG .com.au ©2021 The University of Queensland  The information contained in this document is of a general nature and is not intended to address the objectives, financial situation or needs of any particular individual  or entity. It is provided for information purposes only and does not constitute, nor should it be regarded in any manner whatsoever, as advice and is not intended to  influence a person in making a decision, including, if applicable, in relation to any financial product or an interest in a financial product. Although we endeavour to  provide accurate and timely information, there can be no guarantee that such information is accurate as of the date it is received or that it will continue to be accurate  in the future. No one should act on such information without appropriate professional advice after a thorough examination of the particular situation. To the extent permissible by law, KPMG and its associated entities shall not be liable for any errors, omissions, defects or misrepresentations in the information or  for any loss or damage suffered by persons who use or rely on such information (including for reasons of negligence, negligent misstatement or otherwise). ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International  Limited, a private English company limited by guarantee. All rights reserved.  The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation. Liability limited by a scheme approved under Professional Standards Legislation.  March 2021 . 573005795MCNicole Gillespie KPMG Chair in Organisational T rust   Professor of Management,   The University of Queensland T: +61 7 3346 8076   E: n.gillespie1@uq.edu.auDr Steve Lockey Research Fellow (T rust) The University of Queensland T: +61 7 3443 1206   E: s.lockey@uq.edu.auDr Caitlin Curtis Research Fellow (T rust) The University of Queensland T: +61 7 3346 8083   E: c.curtis@uq.edu.au James Mabbott National Leader, KPMG Futures KPMG Australia T: +61 2 9335 8527   E: jmabbott@kpmg.com.au Richard Boele Global Leader, Business   & Human Rights Services KPMG Australia T: +61 2 9346 585   E: rboele@kpmg.com.au Jon Stone Partner, KPMG Digital Delta KPMG Australia T: +61 3 9288 5048   E: jonstone@kpmg.com.auZoe Willis National Leader, Data & RegT ech KPMG Australia T: +61 2 9335 7494   E: zoewillis@kpmg.com.au Dr Sanjay Mazumdar Chief Data Officer KPMG Australia T: +61 8 8236 7237   E: skmazumdar@kpmg.com.au Ali Akbari Artificial Intelligence   Capability Lead KPMG Australia T: +61 2 9335 7740   E: aakbari@kpmg.com.auRossana Bianchi Associate Director,   Strategy, Growth & Digital KPMG Australia T: +61 2 9335 7036   E: rbianchi2@kpmg.com.au Rita Fentener van Vlissingen Associate Director, Human   Rights & Social Impact KPMG Australia T: +61 2 9346 6366   E: ritafentener@kpmg.com.auKey contacts University of Queensland KPMG
