Responsibility  and AI P repared by the Expert Committee on human righ ts dimensions of automated da ta processing and different forms of ar tificial intelligence (MSI-AUT)Council of Europe study D GI(2019)05R apporteur: Karen Yeung
                   DGI(2019)05 A  study of the implications of advanced digital technologies ( including AI systems) for the concept of responsibility within a  human rights framework  P repared by the Expert Committee on human rights dimensions of au tomated data processing and different forms of artificial intelligence ( MSI-AUT) R apporteur: Karen Yeung  
Council of Europe Study 2  French edition: R esponsabilité et IA T he opinions expressed in this work are the r esponsibility of the authors and do not necessarily r eflect the official policy of the Council of Europe. A ll requests concerning the reproduction or translation o f all or part of this document should be addressed to t he Directorate of Communication (F-67075 Strasbourg Ce dex or publishing@coe.int). All other correspondence c oncerning this document should be addressed to the Di rectorate General Human Rights and Rule of Law. Co ver design: Documents and Publications Pro duction Department (SPDP), Council of Europe Pho tos: Shutterstock T his publication has not been copy-edited by the SPDP E ditorial Unit to correct typographical  an d grammatical errors. ©  Council of Europe, September 2019 Pri nted at the Council of Europe 
DGI(2019)05 3  TABLE OF CONTENTS  I ntroduction .................................................................................................................................. 5  E xecutive Summary ...................................................................................................................... 6  C hapter 1.  Introduction ............................................................................................................. 16  1 .1 Scope of this study ....................................................................................................................... 16  1 .2 Structure of this study ................................................................................................................. 17  1 .3 Understanding the implications of AI for concepts of responsibility ......................................... 18  1 .4 Implications for the concept of responsibility from a human rights perspective ...................... 25  C hapter 2. Threats, risks, harms and wrongs associated   w ith advanced digital technologies ........................................................................................... 28  2 .1 The rise of algorithmic decision-making (ADM) systems ........................................................... 28  2 .1.1 How do ADM systems systematically threaten particular rights? .................................... 29  2 .1.2 Societal risks associated with data-driven profiling ......................................................... 33  2 .2 Collective societal threats and risks generated by other AI technologies ................................. 38  2 .2.1  Malicious attacks, unethical system design or unintended system failure ..................... 38  2 .2.2 Loss of authentic, real and meaningful human contact ................................................... 38  2 .2.3 The chilling effect of data repurposing ............................................................................. 39  2 .2.4 Digital power without responsibility ................................................................................ 39  2 .2.5 The hidden privatisation of decisions about public values ............................................... 40  2 .2.6 Exploitation of human labour to train algorithms ............................................................ 41  2 .3 Power asymmetry and threats to the socio-technical foundations   o f moral and democratic community ................................................................................................ 41  2 .4 Summary ...................................................................................................................................... 43  C hapter 3.  Who bears responsibility for the threats, risks, harms   a nd wrongs posed by advanced digital technologies? .............................................................. 44  3 .1 What is responsibility and why does it matter? ......................................................................... 45  3 .2 Dimensions of responsibility ....................................................................................................... 48  3 .3 How do advanced digital technologies (including AI) implicate existing conceptions of  r esponsibility? .................................................................................................................................... 49  3 .3.1 Prospective responsibility: voluntary ethics codes   a nd the ‘Responsible Robotics/AI’ project ................................................................................ 51  3 .3.2 Machine autonomy and the alleged ‘control’ problem .................................................... 53  3 .4 Models for allocating responsibility ............................................................................................ 55  3 .4.1 Intention/culpability-based models .............................................................................. 57  3 .4.2 Risk/Negligence-based models ..................................................................................... 58  3 .4.3 Strict responsibility........................................................................................................ 60  
Council of Europe Study 4  3.4.4 Mandatory Insurance .................................................................................................... 61  3 .5 Responsibility challenges posed by complex and dynamic socio-technical systems ................ 62  3 .5.1 The problem of ‘many hands’ ....................................................................................... 62  3 .5.2 Human-Computer Interaction ....................................................................................... 64  3 .5.3 Unpredictable, dynamic interactions between complex socio-technical systems ....... 66  3 .6 State responsibility for ensuring effective protection of human rights ..................................... 67  3 .7 Non-judicial mechanisms for enforcing responsibility for advanced digital technologies ........ 68  3 .7.1 Technical protection mechanisms ................................................................................ 69  3 .7.2 Regulatory governance instruments and techniques ................................................... 70  3 .7.3 Standard setting, monitoring and enforcement ........................................................... 72  3 .8 Reinvigorating human rights discourse in a networked digital age ........................................... 72  3 .9 Summary ...................................................................................................................................... 75  C hapter 4.  Conclusion ................................................................................................................ 77  A ppendix A .................................................................................................................................. 80  R eferences  .................................................................................................................................. 83  
DGI(2019)05 5  Introduction In  the terms of reference for the Steering Committee on Media and Information Society (CD MSI) for the biennium 2018 – 2019, the Committee of Ministers of the Council of Europe aske d the CDMSI to “ study the development and use of new digital technologies and services, i ncluding different forms of artificial intelligence, as they may impact peoples’ enjoyment of h uman rights and fundamental freedoms in the digital age, with a view to giving guidance for fu ture standard-setting in this field ” and approved the committee of experts on human rights d imensions of automated data processing and different forms of artificial intelligence (MSI-AU T) as a subordinate structure to facilitate the work of the CDMSI.  In  its first meeting on 6-7 March 2018, the expert committee decided to focus the study on the the  implications of AI decision-making for the concept of responsibility within a human rights frame work. Prof. Karen Yeung was appointed as rapporteur for the preparation of the study. C omposition of the Committee of Experts MSI-AUT Ab raham BERNSTEIN, Professor of Informatics, University of Zürich J orge CANCIO, International Relations Specialist, Federal Office of Communications, Swi tzerland L uciano FLORIDI, Professor of Philosophy and Ethics of Information, Oxford University Sed a GÜRSES, Assistant Professor, Technical University Delft Gab rielle GUILLEMIN, Senior Legal Officer, ARTICLE 19 N atali HELBERGER, Professor of Information Law, University of Amsterdam L uukas ILVES (Chair), Deputy Director and Senior Fellow, Lisbon Council Tanja  KERŠEVAN SMOKVINA , State Secretary, Ministry of Culture, Slovenia J oe MCNAMEE, Independent Consultant Ev genios NASTOS, Head of Information Unit, Ministry of Digital Policy, Telecoms & Media, Greec e  P ierluigi PERRI, Professor of Computer Law, University of Milan  Wo lfgang SCHULZ (Vice-Chair), Professor of Law, University of Hamburg  Karen Y EUNG, Interdisciplinary Professorial Fellow in Law, Ethics and Informatics, University of B irmingham  
Council of Europe Study 6  Executive Summary Ad vanced digital technologies and services, including task- specific artificial intelligence (‘AI’) b ring with them extraordinary promise. They have already generated very substantial benefits, p articularly in the form of enhanced efficiency, accuracy, timeliness and convenience across a wi de range of digital services.  Ye t the emergence of these technologies has also been accompanied by rising public anxiety co ncerning their potentially damaging effects: for individuals, for vulnerable groups and for so ciety more generally.  If these technologies are to be a force for good which enables, rather than  undermines, individual and societal flourishing, then it is imperative that we acquire a d eeper understanding of these concerns. Not only does this require us to acquire a deeper u nderstanding of their impact on the enjoyment of human rights and fundamental freedoms, b ut it also entails careful consideration of questions concerning where responsibility should lie fo r their adverse consequences.   Thi s study begins from the premise that, within contemporary constitutional democratic o rders, a society’s concepts, institutions and practices of responsibility are of critical i mportance.  This is necessary in order to ensure that individuals and organisations are ap propriately held to account for the adverse effects of their actions on others, and in order to est ablish and maintain the foundations for trustworthy and peaceful social cooperation and co ordination.   Acco rdingly, the purpose of this study is to examine the implications of advanced digital te chnologies (including AI) for the concept of responsibility, particularly in so far as they might i mpede the enjoyment of human rights and fundamental freedoms protected under the ECHR an d how responsibility for those risks and consequences should be allocated.   Its  methodological approach  is interdisciplinary, drawing on concepts and academic scho larship from law, the humanities, the social sciences and, to a more limited extent, from co mputer science.  It concludes that, if we are to take human rights seriously in a globally co nnected digital age, we cannot allow the power of our advanced digital technologies and sy stems, and those who develop and implement them, to be accrued and exercised without respo nsibility. Nations bear the primary duty to protect human rights. They must therefore ensu re that those who wield and derive benefits from designing, developing and deploying these  technologies are held responsible for their adverse impacts.  This includes obligations to ensu re that there are effective and legitimate institutional mechanisms that will operate to p revent and forestall  violations to human rights which these technologies may threaten, and to att end to the health of the larger collective and shared socio-technical environment  in which h uman rights and the rule of law are anchored.  This summary gives a brief overview of the m ain content of the report. C hapter 1:  Introduction Ch apter 1 outlines what AI is and how task-specific AI technologies work . It refers to AI as a set  of advanced general-purpose technologies which use techniques from statistics, computer sci ence, and cognitive psychology to enable machines to do highly complex tasks efficiently. The se technologies aim either to reproduce or surpass abilities that would require ‘i ntelligence’ in humans; e.g. reasoning, autonomy, creativity, etc.  It describes how AI te chnologies work using machine learning, enabling computational systems to learn from 
DGI(2019)05 7  examples, data, and experience and consequently to perform specific tasks intelligently. It ex plains how machine learning technologies raise issues of responsibility due to their capacity to  enable task automation and to enable machines to make decisions and perform tasks to so me extent independently from their human developers.  The  Chapter draws attention to the capacity of machine learning systems to learn and change o ver time, dynamically setting their own sub-goals, and their ability to adapt to local co nditions via external sensor information or updated input data. Human designers of these sy stems decide upon and set their initial parameters and the overarching goal which these sy stems are intended to optimise. At the same time, machine learning systems are designed to o perate by making independent decisions that choose between alternatives in ways that are n ot pre-programmed in advance, and to do so without any human intervention.  Because these  systems learn dynamically and iteratively from their environment, which is itself often v olatile and continuously changing, this has implications for the stability and predictability of thei r operation.  In particular, these systems have the potential to evolve in unexpected ways (se ction 1.3).   Ch apter 1 then explains how, in the context of our contemporary global data infrastructure, AI te chnologies display a range of other properties that have direct implications for the concept o f responsibility, including their: •  inscrutability and opacity •  complex and dynamic nature  •  reliance on human input, interaction and discretion •  general purpose nature  •  global interconnectivity, scalability and ubiquity  •  reliance on large data-sets  •  automated, continuous operation, often in real-time  •  capacity to generate ‘hidden’ insight from merging data sets   •  ability accurately to imitate human traits  •  greater software complexity (include vulnerability to failure and malicious attack)   •  capacity to ‘personalise’ and configure individual choice environments , and •  capacity to configure social choice environments, thus redistributing risks and benefits to o ptimise a pre-specified goal (section 1.3) The  chapter also explains the interdisciplinary ‘human rights perspective’  adopted in the stud y, which draws on the human rights and fundamental freedoms protected under the ECHR i n order to: •  understand the nature of the risks and adverse consequences generated by advanced d igital technologies,  •  help identify how responsibility for those threats, risks and consequences should be att ributed and allocated, and  •  inform consideration of the kinds of institutional mechanisms that may be needed to ensu re that human rights are effectively protected.   Fi nally, the discussion draws attention to existing work concerning the adverse impact of AI te chnologies on human rights and fundamental freedoms, and upon which the discussion in Ch apter 2 seeks to build. 
Council of Europe Study 8  Chapter 2: Threats, risks, harms and wrongs associated with advanced digital technologies Ch apter Two examines a range of adverse consequences potentially associated with the use of ad vanced digital technologies.  It begins by considering the socio-historical context of te chnological innovation, suggesting that on-going advances in networked digital technologies are  likely to prompt far-reaching changes to social and economic life of a scale and magnitude as  unsettling and disruptive as the original Industrial Revolution.  The resulting ‘New’ Industrial Rev olution now dawning may resemble the original Industrial revolution in that it is likely to g enerate myriad benefits but, in so doing, might also generate unintended adverse effects that we re not recognised at the time of the revolution’s unfolding. Accordingly, making reliable p redictions about the aggregate, cumulative effects of the current networked digital rev olution over time is extremely challenging.   The  discussion then considers how the use of algorithmic decision- making  (‘ADM’) systems that  rely on data-driven profiling techniques may threaten several human rights (section 2.1), in cluding:  •  rights to a fair trial and to ‘due process’  (Art 6 ECHR), particularly where ADM systems are u sed to automate decisions that significantly affect individuals, yet typically deny the affecte d individual the opportunity to participate, contest or otherwise challenge the o utcome of the decision or the decision-making inputs. Some of these systems are i ncapable of producing an explanation of its underlying logic in terms that are intelligible an d comprehensible to the individual; •  rights to freedom of expression and information  (Art 10 ECHR), particularly given the p owerful influence which global digital platforms now exert over the informational env ironments of both individuals and societies, in which automated algorithms typically d ecide how to handle, prioritise, distribute and delete or remove third-party content o nline, including during political and electoral campaigns.   Although platforms have been w ell-intended in seeking voluntarily to identify and remove ‘extremist’ content, there are seri ous risks that these activities may not meet Art 10(2)’s  requirements of legality, l egitimacy and proportionality for permissible interference with freedom of expression; •  rights to privacy and data protection  (Art 8 ECHR), due to the reliance of data-driven p rofiling technologies on the collection and processing of digital data gleaned from tracki ng the on-line behaviour of individuals at a highly granular level, across a population, the  use of these techniques invariably affect the Article 8 right to private and family life.  Al though contemporary data protection regimes (such as modernised Conv 108) play an i mportant role in safeguarding the rights and interests of data subjects, they might not in p ractice provide effective and comprehensive protection; •  rights to protection against discrimination in the exercise of rights and freedoms  (Art 14 ECHR ), may be implicated due to the significant risks of bias and discrimination arising fro m the use of machine learning algorithms, due to the opportunities for bias of the al gorithm’s developers, bias built into the mo del upon which the systems are built, biases i nherent in the data sets used to train the models, or biases introduced when such systems are  implemented in real world settings.  Such biases might not only violate the right to p rotection against discrimination in the exercise of rights and freedoms protected under Ar t 14, but may also reinforce biases against groups that have historically been 
DGI(2019)05 9  disadvantaged, thereby compounding and exacerbating discrimination and structural d isadvantage. The  discussion then considers how data-driven profiling techniques, when employed at scale, m ay implicate collective values and interests because they make practices of pervasive sur veillance, personalisation and manipulation possible at a population level in ways that risk u ndermining human dignity and autonomy, for example, by systematically treating individuals as o bjects rather than as moral subjects (section 2.1.2).  The  adverse social implications that might accompany the development and use of AI te chnologies generally, including those that do not rely on the profiling of individuals, are then co nsidered (section 2.2). They include: •  risks of large-scale harm from malicious attacks  •  unethical system design or unintended system failure  •  loss of authentic, real and meaningful human contact  •  the chilling effect of data repurposing •  the exercise of digital power without responsibility  •  the hidden privatisation of decisions about public values (including distributive justice) &  •  the exploitation of human labour to train algorithms. Fi nally, the discussion highlights the power asymmetry between those who develop and em ploy AI technologies, and those who interact with and are subject to them (section 2.3). Whi le digital service providers (and relevant third parties) that utilise AI systems can acquire v ery detailed, fine-grained data about the users of their services which they can mine to g enerate predictions about user traits, tastes and preferences with considerable accuracy, the u sers themselves (typically) do not understand the complexities of the digital technologies that they  use.  Nor do they have equivalent access to detailed information about the organisations an d firms whose services they use. This opacity and asymmetry not only expands opportunities fo r potential exploitation, but may substantially threaten collective values and interests that are  not readily expressed in existing human rights discourse, including threats to the socio-te chnical foundations of moral and democratic community. These collective threats and risks are  exacerbated by the capacity of these technologies to operate at unprecedented speed and scal e, generating novel threats, risks and challenges which contemporary societies have not h istorically had to confront.  At the same time, they are also likely to generate problems of co llective action: although the aggregate adverse effects may be very large, the effect on any p articular individual may be relatively minor and remedial action may not be sought.  C hapter3: Who bears responsibility for the threats, risks, harms  a nd wrongs posed by advanced digital technologies? Ch apter 3 considers who bears responsibility for the adverse consequences posed by advanced d igital technologies. It begins by clarifying what we mean by responsibility and why respo nsibility matters, emphasising the vital role of our institutions and practices of respo nsibility in holding to account those whose actions have adverse impacts upon others an d on collective interests and values.  These institutions and practices serve the vital role of sec uring and enabling peaceful, trustworthy social co-operation, and giving expression to the ru le of law.  Although the concept of responsibility can be understood in many different senses,  it highlights the distinction between: 
Council of Europe Study 1 0 • historic (or retrospective) responsibility:  which looks backwards, seeking to allocate respo nsibility for conduct and events that occurred in the past; and  •  prospective responsibility: which establishes obligations and duties associated with roles an d tasks that looks to the future, directed towards the production of good outcomes and the  prevention of bad outcomes.  Prospective responsibilities serve an important guiding fu nction, offering guidance about our rights and obligations vis-à-vis others, and about the way  we should behave in our dealings with others.  It  argues that we must attend to both the prospective and historic allocation of responsibility fo r the adverse consequences associated with AI technologies (section 3.2).  Only then can we h ave confidence that efforts will be made to prevent harms and wrongs from occurring (and if they  do occur, then these will be brought to an end) as a result of the development and i mplementation of these technologies. Societies must therefore ensure that they have i nstitutional structures and mechanisms that can be relied upon to ensure appropriate repa ration, repair, and the prevention of further harm or wrongdoing arising from the o peration of AI technologies. It  then investigates how advanced digital technologies (including AI) implicate existing co nceptions of responsibility (section 3.3).  To this end, it highlights differences between the co ncept of moral responsibility, on the one hand, and legal responsibility on the other.  Unlike m orality, the law has a highly developed system for institutionalising and enforcing respo nsibility (including the application of sanctions) because it must adjudicate real world d isputes. It is also important to bear in mind the distinction between two separate and distinct (al beit sometimes overlapping) types of adverse effect that can arise from the operation of AI sy stems:  •  violations of human rights (including the rights protected under the ECHR) and •  tangible harm to human health, property or the environment. Thi s study is primarily concerned with analysing responsibility for human rights violations rath er than for tangible harm, focusing primarily on those who create, develop, deploy and p reside over AI systems and their settings, and on the responsibilities of nation states to ensu re that human rights are adequately protected.   Because  AI systems can operate in time and space in new and unprecedented ways, these te chnologies may challenge our existing conceptions of responsibility.  Chapter 3 considers tw o core themes raised in contemporary discussions concerning the adverse effects of AI te chnologies.  First, the role of the tech industry in promulgating and voluntarily committing them selves to abide by so- called ‘ethical standards’ .  It argues that although these voluntary i nitiatives are in many ways welcome, these codes and standards typically lack any enfo rcement and sanctioning mechanisms and cannot therefore be relied upon to provide effec tive protection (section 3.3.1).   Secondly , the alleged ‘control problem’ that is claimed to fl ow from the capacity of AI-driven systems to operate more or less autonomously from their creat ors is claimed to create a ‘responsibility gap’ because the developers of those systems can not fairly be blamed for their outputs.  Chapter 3 demonstrates that the so- called ‘control pr oblem’ is based on a very particular moral theory of responsibility, one which places undue att ention on the conduct of the agent, and fails to give due weight to the interests of victims in sec urity of the person and property (section 3.3.2).   
DGI(2019)05 1 1 The discussion in Chapter 3 then identifies and briefly outlines a range of different ‘ responsibility models’  that could be adopted to govern the allocation and distribution of respo nsibility for different kinds of adverse impacts arising from the operation of AI systems (se ction 3.4), including models based on:  •  intention/culpability (section 3.4.1) •  risk/negligence (section 3.4.2) •  strict responsibility (section 3.4.3), and  •  mandatory insurance schemes (section 3.4.4).  In  order to identify which of these models is most suited for the allocation of historic respo nsibility for the adverse effects of AI systems, the analysis emphasises the importance of d istinguishing between human rights violations, on the one hand, and tangible harm to human h ealth, property or the environment on the other (although a single event may result in both tang ible harm and a violation of human rights).  Responsibility for rights violations of any kind, i ncluding human rights violations, is widely understood as ‘strict’.  Thus, provided that a h uman rights violation has been established, there is no need for proof of fault.  In contrast, the  allocation of obligations of repair for tangible harm to health or property may be legally d istributed in accordance with a variety of historic responsibility models.  Each model strikes a d ifferent balance between our interest, as agents, in freedom of action and our interest, as v ictims, in rights and interests in security of person and property.  It is argued that none of these  models are self-evidently the ‘correct’ or ‘best’ model for allocating and distributing the v arious threats, risks and harms associated with the operation of advanced digital te chnologies.  Rather, identifying which (if any) of these models is most appropriate will entail a  social policy choice concerning how they should be appropriately allocated and distributed.   Ch apter 3 then draws attention to several acute challenges that arise in seeking to allocate respo nsibility for the risks and other adverse impacts arising from the operation of complex an d interacting socio-technical systems (section 3.5):  a)  the ‘many hands’ problem , which arises because the development and operation of AI sy stems typically entails contributions from multiple individuals, organisations, m achine components, software algorithms and human users, often in complex and d ynamic environments.   The problem o f ‘many hands’ is not new, and rests largely on a  ‘choice theory’ of responsibility in moral philo sophy.  Contemporary legal systems h ave developed a relatively sophisticated set of principles and procedures for d etermining liability involving multiple defendants who can all be understood as h aving causally contributed to some adverse event.  The law’s  ability to devise p ractical and effective responses to the many hands problem is partly due to the g reater emphasis which the law places on the legitimate interests of victims (and p otential victims) in security of the person.  In this respect, the law’s response differs fro m choice theories of responsibility in moral philosophy which focus almost ex clusively focus on the moral agent.  Moreover, in relation to the human rights v iolations arising from the operation of AI systems, the discussion highlights the i mportance of mechanisms that prevent and forestall human rights violations arising fro m the application of advanced digital technologies. The need for effective p revention is particularly important because the aggregate and cumulative effect of these  technologies could seriously threaten the collective foundations necessary for 
Council of Europe Study 1 2 human rights and fundamental freedom to operate in practice.  These threats point to the nee d to enhance and reinvigorate human rights discourse and protection in a data-d riven age (section 3.5.1); b)  Human-computer interaction: Acute challenges arise in appropriately allocating and d istributing responsibility between humans and machines, particularly when there is a ‘h uman in the loop’ .  A recurring concern has been that, in order to ensure that   co mplex socio-technical systems that incorporate AI always operate in the service of h umanity, they should always be designed so that they can be shut down by a human o perator.  Yet individuals entrusted with the responsibility to supervise the operation o f these systems may be understandably reluctant to intervene. This risks turning h umans placed in the loop into ‘moral crumple zones’, largely totemic humans whose central  role becomes soaking up fault, although they have only partial control of the sy stem, and who are vulnerable to being scapegoated by tech developers and o rganisations seeking to avoid responsibility for unintended adverse consequences (se ction 3.5.2); and  c)  Interacting algorithmic systems: Even more intractable challenges arise in seeking to i dentify, anticipate and prevent adverse events that arise from the interactions b etween complex, algorithm-driven socio-technical systems that can occur at a speed an d scale that was simply not possible in a pre-digital, pre-networked age (eg the stock m arket ‘flash crash’ of 2010).  T he unpredictable nature of interactions between m ultiple algorithmic systems generates novel and potentially catastrophic risks, which we  have barely begun to grasp, let alone anticipate and forestall (section 3.5.3).   Al l of these problems warrant further sustained attention and consideration.  Whi le most of the discussion in Chapter 3 focuses on the responsibility of technology d esigners, developers and those who own and implement the systems which rely upon these te chnologies, the discussion in section 3.6 reminds us that it is states that bear the primary o bligation to ensure that human rights are effectively protected.  It draws attention to the p roblem of collective action that the operation of AI systems in a global networked age is likely to  generate, highlighting the vital importance of a) national legislation to ensure that human ri ghts are protected, b) the need for properly resourced national enforcement authorities with ad equate enforcement powers and c) the valuable role which accessible and convenient co llective complaints mechanisms, in addition to individual legal remedies, may play to ensure effec tive human rights protection. The  discussion then draws attention to a range of non-judicial mechanisms that have potential to  help secure both prospective and historic responsibility for the adverse impacts of AI sy stems, including various kinds of impact assessment, auditing techniques and technical p rotection mechanisms (section 3.7). Technical protection mechanisms, in particular, have co nsiderable promise.  This study emphasises the need to embed these mechanisms within a g overnance framework that enables the relevant technical standards to be set in a transparent an d participatory manner, and to ensure independent external oversight and review of their o peration.    Befo re summarising the various findings in Chapter 3, the discussion in section 3.8 briefly co nsiders whether our existing conceptions of human rights, and the mechanisms through whi ch they are protected and enforced, are fit for purpose in a global and connected digital ag e.  It suggests that the power of networked digital technologies that have emerged in recent 
DGI(2019)05 1 3 years make possible practices and actions that were previously impossible, and thereby create n ovel threats, risks and forms of wrongdoing. Accordingly, we may need to reinvigorate human r ights discourse in a networked digital age, in order to protect and nurture the socio-technical fo undations necessary for human agency and responsibility, without which human rights and freedo ms cannot be practically or meaningfully exercised.  The development of an enhanced an d reinvigorated conception of human rights could lead to the development of new i nstitutional mechanisms which are better placed to safeguard against the adverse effects of n ew digital technologies in a data-driven age. The  findings of Chapter 3 are summarised at the end of the chapter (section 3.9). Ch apter 4: Conclusion  Ch apter four concludes by summarising the argument made in the preceding sections.  It h ighlights four findings arising from this study: Fir st, it is vital that we have effective and legitimate mechanisms that will prevent and forestall h uman rights violations, given the speed and scale at which many advanced digital systems o perate in ways that pose substantial threats to human rights without necessarily generating sub stantial risks of tangible harm.  A preventative approach is especially important given that such  threats could seriously erode the social foundations necessary for moral and democratic o rders, which are essential preconditions for the exercise of individual freedom, autonomy and h uman rights.  This may include both a need to develop collective complaints mechanisms to faci litate effective rights protection, and to enhance and reinvigorate our existing conceptions an d understandings of human rights. Se cond, the model of legal responsibility that applies to human rights violations is widely unde rstood as one of ‘strict responsibility’ without the need for proof of fault.  In contrast, o bligations of repair for tangible harms may be legally allocated and distributed in accordance wi th a range of responsibility models, each striking a different balance between our interests as  agents in freedom of action, and our interest as victims in rights and interests in security of p ersons and property.  Identifying which (if any) of these models is appropriate for preventing the  various threats and risks associated with the operation of advanced digital technologies is n ot self-evident: rather, it will entail a social policy choice.  In constitutional democratic so cieties committed to protecting and respecting human rights, states bear a critical respo nsibility for ensuring that these policy choices are made in a transparent, democratic m anner and in ways that will ensure that the policy ultimately adopted will effectively safegu ard human rights.  T hird, we should nurture and support technical research concerned with securing prospective an d historic responsibility for ensuring due respect for many of the values underpinning h uman rights protection, which may facilitate the development of effective technical p rotection mechanisms and meaningful ‘algorithmic auditing’.  This research needs to be d eveloped by interdisciplinary engagement between the technical community and those from l aw, the humanities and the social sciences, in order to identify more fully how human rights p rotections can be translated and given expression via technical protection mechanisms em bedded within AI systems, and to understand how a human rights approach responds to p roblems of value-conflict. Fo urth, the effective protection of human rights in a global and connected digital age requires that  we have effective and legitimate governance mechanisms, instruments and institutions to 
Council of Europe Study 1 4 monitor, constrain and oversee the responsible design, development, implementation and o peration of our complex socio-technical systems.  This requires, at minimum, both d emocratic participation in the setting of the relevant standards, and the existence of properly reso urced, independent authorities equipped with adequate powers systematically to gather i nformation, to investigate non-compliance and to sanction violations, including powers and ski lls to investigate and verify that these systems do in fact comply with human rights stand ards and values. Fi nally, the study concludes that if we are serious in our commitment to protect and promote h uman rights in a global and connected digital age, then we cannot allow the power of our ad vanced digital technologies and systems, and those who develop and implement them, to b e accrued and exercised without responsibility.   The fundamental principle of reciprocity ap plies: those who deploy and reap the benefits of these advanced digital technologies (i ncluding AI) in the provision of services (from which they derive profit) must be responsible fo r their adverse consequences. It is therefore of vital importance that states committed to the p rotection of human rights uphold a commitment to ensure that those who wield digital p ower (including the power derived from accumulating masses of digital data) are held respo nsible for their consequences.   It follows from the obligation of states to ensure the p rotection of human rights that they have a duty to ensure that there are governance arr angements and enforcement mechanisms within national law that will ensure that both p rospective and historic responsibility for the adverse risks, harms and wrongs arising from the o peration of advanced digital technologies are duly allocated.  
DGI(2019)05 1 5 A study of the implications of advanced digital technologies  ( including AI systems) for the concept of responsibility within a hum an rights framework  b y Karen Yeung  “A great global challenge confronts all those who promote human r ights and the rule of law: how can States, companies and civil society en sure that artificial intelligence technologies reinforce and respect, r ather than undermine and imperil, human rights?”  D avid Kaye, Special Rapporteur on the promotion and protection of the r ights to freedom of opinion and expression, Un ited Nations General Assembly (2018)                                                               W ith contributions from colleagues Ganna Pogrebna and Andrew Howes, and research assistance from Ch arlotte Elves and Helen Ryland, The University of Birmingham.  I am grateful to Imogen Goold for her ad vice concerning the content and contours of Anglo-American tort law. 
Council of Europe Study 1 6 Chapter 1.  Introduction 1 .1 Scope of this study Thi s study examines the implications of ‘new digital technologies and services, including arti ficial intelligence’  for the concept of responsibility from a human rights perspective. It fo cuses on technologies referred to as ‘Artificial Intelligence’  (AI).   AI is notoriously difficult to d efine, and even technical AI researchers do not appear to have settled upon a widely agreed d efinition. For the purposes of this study, the definition of AI proposed within the EU Co mmission Communication on AI will be adopted. 1  It provides that:  Arti ficial Intelligence (AI) refers to systems that display intelligent behaviour by an alysing their environment and taking actions – with some degree of autonomy – to  achieve specific goals.  AI-based systems can be purely software-based, acting in th e virtual world (eg voice assistance, image analysis software, search engines, s peech and face recognition systems) or AI can be embedded in hardware devices (e g. advanced robots, autonomous cars, drones or Internet of Things a pplications)…Many AI technologies require data to improve their performance.  On ce they perform well, they can help improve and automate decision making in th e same domain. A ccordingly, this study uses the term AI to describe a set of advanced general purpose te chnologies that enable machines to do highly complex tasks effectively that draw upon a set o f complementary techniques that have developed from statistics, computer science and co gnitive psychology.2  These technologies aim to reproduce or surpass abilities (in co mputational systems) that would require ‘intelligence’ if humans were to perform them , i ncluding the capacity for learning and adaptation; sensory understanding and interaction; reaso ning and planning; optimisation of procedures and parameters; autonomy; creativity; an d extracting knowledge and predictions from large diverse digital data.3  The scope of this i nquiry is limited to AI technologies that are currently available (at least as initial research and d evelopment demonstrations) or are plausible in the next five years, with a particular focus on te chnologies leveraging machine learning, and it proceeds on the assumption that advances wi ll continue to improve the performance of task-specific AI rather than the achievement of ‘g eneral AI’.4  It is concerned only with the use of AI as a technology, that is, for the purposes o f undertaking useful tasks, rather than as a scientific research tool by academic and other rese archers.5   It  is undeniable that AI technologies have generated extensive benefits, particularly by enh ancing the efficiency, accuracy, timeliness and convenience with which many services are p rovided.  Many such applications can be understood as enhancing the practical reach and e xtending enjoyment of human rights and freedoms. For example, without the use of AI-driven search eng ines, the massive volume of information now available via the internet would not be p ractically useful and accessible, thus enhancing the right to freedom of information ( protected under Art 10 of the European Convention on the Protection of Human Rights and                                                            1   E uropean Commission 2018a.  This definition is elaborated more fully in EU High Level Expert Group on A rtificial Intelligence 2019b. 2   E PSRC; Hall and Pesenti 2017. 3   E PSRC. 4   Bo strom 2014. 5   T he use of machine learning in commercial research and scientific research is not without difficulties.  See fo r example Leonelli 2018; Metcalfe and Crawford 2016. 
DGI(2019)05 1 7 Fundamental Freedoms, hereinafter ‘ ECHR’).  Many national governments and regional o rganisations around the world are devoting considerable resources into developing strategies to  foster innovation and development in AI technologies based on a widely shared belief that these  technologies can and will deliver very significant benefits in terms of enhanced effi ciency, productivity and service delivery.6  Yet early triumphs associated with these ad vanced networked digital technologies that have fuelled the so- called ‘AI boom’ and resul ting ‘AI arms race’7 have been accompanied by rising public anxiety concerning the p otential damaging effects of these technologies for individuals and for society more g enerally.8  These concerns have drawn attention to questions about where responsibility lies fo r these adverse impacts, threats and risks.  For this purpose, the importance of responsibility rest s on the need to ensure, within constitutional democratic orders, that individuals and o rganisations are held to account for the adverse ‘other-regarding’ effects of their actions.9  Acco rdingly, the primary purpose of this study is to examine the implications of advanced d igital technologies (including AI) for the concept of responsibility, particularly in so far as they m ight impede the enjoyment of human rights and fundamental freedoms.   For this purpose, it co nsiders adverse effects, both intended10 and unintended,11 arising from the development an d use of AI that can be understood as bearing directly upon the enjoyment of human rights an d freedoms. However, the indirect adverse effects of AI, including those associated with the ri sks of mass unemployment, and other second- or third-order effects are excluded from sco pe, as are the implications of their use in military applications (including autonomous we apon systems). This is not to suggest that these risks are unimportant, but merely that they rai se particular concerns that are beyond the scope of this inquiry. 1 .2 Structure of this study The  aim of this study is to examine where responsibility should lie for the adverse individual an d societal threats, risks and consequences associated with the actual and anticipated d evelopment and application of advanced digital technologies, particularly as they continue to g row in power and sophistication.  It adopts what might be understood as a ‘human rights p erspective’, in so far as the human rights and fundamental freedoms protected under the ECHR,  can help both to (a) understand the nature of those threats, risks and consequences; (b) h elp identify how responsibility for those threats, risks and consequences should be attributed an d allocated, and (c) consider the kinds of institutional mechanisms that may be needed to ensu re that human rights are effectively protected and that responsibility for the protection of h uman rights is duly allocated.12  To this end, this study draws on concepts and academic                                                            6   T he European Commission has committed “at least €20bn” to AI technologies to be spent by 2020( White 2 018) while the UK has recently committed £1bn: UK Department for Digital, Culture, Media and Sport 2018; U K Department for Business, Energy and Industrial Strategy 2018.  7   S ee Financial Times 2018.  On rivalry between China, US, and EU see European Political Strategy Centre 2018.   8   S ee the literature cited at n.50 below.  9   S ee section 3.1 for a discussion of the concept of responsibility and its importance.  While questions may ari se concerning how responsibility for the positive and otherwise beneficial other-regarding effects of AI s hould be allocated, because the concern of this report is on considering the potential adverse impacts of ad vanced digital technologies on human rights, this study focuses on responsibility for the adverse impacts, th reats and risks associated with these technologies. 10   Intentional attacks on others using AI have been describ ed as the ‘malicious use’ of AI: Brundage et al 2018 .  O ther adverse effects might be intended but not necessarily malicious.  See the examples discussed by S andvig et al 2014.  11   O’Neil 2016. 12   As the Australian Human Rights Commssion has observed, a human rights approa ch provides ‘a more s ubstantive mechanism by which to identify, prevent and mitigate risk’ compared to that of ‘technology 
Council of Europe Study 1 8 scholarship from law, the humanities and the social sciences, including moral, legal and p olitical philosophy and political economy, and from computer science, rather than focusing o n the case law jurisprudence of the European Court of Human Rights.  It proceeds in four cha pters.   C hapter 1 provides a basic outline of how these AI technologies work, before identifying the respo nsibility-relevant attributes or properties which these technologies, and their co ntemporary and near-term applications, possess.     C hapter 2 examines the potential adverse individual and collective consequences that the ap plication of advanced digital technologies may pose. It begins by focusing on the use of data-d riven profiling technologies, highlighting how they may systematically threaten particular ri ghts, as well as threatening more general collective values and interests.  It then considers the  threats and risks posed by other AI technologies and their contemporary and anticipated ap plications.  Chapter Two concludes by drawing attention to the growing power asymmetry b etween those with the capacity and resources to develop and employ AI technologies, and the i ndividuals, groups and populations directly affected by their use.   C hapter 3 then considers where responsibility lies for addressing these potential adverse co nsequences, particularly if they ripen into rights violations and/or harm, including harm to co llective values and interests, including those which might threaten the socio-technical fo undations of democratic freedom in which human rights are anchored.   It considers several l egal ‘models of responsibility’ that might be relied upon to allocate and distribute these risks an d consequences.  It also identifies several challenges associated with seeking to ascribe and assi gn responsibility for the operation of highly complex socio-technical systems, which have ty pically involved multiple organisations, individuals, and interacting software and hardware co mponents.  It then identifies a range of potential mechanisms that might help to address so me of these challenges in order to secure effective and legitimate human rights protection.  C hapter 4 concludes. 1 .3 Understanding the implications of AI for concepts of responsibility In  order to examine the implications of AI for the concept of responsibility from a human rights p erspective, it is necessary to acquire a basic understanding of how these technologies are d eveloped and how they operate. ( a) Machine intelligence and machine learning  M uch of the excitement about the promise and potential of AI to generate advances and i mprovements across a wide range of social domains, including industrial productivity, health, m edicine, environmental management and food security, rely on the power and potential of m achine learning.13  Machine learning is the technology that allows computers to perform speci fic tasks intelligently by learning from examples, data and experience.14  Although                                                                                                                                                                           e thics’ by ‘turning concepts of rights and freedoms into effective policies, practic es and practical realities. In ternational human rights principles embody these fundamental values, and the human rights approach g ives mechanisms and tools to realise them through implementation and accountabilities. ’ Australian H uman Rights Commission 2018:17. 13   Russell and Norvig 2016. 14   Royal Society 2017:16. 
DGI(2019)05 1 9 machine learning techniques have been available for some time, they have experienced major ad vances in recent years due to technological developments, enhanced computing power and the  radical increase in the availability of digital data.  These advances have enabled the d evelopment of machines that can now out-perform humans on specific tasks (such as l anguage processing, analysis, translation as well as image recognition) when, only a few years ag o, they struggled to achieve accurate results.15  These technologies are now ubiquitous in the  everyday lives of those living in highly industrialised, contemporary societies.  In these so cieties, people now regularly interact with machine learning systems that enable digital serv ices (such as, for example, search engines, product recommendation systems and n avigation systems) to provide accurate, efficient responses to user queries in real-time, while co ntinually improving their performance by learning from their mistakes.16     ( b) Responsibility-relevant properties of AI In  order to identify how advanced digital technologies (including AI) challenge our existing l egal, moral and social conceptions of responsibility, it is important to identify the “ responsibility-relevant” attributes or properties which these technologies possess, ie. the p roperties of these technologies that are likely to affect their impact upon others.  Task  automation Fo r this purpose, one of the most important properties of these technologies lies in their cap acity to undertake tasks (many of which formerly required human operators) “ automatically”, that is, without the need for direct human intervention.17   M achine autonomy Ad vances in machine learning techniques have resulted in the development and increasing use o f systems that are not only automated, but they operate in ways that exhibit autonomy. Al though the term ‘autonomy’ is commonly used to describe many AI -enabled applications in p ublic and policy discussion, within the technical community there does not appear to be any wi dely used consensus about what, precisely, this term means, and the preconditions for cha racterising a non- human entity as ‘autonomous’.  However, in the policy literature, the te rm ‘autonomy’ is often used to refer to the functional capacity of computational agents to p erform tasks independently that require the agent to m ake ‘decisions’ about its own behavior wi thout direct input from human operators and without human control.    Computational ag ents of this kind operate by perceiving their environment and adapting their behaviour in respo nse to feedback concerning their own task performance, so that their decisions and acti ons are thought not to be ‘fully deterministic’ at the outset (and therefore not fully p redictable in advance) due to the almost infinite variety of contexts and environments in whi ch these agents might operate.18  So understood, autonomy is a range property which may                                                            15   Royal Society 2017: 16.  For example,  radiologists can be outperformed by image recognition algorithms ( The Economist 2018a) while lawyers can be outperformed by AI in some of their functions (Mangan 2017). 16   For an example of the co-evolution of human behaviours in response to machine-learning driven navigation s ystems, see Girardin and Blat 2010. 17   Liu 2016. 18   European Group on Ethics in Science and New Technologies (EGE) 2018.  The EGE also observes that there s eems to be a push for even higher degrees of automation and ‘autonomy’ in robotics, AI and mechatronics ( a combination of AI and deep learning, data science, sensor technology, IoT, mechanical and electrical eng ineering) yet at the same time they see ‘development toward ever closer interaction between humans an d machines’ noting that well aligned teams of AI systems and human professionals perform better in s ome domains than humans or machines separately. 
Council of Europe Study 2 0 be more or less present in degrees (rather than an all-or-nothing property), depending upon the  extent to which human oversight and intervention is required for the operation of the sy stem.19   So me machine learning systems are distinguished by their capability to learn and change over ti me, dynamically setting their own sub-goals, and their ability to adapt to local conditions via ex ternal sensor information or updated input data.20  The individual designers of the system m ay decide and set its  initial state and parameters, including the overarching goal that it is i ntended to optimise, but once deployed, the operation and outputs of the system will evolve wi th use in different environments.   In particular, these computational systems are intended to  operate in ways that allow the system to make independent decisions that choose between al ternatives in ways that are not pre-programmed in advance, and to do so without any human i ntervention.  Current AI systems cannot determine the overarching goal which the system is d esigned to optimise (which must be specified by the systems’ human developers) but they are c apable of determining their own intermediate sub-purposes or goals.                                                              19    The range of levels of control or involvement that human operators can have in a system has been d escribed by The Royal Academy of Engineering into four different grades of control: (a) controlled systems: w here humans have full or partial control, such as an ordinary car (b) supervised systems: which do what an o perator has instructed, such as a programmed lathe or other industrial machinery (c) automatic systems: th at carry out fixed functions without the intervention of an operator, such as an elevator, and (d) au tonomous systems that are adaptive, learn, and can make ‘decisions’: Royal Academy of Engineering  2009:  2.  The SAE International has developed standard J3016_201806: Taxonomy and Definitions for Terms R elated to On-Road Motor Vehicle Automated Driving Systems (SAE International 2018) which has been u sed, for example, by the US Department of Transportation as part of its Federal Automated Vehicles Policy: US  Department of Transportation 2017. 20   Michalski et al (2013). Box 1: Machine autonomy and sensitivity to context Co ntrast a self-driving vacuum cleaner with a self-driving car  Fundamentally the same technical architecture applies: their overarching purpose i s set by the system’s human designers but both machine agents are capable of d etermining their own sub-goals in order to achieve that purpose  The behaviour of both kinds of machine agents cannot be fully determined at the o utset  Each is capable of perceiving their environment and adapting decisions and acti ons accordingly  Yet these machines are expected to operate in highly contrasting contexts (home env ironments are relatively contained and stable in contrast to the dynamism and co mplexity of on-road conditions) Acco rdingly, the greater the stability and predictability of the environment or context i n which these systems operate, the more foreseeable their possible outputs and respo nses.  Hence the anticipated behaviour of the self-driving vacuum cleaner is l ikely to be easier to foresee and anticipate when compared to that of the self-driving car.  
DGI(2019)05 2 1 For the purposes of identifying where responsibility lies for the outputs and the consequences o f these systems, of particular importance is their stability and predictability  (see Box 1).  Because  these systems learn dynamically and iteratively from their environment (which is itself o ften volatile and continuously changing) this means that these technologies, and their o utputs, have the potential to evolve in unexpected ways. This means that, in practice, these te chnologies are sometimes characterised by their opacity and the unpredictability of their o utputs (discussed below), which may have direct implications for whether, and in what ways, the  concept of responsibility can be applied to their decisions, actions and the resulting co nsequences.   In  addition to their capacity to operate without direct human oversight and control, these te chnologies have a number of other responsibility-relevant characteristics, including their: a.  Inscrutability and opacity: Concerns about the opacity of these technologies21 can be u nderstood in three distinct but related senses.22  First, unlike early forms of AI, including so -called ‘expert systems’ which relied on rule -based ‘if-then’ reasoning, contemporary m achine learning systems create and utilise more complex models which can make it d ifficult to trace their underlying logic in order to identify why and how they generated a p articular output.   While some forms of learning systems enable the underlying logic to be traced  and understood (for example, those which utilise decision-trees), others (including tho se that utilise neural networks and back propagation) do not.23  Secondly, even for sy stems that utilise algorithms whose underlying operation and logic can be understood an d explained in human terms, those that have been developed by commercial providers m ay not be openly available for scrutiny because they are the subject of intellectual p roperty rights, entitling the owner of those rights to maintain the secrecy of their al gorithms.24  Thirdly, even if information about a system is provided (such as the te chnique used to train the machine learning algorithm, or the formal rules of a rule-based co mputational system), those who lack technical expertise will not be able to understand o r meaningfully comprehend this information, effectively reducing the practical tran sparency of the system.25  The combined effect of the inscrutability and opacity of al gorithms results in their cha racterisation as ‘black boxes’ ,26 and these properties have d irect implications for the transparency, explainability and accountability of and for the ap plications that utilise them.27b.  Complexity and dynamism : Technological applications that utilise AI for specific social p urposes can be understood as highly complex socio-technical systems, in that both the u nderlying mechanisms through which they work, and their dynamic and continual i nteraction with the environments in which they operate, are complex in their operational l ogic, generating outcomes that are often difficult to predict, particularly for those em ploying machine learning algorithms.28  This means that understanding and anticipating                                                            21   See Wagner 2017: 36-37. 22   Burrell 2016.  23   A growing body of technical research on ‘explainable AI’ has emerged, seeking to identify methods through w hich these systems might be rendered intelligible to humans.  This form of opacity recognises human l imitations in fully comprehending or explaining the operation of complex systems, because they reason d ifferently to machines: Zalnieruite 2019. See below at section 3.7.1. 24   See for example State vs Loomis 881 N.W. 2d 749 (Wis. 2016).  Noto La Diega (2018). 25   Burrell 2016: 4.  26   Pasquale 2015. 27   Burrell 2016; Datta et al 2016.  Weller 2017; Yeung and Weller 2019.   28   Schut and Wooldridge 2000. 
Council of Europe Study 2 2 how they function in real world contexts can be extremely challenging, even for those with the re levant technical expertise, typically requiring expertise from multiple domains. c.  Human input, interaction and discretion: Although advances in AI are strongly associated wi th the so-called ‘rise of the machines,’ it is important to recognise that humans are i nvolved at every stage of the development and implementation of AI-driven technologies: fro m the origination of ideas and proposal for development, design, modelling, data-g athering and analysis, testing, implementation, operation and evaluation.29  In addition, these  systems are also often expected to operate in real world environments in which the sy stems are designed dynamically to interact with humans, and in many cases are i ntended to do so at scale (eg Facebook’s News Feed system).  In particular, many ap plications that utilise AI are designed formally to preserve human discretion, so that the sy stem’s output is offered to the user as a ‘recommendation’ rather than executing some p re-specified automated decision or function.30 Thus, for example, digital product reco mmendation engines offer product suggestions to users, but the human user retains fo rmal decision-making authority in deciding whether or not to act upon the reco mmendation and this may have significant implications for the concept of respo nsibility.31  d.  A general purpose technology:  AI technologies can be understood as ‘general purpose’, in that  they can conceivably be applied to an almost limitless range of social domains.  This v ersatility means that AI technologies can be characterised as classic ‘dual use’ te chnologies, in that the motivations for their application may range from benevolent, to sel f-interested through to malevolent.32   e.  Global interconnectivity, ubiquity and scalability:  It is important to recognise that the g lobal interconnectivity and reach of the internet (and internet-connected technologies) h ave enabled the swift roll-out of AI technologies on a massive scale, particularly with the rap id and widespread take- up of ‘smart’ networked devices .  Many AI applications used d aily by individuals in the industrialised world have now become ubiquitous.  Given the effi ciency and convenience which they offer in managing the routine tasks involved in co ntemporary life, this means that, in practice, it is rapidly becoming impossible to co nceive of modern living without them.33   Yet the reach and penetration of networked d ata infrastructure, and the take-up of smart connected devices into the global south, rem ains poor and limited compared with the global north, so that those living in these areas  do not have equivalent access to the services and improvements in efficiency and co nvenience that are available to those living in wealthier, highly industrialised states.34   f.  Real-time automated and continuous operation:  The efficiency and convenience which m any AI applications offer can be attributed, in no small measure, to their ability to o perate automatically and in real-time.35  Thus, for example, AI-enabled navigation sy stems can offer invaluable guidance to individuals as they seek to find their way to a d estination which is entirely foreign to them by providing real-time guidance concerning                                                            29   Bryson and Theodorou 2018 . 30   Su and Taghi 2009. 31   See discussion of ‘humans in the loop’ at section 3.5.2 below.  32   On the self-interested design of algorithmic systems, see the discussion of the SABRE airline reservation s ystem in Sandvig et al 2014. On malevolent applications of AI, see Brundage et al (2018). 33   Zuboff 2015; Royal Society 2017. 34   McSherry 2018. 35   For examples of real-time AI applications, see Narula (2018). 
DGI(2019)05 2 3 which direction to take and, at the same time, can advise on the anticipated journey time o f alternative route options.36  These applications are possible because of the capacity of AI  technologies to collect digital data from sensors embedded into and collected from i nternet-enabled devices, enabling them to track the activities and movements of i ndividuals at a highly granular lev el, and often without the individual’s awareness.  These te chnological capacities have direct implications for concepts of responsibility in ways that m ay affect the enjoyment of human rights and freedoms in at least three ways.  Firstly, the n etworked nature of many of these technologies which the internet (and internet co nnected technologies) have made possible means that they can operate at scale and in real  time.  As a result, there may be considerable distance in both time and space between the  design and implementation of these systems, and the point at which their decisions an d consequences arise  and are directly and immediately felt.  Secondly, this capacity to o perate in real-time and at scale generates very significant challenges for their supervision an d oversight, discussed more fully below.  Thirdly, in order to provide highly personalised ad vice that is contextualised against wider population trends (eg traffic congestion) in real-ti me, this necessitates the continuous surveillance of individuals at a population wide-l evel, entailing constant personal data collection and processing, which necessarily i mplicates the human rights to, and collective value of, privacy and data protection.37  g.  Reliance on large data-sets :  While the model upon which a computational algorithm is b ased will determine its operation, machine learning systems rely critically on the u nderlying data-sets for their accuracy and operation.38  Without access to relevant data set s, machine learning algorithms are but hollow shells.  Accordingly, the availability, size, an d quality of the underlying datasets upon which algorithms are trained, tested and v alidated plays a critical role in their performance and the accuracy and legitimacy of their o utputs, as does the availability and quality of the data which these systems rely upon d uring their operation.   h.  Capacity to generate insight from merging data sets: Much of the excitement surrounding AI  technologies arises from their capacity to generate new insight from merged datasets whi ch can then be used to predict and inform decision-making.  In particular, a data set m ight contain fairly mundane, innocuous data about individuals. But when multiple such d ata sets are merged and mined, this may generate insight that can enable quite intimate p ersonal information to be inferred at a very high level of accuracy.39  Accordingly, issues co ncerning how to govern the collection and processing of digital data have far-reaching i mplications for human rights and for the concept of responsibility which, given the ease an d almost negligible cost associated with transferring and copying digital data and the co mplexity of the contemporary global data eco-system, have become especially cha llenging and important.  i.  Capacity to imitate human traits : In recent years, the ability of AI technologies to imitate h uman traits, including voice simulation, visual representations of human behaviour and ro bots capable of interacting with humans with apparent emotional sensitivity, has b ecome so high quality that it may be extremely difficult for ordinary humans to detect that  those traits are artificially generated. This has provoked concern about their capacity                                                            36   Swan 2015. 37   See discussion at Section 2 below.   38   Kitchin 2014.  Prainsack 2019. 39   Kosinski et al 2013. 
Council of Europe Study 2 4 to deceive humans (particularly the production of so- called ‘deep fakes’) and harnessed fo r unethical or other malicious purposes.40j.  Greater software complexity: Machine learning and deep learning systems become p rogressively complex, not only due to the availability of data, but also due to increased p rogramming complexity. As a result, these systems are subject to three types of v ulnerability: first, increased programming complexity increases the propensity of these sy stems to generate stochastic components (i.e. make mistakes)41; secondly, this co mplexity opens the door to a wide range of adversarial attacks;42 and thirdly, the u npredictability of their outputs can generate unintended yet highly consequential ad verse third party effects (‘externalities’) .  k.  Capacity to ‘personalise’ and configure individual choice environments: One way in whi ch AI systems have contributed to the achievement of greater efficiency and precision acro ss a wide range of processes and operations has been through the ‘personalisation’ of serv ice provision. For example, the use of profiling techniques enables digital retailers (suc h as Amazon) to provide ‘personalised’ product recommendations to each customer, b ased on data-driven predictions (gleaned from the continuous collection and analysis of that  customer’s digital traces when analysed in conjunction with those of other custo mers).43  While the personalisation of digital services and offers benefits to users in redu cing the volume of irrelevant offers and services which they receive, it has the effect o f segmenting individual users from each other, such that one user sees only his or her p ersonalised informational environment, which may be very different from that seen by o ther users.  When AI driven personalisation takes place routinely and at scale, this risks fo stering social fragmentation44 and eroding social cohesion and solidarity.45  l.  Capacity to redistribute risks, benefits and burdens among and between individuals and g roups via the use of AI-driven optimisation systems which reconfigure social e nvironments and choice architectures:   AI systems can operate in real time and at a scal e via the internet’s global networked architecture.  As a result, these systems can be co nfigured to operate in a manner designed to optimise the over-arching goal prespecified b y its human developers at a scale that was previously impossible in a pre-internet enab led age.46  The capacity to harness AI systems to personalise the informational choice env ironments of each individual user is particularly powerful when configured to operate at  scale.  It enables the design and deployment of AI content-distribution systems aimed at i nfluencing and directing the behaviour of an entire population of users, rather than one i solated user, in accordance with the developer’s chosen optimisation function,  these sy stems inevitably prioritise certain values over others, and will do so in ways that co nfigure and shape social and informational environments that may be beneficial for so me individuals and groups, while detrimental to others.  For example, the optimisation                                                            40   The Economist 2017.  Chesney and Citron 2019.  See discussion at Section 2.2.2 below. 41    Recent research in image recognition demonstrated the lack of ability of technology to distinguish noisy i nformational inputs: chihuahua dogs pictures were mixed with muffin pictures and the AI algorithm could n ot tell them apart: Yao 2017.  42    Current AI technologies can be easily and successfully attacked by cybercriminals who can use AI system vu lnerabilities for their own benefit.  Cybercriminals can falsify voice recognition and CAPTCHA systems to b reak into personal and business accounts: Polyakov 2018.  43   Yeung 2016. 44   Pariser 2012. 45   Yeung 2018a. 46   Yeung 2016. 
DGI(2019)05 2 5 function of AI-driven navigation systems might be to enable each user to find the fastest p ossible route to her desired destination, given the volume and location of traffic p revailing at the user’s time of travel .   The routes identified by the system and reco mmended to users will, when aggregated, have distributional effects: residents in areas  in which traffic is routed being confronted with greater noise levels, vehicle em issions and congestion, while these effects will not be experienced by residents in areas where  traffic is not routed.  Accordingly, these optimisation systems raise questions about acco untability and responsibility for their resulting distributional outcomes, particularly g iven that there is typically no consultation input or deliberation from affected individuals, g roups and populations concerning the distribution of risks and benefits arising from their o peration.47     m.  The capacity to generate problems of collective action: The capacity of AI optimisation sy stems to operate in a highly targeted manner which is personalised to individual users, an d to do so at scale across an entire population of users, means that these systems can o perate in ways that may have a relatively minor effect at the individual level, whilst h aving a serious and significant impact at the collective and/or societal level.  It is not d ifficult to imagine circumstances in which the operation of AI optimisation systems may theref ore generate a ‘collective action’ problem .  Collective action problems arise when all i ndividuals would be better off cooperating with other users, but each individual user fails to  take action because the impact on each individual is too small to justify the effort and reso urces associated with so doing.48  Consider, for example, the problem of political m icro-targeting and the provision of misleading, inaccurate or dubious political i nformation to individual voters with the intention of encouraging them to vote for a p articular candidate.   Even if a particular individual is misled into voting for a candidate that  she might not otherwise have supported, she is in practice unlikely to be sufficiently m otivated to initiate a complaint or other legal proceedings against those responsible for i ts dissemination.  Yet because these effect are felt at the population/collective level, they m ay pose real and potentially serious threats to the integrity of democratic elections, and t o democratic processes more generally.49  In other words, one of the distinctive and novel cha llenges which AI systems now pose arises from their capacity to operate in a highly targ eted and personalised manner, yet in real-time and at a population-wide scale, which co uld pose serious societal threats but for which the motivation for any individual to try an d counter these threats may be extremely weak. 1 .4 Implications for the concept of responsibility from a human rights perspective   The  importance of understanding the human rights dimensions of AI is reflected in the various i nquiries and reports commissioned and produced by a growing number of civil society o rganisations and is increasingly the focus of academic scholarship concerned with the ‘ethics o f AI’50.  This includes the work of the Council of Europe, including its study on the human                                                            47   Yeung 2017a. 48   Olsen 1965 49   UK Information Commissioner’s Office 2018 .  UK House of Commons Digital Culture Media and Sports Co mmittee 2019. 50 See for example Amnesty International 2017; Access Now 2018; Australian Human Rights Commission 2018; Cath  2017; Hildebrandt 2015; Executive Office of the President 2016;  The Montreal Declaration for R esponsible AI 2017; The Toronto Declaration: Protecting the Rights to Equality and Non-Discrimination in Mac hine Learning Systems 2018; Latonero 2019; Mantelero 2018; Raso et al 2018; Risse 2018; Rouvroy 2016;  UN General Assembly 2018; Mantalero 2019; Nuffield Foundation and Leverhulme Centre for the F uture of Intelligence 2019;  EU High Level Expert Group on AI 2019a. 
Council of Europe Study 2 6 rights dimensions of automated data processing techniques and possible regulatory i mplications,  prepared by the Committee of Experts on internet intermediaries (MSI-NET) (hereaft er the ‘Wagner Study’ ).51  The Wagner Study identifies examples of algorithmic d ecision-making systems currently in use that may violate or undermine the enjoyment of ‘the m ost obviously implicated rights that are to a stronger or lesser degree already in public d iscussion’,52 including rights to: •  a fair trial and due process (Art 6)53; •  privacy and data protection (Art 8)54;  •  freedom of expression (Art 10);  •  freedom of association (Art 11)55;  •  an effective remedy (Art 13)56•  the prohibition on discrimination (Art 14)57  and •  the right to free elections (Art 3, Protocol 1)58  Ye t, as the Report commissioned by the Parliamentary Assembly of the Council of Europe (P ACE) undertaken by the Rathenau Instituut concluded: De spite [the]…wide-ranging impact of digital technologies on human rights, so far l ittle attention has been paid to this crucial topic and there has been scarcely any f undamental political and public debate on it.  As a result, a serious erosion of h uman rights is taking place.  Therefore, the human rights debate, which is s eriously lagging behind the fast-growing technological developments, needs to be s trengthened rapidly.59T he present study builds on the Wagner Study by critically examining how advanced digital te chnologies may implicate the concept of responsibility.  Chapter 2 begins by identifying and ex amining the adverse individual and societal risks posed by AI.  It adopts a ‘human rights p erspective’ by focusing on how these technologies may undermine the practical capacity to                                                            51   The Wagner Study focused primarily on the implications for human rights of algorithmic decision-making s ystems that affect the public at large, identifying various human rights concerns triggered by the increasing r ole of algorithms in decision-making, observing that these concerns are bound to expand and grow as al gorithms, automated data processing techniques and related systems become increasingly complex and i nteract in ways that become ‘progressively impenetrable to the human mind’ : Wagner Study 2017: 5.   52   Wagner Study 2017: 32. 53   See Section 2.1.1(a). 54   See Section 2.1.1(b). 55   Although the internet and social networking sights have enhanced the capacity for individuals to exercise th eir Art 11 ECHR rights to freedom of association, there are concerns that the automated sorting and p rofiling of protested on-line may erode these rights: Wagner Study 2017:  23-24. 56   Art 13 ECHR requires that states ensure that individuals have access to judicial or other procedures that can i mpartially decide on their claims concerning violations of human rights, including on-line violations, i ncluding effective non-judicial mechanisms, and to ensure that private sector actors respect those rights by e stablishing effective complaint mechanisms that promptly remedy the grievances of individuals.  Yet the o pacity of automated decision-making processes may impede the ability of individuals to obtain an effective re medy and the increasing use of automated decision- mechanisms for complaints handling raises ‘serious c oncerns’ about whether such mechanisms can be regarded as offering an effective remedy : Wagner Study 2017: 24.  57   See Section 2.1.1 (d). 58 Art 3 of Protocol 1 ECHR requires states to support the individual right to free expression by holding free e lections at reasonable intervals. These elections must enable you to vote in secret.  However the rise of s ocial media and the use of automated content recommendation systems may be used for the purposes of p olitical manipulation and could threaten the right to free elections: Wagner Study 2017: 30-32. 59   Van Est and Gerritsen 2017: 46. 
DGI(2019)05 2 7 exercise particular human rights and freedoms on a systematic basis in an era pervaded by ad vanced AI technologies, rather than engaging in detailed analysis of particular AI ap plications that may adversely impact specific human rights and fundamental freedoms.  Two d imensions of these systematic impacts are considered: firstly, the threats to a set of rights p osed by algorithmic decision-making systems.60  Secondly, the wider adverse collective social i mpacts of AI technologies (including but not limited to those incorporated into algorithmic d ecision-making systems), only some of which can be readily expressed in the language of ex isting human rights discourse.  Over time, these wider adverse effects could systematically threat en the socio-technical foundations which the very notion of human rights presupposes an d in which they are rooted.                                                             60   A number of these rights are examined in the Wagner Study 2017. 
Council of Europe Study 2 8 Chapter 2. Threats, risks, harms and wrongs associated with ad vanced digital technologies  M any commentators claim that advances in networked digital technologies, including those cur rently referred to as AI technologies, are powering the emergence of a ‘New Industrial Rev olution’ that will provoke far -reaching changes across every aspect of social life, of a m agnitude and scale that will be as disruptive and unsettling as those wrought by the original In dustrial Revolution.61 Before examining the potential threats and risks associated with these em erging technologies, it is helpful briefly to highlight the broader social-political and eco nomic context which affect and condition their development, implementation and ad option, and the broader historic context and experience of modern scientific and te chnological innovation.   To  this end, there may be parallels between the larger societal effects of the original industrial rev olution and the anticipated effects of the ‘New’ Industrial Revolution that is now dawning.  Fo r example, while the 19th century Industrial Revolution brought about myriad benefits to b oth individuals and society, and can be credited with very substantial and widespread i mprovements to living standards and individual and collective well-being, it generated u nintended adverse effects.  These include both direct adverse effects on human health and safety  associated with early forms of industrial production, and the burning of fossil fuels to p ower industrial activity which has led to a serious climate change problem at a global scale, an d which we have not yet adequately addressed or resolved.  Yet the adverse effects on cl imate change arising from the technologies that provoked the original Industrial Revolution d id not become apparent until over a century later, by which time it was too late to address an d reverse them effectively.  Contemporary societies might now face a similar dilemma.  One o f the difficulties in seeking to identify and anticipate the larger adverse societal effects of te chnological innovation arises not only from difficulties in predicting their likely applications an d take up, but especially from difficulties in anticipating their aggregate, cumulative effects o ver time and space.     2 .1 The rise of algorithmic decision-making (ADM) systems  Co mputational systems that utilise machine learning algorithms, combined with the rapid and wi despread take-up of ‘smart’ devices,  have fuelled the emergence of algorithmic decision-m aking systems which seek to harness (and frequently to monetise) the digital data which can n ow be gleaned by systematically tracking and collecting th e digital traces left from individuals’ o n-line behaviours, and utilising advanced digital technologies (including AI) in order to p roduce new knowledge that can be used to inform real-world decisions. Many of these sy stems rely upon data-driven profiling techniques that entail the systematic and bulk co llection of data from a population of individuals in order to identify patterns and thereby p redict preferences, interests and behaviours of individuals and groups, often with very high d egrees of accuracy.  These data profiles can then be used to sort individuals to identify ‘can didates of interest’ with the aim of producing ‘actionable insight’ –  that is, insight that can b e used to inform and automate decision-making about individuals by those undertaking the p rofiling (or their clients).62  These systems are widely used by retailers seeking to target p roducts to individuals identified as most profitable and most likely to be interested in them,63                                                           61   boyd and Crawford 2013.  Skilton and Hovsepian 2017. 62   Mayer-Schonenberg and Cukier 2013. 63   Draper and Turrow 2017; Gandy 1993. 
DGI(2019)05 2 9 by political actors and organisations seeking to tailor and target campaign messages to i ndividuals who are identified as most likely to be persuaded by them,64 and, increasingly, by cri minal justice authorities who seek to assess the ‘risk’ which particular individuals are al gorithmically identified as posing to public safety in order to make custody decisions about i ndividuals (whether criminal suspects or those convicted of criminal offences).65    It  is in this socio-economic context that public anxieties have emerged concerning the societal e ffects of advanced digital technologies (including AI), particularly given the increasing use of d ata-driven profiling.  Recent attention has focused on the way in which social media and o ther content distribution platforms utilize profiling technologies in ways that have profound i mplications for the Article 10 right to freedom of expression and information, particularly fo llowing the Cambridge Analytica scandal in which it is alleged that millions of profiles of Fa cebook users were illegally collected to microtarget individuals with political messages with the  aim of swaying voter behavior.66  The following discussion, however, is concerned primarily wi th the way in which data-driven algorithmic decision-making systems more generally may sy stematically threaten particular human rights, rather than focusing on their application to speci fic domains of activity.   2 .1.1 How do ADM systems systematically threaten particular rights? The  use of algorithmic decision-making systems may systematically threaten several rights in cluding: ( a) The right to a fair trial and rights of ‘due process’ : Art 6.   M any ADM systems utilise data driven profiling techniques to create digital profiles of  i ndividuals and groups across a wide range of contexts, sifting and sorting individuals into cate gories in order to assist decision-making.  When used to automate and inform decision-m aking that substantially affect the rights and significant interests of individuals, data-driven p rofiling may have serious consequences.  For the affected individual, the opportunity to p articipate in, contest or otherwise challenge the outcome of the decision and/or the u nderlying reasoning upon which that decision was based, or the quality or integrity of the d ata that was used to inform the decision, are in practice, almost non-existent.67  While the ri ght to a fair hearing (per Article 6) encompasses a series of more specific procedural rights,68these  include a person’s right to know the reasons for decisions which adversely and si gnificantly affect that individual, yet the ADM systems used to inform decision-making may n ot be configured to, nor capable of, produce meaningful explanations in terms that are i ntelligible to the affected individual, or even (in the case of neural networks that rely on back p ropagation) in terms that are intelligible to the algorithm developers.69  These concerns are ex acerbated by the opacity of these systems which can arise from their technical complexity, d ifficulties in assessing the quality and provenance of the underlying training data that was u sed to train the decision-making model,70 or because the algorithm enjoys intellectual p roperty protection as a trade secret and therefore need not be publicly disclosed,71 a stance                                                            64   Gorton 2016. 65   Oswald et al 2018; Ferguson 2016.  66   UK House of Commons, Digital Culture Media and Sport 2019. 67   Hildebrandt 2015; Hildebrandt and Gutwirth 2008. 68   Galligan 1997. 69   Weller 2017; Matthias 2004; Burrell 2016. 70   Lohr et al 2019.  71   Pasquale 2015. 
Council of Europe Study 3 0 which organisations utilising these systems typically defend on the basis of that it prevents u sers from ‘gaming’ the s ystem.72   Accordingly, these systems risk interfering with rights to d ue process protected under Article 6 (including the presumption of innocence), particularly in ci rcumstances where the consequences for the affected individual are serious and life-li miting.73   Particularly worrying is the increasing use of AI systems in criminal justice contexts to  inform custodial and sentencing decisions, primarily in the USA, although they are being take n up elsewhere (including the UK).74  Yet, as Hildebrandt has observed, we have become resi stant to the notion that the outcomes of an AI tool might be incorrect, incomplete or even i rrelevant with regard to potential suspects.75   ( b) The right to freedom of expression: Art 10  The  operation of algorithmic profiling may significantly affect the Art 10 right to freedom of ex pression, which includes the right to receive and impart information, given the powerful i nfluence which global digital platforms now exert over our informational environment at both an  individual and societal level. For example, automated search engines act as crucial g atekeepers for human beings who wish to seek, receive or impart information, as content whi ch is not indexed or ranked highly is less likely to reach a large audience or to be seen at all.  Ye t search algorithms are intentionally designed to serve their owner’s commercial interests, an d are therefore inevitably biased towards certain types of content or content providers. It is ty pically automated algorithms, rather than humans, that decide how to handle, prioritise, d istribute and delete third-party content on online platforms, including content handling d uring political and electoral campaigns.   These practices not only implicate the individual ri ght to freedom of expression, but also Article 10 ’s inherent aim of creating an enabling env ironment for pluralist public debate that is equally accessible and inclusive to all.76    In  addition, online platforms are increasingly under pressure to actively counter online hate speech  through automated techniques that detect and delete illegal content, particularly fo llowing the live video streaming via social media platforms of the attack on civilians by a lone te rrorist in Christchurch in early 2019. Article 10.2 provides that any interferences with free ex pression, which would therefore include algorithmic systems that block access to content thro ugh filtering or removal, must be prescribed by law, pursue a specified legitimate purpose o utlined in Art 10.2, and necessary in a democratic society.77   Accordingly, the widespread use o f algorithms for content filtering and content removal processes, including on social media p latforms also raises rule of law concerns, raising questions of legality, legitimacy and                                                            72   Bennett-Moses and de Koker 2017. 73   Davidow 2016.   74   These applications not only implicate the rights under Article 6, but also the Article 5 right to liberty and s ecurity of the person, and the non-discrimination principle protected by Article 14.  75   Hildebrandt 2016.  Hildebrandt argues that t he Art 6 ‘equality of arms’ principle should be re -invented the mo ment that the public prosecutor, judge or lawyer is unable to check on how the police’s AI agent reached i ts conclusions, and that these AI agents should be required to log their activity and outputs, purposes, and h ow they reached the outcome to enable proper review .  The Rathenau Institut endorses Hildebrandt’s vi ews, and has suggested that the Council of Europe consider establishing a framework of minimum norms to  be taken into account wh en a ‘court’ (interpreted for this purpose as including all decision -making au thorities within the legal system, particularly those involved in making custody decisions concerning i ndividuals within the criminal justice system) uses AI – helping to prevent member states from devising th eir own individual frameworks which is likely to result in uneven and varying degrees of protection under A rt 6 ECHR provided by individual member states: Van Est and Gerritsen (2017) 42-43. 76   See UN General Assembly 2018. 77 In line with the jurisprudence of the European Court of Human Rights, any restriction of the freedom of ex pression must correspond to a ‘pressing social need’ and be proportionate to the legitimate aim(s) pur sued.  See Yildirim v. Turkey, 18 March 2013, No 3111/10. 
DGI(2019)05 3 1 proportionality, particularly given that that online platforms often face an unclear legislative frame work that encourages them to remove content voluntarily, without a clear legal basis. Whi le their intentions are welcome, there is a lack of transparency and accountability co ncerning the process or about the criteria adopted to establish which content is ‘extremist’ o r ‘clearly illegal’.78  These arrangements create the risk of excessive interference with the ri ght to freedom of expression, and can be understood as ‘handing off’ law enforcement respo nsibilities from states to private enterprises.  National legal regimes which require digital i ntermediaries to restrict access to content based on vague notions such as ‘extremism’ oblige them  to monitor all on-line communication in order to detect illegal content, thereby violating the  established principle that intermediaries should not be obliged to conduct general m onitoring because of their potential ‘ chilling effects’ on freedom of expression.79   In ad dition, process related concerns arise due to the capacity of platforms to decide for them selves what constitutes ‘extremist’ content and therefore subject to removal: the tools an d measures through which identification and removal decisions are made effectively rest wi th private providers and, unless those measures are not subject to meaningful and effective stat e oversight, risk exceeding legally and constitutionally prescribed boundaries, thereby co ntravening the rule of law.80  Whi le the imperative of acting decisively against the spread of hate messages and the i ncitement to racially-motivated offences is indisputable, such practices raise considerable co ncerns related to the legality of interferences with freedom of expression. Extremist content o r material inciting violence is often difficult to identify, even for a trained human, due to the co mplexity of disentangling factors such as cultural context and humor. Algorithms are not cur rently capable of detecting irony or critical analysis. The filtering of speech to eliminate h armful content through algorithms therefore faces a high risk of over-blocking and removing speech  that is not only harmless but might contribute positively to the public debate. On the o ther hand, the capacity of media content platforms to disseminate messages in real time and at  a global scale substantially magnifies the reach, scope and thus the impact of harmful speech .  The turn to automated approaches to on-line content filtering highlights the acute respo nsibility challenges which the increasing reliance on algorithmic systems in contemporary l ife generates: while they offer the benefits of scale, speed and efficiency relative to human d ecision-making, digital platforms claim that human oversight is necessarily inadequate, g enerating a ‘respons ibility gap’ which they typically argue they cannot fairly be expected to f ill.81                                                           78    See Menn and Volz 2017 79    This principle is enshrined in EU-law and in relevant Council of Europe policy guidelines, including the recent  Co uncil of Europe CM/Rec(2018)2.  See also UN General Assembly (2018).  Several states have introduced l aws or initiated law reform initiatives to address the spread of harmful on-line content.  For example, G ermany adopted its Network Enforcement Act (‘NetzDG’) in 2017. This law requires  online platforms with mo re than two million registered users in Germany to remove ‘manifestly unlawful’ content, which c ontravenes specific elements of the German criminal code, such as holocaust denial and hate speech, w ithin 24 hours of receiving a notification or complaint, and to remove all other ‘unlawful’ content w ithin s even days of notification. Non- compliance risks a fine of up to €50 million. This law also seeks to  increase p latform responsibility through imposing greater transparency and significant reporting obligations.  The l aw has been subject to significant criticism on the basis of its restrictive implications for freedom of e xpression.  Eg Access Now 2018: 22.  The UK has recently issued its Online Harms White Paper , which i ntroduces a legal duty of care to make companies take more responsibility for the safety of their users and tac kle harm caused by content or activity on their services, to be enforced by an independent regulator:  UK G overnment 2019. 80   See Wagner Study 2017, 19. 81   See discussion of the so- called ‘control’ problem at Section 3.2.2 below.   
Council of Europe Study 3 2 (c) Right to privacy and data protection: Article 8 T he Article 8 right to respect for private and family life and rights to data protection are being p laced under unprecedented strain due to the ability of algorithms to facilitate the collection an d repurposing of vast amounts of data, including personal data gleaned from digital o bservation of individual users which may generate further data, with entirely unpredictable resul ts for the data subject.82  As the Wagner Study observed, the use of personal data for the p urposes of individual profiling, and its subsequent repurposing, threatens a person’s right to ‘i nformational self-determination’83 particularly given that (as noted in section 2.1) even fairly m undane, innocuous data collected from the digital traces of individuals may be merged with o ther data sets and mined in ways that can generate insight that can enable quite intimate p ersonal information to be inferred at a very high level of accuracy.84  While contemporary d ata protection regimes (including Conv. 108 as modernised) are an important safeguard, co nferring a set of  ‘data protection rights’85 on data subjects, aimed at protecting them from u nnecessary and unlawful data collection and processing, they might not provide co mprehensive and practically effective guarantees against the use of intrusive profiling ap plications.  ( d) The prohibition of discrimination in the enjoyment of rights and freedoms: Art 14 The  potential for bias and discrimination arising from the use of machine learning (ML) te chniques has attracted considerable attention, from both policy-makers and AI researchers al ike.   Concerns about unfair or unlawful treatment directly implicate Article 14 ECHR which p rovides that the enjoyment of the rights and freedoms set out in the Convention shall be ‘sec ured without discrimination on any grounds such as sex, race, colour, language, religion, p olitical or other opinion, national or social origin, association with a national minority, p roperty, birth or other status’.86  There are many opportunities for bias to inadvertently affect the  outputs produced by the use of machine learning techniques, arising from biases of the al gorithms’ developers, bias built into the model upon which the systems are generated, b iases inherent in the data sets used to train the models, or biases introduced when such sy stems are implemented in real-world settings.87  Not only might biased ML systems lead to d iscrimination and generate erroneous decisions, but this can entail significant wrongdoing, resul ting in decisions that are systematically biased against groups that have historically been so cially disadvantaged (and against individuals who are members of those groups), thereby rei nforcing and compounding discrimination and structural disadvantage, even though these effec ts were not intended by the system’s designers .88  These concerns have been particularly                                                            82   See, for example, tension between competition in on-line services and consumer privacy: Oxera 2018. 83   Wagner Study 2017:14. 84   Kosminski et al 2015. 85   The new rights introduced by the recently modernised Conv 108 include: the right not to be subjected to a d ecision significantly affecting him or her based solely on an automated processing of data without having hi s or her views taken into consideration, the right to obtain knowledge of the reasoning underlying data p rocessing where the results of such processing are applied to him or her, and the right to object at any ti me, on grounds relating to his or her situation, and to the processing of personal data concerning him or her , unless the controller demonstrates legitimate grounds for processing which override his or her interest o r rights in fundamental freedoms: Article 5 of the Modernised Convention 108. 86   Protocol No 12 ECHR Article 1 provides that ‘the enjoyment of any right set forth by law shall be secured w ithout discrimination on any ground such as sex, race, colour, language, religion, political or other opinion, n ational or social origin, association with a national minority, property, birth o r other status.’ See also Art 21 CF EU.   87   Veale and Binns 2017. 88    Barocas and Selbst 2016;  Wagner Study 2017: 27-28. 
DGI(2019)05 3 3 acute in relation to the use of machine learning techniques to inform custody and sentencing d ecisions within the US criminal justice system, due to allegations that such techniques o perate in ways that are substantially biased against black and other racial minorities.89  In respo nse to these concerns, a growing body of work concerned with devising technical ap proaches for countering such bias has emerged.902 .1.2 Societal risks associated with data-driven profiling Co ntemporary applications of data-driven profiling technologies may also undermine i mportant collective interests and values, only some of which fall within the scope of existing h uman rights protection.  Much of the value of these technologies lies in their capacity to sort i ndividuals and groups within a population, to automate decision-making, and to enable p ersonalised, predictive interventions  to be scaled and applied  at the population-level .  The fo llowing practices may generate significant societal risks yet these are often overlooked in p ublic and academic debate.  a.  Population-wide, highly granular surveillance    Because  data-driven profiling requires the collection of highly granular data from individuals o n a population-wide basis (i.e. at scale) to profile individuals and groups within and across a p opulation to identify their inferred preferences and interests,91 this necessitates the use of m ass surveillance, often in a highly intrusive yet largely invisible manner.  Although the threats whi ch these practices pose to individual privacy and rights to data protection are readily ap parent (discussed above), these practices also pose serious risks to the collective nature of p rivacy – thereby eroding the fundamental societal conditions in which individual privacy is p ossible and without which individual privacy cannot exist.  As the Council of Europe ’s P arliamentary Assembly92 observes,  ‘s ince many technologies nowadays can operate from a distance, most of us are n ot even aware of this mass surveillance and people are rather defenceless, since th ere are few possibilities to escape these surveillance activities. This creeping d evelopment and its impact on society and human rights have received so far little at tention in political and public debate ….(Yet) there has been little debate about th e cumulative effects of mass surveillance.  Instead, triggered by specific ap plications and incidents ‘mini debates’ have been organised, and the outcome of e ach debate is a balancing act that mostly favours national security or economic i nterests.  The sum of the debates, however, is the gradual but steady dissolving of th e privacy and anonymity of the individual’ .93                                                           89   Angwin et al 2016.  But see Dieterich et al 2016. 90   See below at Section 3.7.1.  As UN Special Rapporteur David Kaye has observed, ‘Tackling the prevalence of d iscrimination in artificial intelligence systems is an existential challenge for companies and governments; fai lure to address and resolve the discriminatory elements and impacts will render the technology not only i neffective but dangerous.’ UN General Assembly 2018, 18.  91   As the Council of Europe, Parliamentary Assembly’s Committee on Culture, Science, Education and Mediah as observed at para 18 ‘The primary business model o f the internet is built on mass surveillance ’: Council o f Europe 2017. 92   Council of Europe 2017, para 60-61. 93   The Wagner Study also draws attention to the risks created by data aggregation and the generation of new d ata, which ‘may then be mined through the use of algorithms, wh ich creates a risk of large-scale s urveillance (‘data-veillance’) by private entities and governments alike…a view echoed by the UN Human R ights Council (22 March 2017) ’: Wagner Study 2017: 15-16.  As the Rathenau Instituut observes, ‘modern-d ay surveillance via the IoT or internet, performed by states or companies, inherently involves the 
Council of Europe Study 3 4 These risks have magnified and deepened as a result of recent advances in AI capabilities that h ave fuelled the emergence of powerful biometric applications that can be used for i dentification purposes in ways that seriously threaten several human rights, including those p rotected under Article 8.  In China, for example, AI-driven facial recognition technology is now b eing introduced in the Beijing subway to enable the facial features of subway users to be i dentified and tracked as they travel. These technologies have already been deployed in train stat ions, used at a pop concert to locate a suspected fugitive, and even implemented in scho ols to monitor student distraction and automatically alert the teacher when distraction is d etected.94   Nor is it difficult to imagine how powerful AI-driven lip-reading  technologies rece ntly developed by DeepMind (which are reported to outperform professional lip-readers95) co uld be deployed by repressive regimes in ways that magnify anxieties that strike at the very h eart of the right to be left alone, and the potentially severe chilling effects that they may have o n freedom of expression, individual self-development and democratic freedom, particularly when  deployed by states to identify and detain individuals identified as political dissidents.96  When  combined with the use of data-driven profiling technologies that enable fairly innocuous an d mundane data to be merged and mined in ways that may reveal highly personal cha racteristics (such as sexual orientation)97 these can be very powerful tools in the hands of g overnmental regimes, whether liberal or repressive, and therefore generate acute threats to the e xercise of all human rights and fundamental freedoms. b.  Population-wide personalisation The  attractions of profiling technologies are readily identifiable: for those wishing to engage in p rofiling, they enable the automated sorting and targeting of candidates of interest in order to p ersonalise the way in which those individuals are treated. These techniques can be applied at scal e, yet in ways that allow for real-time readjustment and reconfiguration of personalised o fferings in response to user behaviour.98   The capacity to engage in population-wide p ersonalisation of digital services has potentially profound implications for social solidarity and co mmunity.  Consider, for example, the practice of ‘personalised pricing’ that data -driven p rofiling and the rise of digital retailing makes possible. Under industrial capitalism, goods we re mass produced and supplied to retailers, and typically made available to consumers at g eographic locations in-store and on terms that applied universally to all customers entering the  store at a particular time at the same price.  In contrast, data-driven profiling now enables g oods and services to be of fered to potential customers at ‘personalised’ prices  (because each custo mer only sees his or her own individualised ‘digital shop front’, and does not have access to  the prices or offers made to others on-line), the level of which can be set by the use of data-d riven profiling in order to identify the maximum ‘willingness to pay’ of each individual, thereby  optimising revenue for the retailer.99  While this kind of intentional discrimination m ight not be unlawful, in so far as it might not directly or indirectly discriminate individuals on                                                                                                                                                                           p rocessing of personal data.  Researchers are still trying to grasp the full extent of the harmful effects on the l ives of individuals caused by such surveillance. The known effects are not comforting.  Not only does s urveillance have a chilling effect on speech…but it also leads to behavioural effects.  For instance, as a re sult of surveillance, individuals conform to perceived group norms.  This conforming effect occurs even w hen people are unaware that they are conforming (Kaminski & Witnov 2015).  Both states and companies re inforce each other in their surveillance activities, as part of the surveillance-innovation complex (Cohen 2016 )’: Van Est and Gerritsen 2017: 20. 94   Cowley 2018. 95   Hutson 2018. 96   Donahoe 2016.  97   Kosinski et al, 2013.  98   Yeung 2016. 99   Townley et al 2017; Miller 2014. 
DGI(2019)05 3 5 the basis of protected grounds under contemporary equality law, nevertheless the effect is a seri ous departure from the pricing practices that prevailed in a pre-digital, pre-data driven age i n ways that, if they become widespread and ubiquitous, may seriously undermine social so lidarity and cohesion.100   c.  Population-wide manipulation The  personalisation of informational environments that data-driven profiling makes possible b rings with it new capacities to manipulate individuals in subtle but highly effective ways.101  At the  individual level, manipulation may threaten personal autonomy and an emerging right to co gnitive sovereignty102 but, as the recent Cambridge Analytica scandal in the run up to the US 2 016 election and the Brexit referendum vividly illustrates, when deployed at scale for the p urposes of political microtargeting to manipulate voting behaviour (which may entail the use o f automated bots operating on social media websites), it may threaten the right to freedom o f expression and information (Article 10) and could seriously undermine the foundations of d emocratic orders by perverting the right to free elections protected under Article 3, Protocol 1  ECHR.103  The manipulative practices which so- called ‘persuasive’ digital technologies enable can  be understood as interfering with rights protected under Articles 8 and 10 because they can  be configured automatically (and continually reconfigured) to tailor the informational cho ice environment and architecture of individuals through the use of data-driven profiling to p redict (often with great accuracy) the behaviours, interests, preferences, and vulnerabilities o f individuals at scale. These applications can be used to manipulate and deceive individuals thu s interfering with both informational and decisional privacy.104   The  capacity to engage in manipulative practices has been exacerbated by the recent em ergence of powerful AI applications that can simulate human traits (including voice si mulation, visual representations of human behaviour and robots capable of interacting with h umans with apparent emotional sensitivity), with such accuracy and precision that it can be ex tremely difficult for humans to detect that those traits are artificially generated.  These te chnologies are likely to be attractive tools for malign actors to deceive and manipulate o thers. For example, some researchers already predict that advanced human-like synthesised v oices will be used to gather information over the phone for deceptive and fraudulent p urposes.  If such attacks become commonplace and widespread, and cannot be readily d etected by targeted individuals, this may seriously threaten the Article 5 right to liberty and sec urity, and the collective security and respect for the rule of law upon which our individual an d collective liberty and security depends.  Opportunities to utilise these technologies to                                                            100   Yeung 2018a.  The European Commission has studied the prevalence of on-line personalisation practices: E uropean Commission (2018b) In the UK, the Competition and Markets Authority (CMA) has commissioned ec onomic research on the use of pricing algorithms and potential competition concerns, including collusion an d personalised pricing: UK Competition and Markets Authority 2018. 101   Yeung 2016.  For example, a recent study by the Norweigan Consumer Council analysed a sample of settings i n Facebook, Google and Windows 10, and show how default settings and dark patterns, techniques and feat ures of interface design meant to manipulate users, are used to nudge users towards privacy intrusive o ptions:  ForbrukerRadet 2018.   102   There is some academic support for recognition of a n ew right to ‘cognitive sovereignty’ aimed at providing i ndividuals with rights-based protection against the forms of manipulation and deception that advancing d igital technologies increasingly make possible, in order to guarantee to individuals a threshold level of s overeignty over their own minds (see Bublitz 2013).  While this might be a self-standing right, it is also p ossible that such a right might be recognised as falling within Article 9(1) ECHR, which establishes the right to  freedom of thought, conscience and religion. 103   Gorton 2016; Wagner Study 2017: 17.  UK House of Commons, Digital Culture Media and Sport 2019. 104   Yeung 2016; Lanzing 2018; Council of Europe 2017. 
Council of Europe Study 3 6 undermine the integrity of the legal process might also become possible. As Brudage et al o bserve in their report on malicious AI: At  present, recording and authentication technology still has an edge over forgery techn ology.  A video of a crime being committed can serve as highly compelling e vidence even when provided by an untrustworthy source.  In the future, however, A I-enabled high-quality forgeries may challenge the ‘seeing is believing’ aspect of v ideo and audio evidence.  They might also make it easier for people to deny al legations against them, given the ease with which the purported evidence might h ave been produced.  In addition to augmenting dissemination of misleading i nformation, the writing and publication of fake news stories could be automated, as  routine financial and sports reporting often are today.  As production and d issemination of high-quality forgeries becomes increasingly low-cost, synthetic m ulti-media may constitute a large portion of the media and information e cosystem.105d.  Systematic treatment of individuals as objects rather than moral agents  Al though the personalisation of individuals’ informational environments is portrayed by social m edia companies as enabling the provision of m ore ‘meaningful’ content, there are two cha racteristics of the underlying socio-technological system upon which these practices rely that  tend to treat individuals as objects rather than moral subjects.  Firstly, individuals are si ngled out, not on the basis of any causal theory, but simply on the basis of correlations in d ata sets.  As a result, these systems typically do not provide any reasoned account to i ndividuals explaining why they have been singled out for treatment of a particular kind.  Seco ndly, their underlying logic and their processing operations are highly complex and o paque, in ways that are practically, and sometimes technically, incomprehensible (discussed ab ove).  In other words, because many contemporary ML systems are designed to capture, co mmodify and optimise value extraction  in the interests of the system owner, by tracking and an alysing from the digital traces of the daily behaviour of individuals, they are not primarily co ncerned with identifying the reasons why individuals behave in particular ways.  Rieder theref ore refers to commercial applications of these ‘big data’ techni ques as offering ‘i nterested’ readings of reality106 in contrast to the disinterested pursuit of knowledge that cha racterises the pursuit of scientific inquiry for academic purposes.107   The net effect of these ap plications is that humans are increasingly treated as objects rather than moral subjects, to b e sorted, sifted, scored and evaluated by technological systems in ways that appear starkly at o dds with the basic right of all individuals to be treated with dignity and respect, and which lies at  the foundation of all human rights and fundamental freedoms.108  As the EU European Gro up of Ethics (2018) explains,                                                             105   Brundage et al 2018: 46. 106   Rieder 2016. 107   Merton 1942. 108  Law enforcement applications of AI for individual profiling within the criminal justice system are especially t roubling.  As AI Now has observed, Axon is now offering free body camera technologies to any US police d epartment following their acquisition of two machine vision companies.  It reports that ‘Axon’s new focus o n predictive methods of policing – inspired by Wal-Mart’s and Google’s embrace of deep learning to i ncrease sales – raises new civil liberties concerns.  Instead of purchasing patterns, these systems will be l ooking for much more vague, context- dependent targets, like ‘suspicious activity’.  Behind the appearances o f technical neutrality, these systems rely on deeply subjective assumptions about what constitutes s uspicious behaviour or who counts as a suspicious person ’ per AI Now 2017: 25. Thus, individuals become ‘ objects of suspicion’ on the basis of data analysis which ha ve no demonstrable causal basis. 
DGI(2019)05 3 7 ‘AI driven optimisation of social processes based on social scoring systems with w hich some countries experiment, violate the basic idea of equality and freedom in th e same way caste systems do, because they construct ‘different kinds of people’ wh ere there are in reality only ‘different properties’ of people.  How can the attack o n democratic systems and the utilisation of scoring systems, as a basis for d ominance by those who have access to these powerful technologies, be pr evented?’… Human dignity as the foundation of human rights implies that m eaningful human intervention and participation must be possible in matters that con cern human beings and their environment.  Therefore, in contrast to the au tomation of production, it is not appropriate to manage and decide about h umans in the way we manage and decide about objects or data, even if this is techn ically conceivable.  Such an ‘autonomous’ management of human beings w ould be unwelcome, and it would undermine the deeply entrenched European core v alues.’109At  the same time, commercial applications of AI for profiling purposes have been accompanied b y the use of population-wide experimentation on individuals through the use of A/B testing, y et without being subject to the supervisory research ethics oversight provided by academic i nstitutions pursuant to the Declaration of Helsinki. The latter sets out core requirements for the  ethical conduct of human subject research.110  The widespread and routine use of these p ractices again reflects a belief that human users are merely objects ripe for experimentation, so  that fundamental norms and institutional oversight mechanisms that are designed to safegu ard and protect the dignity and rights of individuals are not applicable.  As Julie Cohen h as put it ‘[W]e, the citi zens have been reduced to raw material – sourced, bartered and m ined in a curiously fabricated ‘privatised commons’ of data and surveillance.’111e.  Summary of the threats posed by data-driven profiling technologies Take n together, the cumulative effects of the above practices resonate with the concerns ab out profiling expressed by Korff in his report for the Council of Europe concerning the trend s, threats and implications for private life and data protection from the use of the i nternet and related services, which he expresses in the strongest possible terms.  Because p rofiling systems provide the appearance of infallibility, objectivity, reliability and accuracy in the  assessments that they produce, yet their outputs will inevitably and unavoidably generate erro rs (either false positives or false negatives) or generate discriminatory effects on certain g roups112 which are practically impossible for individuals to challenge, Korff concludes: ‘ Profiling thus really poses a serious threat of a Kafkaesque world in  which p owerful corporations and State agencies take decisions that significantly affect th eir customers and citizens, without those decision-makers being able or willing to e xplain the underlying reasoning for those decisions, and in which those subjects ar e denied any effective individual or collective remedies.  That is how serious the i ssue of profiling is: it poses a fundamental threat to the most basic principles of th e Rule of Law and the relationship between the powerful and the people in a d emocratic society.’113These  observations alert us to the collective and cumulative impacts of contemporary ap plications of data-driven technologies which, when undertaken systematically and at scale                                                            109   European Group on Ethics in Science and New Technologies 2018: 9-10. 110   Kramer et al 2015; Tufecki 2015. 111   Powles 2015. 112   Korff and Browne 2013:6. 113   Korff and Browne 2013: 21. 
Council of Europe Study 3 8 may, over time, seriously erode and destabilise the social and moral foundations that are n ecessary for flourishing democratic societies in which individual rights and freedoms can be m eaningfully exercised.   2 .2 Collective societal threats and risks generated by other AI technologies Al though the concerns listed above can be attributed to the use of data-driven profiling, there are  additional societal-level concerns and threats to collective interests and values that do not ari se from the use of individual profiling.  These include: 2 .2.1  Malicious attacks, unethical system design or unintended system failure Un derstandable and well-grounded fears have emerged concerning the safety and security i mplications of AI technologies, including concerns about the potentially catastrophic co nsequences of malicious attacks  on AI systems (including data poisoning attacks and the use o f adversarial ML) if safety critical systems are successfully targeted.  But even if unintended, m any fear the failure of AI technologies within safety-critical systems (such as autonomous v ehicles) which could seriously harm public safety and security.114  Worse, these systems could o perate in ways that are designed to prioritise the safety of particular classes of persons over o thers, and which many would regard as unethical or even unlawful.  As societies become i ncreasingly dependent upon internet-enabled devices and cyber-physical systems more g enerally (many of which are safety critical), ensuring the safety and security of these systems acq uires even greater importance.115 This is especially due to the rise of various avenues and o pportunities for malicious attack that are not confined to direct attack on the systems them selves, but may also include strategies aimed at exploiting network effects that enable the c apacity to target and communicate to individuals at scale, yet with relative anonymity.1162 .2.2 Loss of authentic, real and meaningful human contact In  addition to above-mentioned concerns about the use of AI technologies to imitate human b ehaviour are diffuse but often deeply-felt anxieties that our collective life may become i ncreasingly ‘dehumanised’ , as tasks previously performed by humans are automated.  Many fear  that values and qualities that we cherish, including the value of real human interaction, of g enuine empathy, compassion and concern, may be replaced by the relentless efficiency and co nsistency of AI driven services.  These concerns are particularly prevalent when AI te chnologies are utilised in care environments (eg robot nurses, care nannies and other ro botic care assistants) or in ways that otherwise threaten to denude our societies of cha racteristic values and features that inhere in real, authentic human contact, connection and rel ationships (such as the use of sex robots, for example) which,  although inescapably fraught an d imperfect, nevertheless contribute fundamentally to the meaning and value of human ex perience.117  These applications have generated concerns about the need to ensure that they are  designed and operate in ways that respect the dignity of those in care and might fall within                                                            114  ‘All technologies are liable to failure, and autonomous systems will be no exception (which is pertinent to t he issue of whether autonomous systems should ever be cr eated without manual override)’ : Royal A cademy of Engineering 2009: 3.   115   Thomas 2017a. 116   Brundage et al 2018.  ForbrukerRadet 2018. 117   Yearsley 2017. 
DGI(2019)05 3 9 the scope of Article 8’s protection of ‘private and family’ life  and have prompted some to arg ue in favour of a ‘right to meaningful human contact ’.118  2 .2.3 The chilling effect of data repurposing A dditional concerns arise from worries that people might refrain from participating in systems that  could improve their life conditions (eg. seeking treatment for cancer) due to fears that p ersonal data taken in highly sensitive contexts might be used by AI systems in other contexts i n ways that may be contrary to their interests.119  Concerns about these ‘chilling effects’  ari sing from the ease with which data obtained for one purpose may then be repurposed for o ther unrelated social ends helps explain the importance of honouring and upholding the ‘p urpose specification’ principle enshrined in many contemporary data protection regimes.  If i ndividual autonomy and freedom is understood to include our capacity as individuals to move b etween multiple roles and identities, and to partition them and keep them separate if we so wi sh, then the systematic use of personal data for profiling and decision-making about i ndividuals may threaten our capacity to do so.1202. 2.4 Digital power without responsibility Wo rries that AI systems essentially treat people as objects rather than as moral subjects can b e understood as part of a wider set of concerns about the exploitation of individuals in the serv ice of so-called ‘Big Tech’.  There are several strands of concern.  Firstly, there are serious co ncerns about the population-wide, instantaneous scale at which AI technologies can operate (e g Facebook News Feed) and the limited practical capacity for ‘meaningful human oversight’ o f systems of this kind.   The wedge between the capacity of machines relative to the capacity o f humans to monitor them is evident in repeated claims by social media firms that they can not realistically be expected to respect fully the rights of individuals by providing co mprehensive, timely content moderation given the scale and speed at which their platforms o perate, because – quite simply – they outpace human performance.121  Yet by allowing AI d riven automation to operate without comprehensive human oversight, this threatens to g enerate a serious responsibility gap, through which Big Tech reaps the benefits of these AI-d riven platforms without the concomitant burdens.122                                                              118   Concerns of this kind prompted the Parliamentary Assembly of the Council of Europe (PACE), to suggest that i n contexts where human contact and interaction play a central role, as in raising children and caring for the e lderly or people with disabilities, a ‘right to meaningful human contact’ could play a role: see   Council of E urope Parliamentary Assembly 2017 para 65. 119   There is evidence that this ‘chilling effect’ has occurred in the US in that indiv iduals have been unwilling to u ndertake genetic testing in circumstances where this would likely assist in their healthcare, owing to fears th at the resulting information may be used be others in ways that will be contrary to their interest, p articularly in employment and life insurance contexts: Farr 2016. 120   Understood in terms of Raz’s conception of autonomy, which requires that individuals have an adequate ran ge of options, then widespread data repurposing to inform organisational decision-making about i ndividuals may effectively diminish our autonomy by reducing the range of options available to us.  A ccording to Raz, ‘If a person is to be maker or author of his own life then he must have the mental abilities t o form intentions of a sufficiently complex kind, and plan their execution.  These include minimum rat ionality, the abiity to comprehend the means required to realize his goals the mental faculties necessary to  plan actions etc.  For a person to enjoy an autonomous ife he must actually use these faculties to choose w hat life to have.  There must in other words be adequate options available for him to choose from.  Finally, h is choice must be free from coercion and manipulation by others, he must be independent .’ Raz 1986: 373. 121   See above discussion at Section 2.1.1(b). 122   Keen 2018.  
Council of Europe Study 4 0 Not only does this constitute a violation of basic norms of social reciprocity, amounting to a ki nd of ‘unjustified taking’  from citizens and communities, but it entails the naked exercise of p ower without responsibility.  In other words,  the ‘responsibility gap’ which Matthias123 claims h as arisen from the emergence of computational systems with the capacity to learn124 now has a  more recent contemporary spin, at least in the context of social media platforms in which au tomated systems may be designed to remove or distribute content to users at a scale and speed  which human content moderators cannot keep pace with, and which social media p latforms claim that they cannot be responsible for.125  Secondly, Big Tech has hitherto successful ly managed to immunise themselves against external regulation by claiming to abide b y ‘ethical principles’, which includes their claimed use of technological solutions (discussed in sec tion 3.7.1) that seek to hard-wire normative values into the design and operation of te chnological systems but which, unless they are subject to external oversight and sanction, are un likely to provide meaningful protection.126   2 .2.5 The hidden privatisation of decisions about public  values  AI  technologies aim to reproduce or improve human performance with respect to some task that  would require ‘intelligence’ if humans were to perform them .  Yet the claim that these te chnologies ‘outperform’ humans is based on a very narrow def inition of the overarching goal –  couched in terms of performance of a narrowly defined task (such as identifying malign ti ssue from x-ray images).  But in seeking to incorporate task-specific AI into complex socio-te chnical systems that are developed to provide services to individuals in real world contexts, thi s invariably implicates a wider range of values beside that of precision and efficiency in task p erformance.    T hese systems will invariably reflect the values and value priorities of the system and its d evelopers and might not be aligned with the collective values of the public or the democratic an d constitutional values that human rights are designed to serve.  Yet, even in relation to AI sy stems that directly affect and interface with the public, citizens and other affected groups an d organisations will typically not be given any meaningful opportunity to participate in i dentifying these values or value trade-offs that these systems are configured to reflect.127  The u se of ML in risk-scoring systems used to evaluate the ‘recidivism risk’ of convicted criminals see king release from custody offers a vivid example: although the criminal justice system in co ntemporary democracies is founded on, and is expected to give effect to, several important cri minal justice values, these scoring systems have hitherto been designed to optimise only wi th one such value: public protection.128  As AI technologies become embedded as tools for o ptimising the efficiency of social coordination (such as smart navigation systems or smart i nfrastructure management, for example), they will inevitably make decisions that prioritise so me values over others and impact directly on individuals and groups, some of whom may b enefit and others who may not.  Yet, as Sheila Jasanoff129 and other STS scholars have repeatedl y highlighted, technological systems reflect normative values.  Given their wi despread effects, the determination of those values should be subject to democratic                                                            123   Matthias 2004. 124   Discussed below at section 3.3.2. 125   But see discussion at n.79 above. 126   See section 3.3.4 below.  These technological strategies can be interpreted as  “handing-off” human rights c oncerns to tech firms, empowering them to define (often with a narrow scope) the scope and content of a u ser right and over which they have exclusive powers of enforcement.    127   Korff and Browne 2013. 128   Zweig et al 2018. 129   Jasanoff 2016. 
DGI(2019)05 4 1 participation and deliberation rather than being resolved privately by private providers m otivated by commercial self-interest.  2 .2.6 Exploitation of human labour to train algorithms AI  and ML systems are often claimed to ‘outdo human performance ’ because the algorithms are  trained by large numbers of human workers. For example, an ML algorithm for answering search  queries will be evaluated against an army of Mechanical Turk workers who act like the al gorithm until the algorithm outperforms their answers.  Even after the algorithm has been trai ned, there may be unwanted side effects of the use of these automated algorithms, requ iring humans to identify and weed them out. This is exemplified in the case of social m edia content moderators who are asked to remove inappropriate content on social n etworks.  Both the training for ML models, as well as the consequent human clean-up acti vities to weed out the models’ externalities, are often concealed to maintain the m ythology of seamless automation.130  The humans who train ML models are often located in p oor communities, often in the global south, and typically work under extremely precarious c onditions.131  Nor are they typically provided with support for dealing with the psychological b urdens that may come with the ‘clean up’ activities.   Some claim that because many ML al gorithms continue to learn on their general user population, this allows the system owners to  ‘free ride’ on user labour, thereby nurturing a mode of AI production that contributes to the creati on of conditions in which unpaid labour is normalised and legitimised, while human wo rkers are denuded of rights or recognition.132  2 .3 Power asymmetry and threats to the socio-technical foundations of moral and d emocratic community The  above-mentioned adverse impacts arising from the increasing power and sophistication of n ew and emerging digital technologies are exacerbated by the radical asymmetry in power b etween those who develop and deploy algorithmic systems and the individual users who are sub ject to them.  This asymmetry in power arises largely due to the former’s unique ability to eng age in synoptic, pervasive real-time surveillance of users, collecting and accessing massive d ata sets gleaned from users’ digital interactions on a continuous and real-time basis.  This, in turn , enables them to subject individuals and populations to algorithmic evaluation in order to ‘ sort and score’ them accordingly133 while empowering platform owners to communicate d irectly to users on a one-to-many basis automatically and at scale.  In contrast, the practical cap acity of individuals to understand and navigate the complexity of the data ecosystems in whi ch they are embedded is extremely limited , as is individual users’ abi lity to identify whet her or not digital information and other services are being made available to them on the sam e terms as to other users.134    Thi s power asymmetry suggests that, at least under current institutional arrangements, we n eed to re-examine the capacity of existing rights and our existing mechanisms of oversight an d enforcement to respond comprehensively to the risks associated with our increasingly p owerful digital technologies.  As the Wagner Study has observed:                                                             130   Irani 2015. 131   See for example Chen 2014. 132   Ekbia and Nardi 2014. 133   Ferraris et al 2013. 134   See the findings reported by Which? 2018. Mireille Hildebrandt refers to the ‘digital unconscious’ which is fl ooded with data, in contrast to the information individuals can connect with: Hildebrandt 2015: 196.  
Council of Europe Study 4 2 the increasing use of automation and algorithmic decision-making in all spheres of p ublic and private life is threatening to disrupt the very concept of human rights as p rotective shields against state interference.  The traditional asymmetry of power an d information between state structures and human beings is shifting towards an asymm etry of power and information between operators of algorithms (who may b e public or private) and those who are acted upon and governed.135In  particular, existing human rights institutions may struggle to provide effective and m eaningful protection  for at least three reasons.    Fi rstly, given the highly complex and opaque nature of these technologies, it is very difficult in p ractice for individuals to identify if their rights have been infringed, and if so in what ways.  O ften individuals will be unaware that these technologies are being used for the purposes of ev aluating them. Even if individuals are willing to assert their human rights against i nfringements that arise from the use of automated decision-making, for example, the rem edies available to them might not provide them with their desired outcome.  Perhaps, for ex ample, it is not so much that individuals want an explanation of why they were treated less fav ourably than others, but they want to insist that they should be entitled to equally fav ourable treatment.136   Seco ndly, even if individuals are aware that their rights may have been interfered with as a resul t of the use of AI systems, one might question the likelihood that individuals will in p ractice seek to initiate remedial action if circumstances where they do not regard the i nterference as sufficiently serious to motivate them to invest the time, energy and resources asso ciated with launching and maintaining a compliant.  The resulting collective action p roblem means that the aggregate adverse impacts of these systems are likely to continue u nremedied, at least in the absence of collective complaints mechanisms or an official body wi th the competence, resources and remit to take enforcement necessary to ensure effective h uman rights protection. Thi rdly, many of the larger adverse societal concerns cannot  be readily expressed in the l anguage and discourse of human rights because they concern collective values and interests, i ncluding threats to the broader and more amorphous moral, social and political culture and co ntext in which advanced digital technologies operate.  At the same time, the speed and scale at  which these technologies now operate poses novel threats, risks and challenges which co ntemporary societies have not hitherto had to contend with.   Yet many of the anticipated cum ulative and collective effect of these systems over time could fatally undermine the social an d technical conditions that are essential for the exercise of human rights and fundamental freedo ms. Because current approaches to the interpretation and enforcement of human rights are  highly individualised in orientation,137 they are likely to struggle to address the collective, a ggregate and cumulative risks and harms that these technologies might generate.  In other wo rds, existing rights-based approaches and rights discourse tend to overlook deeper sy stematic, societal concerns, including threats to the underlying democratic and moral fabric i n which individual rights are anchored and without which they have no meaning.138                                                           135   Wagner Study 2017: 33. 136   Edwards and Veale 2017. 137   Yeung 2011. 138   See section 3.8 below. 
DGI(2019)05 4 3 2.4 Summary  Thi s section has examined the adverse individual and collective threats and risks to society that  the application of advanced digital technologies may pose.  It has emphasized the way in whi ch the widespread and growing use of advanced digital technologies (including AI), p articularly those which rely upon data-driven profiling technologies, may systematically threaten  the exercise of human rights, as well as more general collective values and interests that  fall outside the scope of existing understandings of human rights protection.  It has also co nsidered the threats and risks posed by other AI technologies and their contemporary and an ticipated applications.  These include concerns associated with hostile and malicious ap plications or the unethical or unsafe design and operation of AI-enabled systems,  d iminishing opportunities for authentic, real and meaningful human contact, the chilling effect o f data repurposing, the exercise by digital platforms and others with AI capabilities of power wi thout responsibility, the creeping yet hidden privatisation of decisions about public values, an d the exploitation of human workers to train algorithms.  Finally, it has highlighted the g rowing power asymmetry between those with the capacity and resources to develop and em ploy AI technologies and the individual users, groups and populations directly affected by thei r use, which may substantially diminish their capacity to identify and seek protection and redr ess under existing rights-protecting institutions.   The wide-ranging and potentially serious i ndividual and collective threats and risks associated with the development and application of ad vanced digital technologies inevitably raise important questions about how responsibility for av oiding, preventing, and mitigating them should be allocated.  Furthermore, if those risks ri pen into harm and/or violate human rights, how should responsibility for those co nsequences be attributed and allocated and what institutional mechanisms can be relied u pon to ensure adequate enforcement and redress, particularly given the collective action p roblem faced by individual rights-holders?  It is these questions that Chapter 3 seeks to ad dress, beginning with an examination of the concept of responsibility, why responsibility m atters and an analysis of the ways in which AI technologies challenge existing conceptions of respo nsibility. 
Council of Europe Study 4 4 Chapter 3.  Who bears responsibility for the threats, risks, harms and wro ngs posed by advanced digital technologies? As  the preceding section has demonstrated, advanced digital technologies generate serious threats  and risks to our individual and collective interests and values and may perpetuate the co mmission of substantial and systematic wrongdoing, including human rights violations.  Take n together, these threaten the health of the collective moral and social foundations of d emocratic societies.  Accordingly, this section considers who bears responsibility for their p revention, management and mitigation, and for making reparation if they ripen into harms an d rights violations, to individuals, groups and to society.  The following discussion highlights h ow the concept of responsibility is implicated by the emergence of advanced digital te chnologies (including AI), particularly in light of their implications for human rights protected u nder the ECHR referred to in Chapter 2.   The  following discussion proceeds in several stages.   Fi rstly, it begins by clarifying what we mean by responsibility and why responsibility matters, em phasising its vital role in securing and giving expression to the rule of law and which is esse ntial for peaceful social co-operation.   Seco ndly, it then considers two core themes raised in contemporary discussions of the adverse ri sks associated with AI technologies, notably the role of the tech industry in promulgating and v oluntarily committing themselves to abide by so-called ‘ethical standards’ and secondly, the al leged ‘control problem’ that is claimed to flow from the capacity of AI-driven systems to o perate more or less autonomously from their human creators.   Thi rdly, it identifies a range of different ‘responsibility models’ that could be adopted to g overn the allocation of responsibility for different kinds of adverse impacts arising from the o peration of AI systems, including models based on intention/culpability, risk creati on/negligence, strict responsibility and mandatory insurance schemes.  Because the fo cus of this report is on the implications for human rights, responsibility for human rights v iolations is widely understood ‘strict ly’ (or as ‘strict’ – so that provided a human rights v iolation has been established, there is no need for proof of fault).  In contrast, the allocation o f obligations of repair for tangible harm to health or property may be legally distributed in acco rdance with a variety of historic responsibility models.  Because the allocation of historic respo nsibility for tangible harm arising from the operation of AI systems also has a prospective d imension, through its guiding function in identifying the nature and scope of the obligations o f those involved in the development, production and implementation of AI systems, these respo nsibility models are briefly outlined.   Fo urthly, it draws attention to the acute challenges for the allocation of responsibility g enerated by the operation of complex and interacting socio-technical systems, which entails co ntributions from multiple individuals, organisations, machine components, software al gorithms and human users, often in complex and highly dynamic environments.   Fi fthly, it draws attention to a range of non-judicial mechanisms for securing both prospective an d historic responsibility for the adverse impacts of AI systems, including various kinds of i mpact assessments, auditing techniques and technical protection mechanisms.   Si xthly, it emphasises the role and obligations of states in relation to the risks associated with ad vanced digital technologies, focusing specifically on their obligations to ensure effective p rotection of human rights.   
DGI(2019)05 4 5 Finally, it highlights the need to reinvigorate human rights discourse in a digital age, drawing att ention to the need to protect and nurture the socio-technical foundations necessary for h uman agency and responsibility, without which human rights and freedoms cannot be p ractically or meaningfully exercised.    3 .1 What is responsibility and why does it matter? In  setting out the aims of this study, we have already noted that a society’s conceptions and p ractices of responsibility are of vital importance because they serve to ensure that, within co nstitutional democratic orders, individuals and organisations are held to account for the ad verse ‘other-regarding’ effects of their actions. Despite the extensive legal and philosophical l iterature concerned with responsibility, relatively few academics focus their attention on the fu ndamental role of responsibility for individuals and for society.  Woven beneath the surface o f this scholarship lies recognition that the concept of responsibility serves two critical fu nctions, broadly reflecting what moral philosopher Gary Watson’s refers to as the ‘two faces’ o f responsibility.139  The first face is essential to our sense of ‘being in the world’ as moral ag ents, that is, as authors of our own lives who act on the basis of reasons.  As Watson puts it: Re sponsibility is important to issues about what it is to lead a life, indeed about w hat it is to have a life in the biographical sense, and about the quality and ch aracter of that life. These issues reflect one face of responsibility (what I will call i ts aretaic face).140  Bu t Watson identifies a second face of responsibility which is concerned with practices of h olding people accountable.141  For him, w hen we speak of conduct as deserving of “censure,” or “remonstration,” as “ outrageous,” “unconscionable” (and on some views, even as “wrong”), is to s uggest that some further response to the agent is (in principle) appropriate. It is to i nvoke the practice of holding people morally accountable, in which (typically) the ju dge (or if not the judge, other members of the moral community) is entitled (in p rinciple) to react in various ways. T he difference between these two faces of responsibility, which we might call the ‘self -d isclosure’ view of responsibility on the one hand, and the ‘moral accountability’ view on the o ther, is illuminated in the following scenario: If  someone betrays her ideals by choosing a dull but secure occupation in favor of a ri skier but potentially more enriching one, or endangers something of deep i mportance to her life for trivial ends (by sleeping too little and drinking too much b efore important performances, for example), then she has acted badly —cowardly, s elf-indulgently, at least unwisely. But by these assessments we are not thereby h olding her responsible, as distinct from holding her to be responsible. To do that, w e would have to think that she is accountable to us or to others, whereas in many c ases we suppose that such behavior is “nobody's business.” Unless we think she is r e s p o n s i b l e  t o  u s  o r  t o  o t h e r s  t o  l i v e  t h e  b e s t  l i f e  s h e  c a n —and that is a moral que stion—we do not think she is accountable here. If her timid or foolish behavior                                                            139   Watson 2004. 140   Watson 2004: 262-263. 141   Watson 2004: 264. 
Council of Europe Study 4 6 also harms others, and thereby violates requirements of interpersonal relations, th at is a different matter.142A  similar sentiment is reflected in the concept of ‘basic responsibility’ articulated and d eveloped by legal scholar John Gardner who claims that our basic responsibility is central to o ur sense of being in the world.  It is fundamental to our identity as rational agents, that is, as creatures  who act on the basis of reasons and who, as individuals, want our lives to make rati onal sense, to add up to a story not only of whats but also of whys.143  F or Watson, control is arguably central to the accountability practices that characterize the sec ond face of responsibility.  Be cause some of these practices —and notably the practice of moral acco untability—involve the imposition of demands on people, I shall argue, they ra ise issues of fairness that do not arise for aretaic appraisal. It is these concerns a bout fairness that underlie the requirement of control (or avoidability) as a con dition of moral accountability.  ‘Holding responsible’ can be taken as equivalent to  ‘holding accountable’. But the notion of ‘holding’ here is not to be confused with t he attitude of believing (as in, ‘I hold that she is responsible for x’). Holding people re sponsible involves a readiness to respond to them in certain ways.  To be “on the h ook” in these and other cases is to be liable to certain reactions as a result of f ailing to do what one is required. To require or demand certain behavior of an agent  is to lay it down that unless the agent so behaves she will be liable to certain ad verse or unwelcome treatment. For convenience, I shall call the diverse forms of ad verse treatment “sanctions.” Hol ding accountable thus involves the idea of l iability to sanctions. To be entitled to make demands, then, is to be entitled to im pose conditions of liability.144Because  this study is concerned with identifying where responsibility should lie for the i ndividual and collective threats, risks, harms and human rights violations stemming from ad vanced digital technologies, it focuses primarily on the second face of responsibility, u nderstood in terms of ‘holding accountable’ . Nevertheless, there is a crucial link between these  two faces of responsibility that rests in the status of the individual as a moral agent with the  capacity to make active choices and decisions, including decisions that affect, and have the p otential to cause harm or to perpetuate wrongs to others.  As Gardner puts it, ‘(w)e are moral ag ents only insofar as we are basically responsible .’145  Basic responsibility is therefore central to , and a reflection of, both faces of responsibility.   As Gardner observes, whenever we p erpetrate wrongs or mistakes, we always hunt around for justifications and excuses, not only b ecause, as rational beings we want to avoid (unpleasant) consequential responsibility (the ‘ Hobbesian explanation ’) but also for a deeper reason (which he refers to as the ‘Aristotelian ex planation’) that, as rational beings, we all want to assert our basic responsibility – and this requ ires that I can give a good account of myself.146   Res ponsibility and the rule of law In  other words, basic responsibility is essential, not only for our self-understanding as i ndividuals as authors of our own lives but also as individuals as members of a community of                                                            142   Watson 2004: 265-266. 143   Gardner 2003. 144   Watson 2004: 272-273. 145   Gardner 2008: 140.   146   Gardner claims that this account that we provide need not be to anyone in particular, but can and should be o ffered to all the world.   Hence he rejects the view that responsibility is necessarily ‘relational’.   
DGI(2019)05 4 7 moral agents.   Moral agents have the capacity and freedom to make choices about their d ecisions and actions, and to do so in ways that might be wrongful or cause harm, whether to o ther individuals or to the conditions that are essential to maintain the stability and social co-o peration needed to sustain community life.  It is our basic responsibility and our responsibility p ractices through which members of a community hold each other to account, that cha racterise a political community largely as a moral community (i.e. a community of moral ag ents).   Of critical importance is the mutual respect and self-restraint exercised by members o f a moral community that makes possible and sustains community life, and which ultimately l ies at the foundations of the contemporary rule of law ideal.147  A society that lacks a system fo r institutionalising its responsibility practices in order to hold people responsible for the ad verse impacts of their other-regarding conduct (including conduct that harms others or v iolates their human rights) would not benefit from the vital protective functions that such an i nstitutional system provides and that are essential for peaceful and trustworthy social co-o peration and coordination.  In other words, our system for ensuring that responsibility is duly al located plays a critical role in sustaining the underlying social framework of cooperation wi thout which the law cannot rule.  At the same time, it is important to recognise that the stabi lity and continuity of these social foundations rest, ultimately, on the mutual respect and sel f-restraint of individual members of the moral community and not on a system of te chnological coercion and control.  It is this mutual respect and self-restraint that is absent fro m the ostensibly happy, stable, orderly and efficient society depicted in Huxley’s Brave New Wor ld.148  Inhabitants of Brave New World have no meaningful rights or freedoms. Theirs is not a  moral community but a society comprised of members who are merely passive objects, who se thoughts and actions have been hard-wired and controlled by and through the exercise o f technological power of an authoritarian dictator, and in which notions of freedom, au tonomy and human rights not only fail to flourish, but simply lack any meaning or p urchase.149Ac countability, answerability and transparency The  critical importance of institutionalised systems of responsibility to secure the social fo undations upon which the rule of law is founded highlights the need, within any moral and p olitical community committed to respect for human rights, to establish and implement i nstitutional mechanisms for holding members of the community to account for their other-rega rding conduct.    Although the concept of accountability is contested, for present purposes i t has been usefully described as ‘requiring a person to explain and justify –  against criteria of so me kind – their decisions or acts, and then to make amends for any fault or error. ’150  So u nderstood, accountability mechanisms possess the following four features: setting standards ag ainst which to judge the account, obtaining the account, judging the account, and deciding what  consequences (if any) should follow.  The concept of accountability is of particular i mportance in relationships between a principal and agent, in which the agent is expected to act  for and on behalf of the principal, who is therefore required to give an account – to be ans werable to - the principal on whose behalf the agent acts.   Transparency is directly linked to  accountability, in so far as accountability requires that those being called upon to account can  explain the reasons for their actions, and to justify those actions in accordance with a p articular set of rules or standards for evaluation.  Transparency is therefore important for at l east two reasons: to enable those affected by a decision or action to know the reasons for                                                            147   Galligan 2006. 148   Huxley 1932; Yeung 2017b. 149   Yeung 2011. 150   Oliver 1994: 245. See also Bovens 2007 and literature cited therein. 
Council of Europe Study 4 8 that action or decision, and to enable the affected party to evaluate the quality of those reaso ns.151   M echanisms for accountability have particular importance in relation to the exercise of g overnmental power within liberal democratic societies because governmental officials are rega rded as the servants of the citizens upon whose behalf they act and from whom their p ower is ultimately derived.  Yet, the importance of accountability arises whenever the ex ercise of power has the capacity to affect others in adverse ways.  Accordingly, concerns ab out the power, scale and effects of complex socio-technical systems that rely upon AI te chnologies have given rise to a cluster of concerns that can be understood as united in a co ncern to secure ‘algorithmic accountability’, particularly given the opacity of these systems an d their potential to be utilised in ways that can have highly consequential implications for i ndividuals, groups and society in general.152  Securing accountability and responsibility for h uman rights violations and other adverse consequences resulting from the operation of these te chnologies is therefore essential.  Although existing laws, including data protection law, co nsumer protection law, competition law and constitutional laws that enshrine legal p rotection for human rights within national legal systems, have the potential to play a si gnificant and important role in securing various dimensions of algorithmic accountability, thei r contribution to securing algorithmic accountability is beyond the scope of this study.  Rather,  the following discussion seeks to examine implications of advanced digital te chnologies (including AI systems) for the concept of responsibility , focusing primarily on their i mplications for human rights violations, drawing on both moral philosophy and legal scho larship. 3. 2 Dimensions of responsibility  Thi s general concept of responsibility as ‘holding accountable’ ha s been extensively examined i n the legal and philosophical literature, and various insights from that literature are selectively d rawn upon in the analysis which follows.  Although there are many different ‘senses’ in which the  term ‘responsibility’ is use d,153 for the purposes of this study, the temporal element of respo nsibility is worth emphasising, facing in two directions: ( a) Historic (or retrospective) responsibility : which looks backwards, seeking to allocate respo nsibility for conduct and events that occurred in the past.  As we shall see, considerable d ifficulties are claimed to arise in allocating historic responsibility for harms and wrongs cau sed by AI systems; and ( b) Prospective responsibilities:  which establish obligations and duties associated with roles an d tasks that look to the future, directed towards the production of good outcomes and the p revention of bad outcomes.   Prospective responsibilities serve an important guiding function.  As  Cane puts it, ‘one of the most important reasons why we are interested in responsibility an d related concepts is because of the role they play in practical reasoning about our rights an d obligations vis-à-vis other people, and about the way we should behave in our dealings wi th them.’154 In the context of responsibility for the actions and resulting consequences of                                                            151   Yeung and Weller 2018b.  Zalnieriute et al 2019.  152   Yeung 2017. 153   Hart 1968: 211-230. 154   Cane 2002: 45. 
DGI(2019)05 4 9 autonomous AI/robotic systems, the idea of ‘role responsibility’155 has sometimes been fo regrounded.156  An y legitimate and effective response to the threats, risks, harms and rights violations posed b y advanced digital technologies is likely to require a focus on the consequences for individuals an d society which attends to, and can ensure that, both prospective responsibility aimed at p reventing and mitigating risks, and historic responsibility for adverse effects arising from the o peration of the complex socio-technical systems in which these technologies are embedded, i s duly and justly assigned.  Only if both the historic and prospective dimensions of respo nsibility are attended to can individuals and society have confidence that efforts will be m ade first, to prevent harms and wrongs from occurring, and secondly, if they do occur, then i nstitutional mechanisms can be relied upon to ensure appropriate reparation, repair and to p revent further harm or wrongdoing.  It will necessitate a focus on both those involved in the d evelopment, deployment and implementation of these technologies, individual users and the g roups affected by them and action by the state (and states acting collectively and co operatively) to ensure the establishment and maintenance of conditions needed to safegu ard citizens against unacceptable threats and risks, thereby ensuring that human rights are  adequately protected.    In other words, proper consideration of the responsibility of AI te chnologies and systems will attend to the positions of both the moral agent and the moral p atient, as well as the larger moral community more generally,  in order to answer the q uestions: responsibility to whom and for what? 157  3 .3 How do advanced digital technologies (including AI) implicate existing conceptions of re sponsibility? H aving clarified what we mean by responsibility and highlighted the need to attend to both its p rospective and retrospective dimensions, we are now in a position to consider where respo nsibility lies for the adverse consequences, threats and risks associated with the d evelopment and implementation of AI technologies, including human rights violations and o ther wrongs and harms arising from their operation.  Although this question is simple to stat e, there are considerable conceptual challenges in seeking to answer it.  As the EU Eur opean Group on Ethics158 has observed, AI technologies raise:  … questions about human moral responsibility.  Where is the morally relevant agency  located in dynamic and complex socio-technical systems with advanced AI an d robotic components?  How should moral responsibility be attributed and ap portioned and who is responsible (and in what sense)? In  other words, the complexity of the technologies themselves, and the larger socio-technical co ntexts in which they are implemented and applied, can obscure lines of moral responsibility, p articularly when they operate in unexpected ways that generate harm or violate rights.   But we  must bear in mind that moral responsibility and legal responsibility are distinct, albeit rel ated, concepts.  Unlike morality, the law has a highly developed system for institutionalizing an d enforcing responsibility (including the application of sanctions in certain circumstances) b ecause it must adjudicate real world disputes, and which requires both finality of judgement an d legal certainty.159  A society cannot rely exclusively on individuals ’ inclinations to ‘act                                                            155   Hart 1968: 211-230. 156   See Section 3.3.1 below .  157   Liu and Zawieska 2017; Cane 2002. 158   EU European Group on Ethics 2017. 159   Cane 2002. 
Council of Europe Study 5 0 ethically’ because the lack of any institutional mechanisms to enforce those standards (i ncluding lawful authority to sanction non-compliance) means that such an entirely voluntary sy stem for would fail to provide the stable and reliable social foundations necessary for trustw orthy and peaceful social cooperation within contemporary societies.  The law’s role in sec uring and institutionalising responsibility to ensure the protection of legal rights and enfo rce the performance of legal duties is therefore essential.  As the following discussion d emonstrates, the way in which legal systems have allocated historic responsibility has ty pically been more sensitive to the interests of victims and of society in security of the person an d property in comparison with moral philosophical accounts of responsibility, which have te nded to focus on the conduct of the moral agent and whether it appropriately attracts b lame.  Yet applying these moral and legal concepts of responsibility to the development and i mplementation of advanced digital technologies (including AI) in contemporary contexts may n ot be straightforward.  The capacity of these technologies and systems to operate in ways that  were not previously possible may challenge our existing legal, moral and social co nceptions of responsibility, particularly given the properties identified in section 2.1 above as resp onsibility-relevant, including their: •  inscrutability and opacity •  complex and dynamic nature  •  reliance on human input, interaction and discretion •  general purpose nature  •  global interconnectivity, scalability and ubiquity  •  automated, continuous operation, often in real-time  •  reliance on large data-sets  •  capacity to generate ‘hidden’ insight from merging data sets   •  ability accurately to imitate human traits  •  greater software complexity (include vulnerability to failure and malicious attack), and   •  capacity to ‘personalise’ and configure individual choice environments   •  capacity to redistribute risks, benefits and burdens among and between individuals and g roups via the use of AI-driven optimisation systems which reconfigure social env ironments and choice architectures, and  •  capacity to generate collective action problems.  Befo re proceeding, it is important to clarify the conceptual distinction between two different ty pes of adverse effects that may (and have) arisen from the operation of AI systems: (a)  violations of human rights, including but not limited to, the rights protected under the ECHR;  (b)  tangible harm to human health, property or the environment; These  are separate and distinct concepts and consequences.  It is possible for a human rights v iolation to occur without any tangible harm, and vice versa.   For example, the removal by Fa cebook in 2016 of the iconic photograph of a naked 9-year old girl fleeing napalm bombs d uring the Vietnam War, on the grounds that nudity violated its community standards, can be u nderstood as a violation of the Article 10 right to freedom of expression and information, al though it did not generate any substantial tangible harm.160  Conversely, if a self-driving car co llides with and injures a wild animal, this entails the infliction of harm without any human ri ghts violation.  Yet any given event or series of events may entail both tangible harm and a v iolation of human rights.  Thus, if a self-driving vehicle collides with and fatally injures a                                                            160   See Scott and Isaac. 2016
DGI(2019)05 5 1 pedestrian, this would entail both a violation of the Article 2 right to life and the infliction of tang ible harm.161The  focus of this report is on examining the responsibility implications of AI systems from a h uman rights perspective.  It is therefore primarily concerned with analysing responsibility for h uman rights violations, rather than responsibility for tangible harm arising from the operation o f these systems.   The following discussion focuses primarily upon those who create, develop, i mplement and preside over AI systems.  It asks whether they can be held responsible for the ad verse consequences those systems might generate,  beginning with an examination of two co re themes that have arisen in contemporary responses concerned with identifying where respo nsibility lies for the risks which AI technologies may pose: first, voluntary action by the te ch industry in promulgating and publicly proclaiming their commitment to so- called ‘ethical g uidelines’, and secondly, claims that because AI systems act autonomously, this relieves their creat ors from responsibility for their decisions and any consequential adverse effects.  The o bligations of the state in relation to these adverse effects is considered after various ‘models o f responsibility’ that might apply in ascribing responsibility to those who develop and i mplement AI systems have been described. 3. 3.1 Prospective responsibility: voluntary ethics codes  a nd the ‘Responsible Robotics/AI’ project  Ri sing public anxiety and the recent ‘Techlash’162 in response to the growing power, practices an d policies of the Big Tech firms, particularly following the use of political micro-targeting and the  Cambridge Analytica scandal, have precipitated numerous voluntary ‘ethics’ initiatives by the  tech industry.  These initiatives typically entail the promulgation of a set of norms and stand ards either by individual tech firms or by a group of tech firms (including non-profit o rganisations163 or a technical standard setting organisation164) publicly and voluntarily espo using their commitment to comply with those publicised standards of conduct (often cal led ‘codes of ethical conduct’).165   These initiatives can be understood as part of a m ovement towards what Liu and Zawieska refer to as the ‘responsible AI/robotics’  project.166    T wo features of these initiatives are worth highlighting.  Firstly, they are concerned with p rospective responsibility, seeking to identify and allocate ‘role responsibility’ (or spheres of o bligation) for those involved at each stage of the design, development and deployment of these  technologies with the aim of demonstrating to the public the seriousness of their co mmitment to addressing ethical concerns.167  One notable feature of these initiatives is that they  tend steadfastly to avoid explicitly referring to the historic responsibilities of those i nvolved in the design, development and deployment of these technologies when things go awry .  Neither do they tend to specify upon whom the blame should fall for such co nsequences, nor acknowledge any obligation to compensate  those adversely affected.                                                              161   The scope of adverse effects regarded as constituting legally recognisable ‘harm’ varies between national l egal systems.  In Anglo-Commonwealth common law systems, for example, some forms of non-tangible h arm (such as emotional distress and mental anguish) may be legally recognised as harm for the purposes of c ompensation awards in personl injury cases: Gilliker 2000.  162   The Economist 2018b. 163   For example, the ‘beneficial AI’ movement is supported by the Future of Life Institute: see Conn 2017.  164  See for example, the various recommendations and guidelines developed by the IEEE’s Global Initiative for E thical Considerations in AI and Autonomous Systems 2017.  165   For example, Google’s ‘Objectives for AI Applications’ , see Pichai 2018. 166   Liu and Zawieska 2017. 167   Liu and Zaweiska 2017; Loui and Miller 2007; Eschelman 2016. 
Council of Europe Study 5 2 Rather, as Liu explains, role responsibility describes ‘a sense of responsibility that attaches to an  individual by virtue of the position he/she occupies or the function that he/she is expected to  fulfil and is therefore by the performance of obligations connected to an individual’s role an d which can be pre-defined and specified in advance. ’168  Thus, once an individual has d ischarged the duties attached to his or her role or office, that is regarded as due fulfilment of h is or her responsibilities.169   Seco ndly, these ‘Responsible AI/robotics’ initiatives can be characterised as an emerging p rofessional self-governance movement which can be located within a longer standing social p henomena often discussed under the rubric of ‘corporate social responsibility’.  The character o f these so-called ‘ethical codes’ as ‘social’ (rather than legal) and entirely voluntary, means that t hey the obligations and commitments specified in these codes are not legally enforceable i f violated.  Nor do these initiatives typically make provision for the establishment and m aintenance of enforcement institutions and mechanisms through which an independent, ex ternal body is empowered to evaluate the extent to which those commitments have been co mplied with or to impose sanctions for non-compliance.  Thus, although these initiatives p rovide welcome recognition by the tech industry that the ethical development and d eployment of advanced digital technologies is a matter of public concern that warrants their acti on and attention, these initiatives lack any formal institutional mechanisms to enforce and san ction violations.  Nor is there any systematic representation of the public in the setting of tho se standards.  Accordingly, these initiatives have been roundly criticised as a form of ‘ethics washi ng’170 failing to take ethical concerns seriously.171If  these codes of practice were supported by institutional mechanisms, backed by law, i ncluding provision for external participation in the setting and evaluation of the standards them selves, and independent, external oversight to evaluate whether individual firms and o rganisations have in fact complied with the specified norms and standards, there would be stro nger basis upon which those affected (and society more generally) could have confidence that  meaningful and democratically legitimate safeguards were in place to prevent and m itigate some of the ethical risks associated with these technologies (see section 3.7 below).172  It  is the need for meaningful and effective safeguards that a human rights perspective insists upo n.173  At the same time, prospective approaches cannot ensure that historic responsibility  in the  event that harm or wrongdoing occurs will be duly allocated.  As Liu and Zaweiska argue, al though the ‘Responsible Robotics/AI’ project may be welcomed, it leaves a ‘responsibility ga p’, because it is only concerned with role responsibility rather than causal responsibility.  Un like role responsibility, causal responsibility is a form of historic responsibility.  Its concern is to  identify and establish a relation between cause and effect. It is thus retrospective in nature, i nherently outward-looking, relational in orientation, because it foregrounds the moral patient                                                            168   Cane is critical of the narrowly defined way in which role responsibility is attached to specific roles or tasks, o bserving that ‘being a responsible person involves taking seriously the prosp ective responsibilities, w hatever they are, attaching to whatever activity one is engaged in at any particular time’ : Cane 2002: 32. 169  Liu 2016: 336. 170   Wagner 2019.  Metzinger 2019. 171   Green et al 2019; Hagendorff 2019. 172   Nemitz 2018. 173   See AHRC supra, n.10.  As David Kaye, Special Rapporteur on the promotion and protection of the right to f reedom of opinion and expression to the UN General Assembly stated, ‘The development of codes of ethics an d accompanying institutiona structures may be an important compement to, but not a substitute for, c ommitments to human rights.  Codes and guidelines issued by both public and private sector bodies shoud e mphasize that human rights law provides the fundamental rules for the protection of individuals in the c ontext of artificial intelligence’, UN General Assembly 2018: 18.   
DGI(2019)05 5 3 (that is, the person or persons harmed by the relevant activity)174.  In contrast, the allocation of ro le responsibility focuses on the prospective role responsibilities of those identified as respo nsible agents.  Accordingly, a ‘responsibility gap’ arises, because discharging one’s p rospective or role responsibilities will not necessarily guarantee that causal responsibility will b e duly allocated.175  In other words, the designation of role responsibility cannot ensure retro spective accountability nor allocate blame because it is concerned only with the fu lfilment of pre-established obligations, rather than atonement and accountability for co nsequences.176  3 .3.2 Machine autonomy and the alleged ‘control’ problem  (a)  The alleged ‘control’ problem  An other frequent claim made in response to concerns about the need to identify where respo nsibility lies for the adverse implications of advanced digital technologies is that, because these  systems operate more or less autonomously and without direct human intervention and co ntrol from the outside, those who develop and implement them cannot fairly be regarded as respo nsible for their decisions, actions and corresponding consequences. This view was o utlined by Matthias177, who argues that th e agent can be considered responsible only if he knows the particular facts s urrounding his action, and if he is able to freely form a decision to act, and to s elect one of a suitable set of available alternative actions based on these facts.178Bu t an increasing class of machines, which Matthias refers to as ‘autonomous artificial agents’, are  capable of fulfilling some, often quite narrow, purposes by moving autonomously through so me ‘space’ and acting in it without human supervision .  That agent can be a software p rogramme that moves through an information space (eg an internet search spider) but it can al so have a physical presence (eg a robotic pet) and move through time and space.  These ag ents are deliberately designed to act, and inevitably interact, with other things, people, and so cial entities (laws, institutions and expectations). At least for those which have a physical p resence and can learn from direct interaction in real environments, they can, in return, d irectly manipulate that same environment and share their environment with humans. M atthias argues that a ‘responsibility gap’ arises because, for machine agents of this kind, the h uman agent who programmed it no longer exerts direct control over the machine agent’s b ehaviour, which is gradually transferred to the machine itself. It would therefore be unjust to h old humans responsible for actions of machines over which they could not have sufficient co ntrol.179  He offers several examples of these kinds of machine agents, including those that rel y upon:                                                            174   In the legal literature, the term ‘victim’ or ‘potential victim’ tends to be used rather than ‘moral patient’, the l atter being more common in the applied philosophical literature. 175   Liu and Zaweiska 2017. 176   Liu 2016. 177   Matthias 2004. 178   Matthias 2004: 175. 179   Matthias’s argument has been prominent in shaping the debate, in which the underlying ‘choice’ theory of m oral responsibility upon which his argument rests has not been challenged.  Instead, academic responses h ave either sought to counter his argument via a commitment to methodological and moral individualism s uch that every action is ultimately attributable to human individuals: whatever role non-human objects p layed in bringing about a particular outcome, they are ancillary (Hanson 2009: 92).  On this view, AI 
Council of Europe Study 5 4 (a) the operation of artificial neural networks : instead of clear and distinct symbolic repr esentation of information and flow control, we have a sometimes very large matrix of sy naptic weights, which cannot be directly interpreted.  Rather, the knowledge and behaviour sto red in a neural network can only be inferred indirectly through experimentation and the ap plication of test patterns after the training of the network is finished; (b)  reinforcement learning : usually based on the same neural network concepts, but ad ditionally it lifts the distinction between a training and a production phase.  Reinforcement l earning systems explore their action space while working in their operational environment, whi ch is their central feature (enabling them to adapt to ever-changing environments) as well as  a big drawback concerning their predictability.  The information stored in the network can not be fully checked, even indirectly, because it always changes.  Even if we can prove m athematically that the overall performance of such a system will eventually converge to so me optimum, there will be unavoidable errors on the way to that optimised state.  The creat or of such a system (who Matthias comments is not really a programmer in the traditional sense)  cannot eliminate these errors, for they must be explicitly permitted in order that the sy stem can remain operational and improve its performance; (c)  genetic programming methods  in which an additional layer of machine-generated code o perates between the programmer and the product of programming.  Unlike in neural n etworks, where the designer still defines the operating parameters of the system (the n etwork architecture, the input and output layers, and their interpretation) and at least d efines the alphabet used and the semantics of the symbols, the genetic programmer loses ev en this minimal amount of control, for she creates a machine that programs itself. At  the same time, Matthias observes that autonomous agents deprive the programmer of a spa tial link between the programmer and the resulting machine agent.  Accordingly, the m achine agent acts outside the programmer’s observation horizon and might not be able to i ntervene manually (in the case of a fault or error, which might occur at a much later point in ti me).  Thus, these processes involve the designer of machines increasingly losing control over them , gradually transferring control to the machine itself, in which - according to Matthias – the  programmer’s role changes ‘ from coder to creator of software organisms ’.  As the i nfluence of the creator of the machine decreases, the influence of the operating environment i ncreases such that the programmer transfers her control over the product to the environment (e specially for machines that continue to learn and adapt in their final operating environment).  P articularly given that these agents will have to interact with a potentially great variety and n umber of people (users) and situations, it will typically not be possible for the creator to p redict or control the influence of the operating environment.  According to Matthias, the net resul t is that these machines operate beyond their creators’ control , and may thus cause harm fo r which we cannot justly hold them responsible.  Yet Matthias argues that because we                                                                                                                                                                           te chnologies are conceived of as a tool employed by humans so that responsibility for fault will always re side with humans (be they programmers, coders, manufacturers, or developers, users etc): Johnson 2006; Brys on 2010; Sullins 2005.    Others have responded by considering AI to signal the instantiation of some mo ral or legal person of independent ontological status (eg Gunkel 2017) including the ascription of moral ag ency to computational systems (Dennett 1997; Sullins 2005).  However, the weight of academic opinion d enies that non-human entities can have moral responsibility in their own right because they lack the me ntal qualities (and hence cannot meet the epistemic condition) generally accepted as necessary for moral re sponsibility, which – at least in the philosophical literature, are often expressed in terms of intentionality, th e capacity to act voluntarily and an awareness of their actions and anticipated consequences of those ac tions: Johnson 2006; Kuflick 1999; Sparrow 2007; Asaro 2014 and Hanson 2009: 93.   
DGI(2019)05 5 5 cannot do without such systems, we must find a way to ‘address the responsibility gap in m oral practice and legislation. ’180  (b)  Choice-based theories of moral responsibility M atthias’s claim that those who create autonomous machines cannot be ‘justly’ held respo nsible for their actions rests on a ‘choice -based’ account of moral responsibility which h as tended to dominate contemporary academic reflection concerning the ethical and moral i mplications of AI.  According to choice-based accounts of moral responsibility, conduct rightly att racts blame when it is at fault, fault being understood in terms of being freely chosen.181  On thi s account, an agent (X) is only morally responsible for an unwanted outcome (Y) if X ‘caused’ Y.   To establish that X caused Y, then X must have engaged in conduct for which X can be held cau sally responsible.  Establishing this causal link requires that X voluntarily chose to engage in the  relevant conduct, even if that conduct turns out to have consequences and effects that X d id not intend or want.  According to Matthias, because the developers of computational ag ents which have the capacity to make their own decisions in ways that have not been pre-p rogrammed in advance by human developers, those developers lack the requisite degree of co ntrol and therefore are not morally responsible for the decisions of those computational ag ents or their consequences.182T he validity of the claim that the capacity for computational agents to act autonomously b reaks the chain of causation between the acts of their developers and the decisions taken by tho se agents is highly debatable.183  As a preliminary matter, it is important to recognise that cho ice theories of moral responsibility are particularly unsuitable as a model for identifying respo nsibility for human rights violations .  It is inherent in the nature and concept of rights g enerally, and human rights in particular, that they protect values of such fundamental i mportance that any interference with them attracts responsibility per se, without proof of f ault.184 Consider again the example of Facebook’s removal of the iconic image of the Vi etnamese girl in 2016.  In circumstances where national legislation imposes legal obligations o n both state and non-state actors to respect human rights, Facebook would be regarded as l egally responsible for violating the right to freedom of expression, without the need to d emonstrate that it had the capacity to control whether the image was removed.  In other wo rds, a violation of the right to freedom of expression occurred even if the decision to take-it d own had been taken by an automated algorithmic system acting independently without d irect human intervention, and even if the human designers of the automated system had not i ntended or foreseen that the specific image in question might be automatically removed.   3 .4 Models for allocating responsibility  Al though the model of responsibility that applies to human rights violations  is widely u nderstood as one of ‘strict responsibility’, without the need for proof of fault, the allocation o f obligations of repair for tangible harm to health or property, may be legally distributed in acco rdance with a variety of responsibility models.  Because AI systems might operate in ways that  result in both human rights violations and harm to individuals and/or property, and                                                            180   Matthias 2004: 183. 181   Wallace 1994, cited by Cane 2002. 182   Matthias 2004.  For a recent affirmation, see Gunkel 2017. 183   Ascribing causal responsibility to some action or event is an interpretive act and not a matter of scientific ‘t ruth’ per se.   184   See UN Special Representative of the Secretary General 2011 (the ‘Ruggie Principles’)  
Council of Europe Study 5 6 because the allocation of historic responsibility for harm serves as a guiding function to those i nvolved in the design, development, production and implementation of AI systems by speci fying the nature and scope of their obligations, these models are briefly outlined in the fo llowing discussion.  The variety of legal models that might be applied to allocate and d istribute the adverse effects arising from our other-regarding conduct clearly demonstrates that  it is a mistake to expect one single model of responsibility to apply fairly to all the d ifferent kinds of adverse consequences that might flow from the use of advanced digital te chnologies.    As previously noted, unlike philosophical analysis of responsibility, which tend to  focus on agents at the expe nse of ‘victims’ and of society , legal models of responsibility185are  relational in the sense that they are concerned not only with the position of individuals who se conduct attracts responsibility (i.e. moral agents), but also with the impact of that co nduct on other individuals and on society more generally.186 As legal scholar and philosopher P eter Cane has observed R esponsibility is not just a function of the quality of will manifested in conduct, nor th e quality of that conduct.  It is also concerned with the interest we all share in s ecurity of person and property, and with the way resources and risks are d istributed in society.  Responsibility is a relational phenomenon.187   In  other words, legal responsibility emphasises the relationship between moral agents, moral p atients and society more generally, rather than focusing exclusively on the conduct of moral ag ents and whether that conduct justly attracts responsibility.   Accordingly, academic analysis o f the variety of ways in which national legal systems allocate responsibility for conduct that cau ses harm or other adverse events (including rights violations that may or may not result in h arm) demonstrate how each of these models entails a different balancing of interest between m oral agents and moral patients (or ‘victims’ are they are typically referred to in legal scho larship).188  This discussion does not, however, seek to evaluate whether current legal ap proaches adopted within national legal systems adequately allocate responsibility for harm thro ugh the application of national civil liability rules, particularly given the capacity of national l aw to allocate historic responsibility for harms and wrongs by AI systems is yet to be fully te sted via litigation.189  Instead, the following discussion briefly outlines four broad models of respo nsibility reflected in Anglo-American legal systems, notably (1) intention/culpability-b ased models (2) risk/negligence-based models (3) strict responsibility and (4) mandatory i nsurance schemes,190 as exemplars of different ways in which legal responsibility for risks,                                                            185   The concept of ‘responsibility’ is used much more commonly outside the law to refer to ‘human conduct an d consequences thereof that trigger such responses’ so that we tend to speak of ‘moral responsibility’ on th e one hand and ‘legal liability’ on the other, with the latter referring primarily to formal institutionalised i mposts, sanctions and penalties which are characteristic of law and legal systems but not of morality: Cane 2002: 1 -2. 186   Cane 2002: 4-5. 187   Cane 2002: 109. 188   The European Commission is currently undertaking reviewing these issues.  See for example European Co mmission 2018c. 189   Various bodies are working on seeking to evaluate the capacity of national civil liability rules to respond ad equately to harm arising from the operation of AI systems.  For example, the European Commission i ntends to produce guidance in mid-2019 to address the way in which the EU Product Liability Directive ap plies to artificial intelligence, robotics and the Internet of Things: European Commission 2018c. 190   This study identifies various models of responsibility utilised in Anglo-American legal systems for the simple re ason that the author of this report has been trained in and is most familiar with the Anglo-American legal s ystem.  This should not be taken as an indication that these models are representative of responsibility mo dels reflected in other legal systems, or are in any way superior to models adopted elsewhere. 
DGI(2019)05 5 7 human rights violations and collective harms might be distributed.191  They are intended m erely as heuristics aimed at highlighting the range of potential models of responsibility that m ight be used to allocate and distribute threats, risks and harms associated with the use of ad vanced digital technologies.192  These sketches therefore selectively describe the what I will refer  to as the ‘control/conduct condition ’ and the ‘epistemic condition’, applicable to each m odel, rather than providing a complete and detailed account of each model’s content and co ntours.  Taken together, they reveal how each model strikes a different balance between o ur interest, as agents, in freedom of action and our interest, as victims, in rights and interests i n security of person and property.193 It suggests that identifying which (if any) of these models i s most appropriate for allocating and distributing the various risks associated with the o peration of advanced digital technologies is by no means self-evident194, but will entail a soci al policy choice concerning how these burdens should be appropriately allocated and d istributed.   3 .4.1 Intention/culpability-based models In tention/culpability-based models, which constitute the core model of responsibility that u nderpins the criminal law, focus primarily on the voluntariness of the agent’s conduct .  They can  be interpreted as requiring the satisfaction of two conditions: firstly , the ‘control’ co ndition, demonstrating that the agent was causally responsible for the legally proscribed co nduct in so far as the agent had a free and voluntary choice concerning whether so to act, an d secondly, the ‘epistemic condition ,’ requiring proof of ‘fault,’ broadly understood as requ iring that the agent had actual knowledge and awareness of the particular facts sur rounding the harmful consequences of the agent’ s conduct, and the agent’s action can be u nderstood as based on these facts.195  It is an intention/culpability-based model of respo nsibility that underpins the choice-based accounts of moral responsibility that have p redominated in philosophically-oriented discussions concerning whether the human d evelopers of autonomous computational agents are morally responsible for the actions of tho se agents.  For the time being at least, because computational agents lack the capacity for sub jective knowledge, awareness and intent, these responsibility models cannot be readily ap plied to computational agents per se because they cannot satisfy the requisite epistemic c ondition.196  Intention/culpability-based models can, however, be applied to the human d evelopers or users of such computational agents.  The conduct of individuals who                                                            191   According to the European Parliament ’ in its Draft Motion on the Civil Liability Rules on Robotics, ‘civil l iability for damage caused by robots is a crucial issue which also needs to be analysed and addressed at Un ion level in order to ensure the same degree of efficiency, transparency and consistency in the i mplementation of legal certainty throughout the European Union for the benefit of citizens, consumers and bus inesses alike’: European Parliament Committee on Legal Affairs 2016:  16. 192   In Anglo-American legal systems, the distinction between the civil and criminal law is of critical importance.  T he primary purpose of the criminal law is to impose penalties and punishments on those who engage in c riminal conduct, and hence the paradigm of c riminal liability focuses primarily on the alleged offender’s c onduct and mental state.  In contrast, the primary purpose of the civil law is to identify and allocate legal o bligations of repair on those identified as legally responsible for the relevant harm.  Accordingly, re sponsibility in civil law is two-sided, concerned not only with agent-conduct, but also with the impact of th at conduct on others.  The operation of the civil and criminal law paradigms of cut-across fault based, neg ligence-based and strict responsibility models, and the distinction between the civil and criminal law p aradigms are not further discussed.  For an extensive discussion, see Cane 2002. 193   Cane 2002: 98. 194   Danaher 2016. 195   In Anglo-American law, the mental elements of legal fault criteria are intention, recklessness, kn owledge/belief and malice.  See Cane 2002: 79.   196   Hildebrandt 2013; Himma 2009; Solum 1991; Gless et al 2016; Andrade et al 2007.  
Council of Europe Study 5 8 intentionally develop or deploy AI technologies for dangerous or malicious purposes, for ex ample, in order to commit fraud or misappropriate property, would clearly satisfy the requ irements for establishing responsibility under an intention/culpability-based model.  In these  circumstances197, a prima facie violation of human rights would arise (proof of subjective i ntent could be shown, but there would be no need to do so because legal responsibility for ri ghts violations is typically ‘strict’)  and would also be likely to generate both responsibility u nder the criminal law for offences against the person (or property) as well as triggering civil l aw obligations of repair and restoration.   3. 4.2 Risk/Negligence-based models In  Anglo-American law, risk/negligence-based models of legal responsibility for tangible harm fo rm the basis of a general duty to take reasonable care to prevent foreseeable risks of harm.  These  models of responsibility are conventionally applied to determine whether agents are sub ject to legal obligations of repair towards those who have suffered harm as a result of an ag ent’s failure to discharge this general duty of care.  A ‘control condition’ similar to that whi ch applies to intention/culpability-based models of responsibility also applies to ri sk/negligence-based models (with some modification198), insofar as it must be shown that the ag ent caused the relevant damage or injury.  However, the epistemic condition applicable to ri sk/negligence-based models is considerably less demanding than those applicable to i ntention/culpability-based models.  For example, legal liability in negligence under Anglo-Ame rican law does not require proof of the agent ’s accompanying mental state, thereby see king to strike a fair balance between the interest of agents (in freedom of action) and the i nterests of victims in safety and security.  As legal philosophers have emphasised, in order to h old an agent morally responsible, the agent need not in fact have actual subjective kno wledge of the consequences of her behaviour in order to be justly held responsible for it.199  A s John Oberdiek explains, facts matter morally: they are endowed with a normative force that b ears upon the permissibility of prospective action but only once they have been reasonably d iscoverable.200  In deciding upon a course of action, Oberdiek points out that the ordinary p erson can be morally expected to take ‘reasonable epistemic care’: she cannot be expected to  know all the facts, but nor can she stick her head in the sand and fall back on her subjective u nderstanding if she has failed to take reasonable care to find out or discover the relevant facts.   Acco rdingly, whether or not responsibility based on a risk/negligence model can be ascribed to the  human developers of computational agents and systems in circumstances where those sy stems generate decisions or behaviours that cause harm will depend upon whether that h arm was a reasonably foreseeable consequence  of the computational systems’ actions and d ecisions.  In Anglo-American negligence law, legal responsibility for causing harm is only ascri bed to those who are subject to a legal duty of care. Such a duty arises when, very broadly speaki ng, there is a reasonably foreseeable risk that an action could harm a proximate person.                                                            197   The use of  AI technologies in the commission of a crime might appropriately be regarded as an aggravating fac tor in the commission of a criminal offence: see 6 2002.  See also Hallevy 2015.   198   Causation in negligence may be negated by the application of principles of ‘remoteness of damage’: Horsey an d Rackley 2015, chapter 9. 199   Hart 1968. 200   Oberdiek 2017: 57. 
DGI(2019)05 5 9 Foreseeability therefore operates both to define the kinds of risks for which a person may be l egally responsible, and bounds the harms for which they may be liable.201  Reas onable foreseeability also plays a role in determining how a person is expected to act. The d uty of care is discharged if a person acts as an ordinary person exercising reasonable care to av oid foreseeable risks.202 Hence, reasonable foreseeability operates as the touchstone for d etermining the relevant ‘reference class’ for  evaluating whether risk-taking activities (such as d riving) that may result in tangible harm to others gives rise to a legal duty of care. As Oberd iek observes, this common law standard is a just and appropriate moral standard b ecause, in the case of risk-taking activities, it is important that we should be able to hold each o ther accountable for our respective characterisations of risk.   In other words, we must be ab le to justify that characterisation in a way that withstands moral scrutiny.203  Ye t in order to identify whether or not it is reasonably foreseeable that any given risky action m ight ripen into harm, we encounter the so- called ‘reference class’ problem.  As O berdiek ex plains:  th e reference class problem is…essentially a problem of redescribabi lity – any p articular risk can be infinitely re- described…there is no uniquely correct ge nerative reference class that credible beliefs take as their object.204  Fo r example, consider the fatal injury caused by an Uber vehicle which it collided with a woman  pushing a bicycle with shopping bags hanging from its handlebars in 2018.  The vehicle h ad been operating in self-driving mode for 19 minutes before it mistook the woman for a car (w hich it therefore expected would take evasive action), only recognising its mistake and h anding back control to the vehicle’s human driver seconds before collision, which  the human d river was not able to prevent.205 It seems unlikely th at the car’s developers could have reaso nably foreseen that the vehicle’s AI sensing system would mistakenly believe that a woman  pushing a bicycle with shopping bags dangling from its handlebars was another v ehicle.  On the other hand, it seems well within the bounds of reasonable foresight that the car’s  sensing technologies would fail correctly to classify unusually shaped objects enco untered during normal driving conditions, and that errors of this kind might lead to fatal co llisions.    At  the same time, identifying whether particular events associated with the operation of a p articular technological object is ‘reasonably foreseeable’ will invariably be a product of our ex perience and exposure to them.  In the emerging phases during which a new technology is b eing rolled out, expectations of their behaviours (and consequences) will be relatively u nsettled and unknown.206  However, as time passes and we become more accustomed to                                                            201   When it is an omission or failure to act that causes harm, these criteria manifest in a range of particular w ays, such as one has a duty to protect others from risks that arise as a result of one creating a source of da nger or because one has assumed responsibility for the other person’s interests .   S e e  L u n n e y  a n d  O liphant 2013, chapter 9. 202   Oberdiek 2017: 40. 203   Oberdiek 2017: 48.   204   Oberdiek 2017: 40. 205   Smith 2018. 206   For example, Microsoft’ experimental Tay chatbot was designed to learn to converse in human c onversational terms by observing and interacting with Twitter users, improving its performance via c onversational interaction and in so doing learning how AI programmes can engage with web users in casual c onversation.  Instead, it quickly learned to parrot a slew of anti-Semitic and other hateful invective that h uman Twitter users fed the bot, resulting in Microsoft’s decision to shut the chatbot down.  This kind of re sponse was not in fact anticipated by Tay’s developers, yet it could persuasively be argued that responses 
Council of Europe Study 6 0 their patterns of behaviours and action, those behaviours and actions may become more fam iliar to developers and therefore more likely to be regarded as reasonably foreseeable. T herefore, developers of those technologies should be held responsible for negligently failing to  take steps that would have averted the resulting harm and wrongdoing.207  Yet even then, thi s begs the question about our reasonable expectations of the tech industry in making the d ecision to release emerging technology into real world contexts: we rightly implement d emanding governance regimes for new pharmaceuticals, is this not also true of risky ad vanced digital technologies?208Ad ditional questions arise concerning the minimum standard of care which AI system d evelopers should be responsible for attaining in the design and implementation of au tonomous computational systems.  Consider again the fatal collision of the Uber vehicle whi ch misclassified a pedestrian wheeling a bicycle as an approaching vehicle.  In co ntemporary discussions, a common refrain is that autonomous cars will be ‘safer’ than h uman drivers, thereby suggesting that the relevant comparator is that of a reasonable human d river.  But is it appropriate to apply a model of responsibility and the same standard of care that  we apply to an ordinary human vehicle driver operating a traditional human-directed car to  that of unintended harm resulting from the actions of a self-driving car?   Or is it more ap propriate to apply the model of responsibility which conventionally applies to product m anufacturers to govern the development and operation of self-driving vehicles, which, in co ntemporary European law systems, is a model of strict responsibility for product defects (di scussed below)?  In other words, there are important policy choices to be made and it is by n o means self-evident that the standard of the ordinary human driver provides the most sui table comparator.209    3. 4.3 Strict responsibility  As  this study has already noted, the model of legal responsibility applicable to rights violations (i ncluding violations of human rights and fundamental freedoms) is that of strict responsibility, o r ‘strict legal liability’ as it is called in Anglo -American legal parlance.  On this model, respo nsibility attaches to the agent without proof of fault, so that legal responsibility for rights v iolations attaches to those who cause them regardless of whether the responsible agent eng aged in conduct that breached a legally specified standard of conduct, and regardless of whet her the conduct was intended or accompanied by any particular mental state.210  Of the fo ur varieties of strict liability identified by Cane, three are of direct relevance to this study: ri ght-based, outcome-based and activity-based strict liability. (a)  right-based strict liability: arises when legal rights are violated such that any violation of the  sphere of protection bounded by the right triggers liability.  The classic example is tres pass to land: by interfering with the land- owners’ right to exclusive dominion over the l and, all intrusions without the consent of the land-owner constitute unlawful interference ev en if it the intruder was in no sense blameworthy.  As already noted, violations of h uman rights fall into this category of cases.                                                                                                                                                                           o f this kind were reasonably foreseeable, given the volume and frequency with which offensive posts are mad e online via Twitter. See The Guadian 2016. 207   Liu and Zaweiska 2017. 208  Nemitz 2018; Thomas 2017a; Thomas 2017b.  209   Thomas 2017b.  210   Cane 2002: 82. 
DGI(2019)05 6 1 (b) outcome based strict liability: this form of liability rests on the causation of adverse o utcomes (i.e. extrinsic consequences) regardless of fault.  Contemporary European p roduct liability laws are based on this model which imposes strict liability on m anufacturers for defective products that cause harm to natural persons or property.211  In rel ation to advanced digital technologies, questions arise concerning what constitutes a rel evant ‘defect’.  Consider again the fatal collision of the Uber vehicle which initially m isclassified a pedestrian wheeling a bicycle as another vehicle, handing back control to the  human driver as soon as it recognised its error but, however, too late for the human d river to prevent the collision.  It could be argued that in these circumstances, the vehicle was  not ‘defective’ in so far as it functioned in precisely the way that its developers i ntended.  On the other hand, if ‘defective’ is interpreted to mean ‘fit for purpose’, then the  vehicle’s failure to correctly classify the pedestrian and take evasive action to avoid the fatal  collision could readily be characterised as defective.212  A similar approach is often ap plied where the risk of damage is linked to the unpredictability of behaviour of specific ri sk groups, such as animals. In these cases, liability is attributed to the persons that are co nsidered responsible for supervising the animal, as they are typically regarded as best p laced to adopt measures to prevent or reduce the risk of harm. (c)  activity-based strict liability arises in connection with a specified activity, such as various ‘ possession’ offences, such as laws which prohibit the possession of guns, knives, illicit sub stances and so forth.  In Anglo-American law, vicarious liability is an important form of acti vity-based strict liability, where the relevant activity is defined primarily in terms of a rel ationship with another person, for whose breach of the law the first person is held stri ctly liable by virtue of that relationship.  Vicarious liability applies to the employment rel ationship, such that an employer will be strictly liable for the unlawful conduct of an em ployee carried out in the course of his or her duty.  Some jurisdictions may adopt a stri ct liability approach towards those who carry out dangerous activities (e.g. the operator o f a nuclear power plant or of an aircraft) or are ultimately responsible for the dangerous acti vity (e.g. the owner of a vehicle). In such cases, the underlying rationale is that this p erson has created a risk, and at the same time also derives an economic benefit from this acti vity.213      These  various forms of strict liability distribute the risks associated with potentially harmful acti vity between agents and victims in ways that accord considerable weight to the interests of v ictims in security of the person and property.  In so doing, they recognise that responsibility is n ot merely a function of the quality of an agent’s will manifested in conduct, nor the quality of that  conduct: it is also concerned with the interest we all share in security of person and p roperty, and with the way resources and risks are distributed in society, thereby delineating the bo undaries of what our responsibilities are.214  3 .4.4 Mandatory Insurance  Rather  than focus on allocating responsibility to potential candidates who can be understood as  contributing to the harms and wrongs that might arise from the operation of advanced d igital technologies, a society might decide instead to prioritise the need to ensure that all                                                            211   See European Union 1985.  212   Strict liability for damage caused by autonomous robots was favoured by the European Parliament’s draft mo tion on Civil Law Rules on Robotics: European Parliament Committee on Legal Affairs 2016. 213   European Commission 2018b. 214    Cane 2002:108-109. 
Council of Europe Study 6 2 those who are harmed by the operation of these technologies should be financially co mpensated.  This may be achieved by instituting some kind of mandatory insurance scheme (w hich could be established on a ‘no -fault’ basis), establishing an insurance fund to which all tho se harmed by the operation of these technologies could have recourse.215  Such a scheme m ight be funded in various ways, including via contributions from the tech industry, but with cl aims administered by some independent or public authority. One could also simply require fi rms involved in the value chain through which these advanced digital systems are designed an d implemented to take out mandatory liability insurance.216  While it is beyond the scope of thi s study to evaluate the desirability of such schemes, they have the benefit of enabling those h armed from the operation of such technologies to seek financial compensation in ci rcumstances where it is difficult to identify precisely which firms ought to be regarded as respo nsible for the harm, or if the relevant firms have become insolvent.  This may become i ncreasingly important as we become more reliant on autonomous intelligent systems which co ntinue to operate long after their human or corporate developers and owners have died or cease d to exist, so that societies may need to develop long-stop institutions such as collective i nsurance in order to ensure that victims are not systematically left uncompensated.217  P roposals to confer legal status on intelligent machines in order to facilitate the administration o f compensation payments to injured victims have been proposed in this context.2183 .5 Responsibility challenges posed by complex and dynamic socio-technical systems The  preceding analysis has proceeded largely on the assumption that, in seeking to assign respo nsibility for adverse consequences of advanced digital technologies, cause-effect rel ations can be readily identified.  In practice, however, these technologies form an essential co mponent of highly complex and sophisticated socio-technical systems, generating acute cha llenges in seeking to identify lines of causal, moral and legal responsibility.  Three such cha llenges are briefly outlined in the following discussion : the problem of ‘many hands’, ‘h umans in the loop’ and the unpredictable effects of complex dynamics that can arise b etween multiple interacting algorithmic systems. 3 .5.1 The problem of ‘many hands’  Exc ept in relation to some forms of strict responsibility, the assignment of responsibility for the  threats, risks, harms and rights violations (including human rights violations) typically                                                            215   The European Parliament’s Committee on Legal Affairs recommended a scheme of this kind for harm caused b y specific categories of robots, recommending that an obligatory insurance scheme, which could be based o n the obligation of the producer to take out insurance for the autonomous robots it produces, should be e stablished,  be supplemented by a fund in order to ensure that damages can be compensated for in cases w here no insurance cover exist: European Parliament Committee on Legal Affairs 2016 at 20.  216   European Commission 2018b. 217   6:2001 at 429. 218   For example, the  European Parliament’s Committee for Legal Affairs called on the European Commission to c onsider creating a specific legal status for robots in the long run, so that at least the most sophisticated a utonomous robots could be established as having the status of electronic persons responsible for making g ood any damage they may cause, and possibly applying electronic personality to cases where robots make au tonomous decisions or otherwise interact with third parties independently : European Parliament Co mmittee on Legal Affairs (2016) at 18.  The European Parliament’s Policy Department for Citizen’s Rights an d Constitutional Affairs (the JURI Committee) has emphatically opposed this particular proposal: Nevjans 2 016 at 14- 16.  These proposals are separate and distinct from academic discussion concerning whether or n ot robots should be regarded as moral agents and entitled to moral rights protection.  An examination of th e appropriate legal and moral status of AI agents as independent agents in their own right is beyond the s cope of this study.  See Solum 1991; Koops 2010; Teubner 2006; Teubner 2018.   
DGI(2019)05 6 3 require an assessment of whether they can be understood as caused by the agent.    Yet when see king to assign causal responsibility for some adverse event219 or effect that could plausibly b e regarded as a direct consequence of the operation of any complex socio-technical system (w hether or not it utilises AI technologies), one immediately encounters the ‘many hands’ p roblem.220 This problem arises if one adopts an intention/culpability based model of respo nsibility.  First identified in the context of information technology by philosopher of te chnology, Helen Nissenbaum,221 the problem of ‘many hands’ is not unique to computers, d igital technology, algorithms or machine learning.  Rather, it refers to the fact that a complex arr ay of individuals, organisations, components and processes are involved in the d evelopment, deployment and implementation of complex systems, so that when these sy stems malfunction or otherwise cause harm, it becomes very difficult to identify who is to b lame, because such concepts are conventionally understood in terms of individualistic co nceptions of responsibility.222 In other words, causal responsibility is necessarily distributed where  complex technological systems are concerned, diluting causation to mere influence.223   The  ‘many hands’ problem may be especially acute in seeking to identify the locus of resp onsibility for harms or wrongs resulting from the development and operation of AI sy stems, given that they rely on a number of critical components, namely (a)  The models that are developed in order to represent the feature space and the o ptimisation goal which the system is intended to achieve; (b)   algorithms, based on these models, which analyse the data to produce outputs whi ch may trigger som e kind of ‘action’ or decisio n; (c)  The input data (which might or might not include personal data) on which those al gorithms are trained; (d)  The human developers involved in the design of these systems, who must make v alue-laden decisions about the models, algorithms and data that are used to train the  algorithms upon which performance is tested. They include human beings who u ndertake the task of labelling the data that is used to train the algorithms224; and (e)  The larger socio-technical system and context  in which the algorithmic system is em bedded and in which it operates. Ev en assuming that we could satisfactorily identify the allocation of moral responsibility for ad verse impacts in relation to each of the above components, this is unlikely to ensure that l ines of moral responsibility for unintended adverse consequences can be readily identified when  they are dynamically combined within a complex integrated system.  These challenges are  compounded by the fact that digital products and services are open to software ex tensions, updates and patches after they have been implemented.   Any change to the so ftware of the system may affect the behaviour of the entire system or of individual co mponents, extending their functionality, and these may change the system’s operational risk                                                            219    The relevant adverse event might be some systemic risks/harm, individual harm or an individual human ri ghts violation not necessarily entailing material loss or damage or harm to collective interests. 220   Thompson 1980. 221   Nissenbaum 1996. 222   Thompson 1980. 223   Liu and Zaweiska 2017. 224   Zalnieriute et al 2019. 
Council of Europe Study 6 4 profile, including its capacity to operate in ways that might cause harm or violate human ri ghts.225    In  responding to these challenges, it may be helpful to bear three considerations in mind.  Fi rstly, issues relating to the allocation of legal responsibility for harm arising from activities i nvolving multiple parties are not new, and many legal systems have therefore developed a rel atively sophisticated set of principles and procedures for determining liability where m ultiple potential defendants are involved.226  As the European Commission has recently o bserved, identifying the distribution of liability for redress amongst multiple actors involved i n the value chain through which emerging digital technologies operate may not be relevant fo r the purposes of ensuring that victims obtain compensation for damage suffered, although reso lving such questions is likely to be important from an overall policy standpoint in order to p rovide legal certainty to those involved in the production and implementation of these te chnologies.227  Secondly, and relatedly, the law’s ability to devise practical responses despite the  apparent intractability of the many hands problem can be at least partly attributed to the g reater emphasis which it places on the legitimate interests of the moral patient in security of the  person, rather than the almost exclusive focus on the moral agent that is reflected in cho ice theories of moral responsibility (and upon which the ‘many hands’ probl em rests).   Thi rdly, because the focus of this report is on responsibility for human rights violations  arising fro m the development and implementation of advanced digital technologies, rather than on respo nsibility for harm, it is particularly important to ensure that we have effective and l egitimate mechanisms that will operate to prevent and forestall  human rights violations, p articularly given that many human rights violations associated with the operation of ad vanced digital technologies may not result in tangible harm to individual health or property.  The  need for a preventative approach is especially important given the speed and scale at whi ch these technologies now operate.  The resulting cumulative and aggregate effects of h uman rights violations caused by the operation of AI systems could seriously erode the social fo undations necessary for moral and democratic orders that are essential preconditions for h uman rights to exist at all, suggesting that existing approaches to human rights protection m ay need to be reinvigorated in a networked, data-driven age.2283 .5.2 Human-Computer Interaction N ot only are many individuals, firms and other organisations involved in the development and i mplementation of advanced digital technologies, but these technologies are often intended to o perate in ways that involve retaining active human involvement.229  This points to serious cha llenges associated with identifying the appropriate distribution of authority and                                                            225   Thomas 2015. 226   See for example models of shared responsibility for liability of on-line hosting platforms: eg De Streel, Bui ten and Peitz 2018; Helberger et al 2018. 227   European Commission 2018b: 20-21 228   See section 3.8 below. 229   Human oversight may be achieved through governance mechanisms such as a human-in-the-loop (HITL), h uman-on-the-loop (HOTL), or a human-in-command (HIC) approach. HITL refers to the capability for h uman intervention in every decision cycle of the system, which in many cases is neither possible nor d esirable. HOTL refers to the capability for human intervention during the design cycle of the system and mo nitoring the system’s operation. HIC refers to the capability to oversee the overall activity of the AI s ystem (including its broader economic, societal, legal and ethical impact) and the ability to decide when an d how to use the system in any particular situation. This can include the decision not to use an AI system i n a particular situation, to establish levels of human discretion during the use of the system, or to ensure th e ability to override a decision made by a system: see EU High Level Expert Group 2019a: 16 
DGI(2019)05 6 5 responsibility between humans and machines, given the complex interaction between them.  In  particular, many tasks previously performed by humans are now undertaken by machines, y et humans are invariably involved at various points throughout the chain of development, te sting, implementation and operation.  As the Royal Academy of Engineering  has observed: T here will always be humans in the chain, but it is unclear in the case of injury w hich human in the chain bears responsibility – the designer, manufacturer, p rogrammer, or user.230  The  interaction between humans and machines within complex and dynamic socio-technical sy stems generate especially challenging questions concerning the appropriate role of humans i n supervising their operation.  One recurring theme has been a concern that, in order to ensu re that increasingly complex socio-technical systems always operate in the service of h umanity, systems should always be designed so that they can be shut down by a human o perator.  Yet, as the Royal Academy of Engineering has again observed: It  might be thought that there is always need for human intervention, but s ometimes autonomous systems are needed where humans might make bad ch oices as a result of panic – especially in stressful situations – and therefore the h uman override would be problematic.  Human operators are not always right nor d o they always have the best intentions.  Could autonomous systems be trusted m ore than human operators in some situations?231  On  the other hand, even if humans are retained ‘in the loop’ with the aim of supervising co mputational systems, individuals placed in these positions may be understandably reluctant to  intervene.  Over a decade ago, Johnson and Powers232 commented: I n the case of future automated air traffic control…there will be a difficult question ab out whether and when human air traffic controllers should intervene in the comp uter-control of aircraft…Those humans who formerly held the role re sponsibility for the duties will either by replaced by caretakers of the technology, o r will themselves become caretakers.  A concern in this environment is that the h umans assigned to interact with these ‘automatic’ systems may perceive i ntervention morally risky.  It is better, they may reason, to let the computer s ystem act and for humans to stay out of the way.  To intervene in the behaviour of au tomated computer systems is to call into doubt the wisdom of the system d esigners and the ‘expertise’ of the system itself.  At the same t ime, a person who ch ooses to intervene in the system brings the heavy weight of moral responsibility u pon him or herself, and hence human controllers will have some incentive to let th e automaticity of the computer system go unchallenged.  This is a flight from r esponsibility on the part of humans, and it shows how responsibility has been re-ass igned, in some sense, to the computer system.233  Y et, as we increasingly rely on the expanding range of services and systems that automation m akes possible, particularly as our digital technologies grow ever more powerful and so phisticated, continued insistence on placing a human in the loop to act in a supervisory cap acity risks turning humans placed in the loop into ‘moral crumple zones’ –  largely totemic h umans whose central role becomes soaking up fault, even if they only had partial control of                                                            230   Royal Academy of Engineering 2009: 2. 231   Royal Academy of Engineering  2009: 3. 232   Johnson and Powers 2005: 106. 233   On the question of how far humans can responsibly transfer decision-making functionality to computer w ithout at the same time reserving oversight-responsibility to humans, see Kuflik 1999. 
Council of Europe Study 6 6 the system, and who are vulnerable to being scapegoated by tech firms and organisations see king to avoid responsibility for unintended adverse consequences.234  As Elish and Twang’s stud y of aviation autopilot litigation highlights, modern aircraft are now largely controlled by so ftware, yet pilots in cockpits remain legally responsible for the aircraft’s operation.  Yet our cul tural perceptions tend to display ‘automation bias’, elevating the reliability and infallibility o f automated technology whilst blaming humans for error (see Box 2).235   B ox 2:  Automation bias and the responsibility of humans in the loop The  collision of a Tesla car in semi-automated mode exemplifies the tendency to blame the p roximate humans in the loop for unintended adverse consequences, rather than the sur rounding socio-technical system in which the human is embedded.   A  semi-automated Tesla collided with a truck in May 2016 due to the vehicle’s autopilot’s fai lure to detect the truck.  The official investigation following the collision revealed that al though the autopilot functioned as designed, it did not detect the truck. The human failed to respo nd, with the investigation concluding that the driver had over-relied on automation and the  monitoring steering wheel torque, which were not effective methods for ensuring driver eng agement.  The  authority undertaking the investigation concluded that the crash was not the result of any speci fic defect in the autopilot system, so that Tesla was not responsible for the accident.  Because  Tesla had provided an adequate warning to customers, indicating that the autopilot sy stem must be operated under the supervision of the human driver, and that the driver’s h ands should remain on the wheel and their eyes on the road, responsibility lay with the h uman driver.  In addition, Tesla’s Terms of Services included provisions that referred to the sem i-autonomous nature of the autopilot, stating that the driver was to take over the control o f the car in 4 seconds if the driver noticed problematic vehicle behaviour.  S ource: European Commission, Staff Working Docum ent, ‘Liability for Emerging Digital Technologies’ (Apr il 2018) 14-15.  3 .5.3 Unpredictable, dynamic interactions between complex socio-technical systems  E ven more intractable challenges arise in seeking to identify, anticipate and prevent adverse ev ents arise from the interactions between complex, algorithm-driven socio-technical systems that  can occur at a speed and scale that was simply not possible in a pre-digital, pre-networked                                                            234   Elish 2016.  235   Elish points to the tragedy of Air France Flight 447 in 2009 (which crashed into the Atlantic Ocean, killing all 228  people on board) as a classic example of the positioning of individual pilots as moral crumple zones.  T he flight had flown into a storm en route from Brazil to France, resulting in ice crystals forming on the p lane’s pitot tubes, part of the avionics system that measures air speed.  The frozen pitot tubes sent faulty d ata to the autopilot which, in turn, reacted in precisely the way in which it was designed to react in the ab sence of data: it automatically disengaged, transferring control of the aircraft back to the human pilots.  T he pilots were caught by surprise, overwhelmed by an avalanche of information – flashing lights, loud w arning signals, and confusing instrument readings, with t he official French report concluding that they ‘lost c ognitive control of the situation’, with a series of errors and incorrect manoeuvres by the pilots resulting in th e fatal crash.  Elish observes that news coverage of the accident report emphasised the pilots’ errors, but fai led to draw attention to the fact that many of these errors were at least partly due to the automation, by c hanging the very kind of control that can be exercised by a human operator, and by creating opportunities fo r new kinds of error: Elish 2016 ibid. 
DGI(2019)05 6 7 age.  The so-called ‘flash crash’ that occurr ed in 2010, during which the stock market went into freefal l for five minutes before correcting itself, for no apparent reason, provides a vivid i llustration.236  While individual AI agents, that have the capacity to learn from their env ironment and to iteratively improve their performance, might be subject to mathematical v erification and testing, identifying how multiple different algorithms might interact with other al gorithmic agents in a complex and dynamic ecosystem generates risks of unpredictable, and p otentially dangerous, outcomes.  In other words, these interactions generate risks that we h ave barely begun to grasp.237  The challenge of devising solutions that will enable us reliably to  predict,  model and take action to prevent unwanted and potentially catastrophic outcomes ari sing from the interaction between dynamic and complex socio-technical systems generates a  new and increasingly urgent frontier for computational research.  Leading computer sci entists Shadbolt and Hampson warn of the dangers of “hyper -complex and super-fast sy stems” generating considerable new risks, and for which:  Ou r response needs to be vigilant, intelligent and inventive.  So long as we are, we wi ll remain in control of the machines, and benefit greatly from them.  We need to d evelop policy frameworks for this.  Beyond the dangers, a world of opportunitya rises.2383 .6 State responsibility for ensuring effective protection of human rights  One  of the most significant concerns about the emergence of algorithmic systems has been the  increasing power of Big Tech firms, including concerns about the radical power asymmetry b etween these firms and the individuals who are subject to them.239  Accordingly, it is in the h ands of these firms that the power to deploy algorithmic systems overwhelmingly resides.  Ye t, the obligation to protect human rights in the international domain law lies primarily on n ation states, given that human rights protection is primarily intended to operate vertically, to p rotect individuals against unjustified interference by the state.  However, it is well established i n ECHR jurisprudence that the rights protected by the Convention ground positive substantive o bligations requiring member states to take action in order to secure to those within their ju risdiction the rights protected by the Convention.240  Accordingly, states are obliged under the  ECHR to introduce national legislation and other policies necessary to ensure that ECHR ri ghts are duly respected, including protection against interference by others (including tech fi rms) who may therefore be subject to binding legal duties to respect human rights.241  It is these  enforceable legal obligations, grounded in the Convention’s protection of human rights, i ncluding the right to an effective remedy, that offers solid foundations for imposing legally enfo rceable and effective mechanisms to ensure accountability for human rights violations, we ll beyond those that the contemporary rhetoric of ‘AI ethics’ in the form of voluntary self -r egulation by the tech industry can realistically be expected to deliver.242                                                           236   Akansu 2017.  237   Smith 2018. 238   Shadbolt and Hampson 2018. 239   Ibid,  Schwab et al 2018; The Economist 2018b. 240   Rainey, Wicks and Ovey 2014: 102. 241   The scope and extent of the required protection will depend upon the particular right in question. Ibid. 242   The Pilot Judgement Procedure of the European Court of Human Rights provides an institutional mechanism th rough which states can be directed to adopt individual remedial measures in their domestic legal orders in o rder to bring to an end violations found by the Court, supervised by the Committee of Ministers.  See Glas 2014 . 
Council of Europe Study 6 8 The discussion of various models for allocating historic responsibility outlined in the section 3.2 d raws largely on Anglo-American legal approaches introduced via legislation (and adjudicated o n by courts) or developed by courts in their interpretation and application of the common l aw in determining legal liability for harm or other wrongdoing.  One significant drawback asso ciated with reliance upon judicial remedies to redress these concerns is that they are b etter suited to remediating substantial harms suffered by the few, as opposed to less si gnificant harms suffered by the many.  The difficulties of seeking redress via the courts are m agnified in the AI space by the challenge of detecting the harm and determining and proving cau sation, to say nothing of the serious practical obstacles and disincentives faced by i ndividuals in invoking the judicial process.243  At the same time, the capacity of AI systems in a g lobally networked environment to generate collective action problems has already been h ighlighted, underlining the need for, and importance of, properly resourced national enfo rcement equipped with adequate enforcement powers authorities and which may also sug gest that accessible and convenient collective complaint mechanisms may be necessary to ensu re that enforcement action is taken in relation to human rights violations resulting from the  operation of AI systems.  At the same time, it is important to recognise that, in addition to co nventional legal mechanisms of redress via the courts, there are many other institutional g overnance mechanisms  that could help secure responsible human rights-compliant d evelopment and implementation of advanced digital technologies.  The following section theref ore provides a brief outline of other possible institutional governance mechanisms ( beyond ‘voluntary’ self -regulatory initiatives currently emerging) that may serve to enhance b oth prospective and retrospective responsibility for the threats, risks, harms and wrongs ari sing from the operation of advanced digital technologies.  It briefly outlines several possible m echanisms and governance institutions that might have an invaluable role to play in securing acco untability for human rights violations that could complement existing legal mechanisms.  3 .7 Non-judicial mechanisms for enforcing responsibility for advanced digital technologies Al though regulatory governance mechanisms can be classified in many different ways, three features  are worth highlighting for the purposes of this study.  Firstly, we can distinguish b etween mechanisms which operate on an ex ante basis, which provide oversight and ev aluation of an object, process or system before it has been implemented into real world set tings and are therefore primarily concerned with securing prospective responsibility. On the o ther hand, there are ex post mechanisms that operate during or after implementation has o ccurred and are therefore primarily concerned with securing historic responsibility.  As this stud y has already emphasised, both dimensions of responsibility must be attended to in order to  secure the responsible development and implementation of AI systems.  Yet because this stud y is primarily concerned with the human rights implications of these technologies, the n eed for effective and legitimate mechanisms that will prevent and forestall  human rights v iolations is of considerable importance, particularly given the speed and scale at which AI sy stems can now operate, combined with a culture of ‘move fast and break things’ that cha racterises the operational strategy of leading tech firms.  This strategy consists of forging ah ead with rapid technological innovation without attending carefully to their potential risks in ad vance, preferring to deal with any adverse ‘blow -back’ after the event by which time it may n ot be practically possible to unwind or roll-back the technological innovations that have al ready been brought to market.244  Secondly, it is important to attend to the legal enf orceability of regulatory governance institutions and mechanisms in order to identify whet her, and to what extent, they are to be regarded as optional mechanisms which the tech                                                            243   Mantelero 2018: 55. 244   Taplin 2018; Vaidhyanathan 2011. 
DGI(2019)05 6 9 industry has the freedom selectively to adopt or ignore altogether, or whether they are legally m andated and for which substantial legal sanctions attach for non-compliance.245  Thirdly, al though regulatory governance mechanisms have conventionally taken the form of social i nstitutions, in the present context, the role of technical protection mechanisms , which rely u pon a modality of control sometimes referred to as ‘regulation by design’246 may be equally (if n ot more) important. It is to these that this study now turns. 3. 7.1 Technical protection mechanisms One  of the most promising fields of research that has flourished in response to growing awarenes s of the ethical and legal concerns raised by the use of AI technologies, can be found i n the technical responses that have emerged with the aim of seeking to ‘hard -wire’ particular v alues into the design and operation of algorithmic techniques that are incorporated into AI sy stems.247  One of the features often associated with some of these ‘design -based’ regulatory g overnance mechanisms is their capacity to operate in real time, rather than on an ex ante or ex  post basis.248  Although early work in the field utilising technical measures to secure the p rotection of particular interests and values through the use of ICT focused primarily on te chnological solutions to the protection of IP rights249, parallel work also began to take place i n the field of data privacy, which became known as ‘privacy by design’ or ‘data protection by d esign’.  This work recognised that technology could be applied in the service of interests and v alues that it concurrently threatened, seeking to improve the bite of legal norms on IP rights an d data privacy by seeking to build the norms into information systems architecture.250  In ad dition to the work on ‘privacy engineering’, more recent research in machine learning and so ftware engineering can be understood as building on this approach, seeking to secure what m ay be called ‘human rights protection by design’  and include the following:   ( a) Explainable AI (XAI): Advances in machine learning techniques, including those relying on n eural networks (NN), are often used to aid human decision making,251 yet their logic is not easi ly explainable (i.e., when they opt for a particular choice, we do not know why they do so ) or readily interpretable (i.e., they cannot explain or present outcomes in ways humans can  understand).  There is growing recognition of the need to ensure that outputs g enerated by AI systems can be rendered intelligible to users252 and this has opened up a si gnificant field of computational research in ‘explainable AI’ (XAI).253   ( b) Fairness, Accountability and Transparency in Machine Learning (FATML):  Similarly, a g rowing community of ML researchers have directed their attention towards developing te chniques to identify and overcome problems of ‘digital discrimination’254 referring to bias an d discrimination arising from the use of data mining and other machine learning                                                            245   Nemitz 2018. 246   Yeung 2015. 247   Ibid. 248   Ibid. 249   These were originally referred to as ‘Electronnic Copyright Management Systems (ECMS) and later referred to  as Digital Rights Management Systems (DRMS).  250   Bygrave 2017. 251   See for example, Doshi-Velez, Ge, & Kohane, 2013; Carton et al 2016. 252   Weller 2017; Yeung and Weller 2018b. 253   See for example Samek et al 2017;  Wierzynski 2018.   254   Barocas and Selbst 2016; Criado and Such 2019 ; Zliobaite 2015.  
Council of Europe Study 7 0 techniques (known as ‘discrimination -aware’ or ‘fairness-aware’ techniques for machine l earning).255     3 .7.2 Regulatory governance instruments and techniques M ore conventional, social and organisational forms of regulatory governance instruments have al so emerged in response to recognition that AI technologies might be utilised in ways that co uld undermine important values, including those explicitly concerned with ensuring that these  technological systems operate in ways that respect human rights.  Two are briefly d iscussed here: human rights impact assessment and algorithmic auditing techniques.256( a) Algorithmic/Human rights impact assessment: Various scholars and organisations have p roposed various forms of ‘algorithmic impact assessment’ that are, in effect, proposed ri sk-assessment models that are to be applied by those seeking to procure or deploy al gorithmic systems in order to identify the human rights, ethical and social implications of thei r proposed systems, and to take steps to ameliorate those concerns in the design and o peration of algorithmic systems prior to implementation.  While various general impact assess ment models have been proposed, a number of domain-specific models have also b een proposed.257   These  risk assessment models vary widely in terms of their:  o  Criteria of assessment :  while EU data protection law now mandates the use of ‘Data P rotection Impact Assessments (DPIAs)258’ in certain circumstances, building on pre -ex isting approaches to ‘Privacy Impact Assessment’ they are largely focused on the ev aluation of impacts up on data quality and security.  Other models, such as ‘Human Ri ghts Impact Assessment259’ are concerned with  evaluating the impact of a proposed sy stem on human rights more generally.260o  Party undertaking the assessment: some proposed models are intended to be applied by the  data controller (eg DPIAs) while others propose that the assessment be undertaken by an  external third party or accreditation body, which is the approach reflected in the UN                                                            255   See in particular the annual event organised by FATML (h ttp://www.fatml.org/)  and resources listed at h ttp://www.fatml.org/resources/relevant-scholarship.   256   See UN General Assembly 2018 which endorses both techniques. 257   For example, in relation to the use of algorithmic decision-making systems by the pubic sector, see AI Now In stitute: 2018.  That Report outlines a framework for public sector entities in the US to use in carrying out ‘ algorithmic impact assessments’, prior to purchasing or deploying an automated decision. In relation to c riminal justice risk assessment, see Selbst 2018 and Oswald et al 2018.  In relation to the human rights risks fo r internet registries, see ARTICLE 19: 2017. 258   Article 35 of the EU General Data Protection Regulation (GDPR) requires the preparation of data protection i mpact assessments where data processing is likely to result in a ‘high risk’ to the rights and freedoms of n atural persons. 259   Various models of human rights impact assessment can be understood as more specific forms of ‘human r ights due diligence’, growing out of the UN Guiding Principles on Business and Human Rights, which uses th e term ‘due diligence’ as ‘the essential first step towarad identifying, mitigating and redressing the ad verse human rights impacts of AI’ per Rasso et al 2018: 53. See also Toronto Declaration on Machine L earning 2018.  260   Mantelero 2018. 
DGI(2019)05 7 1 Guiding Principles on Business and Human Rights in relation to ‘human rights due d iligence’261. o  Mandatory or voluntary adoption : Some algorithmic-human rights impact assessment p roposals are intended to be adopted on a voluntary basis, so that it is up to the data co ntroller to choose whether or not to undertake the assessment and what, if any, steps to  take in light of that assessment.262  Others, such as the DPIA, are mandated by law if cert ain threshold conditions are satisfied.263o  Scale of evaluation: While Human Rights Impact Assessment is concerned with scrutinising a  wide range of business operations to assess their conformity with human rights stand ards, other forms of impact assessment, such as the DPIA or PIA, are much narrower i n their scale of evaluation, focusing on a single data processing activity.  Im pact assessment techniques can be valuable in focusing attention on the various ways in whi ch a proposed activity may risk interfering with human rights,  in ways that might o therwise be overlooked or ignored.  Yet, in order for impact assessment approaches to p rovide real and substantive protection, it will be necessary to develop a clear and ri gorous methodological approach that firms and other organisations are willing to adopt co nsistently and in ways that reflect a genuine commitment to identifying human rights ri sks, rather than merely regarding them as a bureaucratic burden resulting in ‘ritual’ d isplays of formal compliance without any genuine concern to respect human rights.264  ( b) Algorithmic auditing:  Unlike impact assessment approaches which are intended to take p lace before system implementation, algorithmic auditing techniques are aimed at testing an d evaluating algorithmic systems once they are in operation.  Algorithmic auditing is em erging as a field of applied technical research, that draws upon a suite of emerging rese arch tools and techniques for detecting, investigating and diagnosing unwanted ad verse effects of algorithmic systems.265  It has been proposed that techniques of this ki nd might be formalised and institutionalised within a legally mandated regulatory g overnance framework, through which algorithmic systems (or at least those algorithmic sy stems that are regarded as ‘high risk’ systems in terms of the seriousness and scale of the  consequences in the event of failure or unintended adverse effects) are subject to p eriodic review and oversight by an external authority staffed by suitably qualified te chnical specialists.  For example, Cukier and Mayer-Schonenberg suggest that a new g roup of professionals are needed (‘algorithmists’) to take on this role, which may co nstitute a profession akin to that of law, medicine, accounting and engineering and who can  be relied upon to undertake the task of algorithmic auditing either as independent and ex ternal algorithmists to monitor algorithms from the outside, or by ‘internal’ al gorithmists employed by organisations to monitor those developed and deployed by the o rganisation, which can then be subjected to external review.266                                                              261   Raso et al 2018: 53. 262   Mantelero 2018. 263   See above n. 241;  Mantelero 2018 ; Edwards and Veale 2017.  264  Power 1997. 265  Desai and Kroll 2017.  Sandvig et al 2014.  See for example the resources available at Auditing Algorithms. 266   Such an approach would resemble the governance of conventional financial auditing, in which the ac counting systems within organisations are subject to both internal auditors employed in house, and also fro m external auditors, who then legally obliged to review those accounts and certify their veracity and val idity: Mayer-Schonberger and Cukier 2013: 180. See also Crawford and Schultz 2014; Citron 2008. 
Council of Europe Study 7 2 3.7.3 Standard setting, monitoring and enforcement The  techniques and approaches described above have significant potential as instruments thro ugh which prospective and historic responsibility for systems that rely upon advanced d igital technologies might be secured.  Yet in order for this potential to be realised, we must al so attend to the legal and institutional governance frameworks in which they are embedded.  Fo r example, the various strands of technical research referred to at section 3.7.2 have co nsiderable potential to facilitate prospective responsibility for digital technologies, providing we lcome recognition by the technical community that digital systems are not ‘neutral’ but are i mbued with values and might act in ways that are not consistent with human rights.  Not only i s it important that this work is nurtured and supported, but it is also important that it emerges fro m interdisciplinary engagement between the technical community and those from law, the h umanities and the social sciences, in order to elaborate more fully how human rights values can  be translated into technical mechanisms of protection, and how a human rights approach respo nds to the problem of value conflict.  It is equally important that we attend to the legal st atus of these techniques.  Although the tech industry has been keen to adopt technical respo nses to ethical problems, merely placing ‘blind faith’ in industry solutions risks becoming m erely another form of ‘ethics washing’.267  In other words, unless these technical approaches are  themselves backed by law and subject to transparent evaluation by and oversight by a co mpetent independent authority to ensure their validity and operation, they may not provide effec tive human rights protection.   As regulatory governance scholarship has emphasised, it is v ital that all three elements of the regulatory governance process are attended to: the setting o f standards, information gathering and monitoring of activity that is required to comply with tho se standards, and enforcement action and sanctions for non-compliance.268    Effective and l egitimate regulatory governance requires both stakeholder participation in the setting of the rel evant standards and a properly resourced, independent authority equipped with adequate p owers systematically to gather information, to investigate non-compliance and to sanction v iolations.269  If we are to have confidence that technological protection mechanisms intended to  ensure that human rights values are respected during the operation of digital processes, then  we must have robust mechanisms of oversight that can investigate and verify that they d o in fact so operate.  Hence technical standards themselves should be developed i ndependently (and ideally through a participatory process in which all affected stakeholders can  be involved) and subject to external scrutiny and examination, and that compliance with tho se standards can and will be scrutinised by an external body who has the power to impose (o r seek to ensure the imposition of) sanctions for violation.  In other words, without m eaningful independent oversight, these mechanisms are unlikely to provide the foundations fo r securing meaningful human rights accountability.   Various national and local governments are  increasingly recognising the need for more formal, institutionalised and systematic co nsideration and evaluation of algorithmic systems, reflected in the various task-forces and p ublic authorities commissioned to provide review and/or oversight of data-driven socio-te chnical systems.2703 .8 Reinvigorating human rights discourse in a networked digital age As  we enter a new globally networked digital age, the need to protect human rights and the u nderlying value commitments upon which they rest, is of paramount importance.  This                                                            267   Greene et al 2019. 268   Morgan and Yeung 2007; Lodge and Wegrich 2014. 269   Nemitz 2018. 270   For a summary of national initiatives across Europe, see Access Now 2018. 
DGI(2019)05 7 3 prompts consideration of whether our existing conceptions of human rights and the m echanisms through which they are enforced are fit for purpose in this new socio-technical l andscape.  The powerful networked digital technologies that have emerged in recent years m ake possible practices and actions that were previously impossible and thereby create novel threats,  risks and forms of wrong-doing, provoking reflection on whether additional new h uman rights and regimes of institutional governance are required to ensure that those risks can  be meaningfully addressed in practice.271  Although the basic structure and institutional frame work for human rights protection, which is well-established and universally recognised, can  reasonably be expected to develop effective responses to many of the threats and cha llenges wrought by the rising power of digital automation and machine intelligence, there are  several reasons why our existing rights discourse and enforcement mechanisms may requ ire reinvigoration if they are to provide effective protection.   Firstly, many of the rights co nferred upon data subjects are difficult to assert in practice, largely due to the opacity of m any of the socio-technical systems in which these technologies are embedded.  Secondly, our u nderstanding of the scope and content of existing rights were developed in a pre-networked ag e.  So conceived, these rights might fail to provide comprehensive protection against the full ran ge of threats and risks to individuals which these technologies may give rise to, particularly i n relation to illegitimate attempts to deceive and manipulate individuals that so-called ‘ persuasive technologies may enable (see above) and problems of discrimi nation (see above).  Fo r example, although the rights to data protection confer upon the data subject a right to i nsist upon human intervention, to express her view or to contest a fully automated decision that  has ‘profound effects’ on her, these rights do not apply to partially automated decisions.  N or do they necessarily ensure that, in practice, an affected individual can readily detect whet her she has been treated unequally vis-à-vis others, and if so, whether such differential treat ment amounted to discrimination and was thus prima facie unlawful.  Thirdly, and p erhaps most importantly when considering the adequacy of existing human rights and fu ndamental freedoms to address the new risks associated with new digital technologies, is th e data subject’s freedom to waive some of these rights by consenting to specific practices that  would otherwise constitute a rights-violation, thereby forgoing the protections these ri ghts provide.272  For example, if individuals were to rely only on Article 8 to protect the rights an d interests implicated in the provision of data-driven services, there is a significant risk that these  rights would be too readily waived by individual right-holders in a networked age built u pon a ‘free services’ business model: thus, in return for ‘free’ access to digital services and the  efficiency and convenience they offer, individuals willingly exchange their personal data.273  In  contrast, the core data protection principles upon which contemporary European data p rotection regimes (including modernised Convention 108) rest (and reflected in the ju risprudence of the European Court on Human Rights under Article 8), include mandatory o bligations imposed on data controllers that cannot be waived by individual right-holders,                                                            271   Brownsword, Scotford and Yeung 2017.   272  The extent to which the European Court On Human Rights is willing to recognise the possibility of individuals w aiving their ECHR rights, and the conditions required for an effective waiver, is likely to depend upon the r ight in question and the specific context in which a claimed waiver is alleged to arise.  For example, S coppola v. Italy (No. 2),  17 September 2009, no. 10249/03, para. 135, the Court stated “Neither the letter n or the spirit of Article 6 prevents a person from waiving them of his own free will, either expressly or tac itly. However, such a waiver must, if it is to be effective for Convention purposes, be established in an u nequivocal manner and be attended by minimum safeguards commensurate with its importance [...]. In ad dition, it must not run counter to any important public interest [...].”  In relation to private law and c ontractual relationships between non-state actors, the Court is likely to consider the issue of waiver in te rms of the positive duty of states to take reasonable measures to protect individuals from infringement of C onvention rights by other private persons, including the obligation to ensure (through legal regulation or o ther measures) that the relevant rights are ‘practical and effective’ in their exercise.  273   Solove 2012. 
Council of Europe Study 7 4 including the principles of lawfulness of the processing, of purpose specification and data m inimisation, thereby offering more systematic protection of the core underlying values and co llective interests which these regimes ultimately seek to protect. Bu t quite apart from these potential weaknesses, the individualised orientation of co ntemporary conceptions of human rights and  existing mechanisms for their enforcement, m ay fail to give due attention to the threats which these technologies may pose to collective g oods, particularly the need to preserve and nourish the underlying socio-technical fo undations that make it possible for moral agency and human rights to have space to operate.  L eading philosopher of law and technology, Mireille Hildebrandt, expresses these concerns in te rms of the technical conditions that are assumed to exist in order for the law (and co ntemporary understandings of the rule of law) to fulfil its functions.274  Yet within ‘smart’ env ironments that operate by continuously collecting digital data from the material world in o rder to infer, predict and therefore anticipate future behaviour of things, people and systems, these  technical conditions are both supplanted and augmented, thereby altering the very p ossibility for the exercise of what we currently understand as thought, choice and reason b ecause smart technologies operate continuously and immanently, and because they are d esigned to learn, producing outcomes that their designers did not specify.275  N orth American jurist Julie Cohen develops Hildebrandt’s insights, drawing on both legal scho larship and a growing body of work in the sociology of science referred to as Science and Te chnology Studies (or ‘STS’).276  Cohen argues that to ensure that human rights can be o perationalised in an era of smart environments, we must ‘take affordances seriously’, o therwise our rights will be ineffective.  According to affordance theory, the design of our te chnological objects and environments condition and constrain the possibilities for action, i ncluding the range of actions and responses which the design of the object ‘affords’ to the u ser.   Thus, once we recognise that smart digital technologies continually, immanently and p re-emptively mediate our beliefs and choices, then our legal discourse about human rights (i ncluding privacy) can be understood as incomplete.  Cohen therefore persuasively argues that  this requires more than merely extending rights discourse.  Rather, it will require us to r econceive of rights in new ways , as well as developing a different vernacular for rights d iscourse – one that recognises the central role of sociotechnical configurations in affording an d constraining the freedoms and capabilities that people in fact enjoy.277  In particular, our ri ghts discourse has operated on a set of often unexamined assumptions about the built env ironment’s properties, about both constraint (such as the physical impossibility of universal sur veillance) and lack of constraint (such as the open-ended possibilities for spaces people use to  gather and assemble, for various purposes including democratic protest).  But advances in n etworked digital technologies are challenging these assumptions, and we are only now l earning that the relevant constraints and affordances include not only those affecting our p hysical space, but also the affordances that govern the flow of data and information, and that these  have direct impacts for our rights and freedoms.  We therefore need to expand our frame  of rights discourse to encompass our socio-technical architecture, in which rights can be co nceived in terms of affordances as a practical matter in ways that ‘speak with effective force to  new kinds of material and opera tional considerations.’278                                                             274   Hildebrandt 2015. 275   Cohen 2017 at 3 citing Hildebrandt 2015 at 88-102. 276   Cohen 2017. 277   Cohen 2017: 7. 278   Cohen 2017: 9. 
DGI(2019)05 7 5 In other words, the inability of rights to provide a comprehensive response to the threats p osed by AI technologies is more deeply rooted in the inherent limitations of rights-based ap proaches effectively to address  systematic harms that are experienced primarily at a co llective, societal level, rather than at the level of the individual right-holder.  For example, the  introduction of a new ‘right to meaningful human contact’279 has its attractions, but it m ight not be effective in addressing the concerns about systematic, societal dehumanisation whi ch lies at the foundation of many of the anxieties expressed about our increasing reliance o n computational technologies.  In other words, it is the aggregate and cumulative effects of these  technologies over time, and at scale, that may systematically threaten the socio-te chnical foundations which the very notion of human rights presupposes and in which they are ro oted.280   B ecause smart digital technologies are ‘radically different in kind’ from other kinds of te chnologies, the societal challenge is to contend with their difference and power.281  By fo cusing on the architectural implications of these technologies, our attention is drawn to a p erspective that Cohen describes as ‘inherently communal’.  It highlights the responsibility of stat es, and our collective responsibility as a moral community, to attend to the socio-technical fo undations of moral and democratic freedom, and the way in which the aggregate, cum ulative impact of the adverse social concerns referred to above could fundamentally u ndermine the ‘moral and democratic commons’282 and without which human rights and fu ndamental freedoms cannot, in practice, be realised or asserted.   These social foundations m ust, at minimum, ensure that conditions necessary for moral agency and responsibility are p resent and secure, for in their absence, there is no freedom, and human rights have no m eaning.283  Yet we lack institutional mechanisms for monitoring the health of the socio-te chnical foundations in which our human rights and democratic freedom are anchored, and thi s may require us to develop both a new ‘vocabulary’ of rights, and institutional mechanisms fo r ensuring the health and sustainability of these foundations to secure meaningful human ri ghts protection in a new hyper-connected digital age.2843 .9 Summary Thi s section has highlighted the importance of ensuring that responsibility for the actual and p otential adverse consequences associated with the development and operation of advanced d igital technologies is allocated prospectively and retrospectively.  The fair and effective al location of responsibility for these threats, risks and adverse impacts is vital, not only to p rotect human rights and safeguard the welfare of individuals, groups and society at large , but al so, and even more fundamentally, to ensure that our society remains a moral community.  Ye t attributing responsibility for the adverse risks and effects of our increasingly powerful and so phisticated digital technologies generates considerable challenges, owing to the fact that there  are a great many individuals and organisations involved in their development and i mplementation, and because they may operate in unexpected ways.   We lcome recognition of the need to take seriously responsibility for the risks and other ad verse effects of advanced digital technologies can be found in the proliferation of voluntary                                                            279   Discussed above at section 2.2.2. 280   Yeung 2011. 281   Hildebrandt 2015; Cohen 2017. 282   Yeung 2011; Yeung 2017b.  283   Brownsword 2005. 284   Yeung 2011. 
Council of Europe Study 7 6 initiatives through which the tech firms and the tech industry have promulgated codes of good et hical practice, which they publicly proclaim they will aspire to meet.  Yet because these v oluntary self-regulatory initiatives lack any institutional mechanisms for meaningful public p articipation in the setting of the relevant standards, nor any external enforcement and san ctioning mechanisms, they do not constitute legitimate and effective safeguards.   Al though the capacity of advanced digital systems to operate more or less autonomously has b een claimed to distance their developers from responsibility for their operation, this claim rest s on a very particular and narrow conception of moral responsibility.  We have seen that a ran ge of responsibility models might be available for allocating responsibility for the adverse i mpacts of AI systems, noting that in relation to human rights infringements, responsibility is ap propriately assigned on a strict basis, without proof of fault.  As states bear the primary duty fo r ensuring effective protection of human rights, this grounds a legal obligation to introduce n ational legislative frameworks that give rise to legal duties and obligations on non-state acto rs. In addition, the fundamental value of human rights is of such strength and importance that  they are increasingly recognised as grounding horizontal effects on non-state actors, i ncluding tech developers.285  While judicial remedies constitute an important avenue through whi ch those adversely affected by the operation of AI technologies might seek redress, we h ave also identified a range of other governance instruments (including technical protection m echanisms) that could be utilised to secure meaningful and effective accountability and whi ch warrant further consideration. Ye t although there are various governance mechanisms (described above) that, if backed by l aw, can help to secure meaningful human rights protection, they are – in and of themselves – u nlikely to provide adequate and comprehensive protection.  In particular, our advanced n etworked digital technologies are now of such power and sophistication that they can be u nderstood as ‘radically different in kind’ from other kinds of technologies, particularly given thei r profound implications for our collective and shared technical, social, democratic and m oral architecture of our societies.  We must therefore reinvigorate our existing human rights d iscourse and instruments in ways that foreground our collective responsibility to attend to the  socio-technical foundations of moral and democratic freedom, and the way in which the ag gregate, cumulative impact of the adverse social concerns referred to above could fu ndamentally undermine the ‘moral and democratic commons’and without which human ri ghts and fundamental freedoms cannot, in practice, be realised or asserted.                                                               285   For the private sector, this has most comprehensively been developed by UN Special Rapporteur Ruggie w ho ‘codified’ the corporate social responsibility to respect human rights and act accordingly even in c ountries where national legislation does not demand that. 
DGI(2019)05 7 7 Chapter 4.  Conclusion  Ad vances in techniques now referred to as artificial intelligence are likely to continue to d evelop and grow in power and sophistication in the foreseeable future.  Relatively recent success  in AI, combined with the global and interconnected data infrastructure that has em erged over time, have enabled the proliferation of digital services and systems. These have al ready delivered very considerable benefits, particularly in terms of the enhanced efficiency an d convenience which they offer across a wide range of social domains and activities, al though access to these remains largely the province of inhabitants of wealthy industrialised n ations.  They bring with them extraordinary promise, with the potential to deliver very sub stantial improvements to our individual and collective well-being, including the potential to enh ance our capacity to exercise and enjoy our human rights and freedoms.  Yet, there are al so legitimate and rising public anxieties about their adverse societal consequences, including thei r potential to undermine human rights protection which, as this study has highlighted, co uld threaten to destabilise the very foundations upon which our moral agency ultimately rest s.  This study has therefore sought to examine the implications of advanced digital te chnologies (including AI) on the concept of responsibility from a human rights perspective.  It h as identified a series of ‘responsibility relevant’ properties of these technologies, outlining a ran ge of adverse impacts which these technologies may generate, and has sought to identify h ow responsibility for preventing, managing and mitigating those impacts (including the risk of h uman rights violations) may be allocated and distributed.   Thi s study has shown that any legitimate and effective response to the threats, risks, harms an d rights violations potentially posed by advanced digital technologies is likely to require a fo cus on the consequences for individuals and society which attends to, and can ensure that, bo th prospective responsibility  aimed at preventing and mitigating the threats and risks asso ciated with these technologies, and historic responsibility , to ensure that if they ripen into h arm and/or rights violations, responsibility for those consequences is duly and justly assigned.  Onl y then can we have confidence that sustained and systematic effort will be made to p revent harms and wrongs from occurring, and that if they do occur, then the underlying acti vities will be brought to an end, and that effective and legitimate institutional mechanisms fo r ensuring appropriate reparation, repair, and prevention of further harm are in place.  It will n ecessitate a focus on both those involved in the development, deployment and i mplementation of these technologies, individual users and the collective interests affected by them , and on the role of states in ensuring the conditions for safeguarding individuals subject t o their jurisdiction against risks and ensuring that human rights are adequately protected.   Fo ur findings of this study are worth highlighting:   1.  It is particularly important to ensure that we have effective and legitimate mechanisms that  will operate to prevent and forestall  human rights violations, particularly given that  many human rights violations associated with the operation of advanced digital te chnologies may not result in tangible harm.  The need for a preventative approach is especi ally important given the speed and scale at which these technologies can o perate, and the real risk that such violations may erode the collective socio-technical fo undations that are essential for freedom, democracy and human rights to exist at all.  Thi s has several implications.  Firstly, it suggests that states have an important respo nsibility to ensure that they attend to the larger socio-technical environment in whi ch human rights are anchored.  Secondly, stronger collective complaints m echanisms may be needed to ameliorate the collective action problem that i ndividuals may encounter in responding to rights violations generated by the o peration of AI systems.  Thirdly, our existing conceptions of human rights may need 
Council of Europe Study 7 8 to be reinvigorated in a networked, data-driven age in order to account for the way in whi ch these technologies may reconfigure our socio-technical environments, and the threats  which they may pose to collective goods and values. 2.  The model of legal responsibility that applies to human rights violations  is widely u nderstood as one of ‘strict responsibility’, without the need for proof of fault .  In co ntrast, the allocation of obligations of repair for tangible harm may be legally d istributed in accordance with a variety of responsibility models (briefly outlined in S ection 3.4 above).  This variety of potential legal models that could be applied to al locate and distribute the adverse effects arising from our other-regarding conduct cl early demonstrates that it is a mistake to expect one single model of legal respo nsibility to fairly apply to all the different kinds of adverse consequences that m ight flow from the use of advanced digital technologies.    Legal models of respo nsibility emphasise the relationship between moral agents, moral patients and so ciety more generally, unlike much applied philosophical analysis of responsibility for AI  systems, which has tended to focus on the conduct of moral agents and whether that  conduct justly attracts responsibility agents at the expense of moral patients ( ‘victims’) and of society.  These various legal models of responsibility strike a different b alance between our interest as agents in freedom of action, and our interest as v ictims in rights and interests in security of person and property.  Identifying which (if a ny) of these models is most appropriate for allocating and distributing the various ri sks associated with the operation of advanced digital technologies is by no means sel f-evident, but will entail a deliberate social policy choice concerning how these risks shoul d be appropriately allocated and distributed.  In democratic societies that espo use a commitment to human rights, the state bears a critical responsibility for ensu ring that these policy choices are made in a transparent, democratic manner whi ch ensures that the policy ultimately adopted will effectively safeguard human ri ghts. 3.  Various strands of technical research have considerable potential to help secure p rospective and historic responsibility for advanced digital technologies through the d evelopment of techniques that may enable both effective technical protection m echanisms and meaningful ‘algorithmic auditing’ .  This research should be nurtured an d supported, and needs to be developed through interdisciplinary engagement b etween the technical community and those from law, the humanities and the social sci ences, in order to elaborate more fully how human rights norms can be translated i nto technical mechanisms of protection, and how a human rights approach responds to  the problem of value conflict.   4.  Taking human rights seriously in a hyperconnected digital age will require that have effec tive and legitimate governance mechanisms, instruments and institutions are in p lace to monitor and oversee the development, implementation and operation of our co mplex socio-technical systems.  Some suggestions for how we might take forward the  need to ensure that we have governance mechanisms and institutions that have the  capacity to do this are set out in Appendix A.  Voluntary initiatives by the tech i ndustry via the promulgation of so- called ‘ethical’ standards of conduct which they p ublicly claim they will seek to honour constitute welcome recognition by the tech i ndustry that the technologies which they develop may produce adverse effects for whi ch they bear some responsibility. They do not, however, provide adequate and ro bust human rights protection.  At minimum, responsible development and i mplementation of AI requires both democratic participation in the setting of the rel evant standards and the existence of properly resourced, independent authorities 
DGI(2019)05 7 9 equipped with adequate powers systematically to gather information, to investigate n on-compliance and to sanction violations.    In particular, if we are to have confidence that  technological protection mechanisms intended to ensure that human rights v alues are respected during the operation of digital processes, then we must have ro bust independent mechanisms of external oversight that can investigate and verify that  they do in fact so operate, otherwise they are unlikely to provide the foundations fo r securing meaningful AI accountability.   In this respect, it is the obligation of states to  ensure that these governance mechanisms are established and implemented in way s that will ensure the protection of human rights. I f we are serious in our commitment to protect and promote human rights in a h yperconnected digital age, then we cannot allow the power of our advanced digital te chnologies and systems, and those who develop and implement them, to be accrued and ex ercised without responsibility.   The fundamental principle of reciprocity applies: those who d eploy and reap the benefits of these advanced digital technologies (including AI) in the p rovision of services (from which they derive profit) must be responsible for their adverse co nsequences. It is therefore of vital importance that nations committed to protect human ri ghts uphold a commitment to ensure that those who wield digital power (including the p ower derived from accumulating masses of digital data) are held responsible for their co nsequences.   It follows from the obligation of states to protect human rights that they have a  duty to introduce into national law, governance arrangements that will ensure that both p rospective and historic responsibility for the adverse risks, harms and rights violations arising fro m the operation of advanced digital technologies are duly allocated. 
Council of Europe Study 8 0 Appendix A  Thi s appendix identifies a range of measures and institutional mechanisms that might warrant fu rther consideration and research in order to help ensure that human rights are protected in an  age of advanced networked digital technologies. They are not intended as reco mmendations, but merely to invite further reflection and discussion. P rospective responsibility Co nsider offering additional funding to support and encourage interdisciplinary research ai med at developing techniques, mechanisms and standards that can help ensure that p rospective responsibilities for preventing and mitigating risks of harm or wrongs arising from the o peration of advanced digital technologies are duly assigned.   Co nsider measures to encourage states and interstate cooperation to work towards d eveloping legally supported institutional governance mechanisms to facilitate the protection o f human rights against threats and risks posed by advanced digital technologies.  These might in clude: a.  Legal requirements to undertake ‘human rights impact analysis’ (incorpo rating al gorithmic impact analysis) prior to deployment of advanced digital technologies, i ncluding a publicly available statement identifying how potential interferences with h uman rights and value conflicts are resolved in system architecture and operation; b.  Develop, in conjunction with a wide range of stakeholders, a code of best practice for p reparing human rights impact analysis for advanced digital technologies. c.  Clarify the scope and content of legal obligations of all those involved in the d evelopment of digital services (including software developers), particularly o bligations that bear directly upon human rights protection; d.  Consider the need to subject developers and providers to legal obligations to engage i n, and demonstrate adequate verification and testing of, complex computational sy stems that may have a direct and substantial impact on human rights, both prior to rel ease and at periodic intervals following implementation in real-world environments; e.  Encourage the use of technical protection mechanisms (such as ‘human rights by d esign’, fairness-aware data mining techniques, and explainable AI), identifying how they  can serve a valuable role in ensuring human rights adherence.  Consider the need to  provide legal support for these techniques, including by subjecting them to external o versight and review in order to provide a greater level of assurance that these m echanisms operate in fact in ways that are human rights compliant; f.  Encourage further research into the development of techniques and standards that sup port responsible, human-rights compliant innovation in digital tech industry (i ncluding modelling, data provenance and quality, algorithmic auditing, validation, v erification and testing).  g.  Consider establishing a professional accreditation scheme for appropriately qualified te chnical experts trained in algorithmic auditing techniques as a class of professionals who  are subject to fiduciary duties of loyalty and good faith in verifying and certifying the desi gn and operation of algorithms. 
DGI(2019)05 8 1 h. Develop a methodological framework and set of metrics for systematically identifying, an d evaluating the magnitude and seriousness of, potential threats and risks to i ndividual rights (including the threats they pose to the socio-technical foundations in whi ch human rights and fundamental freedoms are anchored) posed by proposed or p otential AI applications.   i.  Consider whether AI applications which pose threats that are judged to be so serious an d disproportionate in their human rights impact that they should be prohibited u nless they are subjected to prior public consultation and approval from an ap propriately constituted independent supervisory authority.  A framework of this ki nd might include a class of AI applications that should be prohibited outright because they  pose unacceptable grave and potentially catastrophic threats to human rights and fu ndamental freedoms.286     H istoric responsibility Co nsider supporting the development of guidance and techniques that can help ensure that h istoric responsibility is duly assigned for individual and collective harms or rights violations resul ting from the operation of advanced digital technologies.  This may include encouraging stat es and intergovernmental cooperation towards developing legally supported institutional g overnance mechanisms that might include: a.  Member state action to review and assess whether national legal systems will operate to  ensure that responsibility for harm caused by advanced digital technologies can be d uly allocated, identifying any potential gaps which may need to be addressed via l egislative reform; b.  Consider the need to develop standard-setting instruments to clarify and locate d efault historic responsibility for the harms and wrongs to those involved in the d esign, developers, deployment, ownership and provision of digital systems.  This co uld include legal liability to make reparation to those harmed or wronged by the o peration of these services, including an obligation to compensate and introduce m easures to avoid future occurrence.  In developing a suitable instrument, co nsideration might be given to the desirability of some kind of ‘due diligence’ defence i n certain clearly and narrowly defined circumstances, leading to a reduction in the ex tent of the developer’s legal responsibility for harm or wrongdoing;  c.  Support further research into the appropriate distribution and allocation of authority b etween humans in the loop of complex computational systems, in light of the ackno wledged problem of ‘automation bias’ and tendency to allocate responsibility to i ndividual humans in the loop, rather than on those who develop and implement the so cio-technical system in which the human is embedded;                                                            286   See also  EU High Level Group on Artificial Intelligence (2019a). 
Council of Europe Study 8 2 d. Consider the desirability of mandating a compulsory insurance regime for the digital te ch industry, including whether to establish a national insurance scheme, funded by d igital tech industry, to ensure that victims are not left uncompensated; e.  Support the development of further capacity to establish new (and extend the capacity o f existing) governance institutions that can meaningfully and rigorously investigate an d enforce prospective and historic responsibilities of digital service developers and p roviders. f.  Consider the desirability of introducing collective complaints mechanisms and whether to  liberalise standing rules in order to overcome the problem of collective action which m ay arise when a large number of individuals may be vulnerable to rights infringement b ut are unlikely to be sufficiently motivated to take action even though their cum ulative effect may be very substantial.   To this end, consider whether the co llective complaints procedure adopted to enhance the effectiveness, speed and i mpact of the implementation of the European Social Charter provides a suitable m odel. g.  Review adequate resourcing and powers of investigation, sanction and remedies for p ublic enforcers.  This may include the need to develop and build technical expertise an d competence in machine learning and other software development and evaluation te chniques within the public sector. Rec onfiguring human rights discourse in a networked digital age Co nsider ways in which existing human rights protection and discourse may need to develop in o rder to ensure the effective protection of human rights in a globally connected digital age, reco gnising the need to attend to the socio-technical foundations that form the basis of the ru le of law and of moral community.  This might include: a.  Consider the desirability for a new Convention on Human Rights in a Networked Digital A ge which would, at minimum,  recognise that both prospective and historic respo nsibility for risks, harms and rights violations must be fully allocated and d istributed;   b.  Consider the need for formal recognition within such a Convention (or other similar m ultilateral instrument) of the role of independent institutional mechanisms to safegu ard against the collective risks which these technologies pose to the social fo undations of democratic orders  in which human rights are anchored;  c.  Consider whether new collective decision-making and monitoring mechanisms may be n ecessary or desirable in order to track and evaluate the aggregate and cumulative effec ts of these technologies on human rights across member states.  To this end, co nsider the need or desirability of establishing a ‘global observatory’ to undertake thi s monitoring and reporting function on a systematic basis; d.  Apply a precautionary approach in cases where interacting algorithmic systems have the  capacity to cause catastrophic harm which could not reasonably have been fo reseen by any individual digital service provider;  consider the prohibition of p articular kinds of algorithmic applications with the potential for causing catastrophic h arms ; consider the need for systematic monitoring structures and expert institutions i n order to prevent such applications from being developed and deployed. 
DGI(2019)05 8 3 References  6,  P. (2001) ‘Ethics, regulation and the new artificial intelligence, part I: accountability and power’ . I nformation, Communication & Society 4(2):199-229.  6,  P. (2001) ‘Ethics, regulation and the new artificial intelligence, part II: autonomy and liability’. I nformation, Communication & Society 4(3): 406-434.  6,  P. (2002) ‘Who wants privacy protection, and what do they want?’ Journal of Consumer Behaviour: An In ternational Research Review . 2(1): 80-100. Acc ess Now (2018) Mapping Regulatory Proposals for Artificial Intelligence in Europe .  Available at h ttps://www.accessnow.org/mapping-artificial-intelligence-strategies-in-europe/ (Accessed 7.11.18) AI  Now (2017) AI Now 2017 Report. Available at h ttps://ainowinstitute.org/AI_Now_2017_Report.pdf(Ac cessed 31.10.2018). Aka nsu, A. N. (2017). ‘The flash crash: a review.’ Journal of Capital Markets Studies  1(1): 89-100. Am nesty International (2017) Artificial Intelligence for Good .  Available at h ttps://www.amnesty.org/en/latest/news/2017/06/artificial-intelligence-for-good/(A ccessed 2.11.20 18).  An drade, F., Novais, P., Machado, J. and Neves, J. (2007) ‘Contracting agents: legal personality and r epresentation’ Artificial Intelligence and Law . 15(4): 357-373. An gwin, J., Larson, J., Mattu, S. and Kirchner, L. (2016) ‘Machine bias’. ProPublica, 23 May.  Available at h ttps://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing (Accessed 5.11.20 18). ART ICLE 19, The Danish Institute for Human Rights and the Dutch Internet Domain-registry (2017) S ample ccTLD Human Rights Impact Assessment Tool .  Available at h ttps://www.article19.org/wp-con tent/uploads/2017/12/Sample-ccTLD-HRIA-Dec-2017.pdf (Accessed 2.11.2018).  As aro, P.M. (2014) ‘A Body to Kick, but Still No Soul to Damn: Legal Perspectives on Robotics’ in Robot E thics: The ethical and social implications of robotics. Edited by P. Lin, K. Abney & G.A. Bekey.  MIT Press.  Au diting Algorithms: Adding Accountability to Automated Authority.  Available at h ttp://auditingalgorithms.science/ (Accessed 5.11.2018). Au stralian Human Rights Commission (2018) Human Rights and Technology Issues Paper. July.  Available at  https://tech.humanrights.gov.au/sites/default/files/2018-07/Hu man%20Rights%20and%20Technology%20Issues%20Paper%20FINAL.pdf (Accessed 5.11.18). Baro cas, S. and Selbst, A.D. (2016) ‘Big data's disparate impact.’ Cal. L. Rev. 104: 671. Barr,  S. (2018) ‘Computer-Generated Instagram Account Astounds Internet’.  The Independent. 1 March.  Av ailable at h ttps://www.independent.co.uk/life-style/fashion/instagram-model-computer-generated-s hudu-gram-internet-cameron-james-a8234816.html (Accessed 7.11.18) Be nnett Moses, L. and de Koker, L. (2017). ‘Open Secrets: Balancing Operational Secrecy and T ransparency in the Collection and Use of Data by National Security and Law Enforcement Agencies’ Me lbourne University Law Review 41(2): 530.  Bos trom, N. (2014) Superintelligence: Paths, Dangers, Strategies. Oxford: Oxford University Press.  Bov ens, M. (2007) ‘New forms of accountability and EU -governance’ Comparative European Politics  5(1): 104 -120. 
Council of Europe Study 8 4 boyd, d., and Crawford, K. (2012) ‘Critical Questions for Big Data’.  Information, Communication and S ociety 15(5):662-79. Browns word, R. (2005) ‘Code, control, and choice: why East is East and West is West.’ Legal Studies 25 (1): 1-21. Browns word, R., Scotford, E., and Yeung., K. (eds.) (2017). Oxford Handbook on Law, Regulation and T echnology. Oxford: Oxford University Press.  Bru ndage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B., Dafoe, A., Scharre, P., Zeitzoff, T., Fi lar, B. and Anderson, H. (2018).  The malicious use of artificial intelligence: Forecasting, prevention, and  mitigation.’ Available at arXiv preprint arXiv:1802.07228  (Accessed 5.11.2018). Bu blitz, J.C. (2013) ‘My mind is mine!?  Cognitive liberty as a legal concept’ in Cognitive Enhancement: An  Interdisciplinary Perspective, edited by E. Hildt, and A.G Franke. Dordrecht: Springer at 233-264.  Bu rrell, J. (2016) ‘How the machine ‘thinks’: Understanding opacity in machine learning algorithms.’ Big Da ta & Society 3(1):1-12. Bry son, J. J. and A. Theodorou (2018) ‘How Society can Maintain  Human-Centric Artificial Intelligence’.  In  M. Toivonen-Noro and E. Saari (eds.) Human-Centered Digitalization and Services .  Bry son, J.J. (2010) ‘Robots should be slaves’ in Close Engagements with Artificial Companions: Key social, ps ychological, ethical and design issues, edited by Y Wilks.  Amsterdam: John Benjamins Publishing at 6374. Bygra ve, L.A. (2017) ‘Hard-wiring Privacy’ in Brownsword, R., Scotford, E., and Yeung., K. (eds.) (2017). O xford Handbook on Law, Regulation and Technology . Oxford: Oxford University Press.  Can e, P. (2002) Responsibility in Law and Morality . Oxford: Hart Publishing. Carto n, S., Helsby, J., Joseph, K., Mahmud, A., Park, Y., Walsh, J., Cody, C., Patterson, C.P.T., Haynes, L. a nd Ghani, R., (2016) ‘Identifying police officers at risk of adverse events.’ In Proceedings of the 22nd AC M SIGKDD International Conference on Knowledge Discovery and Data Mining , at 67-76. Available at h ttps://dl.acm.org/citation.cfm?id=2939698 (accessed 5.11.18). Cath ,  C. (2018) ‘Governing artificial intelligence: ethical, legal and technical opportunities and ch allenges’ 376 Phil Trans A: Mathematical, Physical and Engineering Sciences .  h ttps://doi.org/10.1098/rsta.2018.0080Chen , A. (2014) ‘The Labourers Who Keep Dick Pics and Beheadings Out of Your Facebook News Feed’, W ired, 23 October.  Available at h ttps://www.wired.com/2014/10/content-moderation/ (Accessed 5.11.18 ). Ches ney, B. and D. Citron (2019) ‘Deep Fakes: A Looming Challenge for Privacy, Democracy, and N ational Security.’  California Law Review 107: forthcoming. Ci tron, D. K. (2008) ‘Technological Due Process.’ Washington University Law Review  85: 1249-1313. Coh en, J. E. (2017). ‘Affording Fundamental Rights.’ Critical Analysis of Law.  4(1): 76-90. Con n, A. (2017) ‘Research for Beneficial Artificial Intelligence’ Future of Life Institute. Available at: h ttps://futureoflife.org/2017/12/27/research-for-beneficial-artificial-intelligence/?cn-reloaded=1&cn-r eloaded=1 (Accessed 5.11.18) Cow ley, J. (2018) ‘Beijing subway to install facial recognition as fears grow of China surveillance powers.’ 19 J une. The Telegraph.  Available at h ttps://www.telegraph.co.uk/news/2018/06/19/beijing-subway-i nstall-facial-recognition-fears-grow-china-surveillance/ (Accessed 5.11.18) 
DGI(2019)05 8 5 Council of Europe. Recommendation CM/Rec(2018)2 of the Committee of Ministers to member states o n the roles and responsibilities of internet intermediaries.  Adopted on 7 March 2018.  Available at h ttps://search.coe.int/cm/Pages/result_details.aspx?ObjectID=0900001680790e14 (Accessed 7.11.18) Cou ncil of Europe, Parliamentary Assembly (2017) Committee on Culture, Science, Education and Me dia.  Technological Convergence, Artificial Intelligence and Human Rights .  10 April.  Doc 14288. Av ailable at h ttp://semantic-p ace.net/tools/pdf.aspx?doc=aHR0cDovL2Fzc2VtYmx5LmNvZS5pbnQvbncveG1sL1hSZWYvWDJILURXLWV4d HIuYXNwP2ZpbGVpZD0yMzUzMSZsYW5nPUVO&xsl=aHR0cDovL3NlbWFudGljcGFjZS5uZXQvWHNsdC9 QZGYvWFJlZi1XRC1BVC1YTUwyUERGLnhzbA==&xsltparams=ZmlsZWlkPTIzNTMx.  Accessed 7.11.18. C rawford, K. and Schultz, J. (2014) ‘Big data and due process: Toward a framework to redress predictive p rivacy harms.’ Boston College Law Review 55:93. Dan aher, J. (2016) ‘Robots, law and the retribution gap.’ Ethics and Information Technology  18(4): 299-309.  Dat ta, A., Sen, S. and Zick, Y. (2016) Algorithmic transparency via quantitative input influence: Theory an d experiments with learning systems. In Security and Privacy (SP), 2016 IEEE Symposium on , 598-617. I EEE. Dav idow, B. (2014). ‘Welcome to Algorithmic Prison - The use of Big Data to to profile citizens is subtly, s ilently constraining freedom’. The Atlantic. 20 February. De nnett, D.C., (1997) ‘When HAL kills, who’s to blame’ in HAL's Legacy: 2001's Computer as Dream and Re ality, edited by D.G. Stork. MIT Press. De sai, D.R. and Kroll, J. (2017) ‘Trust but Verify: A Guide to Algorithms and the Law’.  Harvard Journal of La w & Technology 31:1-64 De  Streel, A., Buiten, M.& Peitz, M. (2018) ‘Liability of online hosting platforms: should exceptionalism e nd?’ Centre on Regulation in Europe Report. Available at: h ttp://www.cerre.eu/sites/cerre/files/180912_CERRE_LiabilityPlatforms_Final_0.pdf  (Accessed 5.11.18) Di etrich, W., Mendoza, C and Brennnan, T.  (2016) ‘Compass risk scales: Demonstrating accuracy, equity an d predictive parity’.  Northpointe.  Don ahoe, E. (2016) ‘So Software Has Eaten the World: What Does it Mean for Human Rights, Security an d Governance?’ 18 March, Just Security.  Available at h ttps://www.justsecurity.org/30046/software-e aten-world-human-rights-security-governance/ (Accessed 5.11.18). Doshi -Velez, F., Ge, Y. and Kohane, I. (2014) ‘Comorbidity clusters in autism spectrum disorders: an e lectronic health record time- series analysis.’ Pediatrics 133(1): e54-e63. D raper, N. A. and J. Turow (2017) ‘Audience Constructions, Reputations and Emerging Media T echnologies: New Issues of Legal and Social Policy’ in The Oxford Handbook on Law, Regulation and T echnology, edited by R. Brownsword, E. Scotford & K. Yeung. Oxford: Oxford University Press. T he Economist (2017) ‘Imitating people’s speech patterns could bring trouble’. Available at h ttps://www.economist.com/science-and-technology/2017/04/20/imitating-peoples-speech-patterns-p recisely-could-bring-trouble (Accessed 7.11.18) T he Economist (2018a) ‘Images aren’t everything: AI, radiology and the future of work’. Available at h ttps://www.economist.com/leaders/2018/06/07/ai-radiology-and-the-future-of-work (Accessed 6.11.18 ) T he Economist (2018b). ‘The techlash against Amazon, Facebook and Google - and what they can do ’.  Av ailable at h ttps://www.economist.com/briefing/2018/01/20/the-techlash-against-amazon-facebook-an d-google-and-what-they-can-do (Accessed 6.11.18) 
Council of Europe Study 8 6 Edwards, L. and Veale, M. (2017) ‘Slave to the Algorithm: Why a Right to an Explanation Is Probably Not th e Remedy You Are Looking for.’ Duke L. & Tech. Rev. 16. E kbia, H. and B. Nardi (2014) ‘Heteromation and its (dis)contents: T he invisible division of labor between h uman and machine. First Monday 19(6). Available at h ttps://firstmonday.org/article/view/5331/4090#author (Accessed 7.11.18) E lish, M.C. (2016): ‘Letting Autopilots Off the Hook: Why do we blame humans when automation fails?’ 16 J une. Available at: h ttp://www.slate.com/articles/technology/future_tense/2016/06/why_do_blame_humans_when_autom ation_fails.html (Accessed 5.11.18) E ngineering and Physical Sciences Research Council (EPSRC): h ttps://epsrc.ukri.org/ (Accessed 6.11.18)  E schelman, A., (2016) ‘Moral responsibility’, The Stanford Encyclopedia of Philosophy (Winter 2016 E dition). Edited by Edward N. Zalta. Available at h ttps://plato.stanford.edu/archives/win2016/entries/moral-responsibility/ (Accessed 6.11.18) E uropean Commission (2018a) Communication on Artificial Intelligence. Communication from the Com mission to the European Parliament, the European Council, the Council, the European Economic an d Social Committee and the Committee of the Regions on Artificial Intelligence for Europe. E uropean Commission (2018b) Consumer market study on online market segmentation through p ersonalised pricing/offers in the European Union, 19 July.  Available at h ttps://ec.europa.eu/info/publications/consumer-market-study-online-market-segmentation-through-p ersonalised-pricing-offers-european-union_en (Accessed 3 May 2019). E uropean Commission (2018c) Liability for emerging digital technologies. European Commission Staff Wo rking Document. COM (2018) 237 final.  Available at h ttps://ec.europa.eu/digital-single-m arket/en/news/european-commission-staff-working-document-liability-emerging-digital-technologies(Ac cessed 5.11.2018) E uropean Commission (2018d) Evaluation of Council Directive 85/374/EEC of 25 July 1985 on the ap proximation of the laws, regulations and administrative provisions of the Member States concerning l iability.  COM(2018) 246 final.  Available at h ttps://eur-lex.europa.eu/legal-con tent/EN/TXT/HTML/?uri=CELEX:52018SC0157&from=EN (Accessed 7.11.18) E uropean Commission (2019a) High Level Expert Group on AI, Ethics Guidelines for Trustworthy AI.  E uropean Commission (2019b) High Level Expert Group on AI, A Definition of AI: Main Capabilities and D isciplines. E uropean Economic and Social Committee and the Committee of the Regions on Artificial Intelligence f or Europe. COM (2018) 237 final.  Available at h ttps://ec.europa.eu/digital-single-m arket/en/news/communication-artificial-intelligence-europe (Accessed 5.11.2018) E uropean Group on Ethics in Science and New Technologies (EGE)(2018). Statement on Artificial I ntelligence, Robotics and ‘Autonomous’ Systems . Available at h ttps://ec.europa.eu/research/ege/pdf/ege_ai_statement_2018.pdf (Accessed 6.11.18) E uropean Convention on the Protection of Human Rights and Fundamental Freedoms (ECHR) E uropean Parliament Committee on Legal Affairs (2017) Report with Recommendations  to the C ommission on Civil Law Rules on Robotics.  Rapporteur: M. Delaveux.  2015/2103 (INL)). Available at h ttp://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//NONSGML+REPORT+A8-2017-0005 +0+DOC+PDF+V0//EN (accessed 5.11.2018). E uropean Political Strategy Centre (2018) ‘The age of artificial intelligence: Towards a European Strategy f or Human-Centric Mach ines’. Available at h ttps://ec.europa.eu/epsc/sites/epsc/files/epsc_strategicnote_ai.pdf (Accessed 6.11.2018) 
DGI(2019)05 8 7 European Union (1985) Council Directive 85/374/EEC of 25 July 1985 on the approximation of the laws, re gulations and administrative provisions of the Member States concerning liability for defective p roducts (OJ L 210, 7.8.1985, 29-33). E xecutive Office of the President. (2016) Big Data: A Report on Algorithmic Systems, Opportunity, and Ci vil Rights. Available at: h ttps://obamawhitehouse.archives.gov/sites/default/files/docs/big_data_privacy_report_may_1_2014.pdf  (Accessed 6.1118). Fa rr, C. (2016) ‘If You Want Life Insurance, Think Twice Before Getting A Genetic Test’.  2 July.  Available at  https://www.fastcompany.com/3055710/if-you-want-life-insurance-think-twice-before-getting-ge netic-testing (Accessed 7.11.18) Fe rguson, A.G. (2016) ‘Policing predictive policing.’ Wash. UL Rev. 94: 1109. Fe rraris, V., Bosco, F. and D'Angelo, E.,(20 13) ‘The impact of profiling on fundamental rights’.  Available at : SSRN: h ttps://ssrn.com/abstract=2366753 or http://dx.doi.org/10.2139/ssrn.2366753  (Accessed 6.11.20 18). Fi nancial Times (2018) F T Series ,  The AI arms race. Available at h ttps://www.ft.com/content/21eb5996-89a3 -11e8-bf9e-8771d5404543Forb rukerRadet.  Norweigan Consumer Council (2018) Deceived by Design.  27 June.  Available at h ttps://fil.forbrukerradet.no/wp-content/uploads/2018/06/2018-06-27-deceived-by-design-final.pdf(Ac cessed 7.11.18). G alligan, D.G. (1997) Due Process and Fair Procedures . Clarendon Press: Oxford. G alligan, D.G. (2006) Law in Modern Society . OUP: Oxford. G andy, O.H. (1993) The panoptic sort: a political economy of personal information. Westview. G ardner, J. (2003) ‘The Mark of Responsibility.’ Oxford Journal of Legal Studies. 23(2): 157-171. G ardner, J. (2008) Introduction to H.L.A. Hart, Punishment and Responsibility: Essays in the Philosophy of La w: Second Edition. OUP Oxford.  T he Guardian (2016) ‘Microsoft ‘deeply sorry’ for racist and sexist tweets by AI chatbot’. Available at: h ttps://www.theguardian.com/technology/2016/mar/26/microsoft-deeply-sorry-for-offensive-tweets-b y-ai-chatbot (Accessed 6.11.18) G illiker, P (2000) ‘A “new” head of damages: damages for mental distress in the English law of torts’.  Le gal Studies 20: 19-41. G irardin, F. and Blat, J. (2010) The co-evolution of taxi drivers and their in-car navigation sy stems. Pervasive and Mobile Computing . 6(4): 424-434. G las, L.R. ‘The Functioning of The Pilot -Judgment Procedure Of The European Court Of Human Rights In P ractice’ (2016) Netherlands Quarterly of Human Rights  34(1): 41. G orton, W.A. (2016) ‘Manipulating Citizens: How Political Campaigns’ Use of Behavioral Social Science H arms Democracy’ New Political Science, 38(1), pp.61-80. Gr eene, D., Hoffman, A.L and Stark, L., ‘Better, Nicer, Clearer and Fair er: A Critical Assessment of the Mo vement for Ethical Artificial Intelligence and Machine Learning’ (2019) Hawaii International Con ference on System Sciences, DOI: 10.24251/HICSS.2019.258.  Available at h ttp://dmgreene.net/wp-con tent/uploads/2018/09/Greene-Hoffman-Stark-Better-Nicer-Clearer-Fairer-HICSS-Final-Su bmission.pdf (Accessed 6 May 2019) G unkel, D.J. (2017) ‘Mind  the gap: responsible robotics and the problem of responsibility’, Ethics and I nformation Technology , pp.1-14. 
Council of Europe Study 8 8 Hagendorf, T. (2019) ‘The Ethics of AI Ethics: An Evaluation of Guidelines’.  Available at h ttps://arxiv.org/abs/1903.03425 (Accessed 6 May 2019). H all, W. and Pesenti, J (2017) Growing the Artificial Intelligence Industry in the UK . Available at h ttps://www.gov.uk/government/publications/growing-the-artificial-intelligence-industry-in-the-uk(Ac cessed 7.11.18) H allevy, G. (2015) Liability for crimes involving artificial intelligence systems . Springer International P ublishing. H anson, F.A. (2009) ‘Beyond the skin bag: on the moral responsibility of extended agencies’, Ethics and i nformation technology , 11(1), pp.91-99. H art, H.L.A., ((1968) 2008) Punishment and responsibility: Essays in the philosophy of law . Oxford U niversity Press: Oxford. H elberger, N., Pierson, J., and Poell, T. (2018) Governing online platforms: From contested to coo perative responsibility. The Information Society  34(1): 1-14, DOI:10.1080/01972243.2017.1391913 H ildebrandt, M. and Gutwirth, S., (2008) Profiling the European Citizen.  Springer: Netherlands. H ildebrandt, M. (2013) ‘Criminal Law and Technology in a Data- Driven Society’ in M.D. Dubber and T H ornle (eds), Oxford Handbook of Criminal Law .  Oxford: Oxford University Press 174-197. H ildebrandt, M., (2015) Smart technologies and the end (s) of law: novel entanglements of law and t echnology. Edward Elgar Publishing. H ildebrandt, M. (2016) ‘Data -gestuurde intelligentie in het strafrecht’ In: E.M.L. Moerel, J.E.J. Prins, M.H ildebrandt, T.F.E. Tjong Tjin Tai, G-J. Zwenne & A.H.J. Schmidt (eds.), Homo Digitalis. Nederlandse : Wo lters Kluwer. H imma, K.E. (2009) ‘Artificial Agency, Consciousness, and the Criteria for Moral Agency: What Properties Mu st an Artificial Agent have to be a Moral Agent.’ Ethics and Information Technology  11(1):24. H orsey, K and Rackley, E (2014) Tort Law.  4th ed.  Oxford University Press: Oxford. H utson, M (2018 ) ‘Lip-reading artificial intelligence could help the deaf —or spies.’ 31 July.  Science.  d oi:10.1126/science.aau9601. Available at h ttp://www.sciencemag.org/news/2018/07/lip-reading-ar tificial-intelligence-could-help-deaf-or-spies (Accessed 6.11.18) H uxley, A. (1932) Brave New World. Chatto & Windus. IE EE, Global Initiative for Ethical Considerations in AI and Autonomous Systems (2017).  Available at h ttps://standards.ieee.org/industry-connections/ec/autonomous-systems.html (Accessed 7.11.18) Ira ni, L. (2015) ‘Difference and Dependence among Digital Workers: The Case of Amazon Mechanical T urk’.  The South Atlantic Quarterly 114(1): 225-234. J asanoff, S. (2016) The ethics of invention: technology and the human future . WW Norton & Company. J ohnson, D.G. (2006) ‘Computer systems: Moral entities but not moral agents.’ Ethics and information t echnology. 8(4): 195-204. J ohnson, D.G. and Powers, T.M. (2005) ‘C omputer systems and responsibility: A normative look at techn ological complexity’, Ethics and information technology , 7(2), pp.99-107 Kami nski, M.E. and Witnov, S. (2014) ‘The Conforming Effect: First Amendment Implications of Su rveillance, Beyond Chilling Speech. ’ U. Rich. L. Rev., 49: 465. Ke en, A (2018) How to Fix the Future. Atlantic Books: London.   
DGI(2019)05 8 9 Kitchin, R (2014) The Data Revolution.  Sage: Los Angeles.  Ko rff, D. and Browne, I. (2013) ‘The use of the Internet & related services, private life & data  protection: t rends, technologies, threats and implications’, Council of Europe, T -PD(2013)07.  Available at at SSRN: h ttps://ssrn.com/abstract=2356797 (Accessed 6.11.18) Koo ps, B.J., Hildebrandt, M and Jaquet-Chiffelle, D- O. (2010) ‘Bridging the Accountability Gap: Rights for N ew Entities in the Information Society?’ Minnesota Journal of Law, Science and Technology  497.  Kos inski, M., Stillwell, D & Graepel, T. (2013) ‘Private traits and attributes are predictable from digital r ecords of human behaviours.’ Proceedings of the National Academy of Science  110: 5802-5805. K ramer, A.D., Guillory, J.E. and Hancock, J.T. (2015) ‘Experimental evidence of massive -scale emotional con tagion through social networks.’ Proceedings of the National Academy of Sciences , 8788–8790. K uflik, A. (1999) ‘Computers in control: Rational transfer of authority or irresponsible abdication of a utonomy?’ Ethics and Information Technology  1(3): 173-184. L anzing, M. (2018) ‘”Strongly Recommended” Revisiting Decisional Privacy to Judge Hypernudging in S elf-Tracking Technologies.’ Philosophy & Technology  https.  doi.org/10.1007/s13347-018-0316-4. La tonero, M (2019) Governing Artificial Intelligence: Upholding Human Rights and Human Dignity , Data &   Society.  Available at h ttps://datasociety.net/wp-con tent/uploads/2018/10/DataSociety_Governing_Artificial_Intelligence_Upholding_Human_Rights.pdf(Ac cessed 6 May 2019). Leon elli , S (2018) ‘Rethinking Reproducibility as a Criterion for Research Quality’ in Fiorito, L., Scheall,  S., an d Suprinyak, C.E. (eds.) Including a Symposium on Mary Morgan: Curiosity, Imagination, and Surprise (Resea rch in the History of Economic Thought and Methodology, Volume 36B) Emerald Publishing Li mited, 129 – 146. L iu, H.Y. (2016) ‘Refining  responsibility: Differentiating two types of responsibility issues raised by a utonomous weapons systems’.  Edited by N. Bhuta, S. Beck, R. Geiss, H.Y. Liu, and C. Kress. Au tonomous weapons systems —Law, ethics policy at 325-344. CUP: New York. Li u, H.Y. and Zawieska, K. (2017) ‘From responsible robotics towards a human rights regime oriented to th e challenges of robotics and artificial intelligence.’ Ethics and Information Technology .  19(3):1-13. Lo dge, M. and K. Wegrich (2012). Managing Regulation. London, Palgrave Macmillan.  Lo hr, J. Maxwell, W. and Watts, P. (2019) ‘Legal practitioners' approach to regulating AI risks.’ In Al gorithmic Regulation .  Edited by K. Yeung &  M. Lodge. OUP: Oxford.  In press. Lo ui, M. C. and Miller, K. W. (2008). ‘Ethics and Professional Responsibility in Computing’. In Wiley E ncyclopedia of Computer Science and Engineering . Edited by B. W. Wah. d oi:10.1002/9780470050118.ecse909Lu nney, M and Oliphant, K (2013)  Tort Law.  5th edition.  Oxford University Press: Oxford. Ma ngan, D. (2017) ‘Lawyers could be the next profession to be replaced by computers.’ 17 February.   Av ailable at at h ttps://www.cnbc.com/2017/02/17/lawyers-could-be-replaced-by-artificial-i ntelligence.html (Accessed 6.11.18). Ma ntelero, A. (2018) ‘AI and Big Data: A b lueprint for a human rights, social and ethical impact ass essment.’ Computer Law & Security Review  34(4): 754-772. Ma ntalero, A. (2019), Artificial Intelligence and Data Protection:  Challenges and Possible Remedies.  Re port prepared for Council of Europe, Consultative Committee of the Convention for the Protection of In dividuals with Regard to Automatic Processing of Personal Data, T-PD(2018)09Rev.  Guidelines on 
Council of Europe Study 9 0 Artifical Intelligence and Data Protection.  Available at h ttps://rm.coe.int/artificial-intelligence-and-data-p rotection-challenges-and-possible-re/168091f8a6 (Accessed 6 May 2019). Ma tthias, A. (2004) ‘The responsibility gap: Ascribing responsibility for the actions of learning automata.’ E thics and information technology  6(3): 175-183. Ma yer-Schönberger, V. and Cukier, K. (2013) Big Data–A Revolution That Will Transform How We Live, T hink and Work. London, John Murray.  Me nn, J. and D. Volz (2016) ‘Exclusive: Google, Facebook quietly move toward automatic blocking of e xtremist videos.’ Available at h ttps://www.reuters.com/article/us-internet-extremism-video-exclusive-i dUSKCN0ZB00M. (Accessed 7.11.18) McS herry, M. (2018) ‘Will AI Widen or Weaken the Global Digital  Divide?’ Medium, 21 May (Accessed 1 Ma y 2019). Me rton, R. K. (1942) ‘The Normative Structure of Science’.  In The Sociology of Science: Theoretical and E mpirical Investigations . Edited by R. K. Merton. Chicago, Il, University of Chicago Press: 267-278. Me tcalf, J., & Crawford, K. (2016). ‘Where are hum an subjects in Big Data research? The emerging ethics d ivide’ Big Data & Society. https://doi.org/10.1177/2053951716650211Me tzinger (2019) ‘Ethics Washing Made in Europe’, Der Taggespiegel, 8 April.  Available at h ttps://www.tagesspiegel.de/politik/eu-guidelines-ethics-washing-made-in-europe/24195496.html(Ac cessed 6 May 2019). Mi chalski, R.S., Carbonell, J.G. and Mitchell, T.M. (eds.) (2013) Machine learning: An artificial intelligence appr oach. Springer Science & Business Media.  Mi ller, A.A. (2014) ‘What Do We Worry about When We Worry about Price Discrimination  -The Law and E thics of Using Personal Information for Pricing’ J. Tech. L. & Pol'y 19:41. Mo rgan, B. and Yeung, K., 2007. An Introduction to Law and Regulation: Text and Materials . Cambridge: Cam bridge University Press. Mo ses, L.B. and Koker, L.D. (2017) ‘Open secrets: Balancing operational secrecy and transparency in the col lection and use of data by national security and law enforcement agencies.’ Melb. UL Rev. 41: 530. N arula, G. (2018) ‘Everyday Examples of Artificial Intelligence and Machine Learning.’  29 October.  Av ailable at h ttps://www.techemergence.com/everyday-examples-of-ai/ (Accessed 7.11.18) N emitz, P. (2018). ‘Constitutional Democracy and Technology in the Age of Artificial Intelligence’. Phil T rans. A. 376 N evjans, N. (2016) European Parliament, Legal and Parliamentary Affairs Committee (2016). European C ivil Law Rules for Robotics.  Study for the JURI Committee.  Available at h ttp://www.europarl.europa.eu/RegData/etudes/STUD/2016/571379/IPOL_STU(2016)571379_EN.pdf(Ac cessed 7.11.18) N ilsson, N.J. (2014) Principles of artificial intelligence . Morgan Kaufmann N issenbaum, H. (1996) ‘Accountability in a computerized society.’ Science and Engineering Ethics . 2(1): 2542. N issenbaum, H. (1996). ‘Accountability in a Computerized Society.’ Science and Engineering Ethics 2: 25-42.  N issenbaum, H. (2010) Privacy in Context: Technology, Policy and the Integrity of Social Life . Stanford CA : Stanford Law Books. 
DGI(2019)05 9 1 Nissenbaum, H. (2011) ‘A contextual approach to privacy online.’ Daedalus the Journal of the American Ac ademy of Arts & Sciences . 140(4): 32–48. N oto La Diega, G. (2018) ‘Against the Dehumanisation  of Decision-Making: Algorithmic Decisions at the Cros sroads of Intellectual Property, Data Protection and Freedom of Information.’ J ournal of Intellectual P roperty, Information Technology and E-Commerce Law . 9 (3): 11-16.. N uffield Foundation and the Leverhulme Centre for the Future of Intelligence (2019) Ethical and Social Im plications of Algorithms, Data and Artificial Intelligence: A Roadmap for Research .  Available at h ttps://www.adalovelaceinstitute.org/nuffield-foundation-publishes-roadmap-for-ai-ethics-research/(Ac cessed 6 May 2019). Ob erdiek, J. (2017) Imposing risk: a normative framework . Oxford: Oxford University Press. Ol sen, M. (1965). The Logic of Collective Action - Public Goods and the Theory of Groups . Cambridge, MA,  Harvard University Press. Ol iver, D., (1994) ‘Law, politics and public accountability. The search for a new equilibrium.’ Public Law 238 -238. O' Neil, C. (2016) Weapons of math destruction: How big data increases inequality and threatens demo cracy. Broadway Books. Os wald, M., Grace, J., Urwin, S. and Barnes, G.C., (2018) ‘Algorithmic ri sk assessment policing models: l essons from the Durham HART model and ‘Experimental’ proportionality’ Information & C ommunications Technology Law . 27(2):223-250. Oxera  (2018) Consumer Data in Online Markets .  Paper prepared for Which? 5 June. Pa squale, F. (2015) The Black Box Society: The secret algorithms that control money and information . H arvard University Press. Pa riser, E. (2012). The Filter Bubble. London, Penguin Books. Pa squale, F (2015). The Black Box Society.  Boston: Harvard University Press. Pi chai, S. (2018) ‘AI at Google: our principles’. 7 June. Available at: h ttps://www.blog.google/technology/ai/ai-principles/ (Accessed 6.11.18)   P olyakov, A. (2018) ‘Seven Ways Cybercriminals Can Use Machine Learning’. Available at h ttps://www.forbes.com/sites/forbestechcouncil/2018/01/11/seven-ways-cybercriminals-can-use-m achine-learning/#1e42a2a81447 (accessed 6.11.18) Po wer, M. (1997). The Audit Society. Oxford: Oxford University Press. Po wles, J. (2015). ‘We are citizens, not mere physical masses of data for harvesting ’. The Guardian. 11 Ma rch.  Available at h ttps://www.theguardian.com/technology/2015/mar/11/we-are-citizens-not-m ere-physical-masses-of-data-for-harvesting (Accessed 5.11.18) Pra insack, B. (2019). Logged out: Ownership, exclusion and public value in the digital data and formation c ommons.’ Big Data & Society. https://doi.org/10.1177/2053951719829773. Rai ney, B., Wicks, E. and Ovey, C. (2014) Jacobs, White and Ovey: the European Convention on Human Ri ghts (6th edition). Oxford: Oxford University Press. Ras o, F., Hilligoss, H,, Krishnamurthy, V., Bavitz,C., & Kim,. (2018) Artificial Intelligence & Human Rights: O pportunities & Risks. Berkman Klein Center for Internet & Society, Harvard University. Available at h ttps://cyber.harvard.edu/publication/2018/artificial-intelligence-human-rights (Accessed 5.11.2018) Raz , J. (1986) The Morality of Freedom.  Oxford: Oxford University Press.  
Council of Europe Study 9 2 Rieder, B. (2016) ‘Big data and the paradox of diversity’. Digital Culture & Society  2(2):39-54. Ri sse, M. (2018) ‘Human Rights and Artificial Intelligence: An Urgently Needed Agenda’.  Harvard Ken nedy School Faculty Research Working Paper Series . RWP18-015. Available at  h ttps://research.hks.harvard.edu/publications/getFile.aspx?Id=1664 (Accessed 6.11.18)  Roy vroy, A. (2016) ‘“Of Data and Men.” Fundamental Rights and Freedoms in a World of Big Data’.  R eport for the Bureau of the Consultative Committee of the Convention for the Protection of Individuals Wi th Regard to Automatic Processing of Personal Data , Council of Europe, TD-PD-BUR.  Available at h ttps://rm.coe.int/16806a6020  (Accessed 6.11.18).  T he Royal Academy of Engineering (2009) Autonomous Systems: Social Legal and Ethical Issues.  August.  Av ailable at h ttps://www.raeng.org.uk/publications/reports/autonomous-systems-report (Accessed 6.11.18 ). T he Royal Society (2017). Machine Learning: The power and promise of computers that learn by ex ample. April. Available at h ttps://royalsociety.org/~/media/policy/projects/machine-l earning/publications/machine-learning-report.pdf (Accessed 6.11.18). Ru ssell, S.J. and Norvig, P. (2016). Artificial intelligence: a modern approach . Malaysia: Pearson E ducation Limited. SAE  International (2018) Taxonomy and Definitions for Terms Related to On-Road Motor Vehicle Au tomated Driving Systems .  Available at h ttps://www.sae.org/standards/content/j3016_201806/(Ac cessed 6.11.18). Same k et al (2017). ‘Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learn ing Models.’ ITU Journal: ICT Discoveries , Special Issue No.1: 1-10. San dvig, C., Hamilton, K., Karahalios, K. and Langbort, C., (2014). Auditing algorithms: Research methods f or detecting discrimination on internet platforms. Data and discrimination: converting critical concerns i nto productive inquiry , 1-23. Available at ht tp://www-p ersonal.umich.edu/~csandvig/research/Auditing%20Algorithms%20--%20Sandvig%20--% 20ICA%202014%20Data%20and%20Discrimination%20Preconference.pdf (Accessed 7.11.18) Schu t, M. and Wooldridge, M. (2000) June. Intention reconsideration in complex environments. In  Proceedings of the fourth international conference on Autonomous agents . 209-216. ACM. Schwab , K., Davies, N. and Nadella, S. (2018) Shaping the Fourth Industrial Revolution .  World Economic Foru m. Scott , M. and Isaac, M. (2016) ‘Facebook Restores Iconic Vietnam War Photo It Censored for Nudity’. Av ailable at: h ttps://www.nytimes.com/2016/09/10/technology/facebook-vietnam-war-photo-n udity.html (Accessed 6.11.18) Sh adbolt, N. and Hampson, R. (2018) The Digital Ape. Scribe: Melbourne.  Ski lton, M. and Hovsepian, F. (2017). The 4th Industrial Revolution: Responding to the Impact of Artificial In telligence on Business . Springer.   Sm ith, A. (2018) ‘Franken -algorithms: the deadly consequences of unpredictable code’. The Guardian.  30 Au gust.   Available at h ttps://www.theguardian.com/technology/2018/aug/29/coding-algorithms-f rankenalgos-program-danger (Accessed 6.11.18) Sol ove, D.J. (2012) ‘Introduction: Privacy self -management and the consent dilemma’. Harvard Law Re view 126: 1880. Sol um, L.B. (1991) ‘Legal  personhood for artificial intelligences’ NCL Rev.70: 1231. Sp arrow, R. (2007) ‘Killer Robots’, Journal of Applied Philosophy 24(1): 62. 
DGI(2019)05 9 3 Su, Xiaoyuan, and Taghi M. Khoshgoftaar (2009) ‘A survey of collaborative filtering techniques.’ Ad vances in artificial intelligence .  Sw an, M. (2015). Connected car: quantified self becomes quantified car. Journal of Sensor and Actuator N etworks 4(1): 2-29. S ullins, J.P. (2005) ‘Ethics and artificial life: From modelling to moral agents’ Ethics and Information t echnology, 7(3), p.139. T aplin, J. (2018) Move fast and break things . Politikens Forlag. T eubner, G (2006) ‘Ri ghts of Non-Humans? Electronic Agents and Animals as New Actors in Politics and La w.’ Journal of Law and Society. 33: 497-521.  T ebuner, G (2018) ‘Digital Personhood? The Status of Autonomous Software Agents in Private Law’.  Av ailable via SSRN network (Accessed 21.5.2019). T homas, M (2015) ‘Should We Trust Computers?’ Gresham Lectures: London. 20 October.   Available at h ttps://www.gresham.ac.uk/lectures-and-events/should-we-trust-computers (Accessed 3 May 2019). T homas, M (2017a) ‘Safety Critical Systems’.  Gresham Lectures: London. 10 January.   Available at h ttps://www.gresham.ac.uk/lectures-and-events/safety-critical-systems (Accessed 3 May 2019). T homas, M (2017b) ‘Is Society Ready for Driverless Cars?’ Gresham Lectures, London, 24 October.  Av ailable at h ttps://www.gresham.ac.uk/lectures-and-events/is-society-ready-for-driverless-cars(acce ssed 3 May 2019);   T hompson, D. (1980) ‘Moral Responsibility of Public Officials: The Problem of Many Hands’, The Am erican Political Science Review 74(4): 905-916. doi:10.2307/1954312 T he Toronto Declaration: Protecting the rights to equality and non-discrimination in machine learning s ystems (2018). Available at  h ttps://www.accessnow.org/cms/assets/uploads/2018/08/The-Toronto-De claration_ENG_08-2018.pdf (Accessed 6.11.18) T ownley, C., Morrison, E., & Yeung, K. (2017) ‘Big Data and Personalized Price Discrimination in EU Com petition Law’. Yearbook of European Law 36(1): 683-748. T ufekci, Z. (2015) ‘Algorithmic harms beyond Facebook and Google: Emergent challenges of c omputational agency’. J. on Telecomm. & High Tech. L.  13: 203. U K Competition and Markets Authority (2018) Pricing Algorithms.  8 October.  CMA 94.  Available at  h ttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/746353 /Algorithms_econ_report.pdf (Accessed 3 May 2019). U K Information Commissioner’s Office (2018) Democracy Disrupted – Personal Information and Political In fluence.  11 July.  Available at h ttps://ico.org.uk/media/2259369/democracy-disrupted-110718.pdf(Ac cessed 3 May 2019). U K Government (2019) Online Harms White Paper , CP 57.  Available at h ttps://www.gov.uk/government/consultations/online-harms-white-paper/online-harms-white-paper-e xecutive-summary--2 (Accessed 3 May 2019). U K Department for Business, Energy and Industrial Strategy (2018) Artificial Intelligence Sector Deal .  Av ailable at h ttps://www.gov.uk/government/publications/artificial-intelligence-sector-deal (Accessed 6.11.18 ).   U K Department for Digital, Culture, Media and Sport (2018)  ‘ Up to £50 million to develop world leading A I talent in the UK’.  Available at h ttps://www.gov.uk/government/news/up-to-50-million-to-develop-w orld-leading-ai-talent-in-the-uk (Accessed 6.11.18). 
Council of Europe Study 9 4 UK House of Commons, Digital Culture Media and Sports Committee, Disinformation and ‘fake news’ : Fi nal Report, Eighth Report of Session 2017-2019, 14 February, HC 1791.  Available at h ttps://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/1791/1791.pdf (Accessed 6 May 2019).  U niversite de Montreal (2017) Montreal Declaration for the Responsible Development of AI .  Available at h ttps://www.montrealdeclaration-responsibleai.com/the-declaration (Accessed 6 May 2019). U N General Assembly (2018)  Report by the Special Rapporteur on the promotion and protection of the ri ght to freedom of opinion and expression.  Seventy-third session. 29 August.  A/73/348.  Available at h ttps://freedex.org/wp-content/blogs.dir/2015/files/2018/10/AI-and-FOE-GA.pdf (Accessed 7.11.18) U N Special Representative of the Secretary General (2011) Guiding Principles on Business and Human Ri ghts: Implementing the United Nations ‘Protect, Respect and Remedy’ Framework .  Endorsed by the U N Human Rights Council by Resolution 17/4 of 16 June 2011.  Available at h ttps://www.ohchr.org/Documents/Publications/GuidingPrinciplesBusinessHR_EN.pdf. (Accessed 7.11.18. ) U .S. Citizenship and Immigration Service (2018) Meet Emma, Our Virtual Assistant . Available at h ttps://www.uscis.gov/emma (Accessed 6.11.18). U .S. Department of Transportation (2017) Automated Driving Systems: A Vision for Safety 2.0 .  Available at  https://www.nhtsa.gov/sites/nhtsa.dot.gov/files/documents/13069a-ads2.0_090617_v9a_tag.pdf(Ac cessed 6.11.18) Vai dhyanathan, S. (2011). The Googlization of Everything: (And Why We Should Worry). University of Cal ifornia Press. Van  der Sloot, B. (2014) ‘Do data protection rules protect the individu al and should they? An assessment o f the proposed General Data Protection Regulation’. International Data Privacy Law  4(4):307-325. Van  Est, R. and J.B.A. Gerritsen, with the assistance of L. Kool (2017) Human rights in the robot age: C hallenges arising from the use of robotics, artificial intelligence, and virtual and augmented reality .   E xpert report written for the Committee on Culture, Science, Education and Media of the Parliamentary As sembly of the Council of Europe (PACE). The Hague: Rathenau Instituut.   Available at h ttps://www.rathenau.nl/sites/default/files/2018-02/Human%20Rights%20in%20the%20Robot%20Age-Rat henau%20Instituut-2017.pdf.  (Accessed 6.11.18). Ve ale, M. and Binns, R. (2017) ‘Fairer machine learning in the real world: Mitigating discrimination w ithout collecting sensitive data’ Big Data & Society 4(2) doi: 10.117 7/2053951717743530. Wa gner, B. (2017) ‘Study On The Human Rights Dimensions of Automated Data Processing Techniques (In  Particular Algorithms) And Possible Regulatory Implications’.  6 October.  Council of Europe, Com mittee of Experts on internet intermediaries  (MSI-NET).  Available at h ttps://rm.coe.int/study-hr-d imension-of-automated-data-processing-incl-algorithms/168075b94a (Accessed 6.11.18) Wa gner, B. (2019) ‘Eth ics as an Escape from Regulation: From ethics-washing to ethics- shopping?’ in Bei ng Profiling: Cogitas Ergo Sum.  Edited by M Hildebrandt. Amsterdam University Press, Amsterdam. Fort hcoming. Wa llace, R.J. (1994) Responsibility and the Moral Sentiments . Harvard University Press: Boston. Wa tson, G. (2004) Agency and Answerability: Selected Essays .  Clarendon Press: Oxford. Wel ler, A. (2017) ‘Challenges for transparency’.  Paper presented at 2017 ICML Workshop on Human In terpretability in Machine Learning (WHI 2017), Sydney, NSW, Australia.  Available at arXiv preprint a rXiv:1708.01870. (Accessed 6.11.18). Wh ich? (2018) Control, Alt or Delete?  Consumer research on attitudes to data collection and use.  Policy Re search Report.  June.    
DGI(2019)05 9 5 White, A. (2018) ‘EU calls for $24 billion in AI to keep up with China, U.S.’ Available at h ttps://www.bloomberg.com/professional/blog/eu-calls-24-billion-ai-keep-china-u-s/ (Accessed 6.11.18 ). Wi erzynski, C. (2018) ‘The Challenges and Opportunities of Explainable AI’ 12 January.  Available at h ttps://ai.intel.com/the-challenges-and-opportunities-of-explainable-ai/ (accessed 27.3.18).  Y ao, M. (2017) ‘Chihuahua or muffin? My search for the best computer vision API’.  Available at h ttps://medium.freecodecamp.org/chihuahua-or-muffin-my-search-for-the-best-computer-vision-api-cb da4d6b425d (Accessed 6.11.18). Y earsley, L. (2017). "We Need to Talk About the Power of AI to Manipulate Humans." 5 June.  MIT T echnology Review.  5 June.  Available at h ttps://www.technologyreview.com/s/608036/we-need-to-t alk-about-the-power-of-ai-to-manipulate-humans/ (Accessed 7.11.18). Ye ung, K., (2011) ‘Can we employ design -based regulation while avoiding brave new world?’ Law, I nnovation and Technology . 3(1): 1-29. Y eung, K. (2015) ‘Design for Regulation.’ In Handbook of Ethics, Values and Technological Design, edited b y M. J. Van Den Hoven, P.E. Varmaas and I. van de Poel . Dordecht: Springer.  Y eung, K. (2016) ‘”Hypernudge”: Big Data as a mode of regulation by design’.  Information, C ommunication & Society: 1-19. Y eung, K. (2017a) ‘Algorithmic regulation: a critical interrogation’, Regulation & Governance.  doi 10.111 1/rego.12158. Y eung, K., (2017b) ‘Blockchain, Transactional Security and the Promise of Automated Law Enforcement: T he Withering of Freedom Under Law?’ In  3THICS - The Reinvention of Ethics in a Digital  Age,  Edited by P.  Otto and E. Graf at 132-146. Y eung, K. (2018a) ‘Five Fears About Mass Predictive Personalization in an Age of S urveillance Cap italism’. International Data Privacy Law  8: 258-269 Y eung, K. and Weller, A. (2018b ). ‘How is ‘transparency’ understood by legal scholars and the machine l earning community?’ Being Profiling. Cogitas Ergo Sum . In E. Bayamlioglu, I. Baraliuc,   L.  W. Janssens and M. Hildebrandt (eds.) Amsterdam University Press.. Za lnieriute, M., et al. (2019). "The Rule of Law and Automation of Government Decision-Making." Mo dern Law Review 82: 397-424. Zl iobaite, I. (2015) A survey on measuring indirect discrimination in machine learning. Available at arXiv p reprint arXiv:1511.00148  (Accessed 6.11.18). 
Council of Europe Study 9 6 Zook, M. and Grote M. H. (2017). ‘The microgeographies of global finance: High -frequency trading and th e construction of information inequality’ Environment and Planning A: Economy and Space  49(1): 121-140  Zu boff, S., (2015) ‘Big other: surveillance capitalism and the prospects of  an information civilization’ J ournal of Information Technology  30(1): 75-89. Zwe ig, K. A., Wenzelburger, G. and Krafft, T. D ( 2018) ‘On Chances and Risks of Security Related Al gorithmic Decision-Making Systems’. European Journal for Security Research . 3(2):181-203. ** **** 
The Council of Europe is the continent’s leading human r ights organisation. It comprises 47 member states, including all members of the E uropean Union. All Council of E urope member states have signed up to the European C onvention on Human Rights, a treaty designed to pr otect human rights, democracy and the rule of law. T he European Court of Human Rights oversees the implementation of the C onvention in the member states.ENGPREMS 111819 w ww.coe.intAdvanced digital technologies and services, including AI t ools, come with extraordinary promise, particularly in the f orm of enhanced efficiency, accuracy, timeliness and conve-nienc e across a wide range of services. Yet the emergence of these  technologies is also accompanied by rising public anxi-et y regarding their potentially damaging effects for individu-als , for vulnerable groups and for society more generally. Giv en their pervasiveness in daily life, we must acquire a deeper  understanding of their impact on the exercise of human  rights and fundamental freedoms, and we should car efully consider how to allocate responsibility in case of adv erse consequences. If we are to take human rights seri-ously  in a globally connected digital age, we cannot allow the  power of our advanced digital technologies and systems, and  those who wield and derive benefits from them, to be ac crued and exercised without responsibility. Eff ective and democratically legitimised governance arrange-men ts and enforcement mechanisms must be put in place t o ensure that responsibility for the risks, harms and wrongs ar ising from the operation of advanced digital technologies ar e duly allocated. w ww.coe.int/freedomofexpression
