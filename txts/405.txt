1 Responsible use of AI for public policy:  Data science toolkit Felipe González  Teresa Ortiz  Roberto Sánchez Ávalos Responsible use of   AI for public policy:    Data science toolkit
https:/ /www.iadb.org/ Copyright © 2020 Banco Interamericano de Desarrollo. Esta obra se encuentra  sujeta a  una licencia Creative Commons IGO 3.0 Reconocimiento-NoComercialSinObrasDerivadas  (CC-IGO 3.0 BY-NC-ND) ( https:/ /creativecommons.org/licenses/ by-nc-nd/3.0/igo/legalcode )  y puede ser reproducida para cualquier uso no-comercial  otorgando el reconocimiento respectivo al BID. No se permiten obras derivadas. Cualquier disputa relacionada con el uso de las obras del BID que no pueda resolverse  amistosamente se someterá a arbitraje de conformidad con las reglas de la CNUDMI  (UNCITRAL). El uso del nombre del BID para cualquier fin distinto al reconocimiento  respectivo y el uso del logotipo del BID, no están autorizados por esta licencia CC-IGO y  requieren de un acuerdo de licencia adicional. Note que el enlace URL incluye términos y condiciones adicionales de esta licencia. Las opiniones expresadas en esta publicación son de los autores y no necesariamente  reflejan el punto de vista del Banco Interamericano de Desarrollo, de su Directorio  Ejecutivo ni de los países que representa.Felipe González, Teresa Ortiz y Roberto Sánchez ÁvalosResponsible use of   AI for public policy:    Data science toolkit
3 Responsible use of AI for public policy:  Data science toolkitInter-American Development Bank – Social Sector  The Social Sector is a multidisciplinary team whose actions are  based on the conviction that investing in people can improve their  lives and overcome development challenges in Latin America and  the Caribbean. Together with the countries of the region, the Social  Sector formulates public policy solutions to reduce poverty and  improve the provision of education, work, social protection, and  health services. The objective is to build a more productive region  with equal opportunities for men and women, and with greater  inclusion of the most vulnerable groups. For more information,  see www.iadb.org/en/about-us/departments/scl .   Inter-American Development Bank – IDB Lab  IDB Lab is the innovation laboratory of the IDB Group where  financing, knowledge, and connections are mobilized to catalyze  innovation oriented towards inclusion in Latin America and the  Caribbean. For IDB Lab, innovation is a powerful tool that can  transform the region by creating unprecedented opportunities for  populations in vulnerable situations due to their economic, social,  and environmental conditions. For more information see https:// bidlab.org/.  Organisation for Economic Co-operation and Development  – OECD The OECD is an international organisation that works to build  better policies for better lives. Our goal is to shape policies that  foster prosperity, equality, opportunity and well-being for all.  Together with governments, policy makers and citizens, we work  on establishing evidence-based international standards and  finding solutions to a range of social, economic and environmental  challenges. One example of standard-setting are the OECD  Principles on Artificial Intelligence (AI) that are the first such  principles adopted by governments. The principles promote AI  that is innovative and trustworthy and that respects human rights  and democratic values.    
4 Responsible use of AI for public policy:  Data science toolkitOECD.AI Policy Observatory The OECD.AI Policy Observatory is an inclusive hub for public  policy on AI. It helps countries encourage, nurture and monitor  the development and use of trustworthy AI.  Used by policy makers and other stakeholders in over 170  countries, OECD.AI has become a recognised centre for policyoriented evidence, debate and guidance, supported by strong  partnerships with actors from all stakeholder groups and with  other international organisations. It provides evidence-based  analysis on AI. OECD.AI is a unique source of real-time data and visualisations  on AI developments. It also contains a database of AI policies  from over 60 countries that allows governments to compare  policy responses and develop good practices. OECD.AI measures  our collective progress towards trustworthy AI; its network of  experts and the “AI Wonk ” blog facilitate collaborative AI policy  discussions.  Other OECD work relevant to the specific application of AI in the  public sector can be accessed through the OECD Observatory  of Public Sector Innovation (OPSI), which provides an overview  of AI measures implemented in the public sector, including  on AI governance policy making and public service design and  delivery.  fAIr LAC Initiative In collaboration with partners and strategic allies, the InterAmerican Development Bank (IDB) leads the fAIr LAC initiative  to promote the responsible adoption of Artificial Intelligence  (AI) and decision support systems to improve social services  delivery and create development opportunities to reduce social  inequality. This toolkit is part of a set of documents and tools  to guide technical teams and policymakers towards that end  (Pombo et al. 2020).fAIr LAC 
5 Responsible use of AI for public policy:  Data science toolkitAcknowledgements The authors would like to express special thanks to Cristina Pombo,  coordinator of the IDB fAIr LAC initiative, and to Professor Ricardo  Baeza-Yates, Director of Data Science at Northeastern University,  Silicon Valley Campus, and a member of the Group of Experts and  Experts of fAIr LAC, for their time and valuable contributions. The  authors are also grateful for the contributions from Karine Perset,  OECD.AI administrator, and Luis Aranda, OECD.AI policy analyst. The authors also appreciate the support and comments from  Luis Tejerina, Elena Arias Ortiz, Natalia González Alarcón, Tetsuro  Narita, Constanza Gómez-Mont, Daniel Korn, Ulises Cortés, José  Antonio Guridi Bustos, Cesar Rosales, and Sofia Trejo. 
6 Responsible use of AI for public policy:  Data science toolkitTabla de contenido ACERCA DE ESTE MATERIAL 6 Iniciativa fAIr LAC 6 ¿Por qué este manual? 6 ¿Para quién es este manual? 6 INTRODUCCIÓN 7 ML y sistemas de toma/soporte de decisiones 7 Componentes de un sistema de IA para políticas públicas 8 RETOS DEL CICLO DE VIDA DEL ML 12 Conceptualización y diseño 13 0.1 Definición correcta del problema y de la respuesta de política pública 13 0.2 Necesidad y Proporcionalidad 14 Fuente y manejo de datos 15 1.1 Calidad y relevancia de los datos disponibles 15 1.2 Información incompleta acerca de la población objetivo 18 Desarrollo de los modelos 22 2.1 Ausencia y errores de validación 22 2.2 Fugas de información 24 2.3 Clasificación: probabilidades y clases 25 2.4 Sub y sobreajuste 28 2.5 Errores no cuantificados y evaluación humana 29 2.6 Equidad y desempeño diferencial de predictores 30 Uso y Monitoreo 33 3.1 Degradación de desempeño 33 3.2 Experimentos y recopilación de datos 34 Rendición de cuentas 35 4.1 Interpretabilidad y explicación de predicciones 35 4.2 Explicabilidad de predicciones individuales 36 4.3 Trazabilidad 37 CUADERNILLOS DE TRABAJO  39 Fuente y manejo de datos 40 Desarrollo de los modelos 49 Rendición de cuentas 68 HERRAMIENTAS 72 REFERENCIAS 77TABLE OF CONTENTS fAIr LAC Initiative   Executive Summary  Why This Toolkit?  Who Is This Toolkit For?  Key Terms  Introduction   Machine Learning and Decision-Making or Decision-Support Systems  Components of an AI System for Public Policies  Challenges of the Machine Learning Life Cycle  1. Planning and Design   1.1. Correct Definition of the Problem and the Public Policy Response  1.2. OECD AI Principles  2.  Data Collection and Processing   2.1. Data Quality and Relevance of the Available Data  2.2. Data Qualification and Completeness for the Target Population  3. Model Building and Validation   3.1. Absence or Inappropriate Use of Validation Samples  3.2. Data Leakage  3.3. Classification Models: Probabilities and Classes  3.4. Under and Overfitting  3.5. Unquantified Errors and Human Evaluation  3.6. Fairness and Differential Performance of Predictors 4. Deployment and Monitoring   4.1. Performance Degradation  4.2. Experiments to Evaluate Model Effectiveness   5.  Accountability  5.1. Interpretability and Explainability of Predictions  5.2. Traceability  Tools   Tool 1: Robust and Responsible AI Checklist Tool 2: Data Profile  Tool 3: Model Card   Workbooks  Data Collection and Processing Model Building and Validation  Accountability  References 7  7  7  8  8  10  10  12  13  16  17 17  20  21  23  27  28  29  30  33  34  35 38  39  40  41  42  44  46  47 52  54  56  56 67  89  93 
7 Responsible use of AI for public policy:  Data science toolkitEXECUTIVE SUMMARY    From finance and insurance to agriculture and transportation, Artificial intelligence (AI)  technologies are diffusing apace in all sectors,  creating  opportunities but also raising  distinctive policy issues.   In the public sector, AI promises to generate productivity gains and improve the quality  of public services.  By  analyzing  social network activity  in real time, policy makers can  for  example  leverage  AI systems to  obtain a more accurate, evidence-based assessment of  the most pressing societal problems and needs. The outcomes and predictions made by AI  systems can inform policy formulation, implementation and evaluation.   Against this backdrop, governments around the world are equipping themselves with the  relevant technical skills to leverage the power of AI in support of public policy development.  However, given that AI-enabled public policy can significantly impact people’s lives and wellbeing,  a systematic  approach is needed to ensure that the appropriate safeguards are in  place to seize the opportunities from – and address the challenges posed by – the use of AI  systems by public policy teams.  Using the AI system lifecycle as the guiding framework for analysis, this toolkit provides   technical guidance for public policy teams  that wish to  use AI technologies to improve  their  decision-making processes  and outcomes.  For each phase of the AI system  lifecycle –planning and design, data collection and processing, model building and  validation,  and  deployment and monitoring – the toolkit identifies the most common  challenges of using AI in public policy contexts and outlines practical mechanisms to detect  and mitigate these challenges.   Policy makers and their technical teams should be accountable for the proper functioning of an  AI system at each phase of its lifecycle. In this regard, one chapter of the toolkit is dedicated to  exploring accountability-related issues in the use of AI for public policy and to outlining practical  mechanisms to addressing them.   True to its objective of promoting the responsible use of AI for public policy making each section  of the toolkit includes checklists to help guide practical implementation. A “data profile” tool  and a “model card” are also provided to help assess data issues and to document an AI system’s  characteristics,  the  assumptions  made,  and  the  risk mitigation measures  implemented  throughout the lifecycle. Moreover, the toolkit provides a section with a workbook containing  practical examples of some of the challenges and mitigation strategies covered in the report,  as well as the relevant code to implement them using R or other programming languages.  Through the fAIr LAC initiative and the OECD.AI Policy Observatory, the IDB and the OECD  have partnered to help move the AI policy discussion from high-level principles to practice  and implementation. This toolkit is a concrete step in this direction.    Why This Toolkit? Although there are a significant number of principles in support of ethical AI, they provide  high-level guidance on how it should or should not be developed, and there is very little 
8 Responsible use of AI for public policy:  Data science toolkitclarity on the best practices for putting those principles into operation (Vayena 2019). The  objective of this toolkit is to identify areas of risk and recommend mitigation measures to  avoid outcomes contrary to the aims of decision-makers. Such outcomes include undesirable  consequences, wasting of resources due to inadequate targeting, or any other outcome that  undercuts what decision-makers are seeking to achieve .1 Who Is This Toolkit For? This toolkit is intended for technical teams working on the application of machine learning  algorithms for public policy. However, it covers challenges common to other applications of  this technology. It is assumed that the reader has basic knowledge of statistics and programming, although when concepts are introduced, brief descriptions and additional references  are included. The toolkit includes workbooks with various examples of the challenges and  solutions discussed. Different types of models (linear, tree-based, and others) and different  implementations (R, Keras, Xgboost) are used to show that these problems arise regardless  of the choice of a particular tool or algorithm. Although the codes and examples were developed in R, all the topics and methodologies applied and described in this toolkit can be  implemented in any other programming language. 2  Key Terms •	Artificial intelligence: Machine-based system that is capable of influencing the environment by producing an output (predictions, recommendations or decisions) for a given  set of objectives. It uses machine and/or human-based data and inputs to (i) perceive  real and/or virtual environments; (ii) abstract these perceptions into models through  analysis in an automated manner (e.g., with machine learning), or manually; and (iii) use  model inference to formulate options for outcomes . AI systems are designed to operate  with varying levels of autonomy (adapted from OECD 2019c). •	Algorithmic fairness:   Mathematical representation of a specific definition of fairness  that is incorporated into the model selection and fitting process. It is important to take  into account that different definitions can be exclusive; that is, satisfying one could imply  not satisfying the others (Verma and Rubin, 2018). •	Algorithmic inequality:  Technical flaws in the models that produce a disparity of results  for protected groups, and which must be evaluated in the context of a given definition  of algorithmic fairness.  •	Decision-making systems: Related to the concept of automated and autonomous intelligence. Final decisions and their consequent actions are made without direct human  intervention – that is, the system performs tasks previously done by a human. In many  contexts, these systems are referred to as automated decision-making systems. •	Decision support systems: Related to the concept of assisted or augmented intelli gence, decision support systems generate information used as input for decision-making by a human.  1  This toolkit is not intended to regulate or explain what the aims and objectives of decision-making bodies and actors should  be.  2 All the material in this toolkit is reproducible according to instructions in the repository (https:/ /github.com/EL-BID/ Manual-IA-Responsable ), which contains a Dockerfile describing the infrastructure dependencies for its replication. The R  programming language is used along with the following packages: tidyverse, recipes, themis, rsample, parsnip, yardstick,  workflows, tune, knitr, and patchwork. 
9 Responsible use of AI for public policy:  Data science toolkit•	Machine learning:  Set of techniques that allow a system to learn behaviors in an automated manner through patterns and inferences instead of explicit or symbolic instructions entered by a human (OECD, 2019c). •	Predictive structure: types of models used to make predictions (linear, random forests,  neural networks), the algorithm parameters and hyperparameters and its interactions. •	Probabilistic guarantees: Using samples designed with randomization, it is possible  under certain assumptions to characterize the behavior of estimators and procedures  (with high probability) – for example, a 95% confidence interval for performance metrics  that contains the actual value that will be observed. •	Subpopulations of interest or protected subpopulations:  Subpopulations of the target population for which we want to have concrete performance evaluations of estimates or models. •	Target population:  Entire group of people, households, geographical areas, etc. targeted by a specific policy.
10 Responsible use of AI for public policy:  Data science toolkitIntroduction Machine learning is a subset of artificial intelligence (AI). Machine learning methods are increasingly used by decision-makers to inform actions or interventions in various contexts,  from business to public policy and service delivery. In practice, these methods have been  used with varying degrees of success, and there has been growing concern about how to  understand the positive or negative performance and influence of these methods on society  (Barocas and Selbst 2016; Suresh and Guttag 2019). Machine Learning and Decision-Making or Decision-Support Systems The Organisation for Economic Co-operation and Development (OECD) describes an AI system as a machine-based system that is capable of influencing the environment by producing  an output (predictions, recommendations or decisions) for a given set of objectives. It uses  machine and/or human-based data and inputs to (i) perceive real and/or virtual environments; (ii) abstract these perceptions into models through analysis in an automated manner  (e.g., with machine learning), or manually; and (iii) use model inference to formulate options  for outcomes. AI systems are designed to operate with varying levels of autonomy (adapted  from OECD 2019c). Figure 1. Stylised conceptual view of an AI system   Although machine learning methods are not the only type of algorithms that AI systems can  use, they are the ones that have seen the most growth in recent years. These methods constitute a set of techniques that allows a system to learn behaviors in an automated way  through patterns and inferences instead of explicit or symbolic instructions entered by a  human (OECD 2019c).  This toolkit discusses some of the most common challenges in the use of machine learning  technologies for decision making or decision support. These include detecting and mitigating  implementation errors and biases and evaluating the possibility of undesirable results for a  company, public sector institution, or society. Context /  environmentData / inputPerceiving Acting humans or    machineshumans or    machines Outcome /  outputAI modelAI System
11 Responsible use of AI for public policy:  Data science toolkitTwo archetypes of the use of machine learning in the decision-making process are considered:3 1. Decision-support systems: Related to the concept of assisted or augmented intelli gence, decision support systems generate information used as input for decision-making  by a human.  2. Decision-making systems: Related to the concept of automated and autonomous intelligence. Final decisions and their consequent actions are made without direct human  intervention. That is, the system starts to perform tasks previously done by a human. In  many contexts, these systems are referred to as automated decision-making systems .  There is a wide variety of techniques, expert knowledge of the subject, and modelling in general for the development of a successful decision-making/decision support system based on  machine learning. This toolkit is not intended to discuss particular machine learning methods  or specific hyper-parameter tuning processes (Hastie, Tibshirani, and Friedman 2017; Kuhn  and Johnson 2013; Gelman and Hill 2006), but rather to focus on the evaluation of those  methods and on the most important challenges shared across systems, regardless of the  type of algorithm or technology used. The evaluation of a machine learning system should be carried out on a case-by-case basis.  Questions such as “what is the maximum error rate tolerated?” or “what are unacceptable  biases?” can only be considered and answered within the specific context of their application.  This includes the purposes and motivations of decision-makers, as well as the risk(s) to end  users and stakeholders. In other words, many of the technical criteria should pertain to the  specific problem at hand. If the biases and limitations of decision-making/decision support  systems are known, even a system with low precision can be useful, if used responsibly. On  the other hand, if a system’s limitations are not understood, even high-performing systems  can lead to unintended consequences or misuse. Objectives •	This toolkit focuses on the subset of challenges that are related to technical  processes throughout the lifecycle of AI systems used for decision-making or  decision support.  •	This toolkit describes how different biases and deficiencies can be caused by  training data, problems, and decisions in the development of the model, or in the  validation or monitoring process, which can produce undesirable or biased results  in the decision-making process. 3 These two types of systems are generic, that is, they do not necessarily use machine learning. Also, these systems can be  interactive and learn dynamically using reinforcement learning techniques, but in this toolkit we only consider non-interactive systems.
12 Responsible use of AI for public policy:  Data science toolkitComponents of an AI System for Public Policies Decision-support AI Systems in the Public Policy Life Cycle  AI does not replace public policy making. Its function is to assist the public policy development cycle by providing information for decision-making. The AI-assisted public policy cycle  is made up of the following stages: 1. Identification of the problem: Every AI project should begin with correctly identifying  the issue that public policy seeks to address, along with the possible causes and consequences of that issue.   2. Policy formulation: The intervention or policy that is being considered to be applied to  certain people, units, or processes is formulated. We will generally assume that there is  evidence of the benefit of such a policy when applied to the target population. 3. Decision-making/Decision support system: Once the intervention is defined, the AI  cycle begins with the design and development of the decision-making/decision support  system, the result of which will be used to focus or guide the intervention chosen in the  previous stage (OECD, forthcoming).4   4. Policy implementation:  Public policy is implemented either as a pilot project or on a  larger scale.   5. Policy evaluation:  The effectiveness, reliability, cost, expected and unintended consequences, and other relevant characteristics of the policy measure are evaluated. If its  results are positive, the intervention is continued or scaled.  In parallel to the public policy making cycle, the development of an AI system has its own  lifecycle that includes the following stages (OECD, 2019c): (i) Planning and Design, (ii) Data  Collection and Processing, (iii) Model Building and Validation, (v) Deployment and Monitoring.  These phases usually take place iteratively and are not necessarily sequential (Figure 2).  Figure 2. Public Policy Lifecycle Supported by a Decision-making/Decision Support  System   Source: Prepared by the authors. 4  AI can be used in different ways, including recognition, event detection, forecasting, personalization, interaction support,  goal-driven optimization, reasoning with knowledge structures, or any combination of them embedded in a composite system (e.g., a driverless vehicle).Identification of  the problem Accountabilityii) Data Collection  and Processing iii) Model Building  and Validationiv) Verification   and Validationi) Planning and Design Evaluación PUBLIC POLICY LIFE CYCLE Implementación de políticaPolicy   formulation Decision-making /  support systemAI LIFE CYCLE
13 Responsible use of AI for public policy:  Data science toolkitIn the interrelation of these two cycles, important challenges are generated that should be  evaluated and considered during the development and use of robust and responsible AI systems.  Challenges of the Machine Learning Life Cycle The construction of robust and responsible decision-making or decision support AI systems  requires careful consideration of all possible sources of bias; investigation of deficiencies and  documentation of assumptions; clear definition of the algorithmic fairness objectives and criteria that the system must meet; understanding the limitations and tolerable errors in the  specific context of the system; and implementation of monitoring measures to avoid undesirable or biased results in decision-making.  To achieve this, this toolkit presents the common challenges and mistakes in building and applying machine learning methods during the AI system lifecycle. The most common problems,  the mechanisms to detect them, and suggestions to mitigate them are described according to  each stage of the AI system life lifecycle:  1. Planning and Design: Refers to the information and criteria from the public policy decision-maker necessary to conceptualize an AI project.   2. Data Collection and Processing: Focuses on the data generation process, the selection  and control over data sources and inputs5, and the identification and mitigation of deficiencies and biases in the data.   3. Model Building and Validation: Includes key principles and methods for building robust  and correctly validated models.  4. Deployment and Monitoring:  Evaluation of the model in production and monitoring of  key principles to avoid unexpected degradation. In addition, Accountability is a cross-cutting dimension in the AI system lifecycle that refers  to the explanatory and transparency-enhancing measures to promote understanding of the  mechanisms through which an AI system produces an output, the output’s reproducibility,  and the user’s capacity to identify and challenge errors or unexpected results. AI actors at  each stage of the lifecycle should be accountable for the proper functioning of an AI system  based on their roles and the context, and consistent with the state of art. Three tools are proposed to accompany the development of the AI system:  •	Tool 1: Robust and Responsible AI Checklist: This tool consolidates the main concerns by  stage of the AI life cycle. The checklist must be reviewed continuously by technical teams  and decision-makers.  •	Tool 2: Data Profile: This tool is an initial exploratory analysis during the Data Collection  and Processing stage of the AI lifecycle. It provides information to reassess the quality,  completeness, temporality, and consistency, as well as possible biases, potential damage, and implications of the use of an AI system.  •	Tool 3: Model Card: This tool is a final description of an AI system, reporting its main  assumptions and most important characteristics, as well as the risk mitigation measures  implemented.   5 See for instance the OECD Good Practice Principles for Data Ethics in the Public Sector, in specific the principle on AI systems and data. Available at: https:/ /www.oecd.org/gov/digital-government/good-practice-principles-for-data-ethics-in-thepublic-sector.pdf 
14 Responsible use of AI for public policy:  Data science toolkit Box 1. Sources of Bias in an AI System Bias poses one of the most pressing challenges throughout the AI lifecycle. Many of  the mitigation measures that should be considered during AI model development depend on the correct understanding and treatment of such biases. The problem of bias  should be addressed early in the development process through the implementation of  review points at each different stage of the lifecycle. At each review point, the experts  and end users of the corresponding system should be invited to verify and defend  the hypotheses made during each stage and to validate the results of the model. The  following are concepts that are important to explain. System error is the difference between the predicted value resulting from the model  and the real value of the variable that is being estimated.6 When an error is systemati cally in one direction or for a specific subset of the data or a specific subpopulation, it  is called bias. For example, if a variable’s value is consistently lower for one subgroup  in the data, such as the salary of women with respect to equally qualified men for an  equivalent job, the salary variable is biased. Conversely, if the error is random, it is called  noise .  The bias of an AI system can have ethical implications when its results are used to  formulate public policies that can be considered unfair or prejudicial to certain subgroups of the population. This assessment of bias is subject to a specific algorithmic  fairness definition, to be determined by public policy decision-makers.  An algorithmic fairness definition  is a mathematical representation of a public policy  objective that is incorporated into the model selection and fitting process. Its defini tion is a task for public policy decision-makers and not for technical teams. The modelling team should only carry out validations to ensure compliance. In Section 3 of  this toolkit, different definitions of algorithmic fairness and their implications will be  discussed in depth.  For example, in some cases the objective of a system may be bound by criteria such  as demographic parity, equality of possibilities, and having representation by quotas,  among many other criteria. On some occasions, compliance with one definition of  algorithmic fairness makes it impossible to comply with another, that is, they can be  partially or totally exclusive. There are different sources of bias . Some biases are intrinsic to the data, including  historical biases or undesirable states; that is, pre-existing patterns that should not be  replicated in the model.  Representation bias  occurs when there is incomplete infor mation due either to missing attributes, sample design, or total or partial absence of  data from subpopulations. Measurement biases  arise from the omission (inclusion) of  variables that should (not) be included in the model (Suresh and Guttag 2019). Other  biases appear due to methodological errors: for example, biases arise during training  due to errors in the validation processes, definition of metrics, and evaluation of results  (evaluation bias ), biases arise due to erroneous assumptions  about the target pop ulation that may affect the definition of the model; and biases arise due to the misuse  and monitoring of the models, whether because of inappropriate interpretations of  their results or temporary changes in the patterns in the real world or in the data-cap ture methods.  Throughout the different sections of this toolkit, the main reasons for  these biases will be presented, and different measures to mitigate them proposed. 6 In prediction models, there is a trade-off between variance and bias captured by the model and its learning generalization  goal. Models with high bias can create systems that under-fit and learn very little from the observed data, but models with  high variance can have the opposite effect and over-fit by perfectly learning the training data. The “Model Building and Vali dation” section of this toolkit describes these phenomena in greater detail and measures to mitigate the corresponding risks.
15 Responsible use of AI for public policy:  Data science toolkit Source: Based on figures from Suresh and Guttag (2019).Data   GenerationHistorical   BiasRepresentation   BiasMeasurement   Bias Implications in the real worldEvaluation   Bias Population   SelectionCollection Training Evaluation ValidationMODEL DATA POPULATION
16 Responsible use of AI for public policy:  Data science toolkit  1. PLANNING   AND DESIGN 16 Responsible use of AI for public policy:  Data science toolkit
17 Responsible use of AI for public policy:  Data science toolkit1. Planning and Design The implementation of an AI solution cannot be considered separately from the AI public policy lifecycle.7 The results of the AI system are only as good as the design of the public policy  intervention in which the system is embedded. AI is a tool – and not a substitute – for public  policy. This implies that projects that employ robust and responsible AI must start from the  definition of the problem and not from the technology itself.  1.1 Correct Definition of the Problem and the Public Policy Response This toolkit assumes that there are at least two types of actors involved in the development of  the AI system: the policy maker and/or decision maker and the technical team that develops  and implements the system. The definition of the public policy should always be the responsibility of the decision-maker with knowledge of the social dynamics and issues.  The technical team must be able to understand the problem so as to be able to orient the results of the model towards satisfying the goal of the desired intervention. Likewise, the technical team is responsible for advising and guiding the design of the system, explaining what is  feasible, and clearly delineating the system’s limitations and risks. Constant communication  between the decision-maker(s) and the technical team is required.  For instance, the definition of the population to which the system will be applied, the protected groups and protected attributes, and the algorithmic fairness measures to be applied  need to be discussed and understood by both sets of actors.8 These definitions have a direct  impact on how the quality and coverage of the data – as well as the quality of the model results – can be assessed. 1.2 OECD AI Principles Although AI has significant potential to streamline some processes and expand state capacity, it should also be noted that it is not a silver bullet. Once the problem and the type of  intervention have been defined, it is necessary to contextualize and rethink the use of AI and  machine learning in line with the OECD AI Principles (Box 2). It is important to consider the broader governance that frames the application of an AI system, including the standards and laws in the jurisdiction where the system is to be implemented. It is also important to establish appropriate requirements during the planning and  design of the system, as they can define or narrow development options for the technical  team. For example, explainability requirements in predictions could limit the use of some  algorithms for which it is very difficult to interpret the results.9  7  See the section entitled “Components of an AI System for Public Policies” in the Introduction of this toolkit.   8  Section 3 of this toolkit discusses different definitions of algorithmic justice and their implications in depth.  9  The concept of explainability is described in Section 5.1.2 of this toolkit.
18 Responsible use of AI for public policy:  Data science toolkit Box 2. OECD AI Principles The Organisation for Economic Co-operation and Development (OECD)’s AI Principles  promote the use of Artificial Intelligence (AI) that is innovative and trustworthy and  that respects human rights and democratic values. The principles set standards for AI  that are practical and flexible enough to stand the test of time. They include five values-based principles for the responsible stewardship of trustworthy AI:  • Inclusive growth, sustainable development, and wellbeing: Stakeholders should  engage in creating credible AI that can contribute to inducing outcomes that are  beneficial for people, as well as for the planet.  • Human-centered values and fairness: The values of human rights, democracy,  and the rule of law should be incorporated throughout the AI system’s lifecycle,  while allowing human intervention through safeguard mechanisms.  • Transparency and explainability: AI actors that develop or operate AI systems  should provide information to foster an overall understanding of the systems  among stakeholders that allows for people affected by AI systems to comprehend  the outcome and challenge the decision when needed.  • Robustness, security and safety: AI systems need to function appropriately  throughout their lifecycle. AI actors should ensure traceability and apply systematic  risk management approaches to mitigate risks.  • Accountability:  AI actors developing, deploying or operating AI systems should  respect the principles and be accountable for the proper functioning of those systems. The OECD AI Principles also contain five recommendations for national policies and  international cooperation: (1) Investing in AI research and development; (2) Fostering  a digital ecosystem for AI; (3) Shaping an enabling policy environment for AI; (4) Build ing human capacity and preparing for labor market transformation; and (5) Promoting international cooperation for trustworthy AI (OECD 2019a, 2019b). The principles  were adopted in May 2019 by OECD member countries and are the first international  standard on AI signed up to by governments. Beyond OECD members, other countries  including Argentina, Brazil, Costa Rica, Malta, Peru, Romania, Ukraine, Singapore and  Egypt have already adhered to the AI Principles, with further adherents welcomed. In  June 2019, the G20 adopted human-centred AI Principles that draw from the OECD AI  Principles.
19 Responsible use of AI for public policy:  Data science toolkit Box 3. Planning and Design Checklist   Correct definition of the problem and the public policy response:  • (Qualitative) Is the public policy problem clearly defined? • (Qualitative) Describe how this problem is currently being addressed – considering  responses by related institutions – and how the use of AI would improve the  government response to this problem.  • (Qualitative) Were the protected groups or protected attributes identified within  the project (e.g., age, gender, education level, race, level of marginalization, etc.)?  • (Qualitative) Were the actions or interventions to be carried out based on the  result of the AI system defined?  AI Principles • (Quantitative) Has the need for an AI system been justified, considering other  possible solutions that do not require the use of personal data and automated  decisions? • (Quantitative) Is there evidence that both public policy action and the  recommendation of the AI system will result in a benefit to people and the planet  by driving inclusive growth, sustainable development, and well-being?  • (Qualitative) For the implementation of these technologies, have there been  similar previous projects and have they been reviewed?  • (Quantitative) Have you considered minimizing the exposure of personally  identifiable information (e.g., by anonymizing or not collecting information not  relevant to the analysis)?
20 Responsible use of AI for public policy:  Data science toolkit  2.  DATA   COLLECTION AND  PROCESSING Responsible use of AI for public policy:  Data science toolkit 20
21 Responsible use of AI for public policy:  Data science toolkit2.  Data Collection and Processing  A plethora of data sources can be used to inform public policy decision-making: censuses,  surveys, administrative records, web page usage logs, and even satellite images. These data  become useful information when they describe the target population or the phenomenon  that is being analyzed. However, the data collected do not always have a frequency, disaggregation, or coverage that  make them relevant or afford them the quality required to be used for decision-making. For  example, surveys designed using probability sampling specify the type of analysis that can be  done with them – by design – but tend to be conducted infrequently and thus may be insuf ficient to capture patterns in the data. On the other hand, information from administrative  records or data from the Internet (interaction on social networks, visits and other measurements on web pages, etc.) and telephony (calls, GPS location, etc.) tend to have a much higher  frequency, but only in a few cases represent the whole population, so it is not always possible  to use these data to make decisions for the entire population.  Statistical AI systems are based on data. Regardless of whether a supervised or unsuper vised model is being implemented, training data are key for any machine learning system.  Data quality and qualification can be analyzed using criteria such as volume, completeness,  validity, relevance, representativeness, precision, timeliness, accessibility, comparability, and  interoperability from different sources. Defining these criteria precisely is difficult, as the  context of each problem entails subtle idiosyncrasies. Relevance and precision refer to the  quality of measurement and usefulness to inform the decision, while timeliness refers to the  fact that the data occur with the timeframe necessary to inform the problem to be analyzed.  Accessibility, comparability, and interoperability refer to the fact that the data can be extracted in a timely manner and different data sources have the necessary consistency to be  applied jointly in the analysis.10  This section addresses challenges in data collection and processing related to two common  data-related concerns for machine learning systems11: 1. Data quality and relevance of the available data; and  2. Data qualification and completeness for the target population.  Sections 2.1 and 2.2 touch upon some of the issues highlighted in the OECD Good Practice  Principles for Data Ethics in the Public Sector with regards to data quality and data qualifi cation. The Good Practice Principles aim to support public officials in the implementation of  data ethics in digital government projects, products, and services such that i) trust is placed at  the core of their design and delivery and ii) public integrity is upheld through specific actions  taken by governments, public organisations and, at a more granular level, public officials  (OECD, 2021). 2.1. Data Quality and Relevance of the Available Data Machine learning algorithms capture observed patterns and relationships from the data they  are trained on. Their objective is to identify these same patterns for new cases not observed  during the model training. For this reason, the training data determines the way in which the  10 At this stage it is recommended that the Data Profile ( see Tool 2 in this toolkit) be filled out.  11 While not covered in detail in this section, other data-related concerns – such as data domain and structure – are included in  the Data Profile.
22 Responsible use of AI for public policy:  Data science toolkit algorithm will behave. However, available data are not always ideal for every use case. Two  of the main problems are: 1. Undesirable or suboptimal states in collected data.  2. Bad correspondence between ideal and available variables. 2.1.1. Undesirable or Suboptimal States in Collected Data  The first challenge is to identify training data that may have captured “undesirable states” of  the real world. These “undesirable states” can include biases and inequities that can lead to  harmful outputs for certain subgroups of the population, as well as any other pattern that  could be considered suboptimal or undesirable from a social policy point of view.  Example In 2015, Amazon experimented with a human resources recommendation  system based on supervised learning techniques. The model was trained using  a database from the company’s candidate selection processes stored over the  previous 10 years. That database identified whether a candidate had been  accepted or rejected for the job by the department. The system was based on  the assumption that the algorithm could capture good candidates and reduce  the work of the human resources department when making a first selection of  candidates. What the team had not taken into account is that the technology  industry has been characterized as predominantly male. So the system tended to  recommend a higher proportion of men, since more men had been accepted into  those positions historically, creating a bias that seemed to show that men were  more successful when in fact it was capturing bias.  Box 4. Undesirable or Suboptimal States in Collected Data Checklist   • (Qualitative) Discuss possible historical social inequalities in the use case with specialists in the field. • (Quantitative) Perform an exploratory analysis of the available data with which the  model will be trained to identify possible historical biases or undesirable states. 2.1.2. Poor Correspondence between Ideal and Available Variables When public policy decisions are made, they are based on the definition of one or more “ideal” target variables that the decision-maker has in mind. However, the ideal variables may or  may not be available in the accessible data. In many cases, it is necessary to use substitute or  proxy variables to get closer to the ideal variable. When we introduce these types of variables  into machine learning models, we may be learning implicit biases that may not be desirable.  For example, a scholarship that seeks to benefit the smartest students (ideal variable) will run  into the problem of defining what is meant by “smart” and finding a variable that can describe  this concept. An IQ test assigns a value using a standardized test that is described as a proxy 
23 Responsible use of AI for public policy:  Data science toolkit variable for intelligence. However, the test measures only some dimensions of intelligence,  so it will underestimate the intelligence of some people (Wilson 2014).  The ideal target variables should be clearly stated. The available variables must be analyzed  to understand how suitable they are to be used as a proxy for the ideal variable. Systematic  biases must be identified within the context of their use. Examples The U.S. health care system implemented an algorithm to predict the medical  care needs of different patients. In this case, the public policy decision-maker  wanted a tool that would preventively indicate which patients were at high risk  of requiring more medical care using historical information from hospitals. Given  that the ideal complication risk variable was not available, the algorithm used  the expenditure incurred by patients during their illness as a proxy variable,  under the hypothesis that sicker people would end up spending more on medical  treatment to overcome the disease. Obermeyer et al. (2019) showed that this  system was racially biased because it underestimated the number of black  patients in need of health care. The racial bias was caused by this subpopulation  spending, on average, less money than white patients. Using expenditure as a  proxy for risk of complications showed that healthier white patients appeared  to require more healthcare than sicker black patients. In this case, using health  spending as a proxy for the need for medical care was inappropriate because it  was biased by an omitted variable of economic inequality.  Box 5. Poor Correspondence between Ideal and Available Variables   • (Qualitative) The ideal target variables should be clearly stated. The collected/available  variables must be analyzed to understand how suitable they are to substitute for the  target variable. Systematic biases or validity of the proxy metric should be identified. • (Qualitative) Has the use of the selected response variable been clearly justified for  the purposes of the intervention? 2.2. Data Qualification and Completeness for the Target Population Machine learning models used in the public sector are intended to generate information  that inform actions or policies for a target population. Most of the time, data sources do not  include the entire population (as would be the case of a census), so only a subset or sample  is available (e.g., a survey, administrative database, etc.), from which one should seek to develop extrapolations, predictions, or estimates that help in decision-making.   2.2.1. Probabilistic and Natural Samples  In statistics, a sample is a subset of cases or individuals from a population. There are two  sampling possibilities: 1. Probability sampling: This is the name given to a sample in which the cases are selected from a probabilistic design that infer several possible models to explain data and for  which deciding which model to use is uncertain (e.g., simple, stratified, cluster random 
24 Responsible use of AI for public policy:  Data science toolkitsample, etc.). In this case, all the predictions and estimates that are to be applied to  the target population can be evaluated for their precision with probabilistic guarantees.  That is, error ranges can be provided for estimates of quantities associated with the  entire target population.  For example, a national household survey with a probabilistic design generally consists  of a definition of stratification, with units of random selection at different levels (primary, secondary units, etc.). Each household is selected with a known probability. Even if  the sample is designed in a non-representative way (e.g., more households in rural or  low-income areas), it is possible to make inference for the entire population with certain  guarantees about the size of the estimation error. 2. Natural samples (non-probabilistic):  A natural or non-probabilistic sample occurs  when the cases are not selected randomly but by a flawed process or a partially un derstood natural process. In this case, it is not possible to know what will happen when  we apply a policy resulting from a model in the general population; nor is it possible  to construct error ranges for predictions and estimates using statistical methods that  have probabilistic guarantees. That is, the estimated quantities and predictions have  unknown error, the models and characteristics useful in the sample may not apply in  the target population, and the situation may be aggravated for underrepresented – or  protected – groups (see Williams 1981, who shows that predictive values of anemia may  be different for different racial groups, and that predictions developed for one group  may perform poorly for another.)  A usual case of this type of sample occurs when particular subgroups of the population  are excluded by a flawed data capture mechanism (selection bias). This is the case for  social networks or phone call record data where the population without access to the  Internet or a smartphone is excluded.  Natural samples of data can result in:  • Estimation and/or prediction errors or biases • Predictive structures different from those we would observe in the target popula tion (invalid models) • Extrapolations that are not supported by the data • Under- or over-representation of subsets of the population. Probability sampling would be the preferred method for most machine learning projects. In  this case, it is possible to understand exactly which sub-populations were sampled, at what  rates, and how these rates are related to population rates. However, having a probability  sample is not always possible. This does not mean that natural samples are not useful, since in many cases they are the only  source of data available for decision-making. However, it is important to understand where  the data come from in order to take into account its limitations and identify the risks involved  when making decisions for the entire population.  A typical case is the data samples that come from social networks where the user’s demographic composition differs substantially from the general population. A study for the United  Kingdom found that, on average, Twitter and Facebook users are considerably younger than 
25 Responsible use of AI for public policy:  Data science toolkit the general population and more likely to have higher levels of education than non-users  (Prosser & Mellon, 2016). Any study with these data should explain how these particularities  can affect the results. An important element to take into account is that having balanced samples in terms of popu lation characteristics is neither a necessary nor a sufficient condition to qualify the database  as appropriate for the construction of machine learning models. For example, in the case of  information collected from social networks, having a sample that contains 50 percent men  and 50 percent women does not tell you anything about the type of conclusions that can be  drawn from those data. This is because the selection of these observations, not occurring  through a probabilistic process, could present a bias in some other dimension and will not  necessarily generalize to the total population.  Box 6. Probabilistic and Natural Samples Checklist • (Qualitative) Have the possible differences between the database and the population  for which the AI system is being developed been analyzed? (Use literature related  to the topic and information from experts. Study in particular unmeasured selection  biases.) • (Quantitative) Although models can be built with various data sources, designed or  natural, validation should ideally be carried out with a sample that allows statistical  inference to the target population. The validation sample must appropriately cover  the target population and sub-populations of interest. 2.2.2. Missing or Incomplete Attributes   Many machine learning projects are compromised because of poor data qualification. When  collecting data from the real world through non-probability samples, it is very common for  some observations to have missing data, that is, observations for which not all the attributes  are available. Missing or incomplete attributes are a phenomenon that can have a significant effect on  the conclusions drawn from the data. On the one hand, when crucial information about the  units is unknown, this can result in models with poor performance and little utility for decision-making. The absent information might also be associated with relevant characteristics  of the units for which you want to predict. When there are missing observations, different imputation methods can be implemented,  but it is important to explore the reasons or the “censorship mechanism” behind the missing  values. In the literature there are three main assumptions (Rubin 2002): •	Missing Completely at Random (MCAR): Occurs when the probability of missing is the  same for all observations – that is, the censorship or fault occurs totally randomly. •	Missing at Random (MAR): Occurs when the missing values do not depend on the values that this variable takes, but there is a relationship between the missing values and  other observed data of the individual.  •	Missing Not at Random (MNAR): Occurs when the missing values depend on the values  that this variable takes or on unobserved data. For example, people with higher income  tend not to disclose their income on self-reported income surveys.
26 Responsible use of AI for public policy:  Data science toolkit Box 7. Missing or Incomplete Attributes Checklist • (Qualitative) Has an analysis of missing values and missing variables been performed? • (Qualitative) Have important omitted variables – for which there are no associated  measurements – been identified? (If any) • (Qualitative) Have the reasons for the missing observations been identified? (If any) • (Quantitative) The imputation processes have to be evaluated in terms of their sensitivity to assumptions and data. Preferably, multiple imputation methods should be  used to assess imputation uncertainty (Little and Rubin 2002; Buuren and Groothuis-Oudshoorn 2011). 2.3. Causal Comparison When humans rationalize the world, they try to understand it in terms of cause and effect. If we  understand why something happened, we can alter our behavior to change future outcomes.  A machine learning model can give us results that seem to describe causal relationships that  do not necessarily exist. This can lead to inadequate policies and wrong decision-making.  Econometric techniques such as randomized controlled trials, natural experiments, difference-in-differences methods, and instrumental variables are used to assess causality. They  control for phenomena such as selection bias or endogeneity due to omitted variables,  among others. In recent years, through works such as Athey (2018), machine learning algorithms have begun to introduce these experimental techniques. Processes like A/B testing  have started to be used broadly in digital contexts because they facilitate the creation of  large experiments on the Internet. However, in most cases machine learning algorithms do  not seek to describe causal relationships, so it is necessary to be careful with this type of use  of algorithms (Stuart 2008). Box 8. Causal Comparison Checklist • (Qualitative) Understand and describe the reasons why the response variable is correlated with known and unknown variables. Describe possible biases based on expert  knowledge and analysis.  • (Qualitative) In case no work has been done to ensure causality in the results, were the  limitations of the results explicitly communicated to the public policy decision-maker?  • (Quantitative) In the case of attempting causal inference with models, the hypoth eses, considerations, or methods used to support a causal interpretation must be  described. Robustness checks should be conducted and documented. Activity:Filling out the Data Profile (see Tool 2) is  recommended during the Data Collection and  Processing stage of the AI lifecycle. At the end of this  stage, it is recommended that the Source and Data  Management Section of the Model Card (see Tool 3)  be filled out and that a discussion be held with the  public policy decision-maker.Activity:
27 Responsible use of AI for public policy:  Data science toolkit  3. MODEL BUILDING  AND VALIDATION Responsible use of AI for public policy:  Data science toolkit 27
28 Responsible use of AI for public policy:  Data science toolkit3. Model Building and Validation The process of developing a model involves making many decisions with implications for the  model’s results. Several types of methodological decisions, if flawed, may lead to errors that  generate biases or that prevent the system from generalizing results adequately.  Another group of decisions is conceptual in nature and can substantially change the way the  system behaves. How do we choose between two models? What type of errors do we report?  What definition of algorithmic justice should we choose? As discussed at the beginning of the  toolkit, none of these questions make sense outside the context of the AI system’s specific  application. However, it is possible to create a framework for understanding these errors so  that they can be discussed by technical teams and public policy decision-makers.  3.1. Absence or Inappropriate Use of Validation Samples  Machine learning models are primarily trained to create predictions in unobserved cases. It  is useless to evaluate a system in terms of its prediction performance regarding the observations with which it was trained, since the system could only memorize each answer.12 The  system’s usefulness lies in the extent to which it can make correct predictions using data  outside the training set ( out-of-sample ). Validation generally involves at least two data samples (training and validation), and preferably three (Figure 3): 1. Training data: Subset of the data used to train the model.  2. Validation data: Subset of the data with which the training is evaluated iteratively.   3. Test data: Subset of the data that should be kept hidden until after the model is selected  and used to confirm the results.  To avoid a random partition in training and validation data that favors or hinders the evaluation, a cross-validation is generally carried out. This consists of dividing the data into k pieces,  calculating the average of k evaluations, where the validation data are each of the pieces and  the remaining k-1s are the training data. This is called k-fold evaluation and usually k = 5 or k  = 10 is chosen.  Figure 3. Evaluation Stages Source: Prepared by the authors. 12  This phenomenon is related to overfitting, which will be discussed later.Selection of the  model with the  validation datasetTuning with  validation dataset  Confirmation of  results with test  datasetTrain the model  with training  datasetEvaluation   with validation   dataset
29 Responsible use of AI for public policy:  Data science toolkit The first challenge is not having an appropriate validation process. In this case, the model  results would only represent the training dataset. The performance metrics of this set should  not be used as an indicator of the potential behavior of the model for new cases, as it could  be overestimating its performance.  Successful validation is also related to quality criteria such as the completeness and representativeness of the information (see Section 2). This is because, if the target population is  different from that represented by the data used during training, that population might have  completely different behavior, even if the evaluation process was carried out correctly, Box 9. Absence or Inappropriate Use of Validation Sample Checklist • (Quantitative) Were the validation and test samples constructed properly? Did they  consider an appropriate size, covering subgroups of interest and protected and avoiding information leaks during their construction?  • The construction of the validation sample must be produced under a sampling  design that allows inference to the target population (Lohr 2009). • The validation sample should cover subgroups of interest and protected, so that it  is possible to make inferences as to their subpopulations. That includes appropriate sample sizes according to the sampling methodology (Lohr 2009). • If such a sample is not available, it is essential that there be an analysis of risks  and limitations of the natural sample conducted by experts and professionals who  know the process that generated these sample data. 3.2. Data Leakage  Data leakage occurs when information from outside the designed training dataset is used in  the creation of the model, contaminating the training dataset (Kaufman, Rosset, and Perlich  2011).  This additional information modifies the learning process and casts doubt on model  validation as a way of estimating the production performance of system.  This occurs in two ways: •	Training-test contamination: When the training sample receives data leaks from the  test or validation set.  •	Target leakage: Inclusion of a feature that is not going to be available when the model is  used in production.  3.2.1. Training-Test Contamination Training-test contamination occurs when all or part of the validation or test samples are  used for the construction of the models during training. This error often results in unrealistic  performance levels in the validation set because the model is making predictions based on  observations that it has seen before. This error often occurs when applying pre-processing methodologies that aggregate and  share information from the database composition to individual observations – for example,  scaling a variable, creating co-variables with averages or counts, over- or under-sampling,  etc. These processes should be performed after splitting the training and validation dataset.
30 Responsible use of AI for public policy:  Data science toolkit Box 10. Training-Validation Data Leak Checklist • (Quantitative) Any processing and preparation of training data should avoid using the  validation or test data in any way. A solid barrier must be maintained between training versus validation and testing. This includes data recoding, normalizations, selection of variables, identification of outliers, and any other type of preparation of any  variable to be included in the models. This also includes sample weights or balances  based on over- or under-sampling. 3.2.2. Target Leakage This error occurs when a model is trained with information that will not be available in the  same way or with the same quality when the model is put into production. This generally has  to do with the temporality of the data or groupings. In more subtle cases this error can be  difficult to detect, since the variable is present, but the information is updated retroactively. An example of this can be seen in crime and mortality statistics. Reports of a theft may take  time to be reported in authorities’ databases due to bureaucratic or administrative processes, and the observed incidence of a period could systematically increase as time passes.  In  this example, the target variable is available in production but may not be complete given  certain lags inherent to the reporting. If this phenomenon is not considered during the training and data are used that are already complete, the evaluation of the model may appear  accurate, but in production the accuracy of the data will be significantly degraded. Box 11. Target Leakage The validation scheme should replicate as closely as possible the scheme under  which the predictions will be applied. This includes replicating: • Temporary windows for observation and registration of variables and prediction  windows. • If there are groups in the data, considering whether there will be information available for each group when the prediction is made, or whether it would be necessary  to predict for new groups. 3.3. Classification Models: Probabilities and Classes In machine learning, supervised classification algorithms are systems whose objective is to  assign a category or class label to new observations.  It is called binary classification when  the target variable has two classes (e.g., classifying an email as spam or not spam), and multiclass classification when there are more than two classes (e.g., plant species identification  algorithm). 3.3.1. Imbalanced Data   In a classification problem, an imbalanced dataset occurs when the distribution of observations across the known classes is not equally distributed. These types of datasets will have  one or more classes with many examples referred to as the majority class, and one or more  classes with fewer observations referred to as the minority classes (e.g., groups with less  than 1 percent of total observations). These later groups present considerable difficulties for  predictive models, as there may be little information about them. 
31 Responsible use of AI for public policy:  Data science toolkit In highly imbalanced data, class predictors can perform poorly (e.g., they never predict the  minority class) even if the performance measures are good. Notably, if we always predict the  majority class, the accuracy will be equal to the percentage of elements in this class.  Examples • Consider that you have 1 million data points, with 999,000 in the majority  class and 1,000 in the minority class. It may be a good idea to subsample the  negatives by a given fraction (say 10 percent) by re-weighting each negative  data point by 10. • Consider that you have 1 million data points, with 999,950 in the majority class  and 50 in the minority class. It may be impossible to properly discriminate  the 50 observations. Building validation sets makes the situation worse: you  cannot validate predictive performance or build a well-performing model. In  these cases, it may be best not to build the model until more information is  gathered. Box 12. Class Imbalance Checklist • (Quantitative) Make probability predictions  instead of class predictions. These probabilities can be incorporated into the subsequent decision process as such.  • (Quantitative) When the absolute number of minority cases is very small, it can be  very difficult to find appropriate information to discriminate that class. More data  need to be collected from the minority class. • (Quantitative) Sub-sampling the dominant class (weighting the cases up to avoid losing calibration) can be a successful strategy to reduce data size and training time  without affecting predictive performance. • (Quantitative) Replicate the minority class to better balance the classes (over-sampling). • (Quantitative) Some machine learning techniques allow you to weight each class by a  different weight so that the total weight of each class is balanced. 3.3.2. Arbitrary Cut-off Point   In classification problems for decision-making, it is recommended that   be used instead of  classifying the observation only with its most probable class. The output of a probabilistic  classification algorithm is a probability distribution over the set of classes. These methods  can provide information to the policymaker about the uncertainty regarding the classification. To make the decision on whether the observation should be classified as positive or negative, the technical team must choose the threshold at which the observation is classified as  belonging to each class. A cut-off point of 0.5 is often mistakenly accepted for binary classifications, as this is the default value for many machine learning models. This decision can 
32 Responsible use of AI for public policy:  Data science toolkit have important implications if it is made outside the context of the problem at hand, so it is  important that it be discussed and selected taking into consideration the types of errors and  their implications.  Box 13. Arbitrary Cut-off Point Checklist • (Quantitative) Using probabilistic classification algorithms  is more suitable for decision-making to incorporate uncertainty regarding the classification.  • (Quantitative) Avoid standard probability cut-off points such as 0.5. Choose an optimal interpretation of the predicted probabilities using the receiving operating characteristic curve and other measures to analyze errors. 3.3.3. Adequateness of Assessment Metrics   In classification problems, the cut-off points are taken with criteria related to the context of  the decision. Most are constructed by analyzing the classification confusion matrix, as shown  in Table 1. Table 1. Confusion Matrix   Real   Positive Negative PredictedPositive True positive (TP) False positive (FP) Negative False negative (FN) True negative (TN) Errors in a classification model can be divided into false positives and false negatives. A false  positive is an observation for which the model incorrectly predicts the positive class. And a  false negative is an observation for which the model incorrectly predicts the negative class.  These performance measures can be combined in different ways depending on the use case  and the social policy objective. The most commonly used metrics are: 1. Accuracy: One of the most used metrics to evaluate classification models is the fraction  of predictions correctly made by the model:   2. Precision:  Fraction of those observations classified as positive by the model that were  actually positive.   3. Sensitivity (Recall): Fraction of positives observations that the model classified correctly.  Accuracy =TP + TN TP + TN + FP + FN Precision =TP TP + FP Sensivity =TP TP + FP
33 Responsible use of AI for public policy:  Data science toolkit 4. Specificity: Fraction of negatives observations that the model classified correctly.   The context should be considered when defining the criteria to assess classification models.  For example, if the model is ranking the prevalence of a fatal disease, the cost of not diagnos ing a sick person’s disease (false negative) is much greater than the cost of sending a healthy  person for more tests (false positive). In other words, depending on the application, the cost  of false negatives can be very different from the cost of false positives. For this reason, the  use of cost-benefit analysis is recommended, since it compares the result of the model in the  decision-making context.  These criteria can also be misleading depending on the composition of the training and evaluation database. For instance, where imbalanced data are used, an accuracy of 95 percent  can actually mean significant model underperformance. Partial solutions to this issue include  using measures that combine precision and sensitivity such as the F1 score or the Precision-Recall curve that can help analyze the trade-off between true positives and false positives in the context of the application. Box 14. Adequacy of the Assessment Metrics Checklist • (Qualitative) Were the implications of the different types of errors for the specific use  case and the correct way to evaluate them questioned? • (Qualitative) Were the limitations of the model clearly explained? This implies identi fying both false positives and false negatives and the implications that a system decision would have on the life of the target population. • (Quantitative) Was a cost-benefit analysis of the system conducted and compared  with the status quo or with the use of other decision-making or decision support strategies? (When possible) 3.4. Under and Overfitting Generalization refers to the ability of a model to perform accurately on unobserved data  during the training process. Generalization is important because the data collected are only  a sample, and as such they may be incomplete and noisy. When a model fails to generalize  or performs poorly, it is usually due to one of the following related phenomena (Figure 4):   •	Overfitting occurs when the model memorizes the particularities of the training data but  is unable to generalize to unseen examples. A model that is too complex for the available  data tends to capture non-informative characteristics as part of the predictive structure.  This is often reflected in a model that performs very well on the training data but has a  poor performance on the validation dataset.  •	Underfitting occurs when the model is unable to perform well with the training data or  generalize to new data. This happens when individual characteristics of observations are  over-grouped and given little weight. An underfitted model tends to ignore patterns in the  predictive structure. This is reflected in systematic and identifiable errors – for example,  systematic under or overprediction for certain groups or values of the input variables.Specificity=VN VN + FP
34 Responsible use of AI for public policy:  Data science toolkit Figure 4. Under and Overfitting Source: Prepared by the author Box 15. Underfit and Overfit Checklist • (Quantitative) Overfitting: If necessary, methods should be refined to moderate the  overfitting, including such methods as regularization, restricting the functional space  of possible models, using more training data, or disturbing the training data (Hastie,  Tibshirani, and Friedman 2017).   • (Quantitative) Underfit: Data on protected groups or other sensitive variables should  be reviewed to verify that there are no undesirable systematic errors. 3.5. Unquantified Errors and Human Evaluation  In many cases, some biases in the model are not captured by the chosen performance metrics.   For example, a document search system that performs well in performance metrics may systematically return short documents, producing biased results and over-selecting promotional or briefing-type documents. Reasons for this type of bias can range from pre-processing  errors – including miscalculated data attributes – to selecting attributes that only consider  part of the problem. 3.5.1. Failures Not Measured by the Model Some algorithms produce low-quality results that escape the lens of the validation metrics.  These models may have poor performance when put into production. Reasons for this include: •	Pre-processing errors when calculating predictions. •	Treatment of data that excludes important metrics to make quality or fair predictions. •	Absence of metrics that measure certain types of errors. This can be a difficult problem to solve, as these errors may be non-visible or not directly  measurable. It is necessary to discover these biases or errors outside the technical evaluation context, and if possible include additional evaluation metrics that would capture these  problems. Under Fitted Good Fit Overfitted
35 Responsible use of AI for public policy:  Data science toolkit Box 16. Unmeasured Errors and Human Review Checklist • (Qualitative) Was a human assessment conducted with use case experts to look for  known biases or errors? Establishing monitoring schemes that allow for the identi fication of unmeasured errors or biases is recommended.  For example, panels of  reviewers can be used to examine predictions and consider whether they are reasonable. These panels must be balanced in terms of user type and expertise and include  decision-makers if necessary. 3.6. Fairness and Differential Performance of Predictors Machine-learning-based methods can produce unfair or discriminatory results for subgroups  of the population (Buolamwini and Gebru 2018; Barocas and Selbst 2016; Bolukbasi et al.  2016). This may be caused by all the aforementioned challenges, including poorly designed  sourcing and handling of the data and errors in the design of the model.  Examples of differential performance and bias include different acceptance rates for receiving benefits in different groups or detection errors in human faces that are different depend ing on race. The evaluation of the results of a decision-making or decision support system is carried out  taking into account the objectives of the decision-maker, which may be different from and  even contradictory to the objectives from the point of view of the machine learning problem.  For example, a decision-maker might sacrifice the overall performance of a model to improve  the performance of the model in a subgroup, even though this subgroup is small compared  to the population as a whole (e.g., affirmative action to correct existing social gaps).  Although the analysis of the ethical implications in machine learning models and their relationship with a definition of justice is still an open field of study, there is an important strand  of literature that seeks to implement mathematical definitions of fairness in the models to  describe their impartiality towards subgroups and their ability to make decisions that mitigate unwanted outcomes. 3.6.1. Algorithmic Fairness and Inequality What is meant by “justice” can change according to a culture or tradition and can also be specific to a public policy project or problem. For example, in certain cases policies seek social  inclusion through affirmative action – such as diversity quotas and reparation policies – while  in other cases these policies are simply based on regional or territorial arguments. These  criteria should be integrated in the design process, in the analysis of the training data, during  the error evaluation process, and in the output of the system. This process can be separated  into two important stages: Terms Protected attribute: A protected characteristic or variable is one for which model  predictions should meet a certain fairness criterion. More than one protected variable  can exist in a dataset (e.g., age, gender, race, etc.).
36 Responsible use of AI for public policy:  Data science toolkit•	Algorithmic fairness definition:   Mathematical representation of a specific definition of  fairness that is incorporated into the model selection and fitting process. It is important  to take into account that different definitions can be exclusive; that is, satisfying one may  imply not satisfying the others (Verma and Rubin 2018). •	Algorithmic inequality:  Technical flaws in the models that produce disparity of results  for protected groups, and that must be evaluated under the definition of algorithmic fairness previously determined.  The objective of the model developer is to establish clear guidelines to avoid deficiencies in  the model from producing undesirable disparities for subgroups of a protected variable (e.g.,  gender, race, or level of marginalization). For this, it is necessary to select a definition of algorithmic fairness in advance. The following three definitions of algorithmic justice are among  the most widely used (although others can be defined depending on the particular problem  and objectives of the decision-maker): 1) Omission of protected variables and demographic parity Two widely contested disparity-prevention strategies between groups of a protected variable   are to ignore the variable  and to aim to achieve demographic parity  in predictions. The first strategy is intended to eliminate the possibility of disparity by not including variable    in the model. This approach usually fails to solve the problem because: •	Typically, there are other attributes associated with  that can produce similar results, even  if  is not considered (e.g., geographic area or postal code and socioeconomic level). •	There may be important reasons to include  in predictive models. For example, in the case  of blood pressure, there are variations in racial groups () in terms of predisposition to high  blood pressure (Lackland 2014), so a model that evaluates the risk of heart attack would  be more accurate and appropriate if it includes variable . In the second strategy, demographic parity establishes that each segment of a protected class  (e.g., gender or certain age ranges) must obtain a positive result in the same proportion (such  as the allocation of school scholarships). This is undesirable in and of itself: for example, if  we wanted to construct a classifier for a certain disease, we would need to consider that it  is possible that women and men are affected differently. However, demographic parity can  be an objective of decision-makers in and of itself, which must be taken into account when  building the model. 2) Equality of opportunity The concept of equality of opportunity  (Hardt, Price, and Srebro 2016) is one less dependent on  decision-makers’ objectives.  It refers to the predictive performance across different groups  defined by a protected variable  (Verma and Rubin 2018). If   is the variable we want to predict  and  is our prediction, we say that our prediction satisfies equality of opportunity when  and   are independent given the true value  This means that  should not influence the prediction when we know the true value , or, in  other words, belonging or not belonging to the protected group A should not influence the  result of the classification. Predictors that significantly deviate from this criterion are likely to produce disparities associated with the protected variable A. Under an equality of opportunity assumption, the predic-
37 Responsible use of AI for public policy:  Data science toolkit tive error rates for each subgroup of A should be similar. For instance, for binary classification  models, the false positive and false negative rates should be approximately equal. For example, suppose you want to create a system to select the recipients of a prestigious  scholarship. The institution defines membership in an indigenous community as a protected  variable that, for simplicity, we will assume takes two values: indigenous or not indigenous.  The predictor satisfies equality of opportunity  when both the false positive and false negative  rates are the same for indigenous people and for non-indigenous people. 3) Counterfactual justice  This measure considers that a predictor is “fair” if its result remains the same when the value  of the protected attribute is modified (such as introducing a change in race, gender, or other  condition). In practice, there is no single algorithmic justice measure that works for all problems and, in  most cases, seeking compliance with one definition implies not fully complying with the others. The choice should be made considering the context, and the reasons behind it should be  justified and documented.  Box 17. Algorithmic Fairness and Inequality Checklist • (Qualitative) Identify protected groups or attributes (e.g., age, gender, race, poverty  level, etc.) • (Qualitative) Was the algorithmic fairness criterion to be used in the model defined  with experts and decision-makers?  • (Quantitative) When protected attributes exist, an assessment must be made of how  far predictions deviate from the chosen algorithmic fairness definition. • (Quantitative) There must be proper post-processing of predictions if necessary to  achieve the chosen algorithmic fairness criterion.  • (Quantitative) In the case of classification models, cut-off points for different subgroups can be adjusted to achieve the chosen algorithmic fairness criterion. • (Quantitative) Collect more relevant information from protected subgroups (both cases and characteristics) to improve predictive performance for minority groups. At the end of this phase, it is recommended that the  Model Development sections of the Model Card  be  filled out and that a discussion be held with the public  policy decision-maker. Activity:
38 Responsible use of AI for public policy:  Data science toolkit  4. DEPLOYMENT AND MONITORING Responsible use of AI for public policy:  Data science toolkit 38
39 Responsible use of AI for public policy:  Data science toolkit 4. Deployment and Monitoring When machine learning methods are used to make decisions, it is necessary to: •	Monitor model performance and variables used over time. •	Monitor, in particular, undesirable results that may result from user interaction with the  systems. •	Evaluate the data collection and processing process to improve performance or evaluate  results. 4.1. Performance Degradation  The performance of a model can degrade over time for multiple reasons, including: •	Machine learning models that assume a static relationship between the input and output variables can degrade the quality of their predictions due to changes in the underly ing relationships between the variables.  •	Changes in data collection methods and methodological adjustments can harm model  performance. For example, in the case of administrative data, a ministry could change  the data collection processes, digitize the systems, or systematize data cleaning or processing in a way that makes a given model obsolete.  •	Model degradation also occurs in interactive systems – where the system and its users  form a closed feedback loop – as users can only interact with elements that are decided  by the system, and vice versa. To mitigate these possible errors, it is necessary to monitor input variables and their relation  with model behavior, and to update assumptions accordingly, jointly with decision-makers  and domain experts.   The behavior of error metrics over time should also be monitored, including total positive  and negative rates disaggregated by protected group or variable of interest and the distribu tion of predictions over time. Box 18. Performance Degradation Checklist Performance degradation: • (Qualitative) Is there a plan to monitor the performance of the model and the collection of information over time? • (Quantitative) Monitor various metrics associated with predictions in predefined  subgroups (including protected variables). • (Quantitative) Monitor drift in variable distributions with respect to the training  set. • (Quantitative) Monitor changes in the data collection and processing methodology  that may reduce the quality of predictions. • (Qualitative) When feasible, a fraction of the predictions should be examined by  humans and scored according to some pre-defined rubric or scale for each variable of interest.
40 Responsible use of AI for public policy:  Data science toolkit 4.2. Experiments to Evaluate Model Effectiveness The data collection mechanisms to maintain the model should be designed in a way that  keeps the model up to date and reaching optimal performance.  Improvements to the data collection process and to the overall performance of the model  could be difficult to assess without strong counterfactuals. In this sense, experimental tests  – of type A/B, for example – should be used when possible to understand the desirable or  undesirable consequences of using the model  (Vaver and Koehler 2011).  Box 19. Experiments and Data Collection Checklist • (Quantitative) When possible, plan to assign randomized (or status quo) treatments  to some units under experimental designs. Make performance and behavior comparisons between this sample and the results under the algorithmic regime. • (Quantitative) Identify unobserved variables and seek ways to measure them. If possible, re-fit the model and evaluate model performance using this information. At the end of this phase, it is recommended that the  Use and Monitoring Section of the Model Card be  filled out and that a discussion be held with the public  policy decision-maker.Activity:
41 Responsible use of AI for public policy:  Data science toolkit  5. ACCOUNTABILITY Responsible use of AI for public policy:  Data science toolkit 41
42 Responsible use of AI for public policy:  Data science toolkit5.  Accountability Regulations such as the European Union’s General Data Protection Regulation (GDPR) define  accountability as the requirement for organizations to put in place appropriate technical and  organizational measures and to be able to demonstrate what they did and its effectiveness  when requested to do so.  Although the development of technical standards and norms for AI systems is still a pending  task for the AI community, this toolkit has described the main technical aspects and measures to avoid and mitigate bias during the AI lifecycle. However, several challenges remain  that are related to the social and legal requirements that the use of these systems entails in  real-world applications. This section reviews the concepts of interpretability, explainability, and traceability of AI systems. 5.1. Interpretability and Explainability of Predictions 5.1.1. Interpretability  There is no concrete mathematical definition of interpretability (Molnar 2019). It generally  refers to the degree to which a human can consistently predict a model’s results (Kim, 2016).  The more interpretable a model is, the easier it is for an individual to understand the process  that led to a certain decision (Miller 2019). A model with high interpretability is desirable in a  high-risk social policy application where the criterion of accountability becomes fundamental.   There are several reasons why having some degree of interpretability in the models used to  make decisions is important from a technical perspective (Molnar 2019): 1. To learn more about the problem, including causal relationships. 2. To achieve social acceptability of the use of the model. 3. To detect potential biases in the algorithm. 4. To debug and improve models. Complex algorithms such as deep neural networks can have millions of relationships between their parameters, so obtaining model interpretability in these algorithms is still an  open field in machine learning. When high interpretability is necessary, the use of intrinsically interpretable methods such as linear regression, logistic regression, and decision trees is  recommended. 5.1.2. Explainability of Individual Predictions In many cases, it may be legally or ethically necessary to provide explanations  of how a model reached certain conclusions (e.g., why a person was not granted a loan, or why someone  does not qualify for a social program).13 In research areas such as computer vision and natural language processing, the most successful implementations are usually developed with highly complex models, such as deep  neural networks, that are not very transparent as to the underlying assumptions used to  reach a given prediction (Carrillo, Cantú, and Noriega 2020). 13 In the European Union, for example, Article 22 of the GDPR describes the right of a person to challenge the decision of a  system, especially when it is automatic.
43 Responsible use of AI for public policy:  Data science toolkit While this is an area of ongoing research, several methods already exist to increase the  explainability of predictions (Molnar 2019). Methods such as counterfactual explanations  (Wachter, Mittelstadt, and Russell 2017), Shapley values (Lundberg and Lee 2017), and integrated gradients for deep networks (Sundararajan, Taly, and Yan 2017) can be used.   Box 20. Explainability of Individual Predictions Checklist • (Qualitative) Were the legal and ethical explainability requirements in the project’s  context analyzed? •  (Qualitative) Is there a process in place to provide explanations to particular individu als about why a decision was made? • (Qualitative) Were the pros and cons of the algorithms discussed according to their  level of interpretability and explainability in order to choose the most appropriate  one? • (Quantitative) For simpler models (e.g., linear or decision trees), ad-hoc explanations  can be constructed. • (Quantitative) For deep neural networks, use available methods such as counterfactual explanations, Shapley values, or integrated gradients. 5.1.3. Parsimonious Models It is widely held that an ML model is always better when more covariates are used; this is  partially correct as the model can find patterns among the interrelationship of variables.  However, when interpretability is taken into account, more parsimonious methods that use  fewer but relevant features are preferable to models that use many but perhaps less relevant  features. Potential biases can occur when using data characteristics or variables that, although valid for a given time and dataset, are easily susceptible to change when the data-generating  process evolves. Algorithms or predictive methods that use many irrelevant attributes are  at higher risk of failing both explicitly and silently when data sources or data-generating processes change.  Examples may be the use of variables that are being actively influenced by some policy that  will not continue in the future or learning characteristics from a non-exhaustive training set  (e.g., in image recognition, recognizing animal species by the context in which information  was collected, such as a zoo, camera trap, landscape, etc.). This type of bias harms a system’s explainability and may be difficult to detect, but parsimonious methods and expert knowledge can mitigate the risk.
44 Responsible use of AI for public policy:  Data science toolkit Box 21. Parsimonious Models Checklist • (Qualitative) Including all available features to build and train a model may increase  the risk of disproportionately affecting users. The variables to be included in the learning process must have some theoretical support or explanation of why they can help  in the prediction task. • (Quantitative) More parsimonious methods that use fewer, but relevant, features are  preferable to models that use many, but less relevant, features. • (Quantitative) Methods such as partial dependence plots (Friedman 2001) or permutations-based importance (Breiman 2001; Molnar 2019) can point to problematic  variables that are heavily weighted in prediction against past observations or expert  knowledge.   5.2. Traceability A data-to-decision process that is not traceable is one whose execution steps are poorly documented: they include poorly specified processes or operator decisions, extract data from  undocumented or inaccessible sources, omit necessary code or materials, or do not give the  necessary information to ensure the reproducibility of results.   Traceability allows users to understand the processes followed by an AI system to arrive at  an outcome, including the system’s shortcomings and limitations. When there is little traceability in a model, the risks outlined throughout this document can be difficult to identify and  may even be exacerbated. In contrast, all steps from data collection to decision-making are  clearly documented and unambiguously specified in a traceable project.  Box 22. Traceability Checklist • (Quantitative) Is the AI lifecycle well documented (including data provenance and collection mechanisms, infrastructure used, model dependencies and code, metrics, and  interpretation of results)? The documentation should include: 1. Data sources , including dataset metadata, data collection processes and data processing information. ( see Tool 2) 2. Complete and appropriately documented code, defining necessary libraries and  their appropriate versions, to allow any third party to understand the purpose of  each part of the code. 3. Information  on how the code should be executed, including detailed documentation of the parameters and computing requirements. This information must guarantee reproducibility of the original results  by a third party.  4. Information on how the results of the computational process were used and  included in the decision-making process. 5. Information about the monitoring strategy, including details about performance metrics and thresholds as well as expected model behavior and mitigation  actions. Ideally, the aforementioned steps should be replicable by a third party with minimal or  no intervention from the original system creators and operators.
45 Responsible use of AI for public policy:  Data science toolkit • (Qualitative) Have the deficiencies, limitations, and biases of the model been communicated to stakeholders so that they are considered in decision-making and decision  support? • (Qualitative) Has the technical team completed the Data Profile (see Tool 2) and the  Model Card (see Tool 3), and has a process for continuous updating of these tools  been defined?
46 Responsible use of AI for public policy:  Data science toolkit  TOOLS Responsible use of AI for public policy:  Data science toolkit 46
47 Responsible use of AI for public policy:  Data science toolkit Tool 1: Robust and Responsible AI Checklist This tool consolidates the main concerns by risk dimension of the AI lifecycle. The checklist  must be reviewed continuously by the technical team accompanied by the decision-maker  (Fritzler 2015; drivendata 2019). Planning and Design Correct definition of the problem and the public policy response  • (Qualitative) Is the public policy problem clearly defined? • (Qualitative) Describe how this problem is currently being addressed – considering responses by related institutions – and how the use of AI would improve the  government response to this problem.  • (Qualitative) Were the protected groups or protected attributes identified within  the project (e.g., age, gender, education level, race, level of marginalization, etc.)?  • (Qualitative) Were the actions or interventions to be carried out based on the result of the AI system defined?  AI Principles • (Quantitative) Has the need for an AI system been justified, considering other possible solutions that do not require the use of personal data and automated decisions? • (Quantitative) Is there evidence that both public policy action and the recommendation of the AI system will result in a benefit to people and the planet by driving  inclusive growth, sustainable development, and well-being?  • (Qualitative) For the implementation of these technologies, have there been similar previous projects, and have they been reviewed?  • (Quantitative) Have you considered minimizing the exposure of personally identi fiable information (e.g., by anonymizing or not collecting information not relevant  to the analysis)? Lifecycle Data Collection and Processing Data quality and relevance of the available data  • (Qualitative) Discuss possible historical social inequalities in the use case with specialists in the field. • (Quantitative) Perform an exploratory analysis of the available data with which the  model will be trained to identify possible historical biases or undesirable states.
48 Responsible use of AI for public policy:  Data science toolkit Poor Correspondence between Ideal and Available Variables  • (Qualitative) The ideal target variables should be clearly stated. The collected/ available variables must be analyzed to understand how suitable they are to substitute for the target variable. Systematic biases or validity of the proxy metric  should be identified. • (Qualitative) Has the use of the selected response variable been clearly justified  for the purposes of the intervention?   Data qualification and completeness for the target population   Probabilistic and Natural Samples • (Qualitative) Have the possible differences between the database and the popula tion for which the AI system is being developed been analyzed? (Use literature related to the topic and information from experts. Study in particular unmeasured  selection biases.) • (Quantitative) Although models can be built with various data sources, designed  or natural, validation should ideally be carried out with a sample that allows statistical inference to the target population. The validation sample must appropriately  cover the target population and sub-populations of interest.   Missing or incomplete attributes  • (Qualitative) Has an analysis of missing values and variables been performed? • (Qualitative) Has it been determined whether there are important omitted variables for which there are no associated measurements? (If any) • (Qualitative) Have the reasons for the missing observations been identified? (If  any) Causal comparison • (Qualitative) Understand and describe the reasons why the response variable is  correlated with known and unknown variables. Describe possible biases based on  expert knowledge and analysis.  • (Qualitative) In the event that no work was done to ensure causality in the results,  were the limitations of the results explicitly communicated to the public policy  decision-maker? Model Building and Validation Absence or inappropriate use of validation samples • (Quantitative) Were the validation and test samples constructed properly, considering an appropriate size, covering subgroups of interest and protected subgroups, and avoiding information leaks during its implementation? 
49 Responsible use of AI for public policy:  Data science toolkit Data leakage    Training-Validation Data Leak  • (Quantitative) Any processing and preparation of training data should avoid using  the validation or test data in any way. A solid barrier must be maintained between  training versus validation and testing. This includes data recoding, normalizations,  selection of variables, identification of outliers, and any other type of preparation  of any variable to be included in the models. This also includes sample weights or  balances based on oversampling/undersampling. Target Leakage • The validation scheme should replicate as closely as possible the scheme under  which the predictions will be applied.  Probabilities and classes   Imbalanced Data • (Quantitative) Make probability predictions instead of class predictions. These  probabilities can be incorporated into the subsequent decision process as such.  • Quantitative) When the absolute number of minority cases is very small, it can be  very difficult to find appropriate information to discriminate that class. More data  need to be collected from the minority class. • (Quantitative) Sub-sampling the dominant class (weighting the cases up to avoid  losing calibration) can be a successful strategy to reduce data size and training  time without affecting predictive performance. • (Quantitative) Replicate the minority class to better balance the classes (over-sampling). Arbitrary Cut-off Point • (Quantitative) Using probabilistic classification algorithms is more suitable for decision-making to incorporate uncertainty regarding the classification.  • (Quantitative) Avoid standard probability cut-off points such as 0.5. Choose an  optimal interpretation of the predicted probabilities using the receiving operating  characteristic curve and other measures to analyze errors. Adequateness of assessment metrics • (Qualitative) Were the implications of the different types of errors for the specific  use case, as well as the correct way to evaluate them, questioned? • (Qualitative) Were the limitations of the model clearly explained? This implies  identifying both false positives and false negatives and the implications that a  system decision would have on the life of the target population. • (Quantitative) Was a cost-benefit analysis of the system conducted and compared  with the status quo or with the use of other decision-making or decision support  strategies? (When possible)
50 Responsible use of AI for public policy:  Data science toolkit Underfit and Overfit Checklist • (Quantitative) Overfitting: If necessary, methods should be refined to moderate  the overfitting, including such methods as regularization, restricting the function al space of possible models, using more training data, or disturbing the training  data (Hastie, Tibshirani, and Friedman 2017).   • (Quantitative) Underfit: Data on protected groups or other sensitive variables  should be reviewed to verify that there are no undesirable systematic errors.  Unquantified errors and human evaluation   Failures Not Measured by the Model  • (Qualitative) Was a human assessment conducted with use-case experts to look  for known biases or errors? Establishing monitoring schemes that allow for the  identification of unmeasured errors or biases is recommended.  For example,  panels of reviewers can be used to examine particular predictions and consider  whether they are reasonable. These panels must be balanced in terms of user  type and expertise, including decision-makers if necessary.   Fairness and differential performance      Algorithmic Fairness and Inequality • (Qualitative) Was the algorithmic fairness criterion to be used in the model defined with experts and decision-makers?  • (Quantitative) When protected attributes exist, an assessment must be made of  how far predictions deviate from the chosen algorithmic fairness definition.  (e.g.,  tested for disparate error rates)? • (Quantitative) In the case of classification models, cut-off points for different subgroups can be adjusted to achieve the chosen algorithmic fairness criterion. Deployment and Monitoring Performance degradation: • (Qualitative) Is there a plan to monitor the performance of the model and the collection of information over time? • (Quantitative) Monitor various metrics associated with predictions in predefined  subgroups (including protected variables). • (Quantitative) Monitor drift in variable distributions with respect to the training  set. • (Quantitative) Monitor changes in the data collection and processing methodology that may reduce the quality of predictions. • (Quantitative) When possible, plan to assign randomized (or status quo) treatments to some units under experimental designs. Make performance and behavior comparisons between this sample and the results under the algorithmic  regime. • (Quantitative) Identify unobserved variables and seek ways to measure them. If  possible, re-fit the model and evaluate model performance using this information.   
51 Responsible use of AI for public policy:  Data science toolkit Accountability Interpretability and explanation of predictions   Explainability of Individual Predictions  • (Qualitative) Were the legal and ethical explainability requirements in the project’s  context analyzed? •  (Qualitative) Is there a process in place to provide explanations to particular indi viduals about why a decision was made? • (Qualitative) Were the pros and cons of the algorithms discussed according to  their level of interpretability and explainability to choose the most appropriate  one? Parsimonious Models • (Qualitative) Including all available features to build and train a model may increase the risk of disproportionately affecting users. The variables to be included  in the learning process must have some theoretical support or explanation of why  they can help in the prediction task. • (Quantitative) More parsimonious methods that use fewer, but relevant, features  are preferable to models that use many, but less relevant, features. • (Quantitative) Methods such as partial dependence plots (Friedman 2001) or permutations-based importance (Breiman 2001; Molnar 2019) can point to problematic variables that are heavily weighted in prediction against past observations or  expert knowledge.   Traceability • (Quantitative) Is the AI lifecycle well documented (including data collection, infrastructure used, dependencies, code, metrics, and interpretation of results)? • (Qualitative) Have the deficiencies, limitations, and biases of the model been communicated to stakeholders so that they are considered in decision-making/deci sion support? (Qualitative) Has the technical team completed the Data Profile (see Tool 2) and the  Model Card (see Tool 3), and has a process for continuous updating of these tools  been defined?
52 Responsible use of AI for public policy:  Data science toolkit Tool 2: Data Profile The Data Profile is an exploratory analysis that provides information to evaluate the quality,  integrity, temporality, consistency, and possible biases of a dataset that will be used to train  a machine learning model (Gebru et al. 2018). Data and Input Collection and Origin • Name of dataset used. • What institution created the dataset? • For what purpose did the institution create the dataset used? • What mechanisms or procedures were used to collect the data (e.g., household  survey, sensor, software, API)? Does they comply with existing data protection  regulations? • What is the scale of the dataset? • Obtain documentation for each variable within the dataset. Provide a short description, including its name and type, what it represents, how it is measured, etc.  Data and Input Domains  • What is the data domain (e.g., proprietary, public, personal)?  • If personal, are the data identified, pseudonymized, unlinked pseudonymized, anonymized, or aggregated?  • If proprietary, are any intellectual property rights considerations? Data and Input Structure  • Are the data static or dynamic? If dynamic, how often will they be updated? Data Quality & Qualification • How were the data obtained (observed, derived, synthetic, or provided by individ uals or organizations)? • Is the data representative of the population of interest?  • Analyze spatial and temporal coverage of the data.  • Analyze coverage of protected groups (sex, race, age, etc.). • Describe the type of sampling used to obtain the data. • Describe the important dimensions in which the data sample may differ from the  population, in particular unmeasured selection biases. Use literature related to  the subject and information from experts. • Identify possible “undesirable states” in the data that could lead to prejudicial biases and inequities for a given subgroup, or any other pattern that is considered  suboptimal or undesirable from a social policy point of view. • Are there any missing values? If so, explain the reasons why this information is  not available (this includes information intentionally removed). Identify reasons 
53 Responsible use of AI for public policy:  Data science toolkitfor missing data and think about whether the missing data are associated with  the variable to predict. Document any imputation processes used to substitute  for missing data. • Capture frequency (weekly, monthly, daily) or average number of observations  per individual. What version of the dataset is being used?  • Is this the most appropriate dataset available given the problem at hand? 
54 Responsible use of AI for public policy:  Data science toolkit Tool 3: Model Card  The rubric presented here is a follow-up card that summarizes the main characteristics of  a machine-learning-based decision-making or decision support system and highlights the  main assumptions, the most important characteristics of the system, and the mitigation measures implemented (Mitchell et al. 2019). Planning and Design  1. Basic information • People who developed the model, date, version, type 2. Use cases • Background • Target population and forecast horizon • Actors and components that will interact with the results • Use cases considered during development • Uses not considered and related warnings • Definition of protected groups Data Collection and Processing  3. Training data • Dataset used and its labeling • Preprocessing or data preparation steps • Potential biases and shortcomings depending on the use case (2) Model Building and Validation 4. Modeling • Algorithms that were used for training, assumed parameters or constraints • Input or assumptions made using expert knowledge • Data interaction (no interaction, active interaction, passive interaction) 5. Performance metrics • Technical metrics used to select and evaluate models  • Cost-benefit analysis of the model for its use case according to (2) • Definition of protected groups and selected fairness measures 6. Validation data • Datasets used and their labeling • Pre-processing steps • Evaluation of adaptation of validation data according to the use case (2)
55 Responsible use of AI for public policy:  Data science toolkit • Potential biases and shortcomings depending on the use case (2) 7. Quantitative analysis summary • Validation error reported • Summary of cost-benefit analysis • Report of fairness measures for protected groups Deployment and Monitoring 8. Monitoring recommendations • Monitoring and improvement strategy in production • Human monitoring strategies (if applicable) Accountability 9. Explainable predictions • Strategy to explain particular predictions • Strategy to understand the importance of different attributes 10. Other ethical considerations, recommendations, and warnings
56 Responsible use of AI for public policy:  Data science toolkit  WORKBOOKS Responsible use of AI for public policy:  Data science toolkit 56
57 Responsible use of AI for public policy:  Data science toolkitWorkbooks This section shows several examples of the challenges and solutions explained in the main  document. Different types of models (linear, tree-based, and others) and different imple mentations (R, keras, xgboost) are used to show that these problems arise regardless of the  choice of particular tools. Booklets use decimal point notation to maintain consistency with packages that use it. The  R programming language is used along with the following packages: tidyverse, recipes, the mis, rsample, parsnip, yardstick, workflows, tune, knitr, and patchwork. All material is reproducible according to instructions in this repository14, which contains a  Dockerfile that describes the infrastructure dependencies for its replication.  Data Collection and Processing Data Quality and Relevance of the Available Data Using models that predict the wrong metric can lead to wrong decisions. Sometimes the  problem is clear, as when the proxy metric has obvious shortcomings, and other times it  can be more subtle. The example shown below seeks to predict the demand for a certain product (let’s think  about vaccines or some medicine) in order to make supply decisions.  There is historical data on inventory (80 weeks) and sales, and a predictor  variable associated with sales (in the case of vaccines it could be temperature) and another on inventory  depletion. We separate the data in training and testing, fitting the model with the subset  of training data. In this case, a linear model is used with the dependent variable sales and  week covariates and the predictor  covariate. train <- sales%>% filter (week < 60)  test <- sales%>% filter (week> = 60, week <= 80)  train%>% select (-demand) %>%  head() %>% kable() Week Inventory Sales Forecast Depletion 1 153 110 -27.7014124 0 2 170 148 0.7664636 0 3 158 130 -15.2606032 0 4 162 142 4.2461227 0 5 159 159 28.5107593 1 6 162 162 14.8895964 1 14  https:/ /github.com/EL-BID/Manual-IA-Responsable
58 Responsible use of AI for public policy:  Data science toolkit linear_mod <- lm (sales ~ week + predictor, data = sales) mod_linear ##  ## Call: ## lm (formula = sales ~ week + predictor, data = sales) ##  ## Coefficients: ## (Intercept) week predictor  ##    140.9935       0.8166       0.5535 We evaluate the prediction error. preds <- predict (mod_linear, newdata = test) round (mean (abs (preds - test$sales)) / mean (test$sales), 3)  ## [1] 0.04 The percentage error is low. The fitted data and predictions look as follows: preds <- predict (mod_linear, newdata = sales) sales_long <- sales%>% mutate (pred = preds)%>%>    pivot_longer (cols = all_of (c (“sales” ,”pred”)), names_to = “type”, values_to = “units”)  ggplot (sales_long%>% mutate (units = ifelse (type ==”sales”  &week>  80, NA,  units)),   aes(x = week, y = units, group = type, color = type)) +  geom_line() +   geom_vline (xintercept = 80) +    geom_vline (xintercept = 60) +    annotate( “text”, x = 25, y =105, label = ”train”) +    annotate( “text”, x = 69, y =105, label = ”test”) 
59 Responsible use of AI for public policy:  Data science toolkit But making demand or inventory decisions with this type of model is wrong. The reason is  that there is a difference between the ideal variable (real demand for medicines) and the observed variable (sale of medicines). The difference is that there are inventory depletions, that  is, periods when, although there was demand, there was not enough inventory for all buyers.  This is marked in red in the following graph.  preds <- predict (mod_linear, newdata = sales) sales_long <- sales%>% mutate (pred = preds)%>%>    pivot_longer (cols = all_of (c ( “sales”,”pred”)), names_to = “type”,  values_to = “units”)  ggplot (sales_long%>% mutate (units = ifelse (type ==”sales”  & week>  80, NA,  units)), aes (x = week)) + > geom_line (aes (group = type, color = type, y = units)) +   geom_point (data = filter (sales, depletion ==1, week < 80), aes (y = sales),  color = “red”) +   geom_vline (xintercept = 80) +     geom_vline (xintercept = 60) +   annotate( “text”, x = 25, y =105, label = ”train”) +  annotate( “text”, x = 69, y =105, label = ”test”) 
60 Responsible use of AI for public policy:  Data science toolkit    If we were to use the policy suggested by the predictions (e.g., 5 percent more), we would see  the sales in the first graph below. However, if we used an inventory policy with 280 units, we  would observe the following: preds <- predict (mod_linear, newdata = sales) sales_obs <- sales%>% mutate (pred = preds)%>%>   mutate (inventory =  1.05 * pred)%>%     mutate (sales = ifelse (week>;  80 , pmin (inventory, demand), sales))   sales_long <- sales_obs%>%    pivot_longer (cols = all_of (c ( “sales”,”pred”)), names_to = “type”, values_to = “units”)  g1 <- ggplot (sales_long, aes (x = week)) +  geom_line (aes (group = type, color = type, y = units)) +   geom_point (data = filter (sales_obs, sales == inventory, week> 80),  aes (y = sales), color = “red”) +    geom_vline (xintercept = 80) + labs (subtitle = “Inventory: Predictions +  5%”)  preds <- predict (mod_linear, newdata = sales) sales_obs <- sales%>% mutate (pred = preds)%>%>    mutate (inventory = 280)%>%     mutate (sales = ifelse (week>;  80 , pmin (inventory, demand), sales))   sales_long <- sales_obs%>%    pivot_longer (cols = all_of (c ( “sales”,”pred”)), names_to = “type”, values_to = “units”) 
61 Responsible use of AI for public policy:  Data science toolkit g1 <- ggplot (sales_long, aes (x = week)) +  geom_line (aes (group = type, color = type, y = units)) +   geom_point (data = filter (sales_obs, sales == inventory, week>  80),  aes (y = sales), color = “red”) +    geom_vline (xintercept = 80) + labs (subtitle = “Inventory: 300 cte  units”)  g1/g2 Therefore: • Prediction policy exacerbates the burnout problem. • An unintended use of data without considering its generating process can lead to large  errors in decisions. • In this case, the confusion comes from not separating the concepts of demand and sales.  Other more suitable demand indicators or models would help solve the problem. • Simplistic solutions such as only taking data where stockouts do not occur can make the  situation even worse, as they increase bias (we select weeks where sales tend to be low)  and reduce accuracy. Natural Samples and Bias When the training samples are different from the populations to which the models are to  be applied, there are difficulties in correctly validating the predictions. Natural Samples: Poor Representativeness For this example, data from the national household income and expenditure survey in Mex ico will be used (INEGI 2014) to simulate a scenario that we want to exemplify. set.seed ( 128)   survey_entry <- read_csv( “data / enigh-example.csv” )  income_data <- income_survey%>%  mutate (num_focos = FOCOS)%>%>   mutate (income_miles = (INGCOR / 1000))%>%     mutate (cell_tel = ifelse (SERV_2 == 1, “Yes”, “No”))%>%   mutate (piso_firme = ifelse (PISOS!= 1 | is.na (PISOS), “Yes”, “No”))%>%   mutate (washer = ifelse (WASH!= 1 | is.na (LAVAD), “Yes”, “No”))%>%     mutate (car = VEHI1_N> ; 0)%>%   mutate (marginalization = fct_reorder (marginalization, income_miles,  
62 Responsible use of AI for public policy:  Data science toolkit median))%>%>  rename (occupied = PEROCU)%>%>  rename (educacion_jef = LEVELAPROB)%>%>  select (income_miles, num_focos, tel_cellular,  marginalization, occupied, firm_floor, washing machine, car, education_jef) income_split <- initial_split (data_entry, prop = 0.7)  train <- training (income_split) test <- testing (income_split) Suppose you are interested in estimating household income and you use a cell phone sur vey to conduct it. Furthermore, suppose you only access areas that do not have very high  marginalization. sample_biased <- filter (train,                            tel_cellular == “Yes”,                             marginalization == ”Very low” )  biased_split <- initial_split (bias_sample) bias_train <- training (bias_split) bias_validation <- testing (biased_split) A linear model is built for the logarithm of income with the available data. library(splines)   formula <- as.formula ( “log (income_miles) ~ ns (num_focuses, 3) +   ns (occupied, 3) + washing machine + car + piso_firme +               ns (education_jef, 3) « )  bias_mod <- lm (formula, data = bias_train) # we take a representative sample to compare, the same size as the biased mod_representative <- lm (formula, data = sample_n (train, nrow (train_bias))) And the error is evaluated in a test sample constructed with data with the same biased  characteristics as the training data (households with a cell phone and a very low degree of  marginalization).
63 Responsible use of AI for public policy:  Data science toolkit preds_val <- predict (bias_mod, newdata = bias_validation) mean (abs (preds_val - log (1 + bias_validation $ thousand_income)))%>% round  (2)  ## [1] 0.37 The error in a sample more similar to the population to which the algorithm is intended to  apply is greater. bias_test_preds <- predict (bias_mod, newdata = test) preds_test <- predict (mod_representative, newdata = test) test $ pred_bias <- preds_test_bias test $ pred_rep <- test_preds mean (abs (bias_test_preds - log ( 1 + test $ thousands_income)))%>% round ( 2)  ## [1] 0.42 However, the main problem is reflected in the graphs below, where logarithmic scales are  used to make multiplicative comparisons, which are interesting due to the nature of income.  Each point represents a household, the sample is more similar to the population to which the  methodology will be applied, and the prediction of households using the model is plotted on  the horizontal axis, while the vertical axis corresponds to the income of each household. As a  reference, the line y = x and a smoother are added. The focus is on performance for relatively  low-income households (lessthan 10,000 pesos per month): breaks_y <- c ( 3, 5, 10, 20, 40, 80)  g_bias <- ggplot (test%>% filter (pred_bias <log ( 30)),   aes (x = exp (pred_biased), y = income_miles)) +    geom_point (alpha = 0.5) +    geom_abline () + geom_smooth (method = “loess”, span = 1) +    scale_x_log10 (limits = c ( 5, 30)) + scale_y_log10 (breaks = breaks_y) +     xlab(“Forecast (thousands per quarter)” ) +    ylab(“Current income (thousands per quarter)” ) +    labs (subtitle = “Test performance \ nwith training bias” )  
64 Responsible use of AI for public policy:  Data science toolkit g_bias <- ggplot (test %>% filter(pred_sesgada < log( 30)),   aes (x = exp (pred_rep), y = income_miles)) +    geom_point (alpha = 0.5) +    geom_abline () + geom_smooth (method = “loess”, span = 1) +    scale_x_log10 (limits = c ( 5, 30)) + scale_y_log10 (breaks = breaks_y) +     xlab(“Forecast (thousands per quarter)” ) +    ylab(“Current income (thousands per quarter)” ) +   labs (subtitle = “Performance in test \ nwith representative sample in training” )   g_bias + g_representative Although it is commonly expected to over-predict relatively low observed values, and to do the  opposite for relatively high values, for households with incomes of less than 10,000 pesos per  month the biased model overpredicts true income by around 40 percent: test_low <- test%>% filter (income_miles < 3*10)   bias <- mean (exp (low_test $ pred_biased)) / mean (low_test $ thousands_income)  - 1  round (bias ,3)  ## [1] 0.412
65 Responsible use of AI for public policy:  Data science toolkit When compared with the same trained model with a representative sample, the effect is considerably less. test_low <- test%>% filter (income_miles < 3*10)   bias <- mean (exp (low_test $ pred_biased)) / mean (low_test $ thousands_income)  - 1  round (bias ,3)  ## [1] 0.152 There are then two problems: 1. Bias produces a considerably larger error in implementation than in validation. 2. Even worse, the bias is greater for lower-income households (predictions are high), which  can produce poor targeting if we seek to identify lower-income households. Natural Samples: Causal Comparisons This example is taken from Hastie, Tibshirani, and Friedman (2017) and Rossouw (1983). The  following data are considered, with the goal being to predict heart disease (chd):15 sa_heart <- read_csv( “data / sa-heart.csv” )  sa_heart <- sa_heart%>%  rename (arterial_pressure = sbp, tobacco = tobacco, cholesterol_ldl = ldl,  adiposity = adiposity, fam_history = famhist, type_a = typea, obesity = obesity,  age = age, coronary_en = chd) sa_heart ## # A tibble: 462 x 10 ## arterial_pressure tobacco cholesterol_ldl adiposity history_fam type_a ##               <dbl>  <dbl>          <dbl>      <dbl> <chr>         <dbl> ##  1              160  12              5.73       23.1 Present          49 ##  2              144   0.01           4.41       28.6 Absent           55 ##  3              118   0.08           3.48       32.3 Present          52 ##  4              170   7.5            6.41       38.0 Present          51 ##  5              134  13.6            3.5        27.8 Present          60 15  The data can be accessed at http:/ /archive.ics.uci.edu/ml/datasets/heart+Disease .
66 Responsible use of AI for public policy:  Data science toolkit ##  6              132   6.2            6.47       36.2 Present          62 ##  7              142   4.05           3.38       16.2 Absent           59 ##  8              114   4.08           4.59       14.6 Present          62 ##  9              114   0              3.83       19.4 Present          49 ## 10              132   0              5.8        31.0 Present          69 ## #… with 452 more rows, and 4 more variables: obesity<dbl>, alcohol<dbl>, ## # age <dbl>, enf_coronary <dbl>  library(recipes)   set.seed( 125)  sa_split <- rsample :: initial_split (sa_heart, prop = 0.75)  sa_split ## <Training/Validation/Total> ## <347/115/462> receta_sa <- training(sa_split) %>%   recipe(enf_coronary ~ .) %>%   step_dummy(history_fam) %>%  step_mutate (enf_coronary= factor (enf_coronary))%>%>    prep() sa_entrena <- recipe_sa%>% juice sa_boosted <- boost_tree (trees = 3000, mode = “classification” ,                           learn_rate = 0.001, tree_depth = 2,                           sample_size = 0.5)%>%    set_engine( “xgboost” )%>%   fit (enf_coronary ~., data = sa_entrena) You can evaluate this model and fine-tune parameters as well. Here we are interested in interpreting the effect of the variables in this model. For this, the partial dependence graph of the  prevalence of heart disease and the obesity variable is considered. 
67 Responsible use of AI for public policy:  Data science toolkit library(pdp)  pdp_ob <- pdp :: partial (sa_boosted $ fit, pred.var = “arterial_pressure” ,    plot = TRUE, plot.engine = “ggplot2” , prob = TRUE,  train = sa_entrena%>% dplyr :: select (-enf_coronary))> pdp_ob + xlab ( “Systolic Blood Pressure” ) + ylab( “Average Prediction” )  The correct interpretation of this partial dependence graph (Hastie, Tibshirani, and Friedman  2017) depends on the fact that this is a retrospective study, where some patients at risk of  heart disease underwent interventions to reduce their risk, including taking medicines to reduce blood pressure. A causal interpretation of blood pressure reductions as a promoter of  heart disease is incorrect and potentially dangerous.
68 Responsible use of AI for public policy:  Data science toolkit Model Building and Validation  Leak Training Validation Several examples of how leakages from training to validation produce biased estimates of  predictor performance will be presented here. Selecting Variables before Dividing the Data Any pre-processing step must be done without using validation data. This includes when  methods such as cross-validation are used. This example is originally from Hastie, Tibshirani, and Friedman (2017) and will use synthetic  data generated through the following process: 1. Simulating response variables and with binomial distribution.  2. Simulating 1,000 independent covariates, each with a standard normal distribution. simulate <- function (n = 100, p = 500, prob = 0.5) {    data <- map ( 1: p, ~ rnorm (n))%>%   bind_cols ()   data $ y <- rbinom (n, 1, prob)   data } set.seed ( 8234)  training_data <- simulate (n = 200, p = 1000)  test_data <- simulate (n = 2000, p = 1000)  dim (data_train) ## [1]  200 1001 data_train %>% group_by(y) %>% tally() %>% kable() y n 0 113 1 87 The selection of variables is given by the function below. This function selects the variables  most correlated with the target variable. 
69 Responsible use of AI for public policy:  Data science toolkit select <- function (data, num_var = 10) {  correlations <- data%>%      pivot_longer (cols = matches ( “V”), names_to = “variable” , values_to = “x”)%>%   group_by (variable)%>%>  summarize (corr = abs (cor (y, x)))%>%>  arrange (desc (corr))    # select   selected <- correlations%>%  top_n (num_var, wt = corr)%>%>  pull (variable)   data%>% select (one_of (c ( “y”, selected)))   } Wrong Method Here are the 10 variables that were selected. By itself, this method is not wrong, but when  run on the data to be used in validation (cross-validation), then the performance estimate is  optimistic: filtered_data <- select (data_train) filtered_data%>% head%>%>>    mutate_if (is.numeric, round, 3)%>% kable ()   y V337 V464 V984 V461 V525 V732 V39 V774 V491 V682 0 1.592 -0.587 1.763 -0.847 0.452 -0.604 -0.400 -1.146 -0.938 0.136 0 1.782 0.604 0.739 -0.533 1.752 0.945 1.142 -0.638 -0.342 1.308 0 1.528 0.635 -0.326 0.734 -0.207 0.974 1.574 2,401 0.428 0.176 0 0.799 -1.436 0.724 0.366 1.680 0.476 0.376 -1.673 -0.683 0.161 0 0.759 -0.208 -0.373 0.208 -1.009 -0.028 -1.209 0.759 2,038 1.402 1 -0.377 -1.044 1.358 -0.223 0.469 1,221 0.582 0.378 -0.116 0.173 For whatever validation cut-off is made (whether separating a dataset or cross-validating), the  percentage of hits appears to be greater than 0.5:
70 Responsible use of AI for public policy:  Data science toolkit cut_validation <- filtered_data%>% sample_frac ( 0.7)   validate <- anti_join (filtered_data, validation_cut) model_1 <- glm (y ~., validation_cut, family = “binomial” )  mean (as.numeric (predict (model_1, valid)> 0) == valid $ y)%>% round ( 2 )> ## [1] 0.73 However, the actual performance of the model will be: mean (as.numeric (predict (model_1, test_data)> 0) == test_data $ y)%>% round  ( 2 )> ## [1] 0.49 Correct Method The selection of variables must be done in each round of cross-validation. cut_validation <- filtered_data%>% sample_frac ( 0.7)   cut_filtered_data <- select (validation_cut) validate <- anti_join (data_train, cut_validation) model_1 <- glm (and ~., cut_filtered_data, family = “binomial” )  mean (as.numeric (predict (model_1, valid)> 0) == valid $ y)%>% round ( 2 )> ## [1] 0.52 Oversample before Partitioning One of the ways to solve class imbalance problems is to use oversampling techniques. However, you have to be very careful to avoid information leakage errors when applying these  techniques.  In this example, you will see that oversampling a small class before separating validation data  or cross-validating can produce overly optimistic estimates of prediction error. Suppose we have a severe imbalance between our two classes: set.seed ( 99134)   data_imbalance <- simulate (n = 500, p = 20, prob = 0.1)%>%     mutate (y = factor (y, levels = c ( 1, 0)))   data_desbalance %>% group_by(y) %>% tally() %>% kable()
71 Responsible use of AI for public policy:  Data science toolkit y n 1 41 0 459 Wrong Method Suppose the Synthetic Minority Over-Sampling Technique (SMOTE) (Chawla 2002) is applied  first to try to balance the data. recipe_balance <- recipe (and ~., unbalance_data)%>% step_smote (y)%>%>   prep() data_smote <- juice (recipe_balance)  Obtaining this: data_smote%>% group_by(y) %>% tally() %>% kable() y n 1 459 0 459 Now training and validation will be separated. sep_data_smote <- initial_split (data_smote) train_smote <- training (sep_data_smote) test_smote <- testing (sep_data_smote) And a classification method is generated using a random forest of decision trees. metrics <- metric_set (accuracy, recall, precision) forest <- rand_forest (trees = 500, mtry = 20, mode = “classification” )%>%    set_engine( “ranger” )%>% 
72 Responsible use of AI for public policy:  Data science toolkit fit (y ~., data = train_smote) forest%>%> predict (test_smote)%>%> bind_cols (test_smote)%>%>  metrics (truth = y, estimate = .pred_class)%>%>   mutate_if(is.numeric, round, 3) %>% kable   .metric .estimator .estimate Accuracy Binary 0.926 Recall Binary 0.973 Precision Binary 0.887 At first glance it seems that the performance is excellent. However, since there is no relationship between ‘y’ and the rest of the covariates we know that this is an error. Correct Method Before doing the rebalancing of classes, training and validation are separated. If you like,  this part can be done using stratified sampling, for example, but here it is built with simple  random sampling. sep_data <- initial_split (data_imbalance, prop = 0.5)   train <- training (sep_data) test <- testing (sep_data) recipe_balance <- recipe (y ~., data = train)%>% step_smote (y)%>%>   prep() train_balanced <- juice (recipe_balance) forest_1 <- rand_forest (trees = 500, mtry = 20, mode = “classification” )%>%     set_engine( “ranger” )%>%   fit (y ~., data = balanced_train) forest_1%>%> predict (test)%>%> 
73 Responsible use of AI for public policy:  Data science toolkit bind_cols (test)%>%>  metrics (truth = y, estimate = .pred_class)%>%>    mutate_if(is.numeric, round, 3) %>% kable   kable () .metric .estimator .estimate Accuracy Binary 0.828 Recall Binary 0.000 Precision Binary 0.000 Although the accuracy seems high, the precision and sensitivity are zero. A trivial classifier that  always predicts the ruling class may have better accuracy than the one we have constructed. Leaks in Implementation Variables Not Available at the Time of Prediction In this case, we show an example where a variable is used erroneously that will not be available  at the time of making the predictions (data from Greene 2003). credit <- read_csv( “data / AER_credit_card_data.csv” )%>%   rename (expense = expenditure, dependents = dependents, income = income, age = age, owner = owner)%>%>    mutate (owner = fct_recode (owner, c ( si = ”yes”)))   credit %>% head %>%  mutate_if (is.numeric, round, 3)%>% kable ()   Card Reports Age Income Share Expense OwnerSelfemployedDependents MonthsMajo   CardsActive Yes 0 37.7 4.5 0.0 125.0 Yes No 3 54 1 12 Yes 0 33.2 2.4 0.0 9.9 No No 3 34 1 13 Yes 0 33.7 4.5 0.0 15.0 Yes No 4 58 1 5 Yes 0 30.5 2.5 0.1 137.9 No No 0 25 1 7 Yes 0 32.2 9.8 0.1 546.5 Yes No 2 64 1 5 Yes 0 23.2 2.5 0.0 92.0 No No 0 54 1 1 You want to build a model to predict which applications were accepted and automate the selection process. A logistic regression with Keras and L2 penalty is used.
74 Responsible use of AI for public policy:  Data science toolkit set.seed( 823)   credit_split <- initial_split (credit) train <- training (credit_split) test <- testing (credit_split) #data preparation recipe_credit <- recipe (card ~., credit)%>% step_normalize (all_numeric ())%>%>  step_dummy (all_nominal (), -card)  # model regular_model <-    logistic_reg (penalty = 1)%>%     set_engine( “keras”, epochs = 500, verbose = FALSE)%>%     set_mode( “classification” )   # adjust preprocessing parameters recipe_prep <- credit_recipe%>% prep (train) # preprocess data train_prep <- bake (recipe_prep, train) test_prep <- bake (recipe_prep, test) # adjust model fit <- regular_model%>%  fit (card ~ expense + dependents + income + age + owner_yes, data = train_prep) # evaluate metrics <- metric_set (accuracy, recall, precision) fit%>% predict (test_prep)%>%>> bind_cols (test)%>%>  metrics (truth = factor (card), estimate = .pred_class)%>%>    mutate_if(is.numeric, round, 3) %>% kable   kable ()
75 Responsible use of AI for public policy:  Data science toolkit .metric .estimator .estimate Accuracy Binary 0.833 Recall Binary 0.393 Precision Binary 0.892 And it seems to be performing reasonably well. If we remove the variable expenditure, the  performance of the model is totally degraded: adjustment_2 <- regular_model %>%  fit (card ~ expense + dependents + income + age + owner_yes, data = train_prep) fit%>% predict (test_prep)%>%>> bind_cols (test)%>%>  metrics (truth = factor (card), estimate = .pred_class)%>%>    mutate_if(is.numeric, round, 3) %>% kable   kable () .metric .estimator .estimate Accuracy Binary 0.745 Recall Binary 0.000 Precision Binary n.a. Sensitivity is very poor and precision cannot be calculated because the model does not make  positive predictions for the test set. The reason for this performance degradation is that  spending  refers to the use of credit  cards. This includes the card for which you want to make an acceptance prediction. train%>%>    mutate (some_expenditure = expense> ; 0)%>%   group_by (some_cost, card)%>%>  tally ()%>%>  kable ()
76 Responsible use of AI for public policy:  Data science toolkitsome_expense Card No. False No 212 False Yes 19 True Yes 759 This indicates that some expense probably includes expense on the current card, and the expense variable is measured after the delivery of the card: The performance of this model for new applications will be poor, since the expense variable,  at the time of application, obviously does not count how much each client will spend in the  future. Cut-off Point Evaluation The best cut-off decisions can be made with cost-benefit analysis using lift curves, like those in  the previous example, based on gains and losses for each decision. Although this information  is often not available, it is ideal to evaluate how the model helps and how much the actions we  intend to take are worth. It is possible to do this analysis with uncertain cost-benefit values. Suppose we are thinking of a treatment to retain students in some training or improvement  program. •	The retention treatment costs 5,000 pesos per student. •	It is estimated through experiments or some external analysis that the treatment  reduces the probability of dropping out by 60 percent. •	There is some kind of assessment of the social value of a student persisting in the  program. You can evaluate the model in the context of the problem as follows: •	Assuming that a percentage of the students most likely to rotate will be treated. •	The expected cost is calculated if a percentage of the students is treated: we simulate  reducing their probability of dropping out due to the treatment and we add the costs  of treating them. •	The model is compared against the scenario of not applying any treatment It is not necessary to use highly technical measures to give a summary of how the treatment  and model can help to maintain the value of the portfolio.
77 Responsible use of AI for public policy:  Data science toolkit ggplot (filter (loss_sim, type == ”Model treatment” ),         aes (x = factor (cut), y = - loss / 1e6)) +     geom_boxplot () + ylab( “Incremental profit (millions)” ) +    xlab(“Lower treatment cutoff (probability)” ) +    labs (subtitle = “Profit vs no action” ) + theme_minimal ()   You can choose a cut-off point between 0.2 and 0.3, for example, or do more simulations to  refine your choice. If you want to separate the effect of the treatment with the effect of the treatment applied  according to the model, it can be compared with the action that consists of treating the students at random: ggplot (loss_sim, aes (x = factor (cut), y = - loss / 1e6,   group = interaction (type, cut), color = type)) +    geom_boxplot () + ylab( “Incremental profit (millions)” ) +    xlab(“Lower treatment cutoff (probability)” ) +    labs (subtitle = “Profit vs no action” ) + theme_minimal ()  
78 Responsible use of AI for public policy:  Data science toolkit The bottom line is that the model  helps considerably in the targeting of the program  (the  area between the two curves shown above). Class Imbalance When you have a severe class imbalance, you can face two problems: there are in absolute  terms too few elements of a class to be able to discriminate it effectively (even when you have  the correct attributes or feature), or the usual predictive evaluation methods are deficient to  evaluate the performance of predictions. Consider the following scenario put forth in James et al. (2017): “The data contains 5,822 real customer records. Each record consists of 86 variables, which  contain sociodemographic data (variables 1-43) and product ownership (variables 44-86).  Sociodemographic data is derived from postal codes. All customers living in areas with the  same ZIP code have the same sociodemographic attributes. Variable 86 (Purchase) indicates  whether the client purchased a caravan insurance policy.”16 You want to predict the variable Purchase: caravan <- read_csv( “data / caravan.csv” )%>%   mutate (MOSTYPE = factor (MOSTYPE), MOSHOOFD = factor (MOSHOOFD))%>%>    mutate (Purchase = fct_recode (Purchase, yes = ”Yes”, no = “No”))%>%   mutate (Purchase = fct_rev (Purchase))%>%>  select (-Purchase) nrow (caravan) ## [1] 5822 16  Data and more information are available at http:/ /www.liacs.nl/~putten/library/cc2000/data.html
79 Responsible use of AI for public policy:  Data science toolkit caravan%>% count (Purchase)%>%>>    mutate (pct = 100 * n / sum (n))%>%     mutate (pct = round (pct, 2))  ## # A tibble: 2 x 3 ## Buy n pct ##   <fct>  <int> <dbl> ## 1 yes 348 5.98 ## 2 no 5474 94.0 This is the natural distribution of response seen in the data, and you have relatively little data  in the “Yes” category. Stratified sampling will be used to obtain similar proportions in training and test sets. set.seed( 823)   caravan_split = initial_split (caravan, strata = Buy, prop = 0.9)   caravan_split ## <Training/Validation/Total> ## <5240/582/5822> train <- training (caravan_split) test <- testing (caravan_split) Logistic regression will be used (the same applies for other methods that produce class probabilities, such as boosting, random trees, or neural networks). library(tune)   #data preparation caravan_recipe <- recipe (Buy ~., train)%>% step_dummy (all_nominal (), -Purchase) caravan_receta_prep <- caravan_receta%>% prep # model model_log <- 
80 Responsible use of AI for public policy:  Data science toolkit logistic_reg ()%>%>    set_engine( “glm”)%>%     set_mode( “classification” )%>%   fit (Buy ~., data = caravan_receta_prep%>% juice)> Incorrect Analysis The confusion matrix for the training data is: predictions_ent_glm <- model_log%>% predict (new_data = juice (caravan_receta_prep))%>%> bind_cols (juice (caravan_receta_prep)%>% select (Buy))> predictions_ent_glm%>%> conf_mat (Purchase, .pred_class) ##           Truth ## Prediction if not ## yes 6 9 ## no 299 4926 And the test ones are: test_processed <- bake (caravan_recipe_prep, test) predictions_ent_glm <- model_log%>% predict (new_data = test_processed)%>%> bind_cols (test_processed%>% select (Buy))> predictions_glm%>%> conf_mat (Purchase, .pred_class) ##           Truth ## Prediction if not ## yes 0 4 ## no 43 535 You get poor performance according to this confusion matrix (test and training). The sensitivity  is very low, although the specificity (rate of correct negatives) is high. A typical conclusion is  that  the model has no predictive value, or that it is necessary to oversample the low occurrence class.
81 Responsible use of AI for public policy:  Data science toolkit Correct Analysis Instead of starting with oversampling/undersampling, which modifies the natural proportions of categories in the data, we can work with probabilities instead of class predictions  with a cut-off of 0.5. For example, this can be visualized with a receiving operating characteristic curve (or lift  curve, precision-recall, or some other curve that takes probabilities into account). predictions_prob <- model_log%>%   predict (new_data = test_processed, type = “prob”)%>%   bind_cols (test_processed%>% select (Buy))>  select (.pred_si, Buy) roc_data <- roc_curve (predictions_prob, Buy, .pred_si) autoplot (roc_data) +   xlab( “1 - specificity” ) + ylab( “sensitivity” )  It can be seen that it is possible to achieve good levels of sensitivity if some degradation in  specificity is accepted, which is originally very high. For example, cutting at 0.05 can obtain  specificity and sensitivity that are possibly adequate for the problem. datos_roc %>% filter(abs(.threshold - 0.04) < 1e-4) %>% round( 4)  ## # A tibble: 2 x 3 ## .threshold specificity sensitivity
82 Responsible use of AI for public policy:  Data science toolkit ##        <dbl>       <dbl>       <dbl> ## 1     0.0399       0.553       0.744 ## 2     0.0399       0.555       0.744 What happens if we oversample? Oversample: caravan_recipe <- recipe (Buy ~., train)%>% step_dummy (MOSTYPE, MOSHOOFD)%>%>  step_smote (Purchase) smote_prep <- prep (caravan_receta_smote) # model train_1 <- juice (smote_prep) train_1%>% count (Purchase)> ## # A tibble: 2 x 2 ## Buy n pct ##   <fct>  <int> ## 1 yes 4935 ## 2 no 4935 model_log_smote <-  logistic_reg ()%>%>    set_engine( “glm”)%>%     set_mode( “classification” )%>%   fit (Buy ~., data = train_1) In training the confusion matrix is  apparently better. predictions_ent_glm <- model_log_smote%>% predict (new_data = train_1)%>%> bind_cols (train_1%>% select (Buy))> predictions_ent_glm%>%> conf_mat (Purchase, .pred_class)
83 Responsible use of AI for public policy:  Data science toolkit ##           Truth ## Prediction if not ## yes 3854 1271 ## no 1081 3664 But while testing, the results are very similar. The model built by sub-sampling the ruling  class is also added. train_sub <- caravan_recipe%>% step_downsample (Purchase)%>% prep ()%>% juice>> model_log_sub <-  logistic_reg ()%>%>    set_engine( “glm”)%>%     set_mode( “classification” )%>%   fit (Buy ~., data = train_sub) predictions_prob <- model_log_smote%>%   predict (new_data = test_processed, type = “prob”)%>%  bind_cols (test_processed%>% select (Buy)) %>%  select (.pred_si, Buy) predictions_prob_sub <- model_log_sub%>%    predict (new_data = test_processed, type = “prob”)%>%  bind_cols (test_processed%>% select (Buy)) %>%  select (.pred_si, Buy) roc_smote_data <- roc_curve (predictions_prob, Buy, .pred_si) roc_sub_data <- roc_curve (predictions_prob_sub, Buy, .pred_si) roc_comp_data <- bind_rows (roc_data%>% mutate (type = “natural” ),                              data_roc_smote%>% mutate (type = “with oversampling” ),                              data_roc_sub%>% mutate (type = “with subsample” )  )
84 Responsible use of AI for public policy:  Data science toolkit ggplot (comp_roc_data,        aes (x = 1 - specificity, y = sensitivity, color = type)) +   geom_path () +   geom_abline (lty = 3) +  coord_equal () + theme_bw ()   The original problem was not that the fit was not working, but that the wrong cut-off   point was evaluated. A cut-off point of 0.5 with SMOTE is equivalent to a much smaller one  without SMOTE. Worse still, the  probabilities of the oversampled model do not reflect the occurrence  rates of the response of interest, which can produce misleading summaries of the response rates that are expected to be observed in production. Equality with Protected Attributes The following example is derived from Hardt, Price, and Srerbro (2016). Suppose you have  a protected attribute A that has two values: blue and orange. Orange is the disadvantaged  minority group. Simulated data will be used as follows, with the score attribute associated  with the protected attribute:
85 Responsible use of AI for public policy:  Data science toolkit inv_logit <- function (x) {     1 / (1 + exp (-x))   } simulate_data <- function (n = c ( 10000, 2000)) {     score_azul <- pmax (rnorm (n [ 1], 50, 10), 0)     score_orange <- pmax (rnorm (n [ 2], 40, 10), 0)     blue <- tibble (type = “blue”, score = score_blue)     orange <- tibble (type = “orange” , score = score_orange)   data <- bind_rows (blue, orange)%>%      mutate (coef_0 = ifelse (type == “blue”, 0.0, 0),              prob_real_pos = inv_logit(- 1 + coef_0 + 0.1 * (score- 40))) %>%       mutate(atr_1 = rpois(nrow(.), 3))  data%>% select (-coef_0)%>%>>      mutate (pay = map_dbl (prob_real_pos, ~ rbinom ( 1, 1, .x)))%>%   select (-prob_real_pos) } set.seed ( 1221)   tbl_data <- simulate_data () Using a histogram for the score, a minority group is obtained with values of the lowest score  variable. ggplot (tbl_datos, aes (x = score, fill = type)) + geom_histogram ()   A simple logistic regression model is fitted.
86 Responsible use of AI for public policy:  Data science toolkit reg_log <- glm (pay ~ score + atr_1 + type, tbl_datos, family = “binomial” )  tbl_datos <- tbl_datos%>% mutate (prob_pos = predict (reg_log, type = “response”))  The actual compliance rates are the same for the two groups. First, a strategy is considered  where the same cut-off point is applied for all groups. cut_result <- function (data_tbl, cuts) {   result <- tbl_datos%>%      mutate(recibe = ifelse(type == “Blue”, prob_pos > cortes[ 1], prob_pos >  cortes[2]),             decision = ifelse (receive, “Accepted” , “Rejected” ))   result%>% group_by (type, decision, pay)%>% count ()%>%>>>  ungroup () } count_results <- cut_result (tbl_data, c ( 0.6, 0.6))   results_counting ## # A tibble: 8 x 4 ## decision type pays n ##   <chr>   <chr>     <dbl> <int> ## 1 blue Accepted 0 905 ## 2 blue Accepted 1 2400 ## 3 blue Rejected 0 4 149 ## 4 blue Rejected 1 2546 ## 5 orange Accepted 0 47 ## 6 orange Accepted 1 101 ## 7 orange Rejected 0 1353 ## 8 orange Rejected 1 499 results_counting%>%> group_by (type, decision)%>%>
87 Responsible use of AI for public policy:  Data science toolkit summarize (n = sum (n))%>%>  mutate (total = sum (n))%>%>  mutate (prop = n / total)%>%>    filter (decision == “Accepted” )  ## # A tibble: 2 x 5 ## # Groups: type [2] ## decision type n total prop ##   <chr>   <chr>    <int> <int> <dbl> ## 1 blue Accepted 3305 10000 0.330 ## 2 orange Accepted 148 2000 0.074 Note that the orange group has received considerably less acceptance than the blue group,  both in total and in proportion. Furthermore, with the precision or rate of true positives we  can evaluate what proportion of those who would comply if they were accepted were accepted  according to the cut-off point. results_counting%>%>    filter (pay == 1)%>%   group_by (type)%>%>  mutate (tvp = n / sum (n))%>%>    filter (decision == “Accepted” )  ## # A tibble: 2 x 5 ## # Groups: type [2] ## type decision pays n tvp ##   <chr>   <chr>    <dbl> <int> <dbl> ## 1 blue Accepted 1 2400 0.485 ## 2 orange Accepted 1 101 0.168 It can be seen that the orange group is also at a disadvantage, since among those who comply  there are fewer acceptance decisions. The next step is to consider  demographic parity. In this case, it was decided to give the same  number of loans to each group, depending on their size. 
88 Responsible use of AI for public policy:  Data science toolkit calculate_parity_points <- function (tbl_data, prop) {   tbl_datos%>% group_by (type)%>%>>      summarize (cut = quantile (prob_pos, 1 - prop))   } tbl_parity_cuts <- calculate_parity_points (tbl_data, 0.45)   tbl_parity_cuts ## # A tibble: 2 x 2 ## cut type ##   <chr>   <dbl> ## 1 blue 0.521 ## 2 orange 0.297 The cut-off point for blue is more demanding than for orange. In itself that is not a problem,  but it is observed that: cuts_parity <- cuts_parity_tbl%>% pull (cut) count_results <- cut_result (tbl_data, parity_cuts)  results_counting%>%>    filter (pay == 1)%>%   group_by (type)%>%>  mutate (tvp = n / sum (n))%>%>    filter (decision == “Accepted” )  ## # A tibble: 2 x 5 ## # Groups: type [2] ## type decision pays n tvp ##   <chr>   <chr>    <dbl> <int> <dbl> ## 1 blue Accepted 1 3094 0.626 ## 2 orange Accepted 1 410 0.683 And so in addition to being more demanding with the blue group, those who comply with the  blue group are also given fewer acceptance decisions. Additionally, considerably fewer people  are accepted from the population.
89 Responsible use of AI for public policy:  Data science toolkit The equal opportunity solution is to do a cut-off so that the acceptance rate within the group  of those who pay is similar for both populations, which occurs at approximately 0.35: calculate_opportunity_cuts <- function (tbl_data, prop) {   tbl_datos%>%>      filter (pay ==  1)%>%   group_by (type)%>%>  mutate (rank_p = rank (prob_pos) / length (prob_pos))%>%>  filter (rank_p <prop)%>%>      top_n ( 1, rank_p)%>%   select (type, cut = prob_pos) } cuts_op <- calculate_cuts_opportunity (tbl_datos, 0.35)   count_results <- cut_result (tbl_datos, cut_op%>% pull (cut))  results_counting%>%>    filter (pay == 1)%>%   group_by (type)%>%>  mutate (tvp = n / sum (n))%>%>    filter (decision == “Accepted” )  ## # A tibble: 2 x 5 ## # Groups: type [2] ## type decision pays n tvp ##   <chr>   <chr>    <dbl> <int> <dbl> ## 1 blue Accepted 1 3215 0.650 ## 2 orange Accepted 1 391 0.652 Note: If the positive outcome variable is unfairly assigned, then this method does not solve the  problem. In this case, it is relevant to understand what the criteria are with which a successful result is considered depending on the group of the protected attribute (e.g., if a particular  segment is allowed greater arrears in payments and another group allowed less arrears, or if a  group is considered a repeat offender for a much lesser offense than other groups).
90 Responsible use of AI for public policy:  Data science toolkit Accountability Interpretability Measures such as importance of permutations can be used to examine models. In this example, we return to the credit application acceptance prediction exercise, and we consider importance based on permutations (Molnar 2019): set.seed( 823)  credit_split <- initial_split (credit) train <- training (credit_split) test <- testing (credit_split) #data preparation recipe_credit <- recipe (card ~., credit)%>% step_normalize (all_numeric ())%>%>  step_dummy (all_nominal (), -card)  # model regular_model <-    logistic_reg (penalty = 1)%>%     set_engine( “keras”, epochs = 500, verbose = FALSE)%>%     set_mode( “classification” )   # adjust preprocessing parameters recipe_prep <- credit_recipe%>% prep (train) # preprocess data train_prep <- bake (recipe_prep, train) test_prep <- bake (recipe_prep, test) # adjust model fit <- regular_model%>%  fit (card ~ expense + dependents + income + age + owner_yes, data = train_prep) library(iml)  model <- fit $ fit train_x <- train_prep%>% dplyr :: select (expense, dependents, income, age, owner_yes) predictor <- Predictor $ new (model, data = train_x, y = ifelse (train_prep $ card  ==”yes”,2,1),                             type = “prob”)  imp <- FeatureImp $ new (predictor, loss = “ce”, compare = “difference” )  plot (imp) + theme_minimal ()
91 Responsible use of AI for public policy:  Data science toolkit It is seen that, for this network without hidden layers, the importance is concentrated in a  single predictor,  expenditure, which as seen represents an information leak. This diagnosis is  useful in general, and although not as dramatic as this example, it can point out which variables are important to consider carefully. It is important to also consider the effect of variables associated with protected groups, and  if necessary, carefully examine how they affect predictions. • Parsimonious models, which use fewer attributes, facilitate analysis, maintain data flow,  and reduce exposure to leakage problems or undesirable effects. Explanation of Predictions To explain individual predictions, the Shapley values are used (Molnar 2019; Lundberg and  Lee 2017). These graphs below indicate the assigned contribution of each attribute to an indi vidual prediction, under the idea of considering marginal effects on the prediction depending  on the presence or absence of other attributes. The contributions obtained add up to the  difference between the particular prediction and the average prediction. Averages across interest groups can also be examined. Consider the example of factors for detecting heart disease (Rossouw 1983): model_sa <- sa_boosted $ fit sa_entrena_x <- sa_entrena%>% dplyr :: select (-enf_coronaria) predict_fun <- function (object, newdata) {     new_data_x = xgb.DMatrix (data.matrix (newdata), missing = NA)  results <-predict (model_sa, new_data_x)   return (results)  
92 Responsible use of AI for public policy:  Data science toolkit } predictor <- Predictor $ new (model_sa, data = sa_entrena_x, y = sa_entrena $  chd,                  type = “prob”, predict.function = predict_fun)   # the case of interest is case 15 shapley_values <- Shapley $ new (predictor, x.interest = (sa_entrena_x [ 15,]))  shapley_values $ plot () + theme_minimal () In this case, several measures contribute positively to the likelihood of heart disease, such as  tobacco use, age, and cholesterol measurements. These contributions explain the very high  probability of this particular individual. In contrast, the following person is close to average, with age and cholesterol levels increasing positively, but non-tobacco use and no family history of diabetes:   # the case of interest is case 24 shapley_values <- Shapley $ new (predictor, x.interest = (sa_entrena_x [ 24,]))   shapley_values $ plot () + theme_minimal ()
93 Responsible use of AI for public policy:  Data science toolkit Remark: As in the model and partial dependence graphs discussed above, these coefficients  should  not be interpreted causally (e.g., cholesterol needs to be lowered for these two indi viduals). This is the information that the model uses to build the prediction from the average  prediction over the population. Shapley values can be calculated for two age groups, for example.
94 Responsible use of AI for public policy:  Data science toolkitReferences Athey, S. W. (2018). Estimation and Inference of Heterogeneous Treatment Effects using Random Forests. Journal of the American Statistical Association. Barocas, S., & Selbst, A. D. (2014). Big Data’s Disparate Impact. SSRN eLibrary. Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., & Kalai, A. T. (2016). Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. ArXiv, abs/1607.06520. Breiman, L. (2001). Random Forests. Machine Learning, 45(1), págs. 5-32. Buolamwini, J., & Gebru, T. (23–24 Feb de 2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. En S. A. Friedler, & C. Wilson (Ed.), Conferen ceProceedings of the 1st Conference on Fairness, Accountability and Transparency. 81, págs.  77-91. New York, NY, USA: PMLR. Buuren, S. v., & Groothuis-Oudshoorn, K. (2011). mice: Multivariate Imputation by Chained  Equations in R. Journal of Statistical Software, 45(3), págs. 1-67. Carrillo, A., Cantú, L., & Noriega, A. (2020). Individual Explanations in Machine. IADB. Chawla, N. V. (2002). SMOTE: Synthetic Minority over-Sampling Technique. Journal of Artificial  Intelligence Research 16: 321–57. drivendata. (2019). An ethics checklist for data scientists. Obtenido de https://deon.drivendata.org/ Friedman, J. H. (2001). Greedy Function Approximation: A Gradient Boosting Machine. The  Annals of Statistics, 29(5), págs. 1189-1232. Fritzler, A. (2015). An ethical checklist for data science. Obtenido de http://www.dssgfellow ship.org/2015/09/18/an-ethical-checklist-for-data-science/ Gebru, T., Morgenstern, J., Vecchione, B., Wortman, J., Wallach, H., Daumé, H., & Crawford, K.  (2018). Datasheets for Datasets. Retrieved from https://arxiv.org/pdf/1803.09010.pdf Gelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (1 ed.). Cambridge University Press. Greene, W. (2003). Econometric Analysis. Pearson Education. Obtenido de https://books.google.com.mx/books?id=njAcXDlR5U8C Hardt, M. a. (2016). Equality of Opportunity in Supervised Learning. CoRR, abs/1610.02413. Harini Suresh, J. V. (2019). A Framework for Understanding Unintended Consequences of  Machine Learning. MIT. Obtenido de https://arxiv.org/pdf/1901.10002.pdf
95 Responsible use of AI for public policy:  Data science toolkitHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning. Springer  New York Inc. INEGI. (2014). Encuesta Nacional de Ingresos Y Gastos de Los Hogares (Enigh-2014). Diseño  Muestral. Obtenido de http://internet.contenidos.inegi.org.mx/contenidos/Productos/prod_ serv/contenidos/espanol/bvinegi/productos/nueva_estruc/702825070359.pdf James, G. D. (2017). Data for an Introduction to Statistical Learning with Applications in R.  Obtenido de https://CRAN.R-project.org/package=ISLR Kaufman, S., Rosset, S., & Perlich, C. (01 de 2011). Leakage in Data Mining: Formulation, Detection, and Avoidance., 6, págs. 556-563. Kim, B. R. (2016). Examples are not enough, learn to criticize! Criticism for interpretability.  Advances in Neural Information Processing Systems. Kuhn, M. F. (2020). Rsample: General Resampling Infrastructure. Obtenido de https:// CRAN.R-project.org/package=rsample Kuhn, M., & Johnson, K. (2013). Applied Predictive Modeling. Springer New York. Lackland, D. (06 de 2014). Racial Differences in Hypertension: Implications for High Blood  Pressure Management. The American journal of the medical sciences, 348. Little, R. J., & Rubin, D. B. (2002). Statistical analysis with missing data. Wiley. Lohr, S. L. (2009). Sampling: Design and Analysis. Cengage Learning. Lundberg, S., & Lee, S.-I. (2017). A Unified Approach to Interpreting Model Predictions. ArXiv,  abs/1705.07874. Miller, T. (2019). Explanation in Artificial Intelligence: Insights from the Social Sciences. Artif.  Intell., 267, págs. 1-38. Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., . . . Gebru, T. (2019).  Model Cards for Model Reporting. Retrieved from https://arxiv.org/abs/1810.03993 Molnar, C. (2019). Interpretable Machine Learning.  Obermeyer, Z., Powers, B., Vogeli, C., & Mullainathan, S. (2019). Dissecting racial bias in an  algorithm used to manage the health of populations. Science, 366(6464), págs. 447-453. OECD (Forthcoming). (s.f.). Framework for the Classification of AI Systems . Paris.: OECD Publishing. OECD. (2019c). Artificial Intelligence in Society. Paris: OECD Publishing. OECD. (2021). Good Practice Principles for Data Ethics in the Public Sector.    
96 Responsible use of AI for public policy:  Data science toolkitPombo, C., Cabrol, M., Alarcón, N. G., & Ávalos, R. S. (2020). fAIr LAC: Adopción ética y responsable de la inteligencia artificial en América Latina y el Caribe. doi:http://dx.doi. org/10.18235/0002169 Prosser, C., & Mellon, J. (2016). Twitter and Facebook are Not Representative of the General  Population: Political Attitudes and Demographics of Social Media Users. Available at SSRN:  https://ssrn.com/abstract=2791625 or http://dx.doi.org/10.2139/ssrn.2791625 . Rossouw, J. E. (1983). Coronary Risk Factor Screening in Three Rural Communities. The Coris  Baseline Study. South African Medical Journal = Suid-Afrikaanse Tydskrif Vir Geneeskunde. Rubin, R. J. (2002). Statistical Analysis with Missing Data, Second Edition . John Wiley & Sons,  Inc. Stuart, K. I. (2008). Misunderstandings Among Experimentalists and Observationalists about  Causal Inference. Journal of the Royal Statistical Society, Series A, 171, part 2, págs. 481-502. Sundararajan, M., Taly, A., & Yan, Q. (2017). Axiomatic Attribution for Deep Networks. ArXiv,  abs/1703.01365. Suresh, H., & Guttag, J. V. (2019). A Framework for Understanding Unintended Consequences  of Machine Learning. ArXiv, abs/1901.10002. Vaver, J., & Koehler, J. (2011). Measuring Ad Effectiveness Using Geo Experiments. Google Inc. Vayena, A. J. (2019). The global landscape of AI ethics guidelines. Springer Science and Business Media LLC. Verma, S., & Rubin, J. (2018). Fairness Definitions Explained. ConferenceProceedings of the  International Workshop on Software Fairness (págs. 1-7). New York, NY, USA: Association for  Computing Machinery. Wachter, S., Mittelstadt, B. D., & Russell, C. (2017). Counterfactual Explanations without Open ing the Black Box: Automated Decisions and the GDPR. ArXiv, abs/1711.00399. Washingtonpost. (04 de 2019). 21 more studies showing racial disparities in the criminal justice system. Obtenido de https://www.washingtonpost.com/opinions/2019/04/09/ more-studies-showing-racial-disparities-criminal-justice-system/ Williams, D. M. (09 de 1981). Racial differences of hemoglobin concentration: measurements  of iron, copper, and zinc. The American Journal of Clinical Nutrition, 34(9), págs. 1694-1700. Wilson, J. (2014). What your IQ score doesn’t tell you. CNN.
97 Responsible use of AI for public policy:  Data science toolkit
98 Responsible use of AI for public policy:  Data science toolkit 
