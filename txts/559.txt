Auditing and Achieving Intersectional Fairness in Classification Problems Giulio Morina‚àó QuantumBlack, a McKinsey company London, United Kingdom fairness@quantumblack.comViktoriia Oliinyk QuantumBlack, a McKinsey company London, United Kingdom viktoriia.oliinyk@quantumblack.comJulian Waton QuantumBlack, a McKinsey company London, United Kingdom julian.waton@quantumblack.com Ines Maru≈°iƒá QuantumBlack, a McKinsey company London, United Kingdom ines.marusic@quantumblack.comKonstantinos Georgatzis QuantumBlack, a McKinsey company London, United Kingdom konstantinos.georgatzis@quantumblack.com ABSTRACT Machine learning algorithms are extensively used to make increasingly more consequential decisions about people, so achieving optimal predictive performance can no longer be the only focus. A particularly important consideration is fairness with respect to race, gender, or any other sensitive attribute. This paper studies intersectional fairness, where intersections of multiple sensitive attributes are considered. Prior research has mainly focused on fairness with respect to a single sensitive attribute, with intersectional fairness being comparatively less studied despite its critical importance for the safety of modern machine learning systems. We present a comprehensive framework for auditing and achieving intersectional fairness in classification problems: we define a suite of metrics to assess intersectional fairness in the data or model outputs by extending known single-attribute fairness metrics, and propose methods for robustly estimating them even when some intersectional subgroups are underrepresented. Furthermore, we develop post-processing techniques to mitigate any detected intersectional bias in a classification model. Our techniques do not rely on any assumptions regarding the underlying model and preserve predictive performance at a guaranteed level of fairness. Finally, we give guidance on a practical implementation, showing how the proposed methods perform on a real-world dataset. 1 INTRODUCTION Fairness is a growing topic in the field of machine learning, as models are being built to determine life-changing events such as loan approvals and parole decisions. Thus, it is critical that these models do not discriminate against individuals on the basis of their race, gender or any other sensitive attribute, by learning to replicate or exacerbate biases inherent in society. Much of the algorithmic fairness literature thus far has focused on fairness with respect to an individual sensitive attribute. In this work, we consider fairness for anintersection of sensitive attributes . That is, our focus is on ensuring fairness for groups defined by multiple sensitive attributes, for example, ‚Äúblack women‚Äù instead of just ‚Äúblack people‚Äù or ‚Äúwomen‚Äù. Ensuring intersectional fairness is critical for safe deployment of modern machine learning systems. A stark example of intersectional bias in deployed systems was discovered by Buolamwini and Gebru [4]who showed that several commercially available gender ‚àóAlso with University of Warwick, Department of Statistics. Work done as an intern at QuantumBlack.classification systems from facial image data had substantial intersectional accuracy disparities when considering gender and race (represented via Fitzpatrick skin type), with darker-skinned women being the most misclassified group ‚Äì having an accuracy drop of over 30% compared to lighter-skinned men. Buolamwini and Gebru [4]emphasize the need for investigating the intersectional error rates, noting that gender and skin type alone do not paint the full picture regarding the distribution of misclassifications. Hart [17] notes that medical data, e.g, from randomized control trials, are often biased in favor of white men and therefore any model trained on this data may exacerbate existing healthcare inequalities. In their study on the increased risk of maternal death among ethnic minority women in the UK, Ameh and Van Den Broek [2]note that there was limited data specifically for black and ethnic minority women born in the UK, and emphasized the need for reliable statistics to understand the scale of the problem. Our Contributions. We present a comprehensive framework for auditing and achieving intersectional fairness, consisting of three pillars: (i) metrics for measuring intersectional fairness in both datasets and model outputs, (ii) methods for robustly estimating these metrics, and (iii) post-processing methods for ensuring intersectional fairness in classification problems. First, we define metrics for measuring intersectional fairness in datasets and model outputs by extending well-established fairness metrics to the case of intersectionalities. Our work builds most directly upon the concept of œµ-differential fairness introduced by Foulds et al . [15] . Specifically, we extend their definition of differential fairness for statistical parity to: 1) elift and impact ratio metrics for data, and 2) equal opportunity and equalized odds metrics for model outputs. This enables practitioners to assess intersectional fairness through multiple, not mutually exclusive, lenses. Second, we propose techniques to robustly measure intersectional fairness. These techniques address real-world concerns of marginalized intersectional subgroups being even more underrepresented in the available datasets due to data-collection biases. Importantly, we provide theoretical guarantees and demonstrate the performance of the estimators qualitatively and experimentally on a synthetic dataset. Third, we develop algorithms to mitigate any detected intersectional bias in a binary classification model: post-processing methodologies that threshold risk scores and randomize predictions separately for each intersection of sensitive attributes, combining andarXiv:1911.01468v2  [cs.LG]  8 Jun 2020
extending the work of Hardt et al . [16] and Corbett-Davies et al . [8]. Our methods maximize predictive performance whilst guaranteeing intersectional fairness. Furthermore, our formulation allows the practitioner to simultaneously focus on multiple fairness metrics, thus allowing to control for multiple facets of model bias. We provide implementation details and demonstrate the utility of our methods experimentally on the Adult Income Prediction problem [10]. Paper Structure. We discuss related work in Section 2. We define intersectional fairness metrics in Section 3, proving some of their theoretical properties in Section 3.1 and presenting methods for robustly estimating them in Section 3.2. In Section 4, we frame post-processing as an optimization problem which aims to preserve good predictive performance while ensuring intersectional fairness; we introduce the formulations in Sections 4.2 and 4.3 for binary and score predictors, respectively. We demonstrate the utility of our methods experimentally on a synthetic dataset and on the Adult dataset [ 10] in Section 5. In Section 6, we conclude and suggest future work. Proofs and notes on reproducibility of results are presented in the supplementary material. Running Example. Throughout the paper, we consider a practical application of auditing and mitigating bias by using the 1994 U.S. census Adult dataset from the UCI repository [ 10]. The aim is to predict whether an individual‚Äôs income is greater than $50,000, using socio-demographic attributes. This dataset contains multiple sensitive attributes; in this paper, we focus on the following three: age, gender, and race. 2 RELATED WORK There is no one fairness definition suitable for all use cases and application domains. Indeed, more than 20 different fairness metrics have been proposed [ 31], some of which are mutually incompatible [27,33]. What constitutes an appropriate fairness metric depends on the application, societal context, and any regulatory or other requirements. One can broadly divide the existing fairness metrics into group and individual ones. Group fairness partitions the population into groups according to the sensitive attributes and aims to ensure similar treatment with respect to a fixed statistical measure. Individual fairness seeks for individuals with similar features to be treated similarly regardless of their sensitive attributes. Assessing group fairness of a dataset or model output becomes much more challenging when considering multiple sensitive attributes [ 20]. The number of generated subgroups grows exponentially with the number of attributes considered, making it difficult to inspect every subgroup for fairness due to both computational as well as data sparsity issues. A first challenge is, therefore, to come up with fairness metrics that can accommodate a large number of intersectional subgroups [ 9,19,24]. Our work builds most directly upon theœµ-differential fairness metric introduced by Foulds et al . [15]. Such a metric satisfies important desiderata, overlooked by other multi-attribute metrics [ 19,24]: It (i) considers multiple sensitive attributes, (ii) protects subgroups defined by intersections of and by individual sensitive attributes (e.g., ‚Äúblack women‚Äù and ‚Äúwomen‚Äù respectively), (iii) safeguards minority groups, and (iv)aims at rectifying systematic differences between subgroups. Foulds et al. [15] demonstrate that œµ-differential fairness also satisfies other important properties, such as providing privacy, economical, and generalization guarantees. They also extend the original definition to handle confounders and propose deep neural network classifiers that handle intersectional fairness. Foulds et al . [15] focus mainly on enabling a more subtle understanding of unfairness than with a single sensitive attribute, whereas we present multiple metrics that allow a more nuanced analysis of intersectional discrimination. While Foulds et al . [15] propose a pointwise estimate for intersectional bias, we have found that it can be unstable in practice, as illustrated in Example 5.1. In a later work, Foulds et al . [14] use an elegant hierarchical approach with probabilistic models to overcome the issue of instability and provide uncertainty estimates; their formulation, however, requires careful tuning of hyper-parameters and is computationally more demanding than our proposed ones. Several other methods have been proposed for handling intersectional bias that either make use of ad-hoc algorithms [ 26] or are based on visual analytic tools [ 6]. For intersectional bias detection, Chung et al . [7] suggest a top-down method to find underperforming subgroups. The dataset is divided into more granular groups by considering more features until a subgroup with statistically significant loss is found. In contrast, Lakkaraju et al . [29] use approximate rule-based explanations to describe subgroup outcomes. As well as detecting discriminatory bias, another line of research has focused on achieving ‚Äúfairer‚Äù models. There are three possible points of intervention to mitigate unwanted bias in the machine learning pipeline: the training data, the learning algorithm, and the predicted outputs. These are associated with three classes of bias mitigation algorithms: pre-processing, in-processing, and postprocessing. Pre-processing methods a-priori transform the data to remove bias or extract representations that do not contain information related to sensitive attributes [ 11,21,32].In-processing methods modify the model construction mechanism to take fairness into account [ 22,34,35].Post-processing methods transform the output of a black-box model in order to decrease discriminatory bias [8, 16]. Kearns et al . [24,25]propose and demonstrate the performance of an in-processing training algorithm which mitigates intersectional bias by imposing fairness constraints on the protected subgroups. Their work is a generalization of the ‚Äúoracle efficient‚Äù algorithm by Agarwal et al . [1] to the case of infinitely many protected subgroups. In contrast, we develop a novel post-processing method. Postprocessing methods are popular in practical applications as they do not interfere with the training process and are thus suitable for runtime environments. In addition, these methods are model agnostic and privacy preserving as they do not require access to the model or features other than sensitive attributes [ 21]. The work of Hardt et al. [16] aims to ensure equal opportunity for two subgroups of the population, defined by a single binary sensitive attribute. They achieve this by randomly flipping some of the predictions in order to mitigate discriminatory bias. Corbett-Davies et al . [8] propose another post-processing approach by treating model predictions differently depending on subgroup membership. We combine both approaches and expand them to the case of intersectional fairness. 2
3 METRICS FOR INTERSECTIONAL FAIRNESS In this section, we introduce fairness metrics that can handle intersections of multiple sensitive attributes. Such metrics can be applied to assess fairness in either the data or in model outputs. Robustly estimating them is non-trivial in practice due to subgroup underrepresentation. Indeed, minority groups may be even more severely underrepresented in a dataset compared to their true representation in the general population; one cause of this is bias in the data collection practices. After defining the metrics in Section 3.1, in Section 3.2 we present three approaches for robustly estimating the intersectional impact ratio. The same approach can be applied to any other intersectional fairness metric. Notation. Letpbe the number of different sensitive attributes. We denote by A1, . . . , Apdisjoint sets of discrete-valued sensitive attributes; e.g., A1could represent gender, A2race, A3nationality and so forth. The space of intersections is denoted by A=A1√ó¬∑¬∑¬∑√ó Ap. Therefore, a specific element s‚ààAis a particular combination of attributes; e.g., s=(Woman, Black, Italian )‚ààA1√óA2√óA3. Suppose we have access to a finite dataset with nobservations denoted byD={(xi,yi)}i=1, ...,n, where xirepresents the individual‚Äôs features ‚Äì including their sensitive attributes ‚Äì and yi‚àà{0,1} a binary outcome. We interpret yi=1as a ‚Äúpositive‚Äù outcome and ‚Äúnegative‚Äù otherwise, denoting by Ythe random variable describing the true outcomes. Furthermore, we let Sbe a discrete random variable with support on A. For brevity, we denote its probability mass function by ¬µs=P(S=s); i.e.,¬µsis the probability that an individual has sensitive attributes s‚ààA. Analogously, we denote by¬µ1=P(Y=1)the probability that a given individual has positive outcome. Finally, we will also denote the probability that an individual with sensitive attributes shas positive outcome as ¬µ1|s=P(Y=1|S=s). We do not make explicit assumptions on the distribution of YorSbut we shall assume ¬µs>0,¬µ1|s>0,‚àÄs‚ààA. Given a classifier, we denote by ÀÜyi‚àà{0,1}the prediction for theithindividual and by ÀÜYthe corresponding random variable describing predicted outcomes. Importantly, we do not make any assumptions on how the model has been constructed and regard it as a black box. 3.1 Definitions of Metrics We now introduce intersectional fairness metrics for datasets and model outputs. Our metrics are based on the œµ-differential fairness framework of Foulds et al . [15] . Metrics introduced in this paper can be seen as relaxations of the widely-used fairness metrics for a single sensitive attribute, motivated by the fact that the number of intersections grows exponentially with sensitive attributes. In Table 1 we define fairness metrics to assess intersectional bias in the data, while Table 2 defines metrics to assess intersectional bias in model outputs. With the exception of œµ-differential fairness for statistical parity (introduced by Foulds et al . [15] ), intersectional fairness definitions for other metrics of Table 1 and 2 are, to our knowledge, novel contributions. We prove some of their theoretical properties in Theorem 3.1. Although we restrict our analysis to fairness metrics for binary outcomes, they can be easily extendedTable 1:œµ-differential fairness metrics on the data Fairness metric Intersectional definition elift e‚àíœµ‚â§P(Y=1|S=s) P(Y=1)‚â§eœµ,‚àÄs‚ààA impact ratio (slift)e‚àíœµ‚â§P(Y=1|S=s) P(Y=1|S=s‚Ä≤)‚â§eœµ,‚àÄs,s‚Ä≤‚ààA Table 2:œµ-differential fairness metrics on the model Fairness metric Intersectional definition statistical parity (demographic parity)e‚àíœµ‚â§P(ÀÜY=1|S=s) P(ÀÜY=1|S=s‚Ä≤)‚â§eœµ,‚àÄs,s‚Ä≤‚ààA TPR parity (equal opportunity)e‚àíœµ‚â§P(ÀÜY=1|Y=1,S=s) P(ÀÜY=1|Y=1,S=s‚Ä≤)‚â§eœµ,‚àÄs,s‚Ä≤‚ààA FPR parity e‚àíœµ‚â§P(ÀÜY=1|Y=0,S=s) P(ÀÜY=1|Y=0,S=s‚Ä≤)‚â§eœµ,‚àÄs,s‚Ä≤‚ààA equalized oddsIfœµ-differential fairness is satisfied for both TPR and FPR parity to the categorical case by simply requiring them to hold for all possible outcomes. All metrics are parameterized by œµ‚â•0. Note that œµ=0corresponds to achieving perfect fairness with respect to a given metric. Moreover,œµ-differential fairness allows us to compare bias between two different models. In particular, if we assume that two models achieve œµ-differential fairness for œµ1andœµ2respectively, then the quantity exp(œµ2‚àíœµ1)can be interpreted as a multiplicative increase/decrease of one model‚Äôs bias with respect to the other, a phenomenon known as bias amplification [36]. Let us apply these metrics on our running Adult dataset example, focusing on two sensitive attributes: gender and race. If the income distribution in the population did not differ across race and gender subgroups, the elift ratio would be close to 1 and œµwould be close to 0. We would like to collect a representative sample from each intersection that satisfies these requirements. In this U.S. census data, we see that the high income rate of white men is 30% whilst for black women it is 6%. The œµvalue for elift is driven by the subgroup with the largest absolute difference in log proportion of high incomes from the base rate for the entire population; in this case the subgroup (gender, race)=(women, ‚Äòother‚Äô). For the performance metric of intersectional False Positive Rate (FPR) parity, a fair model should have similar FPRs predicting high income for individuals who are white men and black women, say, as well as other combinations of the sensitive attributes. A key desideratum of any intersectional fairness metric is for intersectional fairness to imply fairness with respect to individual sensitive attributes or arbitrary subsets thereof. Theorem 3.1 proves that this is indeed the case; i.e., if œµ-differential fairness is satisfied forA=A1√ó¬∑¬∑¬∑√ó Ap, then it is also satisfied when only A1is considered, A1√óA2and any other possible combination. Theorem 3.1. LetA‚Ä≤=Ac1√ó¬∑¬∑¬∑√ó Ack, where ci‚àà{1, . . . , p} andk‚â§p. Ifœµ-differential fairness is satisfied for any of the metrics 3
in Tables 1 and 2 on the space of intersections A, thenœµ-differential fairness is also satisfied on the space A‚Ä≤for the same metric. 3.2 Robust Estimation of Metrics We now tackle the problem of auditing discriminatory bias having only access to a finite dataset D. In particular, we are interested in the case where some combinations of sensitive attributes may be underrepresented in the data. This is often the case in real-world datasets, usually due to historical or societal biases. We first make clear what we mean by auditing for intersectional fairness. We then explore three different methodologies to achieve this: (i) smoothed empirical estimation , where fairness metrics are directly computed from the data, (ii) bootstrap estimation , to measure uncertainty in the empirical estimates, and (iii) Bayesian estimation , to provide credible intervals. Byestimating the level of intersectional bias we mean computing the minimum value of œµ‚â•0such that the chosen intersectional fairness conditions (one or more) of Tables 1 and 2 hold. For simplicity of exposition we focus on impact ratio, but the same reasoning can readily be applied to all other metrics. As per Table 1, estimating the level of impact ratio bias means computing: œµI R:=min œµ‚â•0 e‚àíœµ‚â§¬µ1|s ¬µ1|s‚Ä≤‚â§eœµ,‚àÄs,s‚Ä≤‚ààA . (1) In practical applications, it is often of interest to also check which attributes s,s‚Ä≤yield big values of the ratios¬µ1|s ¬µ1|s‚Ä≤. Computing œµI Rmay appear straightforward: we could just calculate¬µ1|sfor all s‚ààAand letœµI R=log max s,s‚Ä≤‚ààAn¬µ1|s ¬µ1|s‚Ä≤o . However, the values of ¬µ1|sare usually unknown and estimating them from the data for all the values of s‚ààAcan be challenging as few instances of a particular combination of attributes smay be present in the dataset D. Moreover, as previously mentioned, minority subgroups may be even more severely underrepresented in the dataset compared to their true representation in the general population, making the problem even harder. For example, the Adult dataset‚Äôs training set contains 32,000 individuals, of which over 85% are white people. This leaves only hundreds of people from the smallest minority groups, who might also have low rates of high income. Splitting the dataset by additional sensitive attributes will produce subgroups consisting of very few high earners, if any. Our methods recognize that subgroups with fewer individuals produce noisier estimates and quantify this uncertainty. 3.2.1 Smoothed Empirical Estimation. A simple approach is to directly estimate ¬µ1|sfrom the data, as proposed by Foulds et al . [15] . In particular, we set ÀÜ¬µ1|s=N1,s+Œ± Ns+Œ±+Œ≤, (2) where N1,sis the empirical count of occurrences of individuals with sensitive attributes sand positive outcome in the dataset D, while Nsis the total number of individuals with attributes s. We introduce smoothing parameters Œ±,Œ≤asNsorN1,smay be small due to data sparsity. Note that Equation (2)represents the expected posterior value of a Beta-Binomial model with prior parametersŒ±,Œ≤. The final estimate of œµis: ÀÜœµI R:=log max s,s‚Ä≤‚ààAÀÜ¬µ1|s ÀÜ¬µ1|s‚Ä≤ =logmax s‚ààAÀÜ¬µ1|s mins‚Ä≤‚ààAÀÜ¬µ1|s‚Ä≤ . This estimation procedure requires computing ÀÜ¬µ1|sfor all possible combinations of attributes s‚ààA, leading to O(|A|)computational complexity. In general, it can be hard to tune the parameters Œ±and Œ≤properly as large values of either Œ±orŒ≤will introduce additional bias, while small values of Œ≤will not solve the data sparsity problem. Therefore, this procedure is not robust; ÀÜœµI Rwill generally be biased and no uncertainty quantification can be provided. Nevertheless we prove in Proposition 3.2 that, as the dataset size grows, the smoothed empirical estimator converges to the true value regardless of the chosen smoothing parameters. Although the result holds for Œ±,Œ≤‚ààR, in practice one would choose them to be non-negative, and set them both to zero when no smoothing is desired. Proposition 3.2. The smoothed empirical estimate of œµfor any œµ-differential fairness metric is consistent for all Œ±,Œ≤‚ààR. 3.2.2 Bootstrap Estimation. We propose a bootstrap estimation procedure to provide confidence intervals for the estimate ÀÜœµI R. We generate Bdifferent datasets by sampling with replacement nobservations from the original dataset D. For each bootstrap sample, we obtain an estimate ÀÜœµ(b) I R,b=1, . . . , Bas in Equation (2). The final estimate ÀÜœµI Ris obtained by averaging over the samples and empirical confidence intervals can be easily constructed. The computational complexity is O(B|A|), but in practice we also observe a computational overhead due to the construction of the Bdatasets. Notice that some of the generated datasets may not contain instances of specific attributes s‚ààA, producing undefined values if the smoothing parameters Œ±,Œ≤are set to zero. 3.2.3 Bayesian Estimation. Motivated by the form of Equation (2), we propose a Bayesian approach by considering the likelihood N1,s|¬µ1|s‚àºBinom(Ns,¬µ1|s)and setting its conjugate prior ¬µ1|s‚àº Beta(Œ±,Œ≤). The posterior is therefore tractable and given by ¬µ1|s|N1,s‚àºBeta(Œ±+N1,s,Œ≤+Ns‚àíN1,s). We use Monte Carlo simulation techniques to get an estimate of œµI R. In particular, we simulate mvalues of¬µ1|sfrom the posterior and use them to compute the estimate of œµI Ras in Equation (1), with a computational complexity of O(m|A|). Averaging the soconstructed sample gives the final estimate of œµI R. Moreover, this procedure promptly provides credible intervals. Finally, we note that the simulated values of ¬µ1|swill always be greater than zero, so that we do not need to resort to any further smoothing. Prior parameters Œ±,Œ≤can be chosen using domain knowledge or set close to zero to suggest no prior information. It follows from Proposition 3.3 that this estimator is also consistent. Proposition 3.3. The Bayesian estimate of œµfor anyœµ-differential fairness metric is consistent ‚àÄŒ±,Œ≤>0. 4 POST-PROCESSING OF CLASSIFIER MODEL We defined in Section 3 different metrics for assessing intersectional fairness of model outputs. In this section, we present postprocessing methods to mitigate any detected intersectional bias in a classification model. 4
We argue that when possible, the best way to ensure fairness is to collect more representative data and retrain the model. Nevertheless, it is commonly the case that only historical data ‚Äî where conscious or unconscious bias is present ‚Äî is available. Training a new classifier may be impractical due to cost and time constraints. Moreover, in practice we often only have access to outputs of a trained classifier, but no knowledge on how such predictions were made ‚Äì either because the model is hard to interpret or because we do not have access to the model itself. This motivates the need to develop post-processing techniques that are model agnostic. Indeed, we make no assumptions on the model training mechanism and only require access to its outputs and sensitive attributes. We will refer to it as a ‚Äúbinary predictor‚Äù if its outputs are 0 and 1 and as a ‚Äúscore predictor‚Äù if its outputs are in [0,1]. We propose a framework to allow the practitioner to make a trade-off between a model‚Äôs accuracy and fairness. Let us return to our running example, but re-interpret it as data for loan applications. A model trained on the Adult dataset without post-processing is likely to have slightly higher overall performance, but one that is driven by the majority subgroup. As the dataset is imbalanced, a model may incorrectly deny loans more often to black women than white men, indicating intersectional bias. Depending on the desired notion of fairness, our proposed post-processing can ensure the model has balanced performance across all subgroups or gives out the same proportion of loans to every subgroup. We construct a derived predictor ÀúYwith improved fairness with respect to one or more chosen metrics. In particular, by combining the approaches of Hardt et al . [16] and Corbett-Davies et al . [8], we propose a class of derived predictors that are able to handle classifiers returning either binary predictions or scores. Section 4.1 presents a general framework for the construction of derived predictors. We explore how to compute them for a binary and score predictor in Sections 4.2 and 4.3, respectively. Crucially, the value of the derived predictor depends only on the given prediction ÀÜY and on the individual‚Äôs combination of sensitive attributes S. Definition 4.1 ([ 16]).Aderived predictor ÀúYis a random variable whose distribution depends solely on a classifier‚Äôs predictions ÀÜY and an intersection of sensitive attributes S. Our aim is to construct a derived predictor that, by transforming predictions of a given classifier, achieves better fairness in terms of one or more œµ-differential fairness metric(s). If the model only returns binary predictions ÀÜY‚àà{0,1}, we can resort to randomization , that is, randomly flipping some of the predictions. On the other hand, when the model returns scores, we can also threshold such scores to retrieve a binary prediction. We combine the two approaches in the following definition: Definition 4.2 (Randomized Thresholding Derived Predictor). Given a classifier returning predictions ÀÜY‚àà[0,1], the Randomized Thresholding Derived Predictor (RTDP) ÀúYis a Bernoulli random variable such that P(ÀúY=1|ÀÜY=ÀÜy,S=s)=Àúp1,sI(ÀÜy‚â•œÑs)+Àúp0,sI(ÀÜy<œÑs) (3) where Iis the indicator function and œÑs,Àúp1,s,Àúp0,s‚àà[0,1], for all s‚ààA, are the tuning parameters.We interpret Equation (3)as follows: given an individual with predicted score ÀÜyand combination of sensitive attributes s, we first construct a binary prediction by thresholding on œÑsand then, with a specific probability, accommodate the possibility to reverse it or keep it. In particular, Àúp0,sis the probability of flipping what would have been a negative prediction, while Àúp1,sis the probability of keeping a positive prediction. Note that Definition 4.2 covers also the case where the model is a binary predictor; we explore this case in more detail in Section 4.2. In consequential applications, randomization may not be desired or permissible due to legal or other requirements. In this case, Definition 4.2 allows us to construct a deterministic derived predictor by setting Àúp1,s=1and Àúp0,s=0for all s‚ààA. 4.1 Formulation as an Optimization Problem We construct the RTDP by solving an optimization problem. In order to assess performance of the post-processed model, we introduce a loss function l(y,Àúy):{0,1}2‚ÜíRthat, given the true and the post-processed outcomes, returns the cost of making such a prediction, following the approach of Hardt et al . [16] . Without loss of generality, we assume l(0,0)=l(1,1)=0, so that making correct predictions does not contribute to the loss. Indeed, if either a bonus or a penalty is desired for correct predictions, it can be incorporated by changing the values of l(0,1)andl(1,0). Therefore, minimizing the expected loss function preserves good predictive performance. Corbett-Davies et al . [8] take a slightly different approach and aim to maximize a utility function, defined as E[YÀúY‚àícÀúY],c‚àà(0,1). An advantage of this approach is that it only requires tuning a constant cthat can be interpreted as the cost of making a positive prediction. We now prove that this approach is a special case of the framework we propose. Proposition 4.3. Maximizing the immediate utility function E[YÀúY‚àícÀúY]for a constant c‚àà(0,1), is equivalent to minimizing E[l(Y,ÀúY)]when setting l(0,1)=cand l(1,0)=1‚àíc. One can control the level of bias in the post-processed model by selecting the desired value of œµfor the chosen (one or more) intersectional metrics of Table 2. We consider two possible approaches to find the unknown parameters œÑs,Àúp0,s,Àúp1,s: (i) minimizing the expected loss subject to the selected fairness metric(s) being satisfied for the chosen œµ, or (ii) adding a penalty term to the expected loss for values of the parameters that do not satisfy the required fairness constraint. The two approaches are in principle equivalent, but their practical implementations may differ as different numerical optimization routines need to be used. For instance, one established fairness guideline is the 80% rule for statistical parity [ 12]; corresponding to requiring œµ-differential fairness for statistical parity to hold for œµ‚â§‚àílog(0.8)(cf. Theorem 3.1). We can either consider this as a constraint in the parameter space of the optimization problem or consider minimizing E[l(Y,ÀúY)]+t¬∑I ‚àÉs,s‚Ä≤‚ààA:P(ÀúY=1|S=s) P(ÀúY=1|S=s‚Ä≤)>0.8 , 5
fortappropriately large. Note that any model-output fairness metric of Table 2 can be considered as a constraint; for instance, in Section 5 we show how to achieve better equalized odds intersectional fairness. We show in Proposition 4.4 that the expected loss can be rewritten as a weighted sum of the False Positive Rate ÀúFPR =P(ÀúY= 1|Y=0)and the False Negative Rate ÀúFNR =P(ÀúY=0|Y=1)of the post-processed model, where the weights depend on ¬µ1=P(Y=1). Proposition 4.4. Minimizing E[l(Y,ÀúY)]is equivalent to minimizing ÀúFPR(1‚àí¬µ1)l(0,1)+ÀúFNR¬µ1l(1,0). (4) 4.2 Post-Processing of a Binary Predictor If the predictor returns solely binary predictions, we set œÑs=1,‚àÄs‚àà Aand tune the probabilities Àúp1,sand Àúp0,sto construct the derived predictor. To find the unknown parameters we minimize the expected loss subject to the required fairness constraint; Proposition 4.5 shows that this optimization problem can be efficiently solved via linear programming. Proposition 4.5. Minimizing E[l(Y,ÀúY)]in Equation (4)in the variables Àúp1,s,Àúp0,s, subject to the constraints that œÑs=1,‚àÄs‚ààAand that any of the œµ-differential fairness model-output metrics (cf. Table 2) is below a user-defined threshold, is a linear programming problem. We conclude that in the case of a binary predictor, an RTDP can be computed in polynomial time [ 23]. The unknown constant base rates¬µs,¬µ1|sand model metrics FPR ,FNR can be estimated from the data via any of the techniques introduced in Section 3. 4.3 Post-Processing of a Score Predictor We now focus on the more generic setting where the model outputs are in the form of scores ÀÜY‚àà[0,1], where high scores indicate high probability of a positive outcome. We assume no further knowledge on how these scores were computed, and treat the underlying model as a black box. To construct the RTDP we can optimize both the probabilities Àúp1,s,Àúp0,sand the thresholds œÑsfor all s‚ààA, corresponding to a total of 3|A|parameters to optimize. Although we do not observe overfitting in our experiments (cf. Section 5), in other applications it may be necessary to use cross-validation or to add regularization terms to reduce the degrees of freedom (e.g., imposing œÑs=œÑs‚Ä≤for some s,s‚Ä≤‚ààA). We explore in detail the ‚Äúdeterministic‚Äù scenario in Section 4.3.1. The case where both the thresholds and the probabilities are optimized is discussed in Section 4.3.2. 4.3.1 Deterministic Post-Processing. If no randomization is desired, we construct an RTDP fixing Àúp1,s=1and Àúp0,s=0,‚àÄs‚ààA. This case is of particular interest as randomization may be undesirable in real-world applications, for instance when assessing judicial decisions [ 3]. We carefully tune the thresholds œÑs, as they drive the predictive performance of the post-processed model. Figure 1 illustrates the constrained optimization routine, where for explanatory purposes we only consider 3 intersections of sensitive attributes. The model performance differs across the 3 intersectional subgroups; this is apparent from the ROC curves for each subgroup. Note that a value of œÑsuniquely determines a point oneach curve. The chosen level of œµ-differential fairness determines a region around each ROC curve where the other ROC curves must also lie. Therefore, the optimal thresholds must be in an intersection of compact spaces in [0,1]. In practice, only a few points on each ROC curve are observed and the optimum can then be found by exhaustive search. Alternatively, ROC curves may be estimated from the data. Note that if the œµ-differential fairness constraints are too strict, the only admissible solution may be to always return only positive or negative predictions. 4.3.2 Post-Processing Using Randomization. We now focus on constructing an RTDP by finding both the optimal thresholds œÑsand probabilities Àúp1,s,Àúp0,s. We first investigate whether applying randomization deteriorates model performance. Intuitively this should be the case if the given model performs reasonably well for every intersection of attributes. This is formalized in Proposition 4.6, where we show that the randomization can improve predictive accuracy only if the model performance metrics are within certain bounds. Proposition 4.6. Given a score predictor ÀÜY‚àà[0,1], solving minœÑs,Àúp0,s,Àúp1,sE[l(Y,ÀúY)], where ÀúYis the RTDP of Definition 4.1, is equivalent to setting Àúp1,s= 1,Àúp0,s=0,‚àÄs‚ààAand solving minœÑsE[l(Y,ÀúY)], if and only if ÀúT NR s ÀúFNR s>¬µ1|s 1‚àí¬µ1|sl(1,0) l(0,1),ÀúTPR s ÀúFPR s>1‚àí¬µ1|s ¬µ1|sl(0,1) l(1,0),‚àÄs‚ààA.(5) Even when randomization worsens predictive performance, it may still improve intersectional fairness. To find the optimal thresholdsœÑsand probabilities Àúp1,s,Àúp0,s, we first consider a simple approach that we name ‚Äúsequential post-processing‚Äù. Here we first find optimal thresholds œÑswhen no fairness constraints are imposed. By applying such thresholds, we convert the scores ÀÜYto binary predictions, so that we can find optimal probabilities Àúp1,s,Àúp0,sthat achieve the desired fairness constraints via linear programming (cf. 0.0 0.2 0.4 0.6 0.8 1.0 (Y=1|Y=0,S) 0.00.20.40.60.81.0(Y=1|Y=1,S) s se se ROC curve and admissible region (1st intersection) ROC curve and admissible region (2nd intersection) ROC curve and admissible region (3rd intersection) Figure 1: Example of deterministic post-processing for equal opportunity for 3 intersections of sensitive attributes. The selected level of œµdetermines the admissible regions. 6
Table 3: Overview of the proposed optimization approaches for post-processing using RTDP (Definition 4.2). Scenario Method Existance Only binary outcomes ÀÜyi(thresholding not possible)Optimize RTDP with randomization only, i.e., choosing Àúp1,s,Àúp0,sby LP using Proposition 4.5Guaranteed to minimise Equation (4)for values of the fairness constraint Randomization not appropriate (e.g., for regulatory reasons)Optimize RTDP deterministically by choosing thresholds œÑsAdmissible region may be trivial solutions œÑs‚àà{0,1} only if the fairness constraints are too strict Randomization and thresholding (sequential approach)Optimize RTDP by first selecting thresholds without fairness constraints then choosing Àúp1,s,Àúp0,sby LP using Proposition 4.5Guaranteed to find a solution for a given fairness constraint, but no guarantee to return global optimum Randomization and thresholding (overall approach)Optimize RTDP jointly for thresholds œÑsand randomly flipping probabilities Àúp1,s,Àúp0,sGuaranteed to find a solution for a given fairness constraint, but no guarantee to return global optimum Proposition 4.5). While this procedure may return an acceptable result for the case at hand, there is no guarantee it will return the global optimum. A different approach, which we will refer to as ‚Äúoverall postprocessing‚Äù, is to solve the following optimization problem: minœÑsf(œÑs),s.t.œÑs‚àà[0,1],‚àÄs‚ààA, (6) where f(œÑs)is the optimal cost function value found by solving the optimization problem only in the variables Àúp1,s,Àúp0,s, for a fixed œÑs(cf. Section 4.2). Although this may seem as adding an extra layer of complexity, we note that values of f(œÑs)can be efficiently computed via linear programming. In general, since the model metrics are estimated from a finite dataset, f(œÑs)is a piecewise constant function. Therefore, gradient-based optimization routines are unlikely to succeed as the gradient of the objective function ‚Äì if defined ‚Äì will be zero at all points. We discuss in the supplementary material the details of the optimizer we use and discuss other viable approaches in the conclusion. We summarize all approaches in Table 3. 5 EXPERIMENTS We perform the following experiments to comprehensively evaluate our methods for auditing and achieving intersectional fairness: in Section 5.1, we apply the techniques of Section 3.2 to estimate the level of intersectional fairness of a synthetic dataset purposefully constructed so that one subgroup is underrepresented ‚Äì a common scenario in practice due to societal and data collection biases. In Section 5.2, we estimate the level of intersectional fairness of a trained classifier and then mitigate the detected intersectional bias using our post-processing techniques of Section 4. Here we consider intersectional fairness for 3 sensitive attributes. 5.1 Underrepresented Subgroup The synthetic dataset contains two sensitive attributes: one binary and one with 3 possible values. Out of the 6 intersectional subgroups, one (denoted s1) is sparse: corresponding to 5%of the dataset. Details of the dataset generation mechanism are in the supplementary material. For concreteness, we focus on intersectional fairness for impact ratio , where the true value of œµI Ris known and equal to log 0.95 0.05 ‚âà2.94. First, we show in Figure 2 how the estimates behave as the size of the dataset increases and analyze the confidence intervals(where applicable). Consistent with the theoretical guarantees of Propositions 3.2 and 3.3, all methods converge to the true value as the dataset size grows. Furthermore for smaller dataset sizes, the confidence intervals provided by the bootstrap method are generally wider than the ones obtained via a Bayesian approach. This is not surprising as the estimate of œµI Ris particularly unstable if any instances of subgroup s1are not replicated in one of the bootstrapped datasets; in this case, it is driven by our chosen values of smoothing parameters. Second, we approximate the Mean Squared Error (MSE) of our three estimators. As shown in Figure 3, the Bayesian estimate performs better for all considered dataset sizes. For small dataset sizes, bootstrap estimate performs slightly worse than the empirical estimate, illustrating that one can get biased estimates of œµI Rif one intersectional subgroup (e.g., s1in our experiment) is poorly represented in a bootstrapped dataset. Overall, we observe that the smoothed empirical estimator requires considerably less computational effort than the other two methods, however unlike bootstrap or Bayesian estimates it does not provide any insight into how reliable the estimate is. Moreover, Bayesian estimation is in general faster than bootstrap, as the posterior parameters need to be computed only once and no computational overhead is observed. 5.2 Adult Income Prediction We return to our running example, focusing on three sensitive attributes: gender, age, and race. We treat age (binned) and gender as binary sensitive attributes, and race as having five values. We treat the model as a black box. Details of the experiment configuration are in the supplementary material. First, we audit intersectional fairness on the dataset and the model outputs. We then compare performances of the different post-processing techniques. Auditing intersectional fairness Figure 4 shows the minimum values of œµsuch thatœµ-differential fairness is satisfied for different intersectional metrics on the data and the classifier outputs. The results indicate unfairness across all the different metrics, with œµ-differential fairness for FPR parity being the worst ( œµ‚âà8.14). Note that confidence intervals for the Bayesian procedure are generally wider: this is due to the model performing poorly for some subgroups, leading to a high variance in the estimates for œµ. Achieving intersectional fairness 7
Table 4: Predictive performance of given binary predictor and post-processed models on the Adult training set with gender, age, and race as sensitive attributes. No fairness constraints With fairness constraint œµ‚â§8.14‚àílog(400)‚âà2.15 Given binary predictorOptimal score modelRandomization onlyDeterministic Sequential Overall TPR 0.5450 0.5481 0.5434 0.5995 0.5465 0.5376 FPR 0.0422 0.0427 0.0426 0.0759 0.0425 0.0400 Expected loss function 0.1416 0.1412 0.1423 0.1540 0.1415 0.1417 0 500 1000 1500 2000 2500 Dataset size12345678Value of IR Empirical Bootstrap Bayesian True value Figure 2: Comparison of different estimators of intersectional impact ratio on synthetic datasets of increasing size. Vertical bars represent 95% confidence intervals for bootstrap and Bayesian estimation where 1,000 bootstrapped datasets and Monte Carlo samples have been drawn, respectively. 0 500 1000 1500 2000 2500 Dataset size02468Estimated MSE of IR Empirical Bootstrap Bayesian Figure 3: Comparison of the estimator‚Äôs MSE on synthetic datasets of increasing size. MSE has been estimated by generating 1,000 different datasets with equal base rates. We now focus on mitigating the detected intersectional bias. We first consider the scenario where we only have access to binary Elift Impact Ratio Stat. Parity TPR Parity FPR Parity Fairness metric024681012141618Estimated value of  Empirical Bootstrap BayesianFigure 4: Estimates of œµ-differential fairness for both data and model outputs metrics on the Adult training set when gender, age, and race are considered as sensitive attributes. Vertical bars represent 95% confidence intervals. predictions. As we assume no further knowledge of the underlying model, the only possible post-processing technique is randomization (cf. Section 4.2). We focus on improving the equalized odds intersectional fairness metric, and set the ambitious aim of reducing bias amplification by a multiplicative factor of 400. This amounts to reaching an œµ-differential fairness for FPR parity equal to8.14‚àílog(400)‚âà2.15. As we do not want to deteriorate the TPR parity score, we impose as a constraint œµ-differential fairness for equalized odds of less than 2.15. We calculate optimal probabilities of changing the predictions here; we refer to this model as ‚Äúrandomization only‚Äù. Next, we consider the scenario where prediction scores are available. The RTDP that achieves the best predictive performance is obtained when no fairness constraints are imposed (cf. Section 4.3.1). This model, henceforth referred to as the ‚Äúoptimal score model‚Äù, represents our baseline for assessing whether imposing fairness constraints deteriorates predictive performance significantly. As before, we aim to achieve the level of œµ-differential fairness for equalized odds of œµ‚â§2.15. Having access to the scores, we construct the following three post-processed models: ‚Ä¢‚ÄúDeterministic‚Äù post-processing, where we optimize the thresholds only; ‚Ä¢‚ÄúSequential‚Äù post-processing, where we consider the optimal score model and apply randomization on top; 8
Original binary predictorOptimal score modelRandom. onlyDeterministic Sequential Overall Model or post-processing technique024681012141618Estimated value of  Empirical Bootstrap Bayesian ConstraintFigure 5: Estimate of œµ-differential fairness for equalized odds for the original and the post-processed models, using the Adult training set when gender, age, and race are considered as sensitive attributes. The constraint is set to œµ‚âà2.15. ‚Ä¢‚ÄúOverall‚Äù post-processing, where we simultaneously optimize the thresholds and probabilities. Figure 5 shows the level of œµ-differential fairness for equalized odds achieved by the different post-processing techniques. We note that all the post-processed models achieve the desired fairness constraint according to the smoothed empirical estimator. The required value is also contained in the 95% confidence intervals produced by the bootstrap and the Bayesian estimators. Table 4 reports models‚Äô predictive performances. Note that there is almost no loss in performance when only randomization is used on top of the given binary predictor. Indeed, in the case of our experiment we found that the model performance was better after randomization for a small, underrepresented intersection; the model produced incorrect predictions more often than correct ones. This illustrates the utility of the post-processed model for assessing quality of the original model. The optimal score model, while having the best predictive performance, does not reach the desired fairness constraint. On the other hand, the ‚Äúdeterministic‚Äù post-processed model reaches the fairness constraint but the expected loss is significantly greater than that of other models. We observe that ‚Äúsequential‚Äù and ‚Äúoverall‚Äù post-processed models perform very similarly and close to the ‚Äúoptimal score model‚Äù. 6 CONCLUSION AND FUTURE WORK Intersectional fairness is crucial for safe deployment of modern machine learning systems, yet most of the algorithmic fairness literature has thus far focused on fairness with respect to a single sensitive attribute. We present a comprehensive framework for auditing and achieving intersectional fairness, i.e., fairness when intersections of multiple sensitive attributes are considered. First, we propose metrics to assess intersectional fairness in the data and the model outputs. Second, we propose 3 methods to robustly estimate these metrics: smoothed empirical, bootstrap, and Bayesian estimation. Using these methods, we can assess confidence in theestimates and rapidly evaluate which subgroups are misrepresented in the data or discriminated by the model. Third, we propose postprocessing techniques that transform the output of a given binary classifier so as to achieve intersectional fairness with respect to the chosen metric. We implemented the proposed auditing and post-processing methods on the Adult dataset. There are many remaining open problems in this area, including defining other intersectional fairness metrics, e.g., for calibration, and further refining estimation procedures thereof, e.g., by weighting the bootstrap samples, differently tuning the prior parameters of the Bayesian estimators, or taking a hierarchical approach as in [14]. Our post-processing techniques can be further improved by introducing a regularization term to avoid overfitting, smoothing the cost functions or by modifying the optimization procedure itself. Although we focused on post processing, research on preand in-processing techniques that achieve intersectional fairness can also be carried out. Another direction for future work is to develop post-processing techniques for regression and categorical classification problems. ACKNOWLEDGEMENTS We thank Imran Ahmed, Anil Choudhary, Philip Pilgerstorfer and Stavros Tsalides for helpful comments and discussions. We would also like to thank anonymous referees for their valuable feedback, which helped us to improve the paper. REFERENCES [1]Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. 2018. A Reductions Approach to Fair Classification. In Proceedings of the 35th International Conference on Machine Learning . [2]Charles Anawo Ameh and Nynke Van Den Broek. 2008. Increased risk of maternal death among ethnic minority women in the UK. The Obstetrician & Gynaecologist 10, 3 (2008), 177‚Äì182. [3]Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine Bias: there‚Äôs software used across the country to predict future criminals. And it‚Äôs biased against blacks. https://www.propublica.org/article/machine-bias-riskassessments-in-criminal-sentencing/. ProPublica. [4]Joy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency . [5]Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. 1995. A Limited Memory Algorithm for Bound Constrained Optimization. SIAM Journal on Scientific Computing 16, 5 (1995), 1190‚Äì1208. [6]√Ångel Alexander Cabrera, Will Epperson, Fred Hohman, Minsuk Kahng, Jamie Morgenstern, and Duen Horng Chau. 2019. FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning. arXiv preprint arXiv:1904.05419 (2019). [7]Yeounoh Chung, Tim Kraska, Neoklis Polyzotis, Kihyun Tae, and Steven Euijong Whang. 2019. Automated Data Slicing for Model Validation: A Big data - AI Integration Approach. IEEE Transactions on Knowledge and Data Engineering PP (05 2019), 1‚Äì1. [8]Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. Algorithmic Decision Making and the Cost of Fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . [9]Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi, and Richard Zemel. 2019. Flexibly Fair Representation Learning by Disentanglement. In Proceedings of the 36th International Conference on Machine Learning . [10] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http: //archive.ics.uci.edu/ml [11] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness Through Awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference . [12] Equal Employment Opportunity Commission, U.S. Civil Service Commission, U.S. Department of Labor and U.S. Department of Justice. 1978. Uniform Guidelines on Employee Selection Procedure. In 43 FR 38295 . Federal Register. 9
[13] John Forrest, Ted Ralphs, Stefan Vigerske, LouHafer, Bjarni Kristjansson, jpfasano, EdwinStraver, Miles Lubin, Haroldo Gambini Santos, rlougee, and Matthew Saltzman. 2018. coin-or/Cbc: Version 2.9.9. https://doi.org/10.5281/zenodo. 1317566 [14] James Foulds, Rashidul Islam, Kamrun Keya, and Shimei Pan. 2018. Bayesian Modeling of Intersectional Fairness: The Variance of Bias. arXiv preprint arXiv:1811.07255 (2018). [15] James Foulds, Rashidul Islam, Kamrun Naher Keya, and Shimei Pan. 2018. An Intersectional Definition of Fairness. arXiv preprint arXiv:1807.08362 (2018). [16] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of Opportunity in Supervised Learning. In Advances in Neural Information Processing Systems . [17] Robert David Hart. July 2017. If you‚Äôre not a white male, artificial intelligence‚Äôs use in healthcare could be dangerous. Quartz (July 2017). [18] Tim Head, MechCoder, Gilles Louppe, Iaroslav Shcherbatyi, Fcharras, Z√© Vin√≠cius, Cmmalone, Christopher Schr√∂der, Nel215, Nuno Campos, Todd Young, Stefano Cereda, Thomas Fan, Rene-Rex, Kejia (KJ) Shi, Justus Schwabedal, Carlosdanielcsantos, Hvass-Labs, Mikhail Pak, SoManyUsernamesTaken, Fred Callaway, Lo√É≈ïc Est√Éƒ∫ve, Lilian Besson, Mehdi Cherti, Karlson Pfannschmidt, Fabian Linzberger, Christophe Cauet, Anna Gut, Andreas Mueller, and Alexander Fabisch. 2018. Scikit-Optimize/Scikit-Optimize: V0.5.2. https://doi.org/10.5281/zenodo.1207017 [19] Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. 2018. Multicalibration: Calibration for the (Computationally-Identifiable) Masses. In Proceedings of the 35th International Conference on Machine Learning . [20] Minna J. Kotkin. 2008. Diversity and Discrimination: A Look at Complex Bias. William and Mary Law Rev. 50 (04 2008). [21] Faisal Kamiran and Toon Calders. 2012. Data Preprocessing Techniques for Classification Without Discrimination. Knowledge and Information Systems 33, 1 (2012), 1‚Äì33. [22] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012. Fairness-Aware Classifier with Prejudice Remover Regularizer. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases . Springer, 35‚Äì50. [23] Narendra Karmarkar. 1984. A new polynomial-time algorithm for linear programming. In Proceedings of the sixteenth annual ACM symposium on Theory of computing . 302‚Äì311. [24] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2018. Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness. In Proceedings of the 35th International Conference on Machine Learning . [25] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2019. An Empirical Study of Rich Subgroup Fairness for Machine Learning. In Proceedings of the Conference on Fairness, Accountability, and Transparency . [26] Michael P. Kim, Amirata Ghorbani, and James Zou. 2019. Multiaccuracy: BlackBox Post-Processing for Fairness in Classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society . [27] Jon Kleinberg. 2018. Inherent Trade-Offs in Algorithmic Fairness. In Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems . [28] Dieter Kraft. 1988. A Software Package for Sequential Quadratic Programming. (1988). [29] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Eric Horvitz. 2017. Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence . [30] Edward B. Manoukian. 1986. Modern Concepts and Theorems of Mathematical Statistics . [31] Arvind Narayanan. 2018. Translation tutorial: 21 fairness definitions and their politics. In Conference on Fairness, Accountability, and Transparency . [32] Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. 2008. Discrimination-aware Data Mining. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . [33] Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q. Weinberger. 2017. On Fairness and Calibration. In Proceedings of the 31st International Conference on Neural Information Processing Systems . [34] Blake Woodworth, Suriya Gunasekar, Mesrob I. Ohannessian, and Nathan Srebro. 2017. Learning Non-Discriminatory Predictors. In Proceedings of the 2017 Conference on Learning Theory . [35] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. 2018. Mitigating Unwanted Biases with Adversarial Learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society . [36] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints. arXiv preprint arXiv:1707.09457 (2017). 10
We provide proofs and experiment configuration in this appendix. A PROOFS OF SECTION 3 Proof of Theorem 3.1. Theorem VIII.1 of Foulds et al . [15] proves the result in the case of œµ-differential fairness for statistical parity. Their proof is based on the following reformulation of the original definition (Lemma VIII.1, [15]): log max s‚ààAÀÜ¬µ1|s ‚àílog min s‚ààAÀÜ¬µ1|s ‚â§œµ, and on proving that log max s‚ààAÀÜ¬µ1|s ‚â•log max s‚ààA‚Ä≤ÀÜ¬µ1|s , (7) log min s‚ààAÀÜ¬µ1|s ‚â§log min s‚ààA‚Ä≤ÀÜ¬µ1|s . An analogous reformulation holds for the definitions of œµ-differential fairness for impact ratio, TPR parity, and FPR parity. Therefore, the desired result holds for these metrics by reproducing the proof of Theorem VIII.1 of Foulds et al. [15]. The definition of œµ-differential fairness for the elift metric can be reformulated as: log max s‚ààAÀÜ¬µ1|s ‚àílog(¬µ1)‚â§œµ,and so from Equation (7) it follows that log max s‚ààA‚Ä≤ÀÜ¬µ1|s ‚àílog(¬µ1)‚â§log max s‚ààAÀÜ¬µ1|s ‚àílog(¬µ1)‚â§œµ, as desired. ‚ñ° Proof of Proposition 3.2. We prove the result for impact ratio, but similar reasoning can be applied to prove consistency for all the œµ-differential fairness metrics introduced in Tables 1 and 2. Assume we have access to a dataset containing nobservations; we make the dependency on nexplicit by using superscript n. We will prove that ÀÜœµn I Rconverges in probability to œµI R, as defined in Equation (1). Recall that N1,sdenotes the number of occurrences in the dataset of individuals with attributes sand positive outcome, while Nsis the number of individuals with attribute s. Define the following estimators of ¬µ1,s:=P(Y=1,S=s)and¬µs:=P(S=s): ÀÜ¬µn 1,s=N1,s n,ÀÜ¬µn s=Ns n, respectively. The two estimators are consistent by the Strong Law of Large Numbers. We can now apply Slutsky‚Äôs theorem [ 30, p. 76] and show: ÀÜ¬µn 1|s=N1,s+Œ± Ns+Œ±+Œ≤=ÀÜ¬µn 1,s+Œ± n ÀÜ¬µns+Œ±+Œ≤ np‚Üí¬µ1,s ¬µs=¬µ1|s, assuming¬µ1|s>0,‚àÄs‚ààA. By Slutsky‚Äôs theorem, it follows: ÀÜ¬µn 1|s ÀÜ¬µn 1|s‚Ä≤p‚Üí¬µ1|s‚Ä≤ ¬µ1|s‚Ä≤. Finally, by the Continuous Mapping Theorem, we conclude that ÀÜœµn I Ris a consistent estimator of œµI R. ‚ñ° Proof of Proposition 3.3. The expected value of the posterior distribution is given by Equation (2), and the variance is o 1 n . Therefore, as n‚Üí‚àû the posterior distribution converges to aDirac delta concentrated on ÀÜ¬µ1|s. In the proof of Proposition 3.2 we showed that ÀÜ¬µ1|sconverges in probability to ¬µ1|s. The Central Limit Theorem now implies that the Monte Carlo procedure yields consistent estimates. ‚ñ° B PROOFS OF SECTION 4 Proof of Proposition 4.3. Consider E[YÀúY‚àícÀúY]=P(Y=1,ÀúY=1)‚àícP(ÀúY=1) =P(ÀúY=1|Y=1)P(Y=1) ‚àíc P(ÀúY=1|Y=0)P(Y=0) +P(ÀúY=1|Y=1)P(Y=1) =ÀúTPR¬µ1‚àícÀúFPR(1‚àí¬µ1)‚àícÀúTPR¬µ1 =(1‚àíc)¬µ1(1‚àíÀúFNR)‚àíc(1‚àí¬µ1)ÀúFPR . Therefore by Proposition 4.4: maxE[YÀúY‚àícÀúY]=minc(1‚àí¬µ1)ÀúFPR+(c‚àí1)¬µ1(1‚àíÀúFNR) =minc(1‚àí¬µ1)ÀúFPR+(1‚àíc)¬µ1ÀúFNR =minE[l(Y,ÀúY)] where l(0,1)=candl(1,0)=1‚àíc. ‚ñ° Proof of Proposition 4.4. Recall that we assumed w.l.o.g. that l(0,0)=l(1,1)=0. This implies that E[l(Y,ÀúY)]=P(Y=0,ÀúY=1)l(0,1)+P(Y=1,ÀúY=0)l(1,0) =P(ÀúY=1|Y=0)P(Y=0)l(0,1) +P(ÀúY=0|Y=1)P(Y=1)l(1,0) =ÀúFPR(1‚àí¬µ1)l(0,1)+ÀúFNR¬µ1l(1,0). It follows that minE[l(Y,ÀúY)]=min{ÀúFPR(1‚àí¬µ1)l(0,1)+ÀúFNR¬µ1l(1,0)}, as desired. ‚ñ° Proof of Proposition 4.5. Denote the FPR for individuals with attribute sof the given model as ÀÜFPR s:=P(ÀÜY=1|Y=0,S=s) and the FNR as ÀÜFNR s:=P(ÀÜY=0|Y=1,S=s). It follows that ÀúFPR s=Àúp0,s(1‚àíÀÜFPR s)+Àúp1,sÀÜFPR s, ÀúFNR s=(1‚àíÀúp0,s)ÀÜFNR s+(1‚àíÀúp1,s)(1‚àíÀÜFNR s). Therefore ÀúFPR(1‚àí¬µ1)l(0,1)+ÀúFNR¬µ1l(1,0)is a linear combination of the variables Àúp0,sand Àúp1,s. By Proposition 4.4, minimizing Equation (4)is equivalent to minimizing E[l(Y,ÀúY)]. Therefore, the objective function is indeed linear. All that remains now is to show that the optimization constraints are also linear. Consider for instance using statistical parity as the fairness constraint, that is e‚àíœµ‚â§P(ÀúY=1|S=s) P(ÀúY=1|S=s‚Ä≤)‚â§eœµfor all s,s‚Ä≤‚ààA. By the law of total probability, it follows that: P(ÀúY=1|S=s)=ÀúFPR s(1‚àí¬µ1|s)+(1‚àíÀúFNR s)¬µ1|s, and we have already shown that ÀúFPR sand ÀúFNR sare linear in the variables to be optimized. The same conclusion holds when equal opportunity or FPR parity are considered as constraints, and therefore also for equalized odds. Indeed, we can require (as our 11
fairness constraint) multiple œµ-differential fairness definitions to hold simultaneously, each one for a possibly different value of œµ.‚ñ° Proof of Proposition 4.6. Following the same steps as in the proof of Proposition 4.4, we first notice that the expected loss function marginalizes as: E[l(Y,ÀúY)]=√ï s‚ààA P(ÀúY=1|Y=0,S=s)¬µs(1‚àí¬µ1|s)l(0,1) +P(ÀúY=0|Y=1,S=s)¬µs¬µ1|sl(1,0) =√ï s‚ààA¬µsÀúFPR s(1‚àí¬µ1|s)l(0,1)+ÀúFNR s¬µ1|sl(1,0) ,(8) so that it suffices to prove the result when solving min œÑs,Àúp0,s,Àúp1,sÀúFPR s(1‚àí¬µ1|s)l(0,1)+ÀúFNR s¬µ1|sl(1,0)	 , for an arbitrary s‚ààA. For brevity we denote: FPR‚ãÜ s=P(ÀÜY‚â•œÑs|Y=0,S=s),T NR‚ãÜ s=1‚àíFPR‚ãÜ s, FNR‚ãÜ s=P(ÀÜY<œÑs|Y=1,S=s),TPR‚ãÜ s=1‚àíFNR‚ãÜ s, so that ÀúFPR s=T NR‚ãÜ sÀúp0,s+FPR‚ãÜ sÀúp1,s, ÀúFNR s=FNR‚ãÜ s(1‚àíÀúp0,s)+TPR‚ãÜ s(1‚àíÀúp1,s), where, although not explicitly stated, ÀúFPR sand ÀúFNR sare functions of the variables œÑs,Àúp1,s,Àúp0,s. Therefore: min œÑs,Àúp0,s,Àúp1,sÀúFPR s(1‚àí¬µ1|s)l(0,1)+ÀúFNR s¬µ1|sl(1,0)	 = min œÑs,Àúp0,s,Àúp1,s [T NR‚ãÜ sÀúp0,s+FPR‚ãÜ sÀúp1,s](1‚àí¬µ1|s)l(0,1) +[FNR‚ãÜ s(1‚àíÀúp0,s)+TPR‚ãÜ s(1‚àíÀúp1,s)]¬µ1|sl(1,0)	 = min œÑs,Àúp0,s,Àúp1,sÀúp1,s[FPR‚ãÜ s(1‚àí¬µ1|s)l(0,1)‚àíTPR‚ãÜ s¬µ1|sl(1,0)] +Àúp0,s[T NR‚ãÜ s(1‚àí¬µ1|s)l(0,1)‚àíFNR‚ãÜ s¬µ1|sl(1,0)] +TPR‚ãÜ s¬µ1|sl(1,0)+FNR‚ãÜ s¬µ1|sl(1,0)	 . Under the assumptions of Equation (5), it follows: FPR‚ãÜ s(1‚àí¬µ1|s)l(0,1)‚àíTPR‚ãÜ s¬µ1|sl(1,0)<0, T NR‚ãÜ s(1‚àí¬µ1|s)l(0,1)‚àíFNR‚ãÜ s¬µ1|sl(1,0)>0, so that to minimize the desired quantity, we must set Àúp1,s=1and Àúp0,s=0as desired. ‚ñ° C CONFIGURATION OF EXPERIMENTS FOR REPRODUCIBILITY We now provide configuration details of our experiments. C.1 Synthetic Dataset (Section 5.1) Dataset Generation. We consider a set A1, consisting of a binary sensitive attribute, and A2, consisting of a different sensitive attribute with 3 possible values. Therefore, the space A=A1√óA2 encompasses 6 intersections of sensitive attributes s1, . . . , s6. We fix true base rates as follows: ¬µs1=0.05,¬µs2=0.55,¬µs3=. . .=¬µs6=0.1, ¬µ1|s1=0.05,¬µ1|s2=0.95,¬µ1|s3=. . .=¬µ1|s6=0.5.(9)The true value of œµI Rcan be exactly computed as log 0.95 0.05 ‚âà2.94. Parameter Configuration. The number of bootstrapped datasets is B=1,000, each of size equal to the original one. The smoothing parameters are Œ±=Œ≤=0.01to avoid divisions by zero. When using Bayesian estimation, we generate m=1,000Monte Carlo samples and consider a non-informative prior Œ±=Œ≤=1 3. To approximate the estimators‚Äô Mean Squared Error (MSE), we generate 1,000 different datasets of increasing size with the same true base rates as in Equation (9). For each dataset, we estimate œµI R using the techniques of Section 3.2. C.2 Adult Income Prediction (Section 5.2) Dataset Preparation. The Adult Income Prediction dataset is publicly available [ 10] and is already split into a training set, consisting of 32,561 observations, and a test set, with 16,281 data points. We removed from the training set individuals originally from the Netherlands, as they are not represented in the test set. We represent age as a binned binary categorical variable indicating which individuals are over 50. Gender is considered as a binary attribute in the Adult dataset. Race is encoded in the dataset into 5 different categories. For the purpose of this experiment, since the dataset contains few instances of categories ‚ÄúEskimos and American Indians‚Äù and ‚ÄúOther‚Äù, we encode them together under the label ‚ÄúOther‚Äù. We also standardized all continuous variables and created dummy variables for the categorical ones. Model. We built a classifier returning scores in [0,1]via Extreme Gradient Boosting1and kept default parameters, except setting 20 boosting iterations and learning_rate = 0.01 . We built a model returning only binary predictions by applying a fixed threshold equal to 0.5. Intersectional Fairness Estimation Parameters. We choose smoothing parameters Œ±=Œ≤=0.01to avoid division by zero when using the empirical and bootstrap estimators. Prior parameters for the Beta distribution are both set to1 3. Post-Processing Parameters and Implementation. We set a loss function that gives equal weights to false positive and false negative predictions; i.e., l(0,1)=l(1,0)=1. We applied different optimization routines, depending on the post-processing method: ‚Ä¢For ‚ÄúRandomization-only‚Äù post-processing: Linear programming using the coin-or branch and cut solver [13], ‚Ä¢For ‚ÄúOverall‚Äù post-processing: Constrained optimization using sequential quadratic programming [28], ‚Ä¢For ‚ÄúDeterministic‚Äù and ‚ÄúSequential‚Äù post-processing: Unconstrained optimization using two different approaches. The first uses the L-BFGS-B algorithm [ 5], which approximates gradient information and therefore we make use of the smoothing technique proposed in the previous paragraph. The second uses a Bayesian optimizer that approximates the objective function with a Gaussian process [ 18], which can thus deal with non-differentiable functions as it does not rely on gradient information. 1Implemented in the XGBoost Python package version 0.81 12
D EXTRA MATERIAL FOR EXPERIMENTS D.1 Adult Income Prediction (Section 5.2) Table 5: Predictive performance of given binary predictor and post-processed models on the Adult test set with gender, age, and race as sensitive attributes. No fairness constraints With fairness constraint œµ‚â§8.14‚àílog(400)‚âà0.157 Given binary predictorOptimal score modelRandomization onlyDeterministic Sequential Overall TPR 0.5216 0.5258 0.5197 0.5785 0.5238 0.5139 FPR 0.0433 0.0451 0.0439 0.0762 0.0452 0.0413 Expected loss function 0.1416 0.1465 0.1470 0.1578 0.1470 0.1464 Table 6: Probabilities of flipping the original predictions for the ‚Äúrandomization-only‚Äù post-processing model, which has been constructed on a binary classifier trained on the Adult training set when gender and age are considered as sensitive attributes. The probability for unreported combinations of sensitive attributes is equal to 0. Model prediction Income‚â§50k Income >50k Female, Age‚â§50, Asian-Pacific Islander0.01 0 Female, Age‚â§50, Black 0.01 0 Female, Age >50, Asian-Pacific Islander0.09 1 Female, Age‚â§50, Black 0.01 0 Female, Age >50, Other 0.07 0 Male, Age‚â§50, Asian-Pacific Islander0 0.01 Female, Age >50, Asian-Pacific Islander0 0.35 Original binary predictorOptimal score modelRandom. onlyDeterministic Sequential Overall Model or post-processing technique024681012141618Estimated value of  Empirical Bootstrap Bayesian Constraint Figure 6: Estimate of œµ-differential fairness for equalized odds across the original and the post-processed models. Results are based on the Adult test set when gender, age, and race are considered as sensitive attributes. The constraint is set at œµ‚â§ 8.14‚àílog(400)‚âà2.15. 13
