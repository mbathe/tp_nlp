1 Contact : Frederico Oliveira da Silva and Kasper Draze wski digital @ beuc.eu BUREAU EUROPÉEN DES UNIONS DE CONSOMMATEURS AISBL | DER EUROPÄISCHE VERBRAUCHERVERBAND Rue d ’ Arlon 80 , B -1040 Brussels • Tel . +32 ( 0 ) 2 743 15 90 • www.twitter.com/beuc • www.beuc.eu EC register for interest representatives : identification number 9505781573 -45 Co-funded by the European Union Ref : BEUC -X-2021-088 - 07/10/2021 REGUL ATING AI TO PROTECT THE CONSUMER Position Paper on the AI Act The Consumer Voice in Europe Why it matters to consumers AI products and services , such as virtual assistants and facial recognition tools , are already changing consumer markets and our societies . It is a technology which carries huge expectations of improving and making consumers ’ lives more convenient . But the use of AI also comes with great risks and has major implications for consumers ’ autonomy and self-determination , their privacy , their capacity to interact with products and services and , ultimately , in the ability to hold bu siness es responsible if some thing goes wrong . The AI Act must provide consumers with the rights and protections they need to be at ease w hen using AI , while also allowing space for innovation and , more broadly , ensuring EU ’ s fundamental rights and values are respected . Summary Regulating AI is crucial to ensure a high level of consumer protection and a fair , safe and sustainable development of our society . BEUC welcomes that the Commission has taken the initiative in this regard . However , the AI Act proposal requires substantial improvements to guarantee consumers have the protections they need and can trust AI to respect their rights and freedoms . This position paper contains a series of recommendations for the Commission ’ s AI Act proposal ( scope , definiti ons , harmonisation , prohibited practices , risk and conformity assessments and other obligations for AI systems , standardisation , enforcement , etc ) . Key BEUC recommendations : The proposal must have a broader scope and impose basic principles and obligations , e.g . on fairness , accountability and transparency , that apply to all AI systems . • The scope of the AI Act should be broadened to adequately regulate AI systems other than those it currently classifi es as ‘ high-risk ’ , such as smart meters , AI -powered connected toys , virtual assistants or AI that organises what people see on social media . • All AI systems employed in the EU , including medium - and low -risk systems , should respect a set of common principles established in the AI Act ( e.g . transpare ncy , fairness , non -discrimination ) . • The existing list of ‘ high -risk ’ applications in Annex III should be expanded to include additional AI applications . For example , AI used to assess insurance premiums and AI payment and debit collection services . The list of forbidden AI practices in Article 5 must be extended and strengthened to include harmful practices which are currently not covered . • Articles 5 ( 1 ) a ) and b ) should include AI practices that manipulate someone in a way that can cause them economic harm . 2 • Articles 5 ( 1 ) a ) and b ) should cover AI practices where its ‘ intended purpose ’ or ‘ reasonably foreseeable misuse ’ can manipulate someone and lead to physical , psychological or economic harm . • Art . 5 ( 1 ) b ) should also apply to AI used to exploit vulnerabilities other than those related to children or of physical ly- or mental ly-disabled people . For example , it should protect consumers made vulnerable through the use of persuasion profiles and personalisation practices ( digital asymmetry ) or through temporary vulnerabilities ( e.g . grief , sorrow , emotional distress ) who are not associated to group of people due to their age , physical or mental disability . • A broad reversal of the burden of proof putting the onus of demonstrating compliance on the entity using the AI system in all disputes involving individuals must be introduced to reinforce the prohibitions of Article 5 . • Regarding social scoring , Article 5 ( 1 ) c ) should introduce a general ban on AI used by both public and private bodies to evaluate the trustworthiness of an individual based on their social behaviour or other personal attributes , such as someone ’ s preferences , emotions , health or intellige nce . • The use of remote biometric identification systems by private entities in public spaces should be banned , without exceptions . • Article 5 of the AI Act should ban the use of emotion recognition AI except in very specific circumstances related to healt h or research purposes , in line with the recommendations of the EDPB and EDPS . • The AI Act should prohibit the use of AI for which the scientific validity is unproven or the claimed benefits have been debunked by science . Consumers must have a strong set of rights and access to effective remedies and redress mechanisms in case of harm , including collective redress . • Consumers should have the right to be given a clear explanation about how an AI system affect ing them works , and the right to object to an algorithmic decision that has a significant impact on them . • The proposal must also grant consumers the means to seek justice and redress in case of harm . The AI Act must include : - A right for consumers to complain to a national authority or launch a legal action in court when an AI system or practice which affects them infringes the Regulation . This should include a right to receive compensation for material or non -material damages s uffered . - An obligation for companies to make a complaint mechanism available to consumers . Companies must be obliged to react to those complaints within a short period of time . - An a rticle allowing consumer organisations , or more broadly civil society organisations , to represent individual consumers in the exercise of their rights under this Regulation . They should also be allowed to act in the ‘ general interest ’ ( i.e . be able to bring forward complaints without a mandate from an individual , when they consi der that an AI system or practice is infringing the rules ) . 3 - A provision that adds the AI Act to the Annex of the Representative Action Directive ( RAD ) , which lists the laws where it is possible to file a representative action . It must be possible to file collective redress or injunctive actions in case of non-compliant AI . The conformity assessment procedure applicable to ‘ high -risk AI systems ’ ( Article 43 ) must be strengthened . • Third party assessment should be the rule to assess the conformity of ‘ high -risk AI systems ’ . Self-assessment should only be allowed when AI systems are not considered to be high -risk . • For high -risk AI systems , the results of the conformity assessment pro cedure and all relevant documentation must be notified to the relevant market surveillance authority before the product is placed on the market and made available to the public . Harmonised standards should only be used to define technical requirements , no t to define or apply legal principles and fundamental rights . • Harmonised standards must not be used to define or apply fundamental rights , legal or ethical principles . Their use should be limited to implement technical aspects . In this regard , a standard s hould , for example , not be used to determine what types of biases are prohibited under Art . 10 ( 2 ) f ) . The governance structure and the enforcement mechanisms of the AI Act by national authorities needs to be clarified and improved . • Enforcement should be reinforced on a technical level with the creation of a highly specialised body of technical experts designated by the Commission . Such a body should assist national authorities and the Commission in the technical aspects of their investigations , and have the competence to issue non -binding opinions about specific cases brought up by the national authorities . • To ensure a coherent and consistent application of the AI Act , it is important to specify in Article 65 ( 2 ) that those authorities which start an investigation into a suspicious AI system must inform their counterparts in the other Member States within the AI Board . • The procedure foreseen for solving disagreements between national authorities in Article 66 should not be limited to actions started by Member States ’ authorities . The European Commission should be able to start an evaluation procedure about an AI system under this provision whenever ( i ) it has sufficient reasons to believe that that an AI system presents a risk , ( ii ) no market surveillance authority started an investigation under Article 65 ( 2 ) and ( iii ) the AI system affects consumers in more than one Member State . Contents 1 . Introduction ................................ ................................ ................................ . 2 2 . Objectives and consumer protection gap ................................ ..................... 3 3 . The implications and risks of maximum harmonisation ................................ 3 4 . Main definitions and concepts ( Article 3 ) ................................ ..................... 4 4.1 . AI system ................................ ................................ ................................ . 4 4.2 . User ................................ ................................ ................................ ......... 5 4.3 . Risk ................................ ................................ ................................ ......... 5 4.4 . Harm ................................ ................................ ................................ ....... 6 4.5 . Intended purpose and misuse ................................ ................................ ..... 8 4.6 . Biometric data ................................ ................................ ........................... 9 4.7 . Remote biometric identification systems ................................ ....................... 9 5 . List of prohibited practices ( Article 5 ) ................................ ........................ 10 5.1 . Subliminal techniques causing behavioural distortion and harm – Art . 5 ( 1 ) a ) .10 5.2 . Vulnerability of groups – Art . 5 ( 1 ) b ) ................................ .......................... 12 5.3 . Social scoring – Art . 5 ( 1 ) c ) ................................ ................................ ...... 14 5.4 . Real-time remote biometric identification – Art . 5 ( 1 ) d ) ................................ 15 5.5 . Emotion recognition ................................ ................................ .................. 16 5.6 . Scientifically unproven AI systems ................................ .............................. 16 6 . Scope restriction and risk assessment ................................ ....................... 17 6.1 . Scope restriction ................................ ................................ ....................... 17 6.1.1 . High-risk systems ................................ ................................ ..................... 17 6.1.2 . Updating the list of high -risk systems ................................ .......................... 18 6.1.3 . Systems other than high -risk ................................ ................................ ..... 18 6.2 . Assessment methodology to determine level of risk ................................ ...... 19 7 . Horizontal AI principles and rights for consumers ................................ ..... 20 7.1 . Transparency , explanation , and objection ................................ .................... 20 7.2 . Accountability and control ................................ ................................ .......... 21 7.3 . Fairness ................................ ................................ ................................ .. 21 7.4 . Non-discrimination ................................ ................................ .................... 22 7.5 . Safety and security ................................ ................................ ................... 22 7.6 . Access to justice and right to redress , including collective redres s .................. 23 7.7 . Reliability and robustness ................................ ................................ .......... 24 8 . Conformity assessment procedure ( Art . 43 ) ................................ .............. 24 9 . Standards ( This chapter is co -authored by ANEC and BEUC ) ...................... 25 10 . Enforcement ................................ ................................ ............................... 27 1 10.1 . Reporting ( only ) of serious incidents and of malfunctioning of high -risk systems ( Article 62 ) ................................ ................................ .............................. 27 10.2 . Governance structure and enforcement ( Article 63 ) ................................ ...... 28 10.3 . Access to data and documentation ( Art . 64 ) ................................ ................. 30 10.4 . Procedure for dealing with AI presenting a risk at national level ( Art . 65 ) ........ 30 10.5 . Union Safeguard Procedure ( Article 66 ) ................................ ....................... 32 11 . AI Board ( Articles 56 – 58 ) ................................ ................................ ........ 32 12 . Interplay with other areas of EU laws ................................ ........................ 33 12.1 . General Data Protection Regulation ( GDPR ) ................................ ................. 33 12.2 . Product Liability Directive ( PLD ) ................................ ................................ .. 33 12.3 . Medical Devices Regulation ................................ ................................ ........ 33 12.4 . AI and consumer law – Addressing digital assymetry ................................ .... 34 12.5 . AI and sustainability ................................ ................................ ................. 34 12.6 . AI and financial services ................................ ................................ ............ 35 12.7 . AI and trade ................................ ................................ ............................ 36 2 1 . Introduction On 21st April 2021 , the European Commission publish ed the proposal for a Regulation laying down harmonised rules on Artificial Intelligence and amending certain Union legislative acts ( ‘ Artificial Intelligence Act ’ ) .1 Artificial intelligence ( AI ) has the potential to bring many benefits for consumers . It can power new products and services and help make daily life easier and less burdensome ( via e.g . personalis ed services , augmented reality applications , AI-powered healthcare tools that can help detect diseases quicker , and automated vehicles ) . However , AI also comes with significant risks and challenges for consumers . For exampl e , the use of AI to maximise user monetisation can lead to increased risks of algorithmic bias and unfair discrimination among different groups of people on the basis of economic criteria , gender or a person ’ s health . More broadly , the use of AI can negatively affect consumers ’ autonomy and freedom of choice . Consumers are conc erned with the rollout of this technology . BEUC member organisations conducted an EU-wide survey to see what consumers think about AI.2 A large majority of respondents perceive AI to be somewhat or even very useful to them in different areas , especially w hen it is used to predict traffic accidents ( 91 % ) , their health ( 87 % ) or financial problems ( 81 % ) . However , respondents have highlighted the risks associated to this technology . In Belgium , Italy , Portugal and Spain , most respondents ( 64 % ) agree or strongl y agree that companies are using AI to manipulate consumer decisions . Also , there is a significant lack of trust : when asked about their level of trust in that their privacy is protected when using AI devices , a large majority of consumers state this to b e medium or low . An average 50 % of Belgian , Italian , Portuguese and Spanish as well as 45 % of Danish , French , German , Polish and Swedish respondents have low trust in the protection of their privacy with wearables . Current EU rules are insufficient to ensure a high level of consumer protection when it comes to AI . For instance , the consumer ’ s right to be informed about the use of AI and to contest automated decisions is very limited in scope and only apply in certain situations . Also , ex isting EU laws do not guarantee that AI systems need to be safe . From this perspective , BEUC welcome s that the European Commission has put forward a much -needed legal instrument aimed at regulating AI . Although the AI Act proposal marks a welcome and necessary step , regretfully , it generally fails to address consumers ’ core concerns and expectations . This is particularly due to its narrow scope , focused on a pre-defined list of so-called ‘ high risk AI ’ 1 Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence ( Artificial Intelligence Act ) and amending certain union legislative acts . 2 https : //www.beuc.eu/publications/survey -consumers -see-potential -artificial -intelligence -raise-seriousconcerns/html 64 % OF RESPONDENTS IN BELGIUM , ITALY , SPAIN AND PORTUGAL AND 52 % OF RESPONDENTS FROM DENMARK , FRANCE , GERMANY , POLAND AND SWEDEN ‘ AGREE ’ OR ‘ STRONGLY AGREE ’ THAT COMPANIES USE AI TO MANIPULATE CONSUMER DECISIONS AND ABUSE PERSONAL DATA BEUC AI survey 2020 3 applications , and the general disregard of the particular dangers posed by AI systems to consumers ’ rights . Moreover , the proposal relies heavily on industry ’ s own unvetted assessment of compliance with legislation and fails to put forward a robust governa nce and public/private enforcement system . Finally , the methodology of listing specific AI system s that the proposed regulation would apply to seems ill-adapted to all future applications of AI that have not yet been d eveloped . In this paper , we address these shortcomings and suggest improvements to ensure that the legislative p roposal provides the rights and protections that consumers need , while also giving space to innovation and respect ing our fundamental rights and values . 2 . Objectives and consumer protection gap The proposal aims to lay down a uniform legal framework for the development , marketing and use of artificial intelligence in conformity with the European Union ’ s values , such as a high level of protection of health , sa fety and fundamental rights .3 The latter are to be understood as consistent with the Charter of Fundamental Rights of the European Union4 and include human dignity , respect for private and family life , protection of personal data , freedom of expression and information , non -discrimination , or a right to an effective remedy and to a fair trial ,5 as well as the duty to ensure a high level of consumer prot ection as enshrined in Article 38 of the Charter . However , beyond the declarat ive non-binding layer in the recitals , consumer protection is lacking in the proposed AI Act . The proposal does not refer to protection of consumers from the adverse impact of AI among the legislative objectives of the AI Act . Consumers are not granted horizontal rights under the proposal and are excluded from the conceptual framework as definition of ‘ user ’ in the proposal is only defined as an institutional or business user.6 BEUC recommendation : - Ensuring a high level of protection for public interests , such as health and safety in general , the protection of consumers , the protection of the environment and of fundamental rights I risks and potential harms caused by artificial intelligence should be explicitly mentioned as legislative objectives of the AI Act in Article 1 . 3 . The implication s and risks of m aximum harmonisation An important aspect in understanding the role and implications of the AI Act is that of the intended level of harmonisation and the very broad material scope cover ing all AI systems that fall under its definition in Article 3 ( 1 ) . A maximum harmonisation instrument leaves no ability for Member States to regulate further and prevents any more restrictive and thus protective regulation within that area on a national level . This is addressed explicitly in the proposal . The proposal states it prevent s “ Member States from imposing restrictions on the development , marketing and use of AI s ystems , unless explicitly authorised by this Regulatio n ” 7 ( emphasis added ) . Coupled with the broad 3 Recital 1 of the proposal . 4 Recital 13 of the proposal . 5 Recital 28 of the proposal , referencing the Charter of Fundamental Rights of the European Union . 6 Article 3 ( 4 ) of the proposal . 7 Recital 1 of the proposal . 4 definition of AI systems included in the text,8 this would leave no room for any further legislation in Member States concerning AI , despite the scope of the proposal covering only high-risk systems . 9 This would mean that the proposal would : • establish regulatory requirements for a narrow group of AI systems defined by the proposal as high-risk , while • preclud ing Member States from establishing further restrictions on a broad array of AI systems without imposing any of its own . BEUC recommendation s : - Introducing m aximum harmonisation rules for all AI systems via the AI Act in its current form can not be deemed fit for purpose due to its substantive scope being focused almost exclusively on a narrow list of systems which it labels as high -risk.10 - Recital 1 should be amended to clarify that Member States can establish additional limitations than those established in the AI Act provided that they are justified on grounds of public interest and the protection of individuals . 4 . Main definitions and concepts ( Article 3 ) 4.1 . AI system Many different definitions of AI and AI systems are in circulation . The Commission ’ s 2020 White Paper on AI speaks broadly of “ technologies that combine data , algorithms and computing power ” ,11 while its 2018 Communication12 defined AI as ” systems that display intelligent behaviour by analysing their environment and taking actions – with some degree of autonomy – to achieve specific goals ” . The Commission ’ s High Level Expert Group13 ( HLEG ) published its own extensive definition of AI systems in 2019 , which now e choes in the approach taken in the AI Act proposal .14 While refraining from defining ‘ artificial intelligence ’ ( AI ) as such , Article 3 ( 1 ) of the proposal defines an ‘ AI system ’ as one able to , for a given set of human -defined objectives , 8 See Section 4.1 below . 9 Michael Veale , Frederik Zuiderveen Borgesius , Demystifying the Draft EU Artificial Intelligence Act ( forthcoming in ( 2021 ) 22 ( 4 ) Computer Law Review International ) at 21 . 10 See also the comments in Section 6.1.3 on systems other than high risk . 11 https : //ec.europa.eu/info/sites/default/files/commission -white -paper -artificial -intelligence -feb2020_en.pdf , p. 2 . 12 Communication fro m the Commission to the European Parliament , the European Council , the Council , the European Economic and Social Committee and the Committee of the Regions on Artificial Intelligence for Europe , Brussels , 25.4.2018 COM ( 2018 ) 237 final . 13 Of which BEUC was a member . 14 HLEG proposed to define AI systems as “ software ( and possibly also hardware ) systems designed by humans that , given a complex goal , act in the physical or digital dimension by perceiving their environment through data acquisition , interpreting the collected structured or unstructured data , reasoning on the knowledge , or processing the information , derived from this data and deciding the best action ( s ) to take to achieve the given goal . AI systems can either use symbolic rules or learn a numeric model , and they can also adapt their behaviour by analysing how the environment is affected by their previous actions . As a scientific discipline , AI includes several approaches and techniques , such as machine learning ( of which deep learning and reinforcement learning are specific examples ) , machine reasoning ( wh ich includes planning , scheduling , knowledge representation and reasoning , search , and optimization ) , and robotics ( which includes control , perception , sensors and actuators , as well as the integration of all other techniques into cyber -physical systems . ” Independent High -Level Expert Group on Artificial Intelligence , A definition of AI : main capabilities and disciplines , 2019 5 generate outputs such as content , predictions , recommendations , or decisions influencing the environments it interact s with , using techniques and approaches defined in Annex I as machine learning approaches , l ogic- and knowledge -based approaches or statistical approaches .15 This definition is broad and tied to the possible uses of technologies and approaches listed in Annex I while only offering suggestions of outputs such systems may produce . In doing so , it avoids the pitfalls of layman defini tions linking the operation of AI to that of human intelligence and thus significantly narrowing down their scope . Since the AI Act aims to lay down a uniform legal framework for ( all ) AI , it is important to ensure that it does not shy away from establish ing ground rules for all AI systems . To this end , the definition of AI systems must be both broad and able to cover applications to be developed in the future , and not be prone to misinterpretation so that simple tools like Excel spread sheets using statist ical formulas don ’ t get included . BEUC recommendation s : - To address the numerous definitions in circulation , the proposal should clarify in Recital 2 that its references to ‘ AI systems ’ also appl y to AI as such , as per the HLEG AI definition paper.16 - To avoid any misunderstandings , Recital 6 should clarify the reach of the definition of ‘ AI system ’ and give some examples of tools and systems , such as spread sheets with statistical formulas , that would typically not fall under the definition of ‘ AI system ’ . In this regard , Annex I should not be amended . 4.2 . User As mentioned above , t he definition of ‘ user ’ in Article 3 ( 4 ) of the proposal refers to “ any natural or legal person , public authority , a gency or other body using an AI system under its authority , except where the AI system is used in the course of a personal non professional activity ” . This approach excludes individuals using A I systems as ‘ users ’ under the AI Act unless they are doing so in their professional capacity , as well as those individuals who are subject to the use of an AI system .17 BEUC recommendation : - To ensure a high level of consumer protection , the AI Act must introduce a definition of ‘ consumer ’ . 4.3 . Risk The proposal presents a ‘ risk -based ’ approach relying on an ex ante classification of AI systems , with the high-risk category being the main subject of regulation ( see section 5 for a discussion on the classification and its implications ) . 15 Annex I ( a ) - ( c ) . 16 See fn . 14 above . 17 Ada Health GmbH : Ada Website , URL : https : //ada.com/app/ 6 While the proposal does not define ‘ risk ’ as such , it offers numerous hints to this effect . A ‘ risk of harm to the health and safety , or a risk of adverse impact on fundamental rights ’ is offered as a criterion for updating the list of high -risk AI systems in Annex I II.18 Moreover , Article 65 ( 1 ) define s an AI system presenting a risk as a “ product presenting a risk ” definition under Article 3 ( 19 ) of Regulation 2019/102019 but only “ insofar as risks to the health or safety or to the protection of fundamental rights of persons are concerned ” ,20 thus leaving out all other risks contemplated in that Article.21 With the exception of a high level of environmental protection , which is included in this assessment under Recital 28 , o ther risks posed by AI to individuals and society ( e.g . risks to democracy or rule of law ) , despite all being highlighted by the HLEG Ethics Guidelines for Trustworthy AI ,22 are thus left outside of the scope of this Regulation . Risks to societies , such as attention -maximising pers onalisation algorithms in recommender systems , are not included . Similarly , other risks which are highly pertinent to consumers in the digital sphere , i.e . risks of adverse impact on consumers ’ agency , autonomy of choice , access to goods and services , unfair discrimination and economic harm , are also not addressed by the proposal . BEUC recommendation : - Risks of AI having an adverse impact on consumers ’ agency , autonomy of choice , access to goods and services , unfair discrimination and economic harm , privacy and data protection , as well as societal risks should also be explicitly addressed in the definition of an ‘ AI system pr esenting a risk ’ under Article 65 ( 1 ) . 4.4 . Harm Similarly to risk , ‘ harm ’ is flagged in the Explanatory Memorandum as needing definition.23 However , the term remains undefined in the proposal despite its frequent use . It is often used in reference to health and safety of persons and fundamental rights24 but also in a general sense , in contexts such as public interests and rights25 and regulatory sandboxes.26 A detailed guidance on harm ( clarifying the types and ways in which AI can ca use detriment ) should be introduced to the recitals to help interpreting the existing references to individual harm . This is particularly the case of use of AI systems intended27 for material 18 Article 7 ( 1 ) in conjunction with Article 73 and 84 ( 1 ) of the proposal . 19 Regulation ( EU ) 2019/1020 of the European Parliament and of the Council of 20 June 2019 on market surveillance and compliance of products ( ‘ Market Surveillance Regulation ’ ) . 20 Article 65 ( 1 ) : ‘ AI sys tems presenting a risk shall be understood as a product presenting a risk defined in Article 3 , point 19 of Regulation ( EU ) 2019/1020 insofar as risks to the health or safety or to the protection of fundamental rights of persons are concerned . ’ 21 These are products having the potential to affect adversely health and safety of persons in general , health and safety in the workplace , protection of consumers , the environment , public security and other public interests , protected by the applicable Union harmonis ation legislation , to a degree which goes beyond that considered reasonable and acceptable in relation to its intended purpose or under the normal or reasonably foreseeable conditions of use of the product concerned , including the duration of use and , wher e applicable , its putting into service , installation and maintenance requirements ( following Art . 3 ( 19 ) , Market Surveillance Regulation ) . 22 https : //ec.europa.eu/newsroom/dae/document.cfm ? doc_id=60419 23 Id . 24 See e.g . Recital 28 , 32 , Article 7 ( 1 ) b and 7 ( 2 ) , Article 13 ( 3 ) b of the proposal . 25 Recital 4 of the proposal . 26 Article 53 ( 4 ) of the proposal . 27 Recital 16 places these prohibitions i n the context of AI systems ‘ certain AI systems intended to distort human behaviour ’ rendering them even more narrow than their wording would suggest . 7 distortion of a person ’ s behaviour that is likely to lead to phy sical or psychological harm , covered under Article 5 ( 1 ) ( a ) and ( b ) . Applying a ‘ common sense ’ definition of harm in this context may lead to difficulty in applying these provisions in practice , particularly given the need to establish inten ded use28 and the extremely narrow scope of these provisions.29 Harms which are collective rather than individual , as well as those which build up over time rather than being caused by a one-off event , are not covered . This is the case of recommender systems combi ned with hyper -personalisation , engagement and impact on childre n.30 Harm resulting from users ’ interaction with a n AI system is seemingly also ruled out by Recital 16 which precludes presumption of intent ” if the distortion of human behaviour results from factors external to the AI system which are outside of the control of the provider or the user ” . This raises the question to what degree harm caused by e.g . social network s or content recommender site s , can even be considered in this context . Lastly , downstream harms , where one agent deploys a n AI for classification of individuals and another entity ( e.g . a dating service ) uses this classification in a harmful manner , are also difficult to address under this proposal .31 BEUC recommendation s : The recitals should include detailed guidance on the concept of harm , to clarify the following : - a clear concept of how ‘ harm ’ is to be understood , e.g . by means of referencing the EU general risk assessment methodology32 definition of ‘ harm ’ as injury or damage to the health of people or damage to property , economic harm to co nsumers , damage to environment , security and other aspects defined in the scope of New Approach directives , complemented by collective harm s such as harm to societ y ; - the manner in which ‘ harm ’ is created , including : o by single events and through exposure over time to harmful algorithmic practices ; as well as o through action distributed among a number of actors where the entity causing the harm is not necessarily that which uses the AI ; o causation of harm through uses other than intended , to avoid narrowing down the framework to systems ‘ intended to distort behaviour ’ as Recital 16 suggests , and to avoid the pitfall of basing harm on the presumption of an intention which may 28 A good example is general -purpos e AI or AI as a service ; see Cobbe , Jennifer and Singh , Jatinder , Artificial Intelligence as a Service : Legal Responsibilities , Liabilities , and Policy Challenges ( April 12 , 2021 ) . Forthcoming in Computer Law & Security Review , Available at SSRN : https : //ssrn.com/abstract=3824736 or http : //dx.doi.org/10.2139/ssrn.3824736 29 See more specifically Section 5 on the list of prohibited practices . 30 Borgesius , Veale ( 2021 ) at 4 . 31 Id , at 5 . 32 EU general risk assessment methodology ( Action 5 of Multi -Annual Action Plan for the surveillance of products in the EU ( COM ( 2013 ) 76 ) , https : //ec.europa.eu/docsroom/documents/17107/attachments/1/translations/en/renditions/native 8 not even exist or be hidden /impossible to prove ( such as in case of AI as a service , or e.g . stalke rware sold as child trackers33 ) . 4.5 . Intended purpose and misuse A prominent parameter in the proposed framework is the ‘ intended purpose ’ of an AI system , which is defined as ” use intended by the provider ” .34 This is used to classif y whether a system pos es a ‘ high -risk ’ 35 of harm to the health and safety or the fundamental rights of persons . Therefore , the ‘ intended purpose ’ affects the requirements of t raining , validation and testing of datasets36 and its change requires a new conformity assessment.37 A related concept is that of ‘ reasonably foreseeable misuse ’ , defined as the use of an AI system in a way that is not in accordance with its intended purp ose , but which may result from reasonably foreseeable human behaviour or interaction with other systems .38 Both the ‘ intended purpose ’ and the ‘ reasonably foreseeable misuse ’ form part of the risk management system required under Article 9 and the mandator y disclosure to users in terms of possible risks posed by a system.39 Notably , both concepts can only be applied to systems classified as high risk . For any other AI applications , no AI specific framework is established that would warrant their inclusion , leaving unregulated the misuse of any AI that is not classified as high risk . This is particularly worrying given that , as put forward in the proposal , misuse of AI technology can provide novel and powerful tools for manipulative , exploitative and social c ontrol practices , declared as particularly harmful and needing to be prohibited due to contradict ing European Union values .40 The proposed framework also falls short when it comes to an AI system which is not classified in principle as high -risk but that can begin to create risks , of a varying degree , to health and safety or fundamental rights of persons as it is customi sed in the course of its use . Such concern applies particularly to general -purpose AI systems sold by tech companies which the buyer is free to develop further.41 Even if a procedure existed for their assessment ( despite the concerns raised in the above paragraph ) , it is dubious whether the ‘ intended purpose ’ – ‘ reasonably foreseeable misuse ’ dichotomy could capture suc h uses , which suggests that it needs to be supplemented with other categories of purpose and use . BEUC recommendations : - To enable assessment of systems other than those currently labelled as high-risk , the proposal must explicitly include a mechanism for evaluating purpose and foreseeable use , including misuse , for all AI systems . This mechanism should be 33 Veale , Borgesius ( 2021 ) citing Harkin D , Molnar A , Vowles E. The commodification of mobile phone surveillance : An analysis of the consumer spyware industry . Crime , Media , Culture . 2020 ; 16 ( 1 ) :33 -60. doi:10.1177/1741659018820562 https : //journals.sagepub.com/eprint/XnAgFVFWyHGUv6dzbFPZ/full 34 Article 3 ( 12 ) of the proposal . 35 Article 6 ( 1 ) a ) of the proposal . 36 Recitals 43 – 44 of the proposal . 37 Recital 66 of the proposal . 38 Article 3 ( 13 ) of the proposal . 39 Article 13 ( 3 ) b ) iii ) of the proposal . 40 Recital 15 of the proposal . 41 For an overview on automated machine learning systems ( ‘ AutoML ’ , e.g . Google 's Cloud AI AutoML , Amazon SageMaker Autopilot , Microsoft Azure automated ML ) allowing business users t he creation of customized machine learning models , see https : //www.kdnuggets.com/2020/02/data -scientists -automl -replace.html ( 2020 ) and https : //www.ibm.com/downloads/cas/RGN4EOZK ( 2019 ) . 9 connected with the framework for classification of AI systems that should complement Article 6 , as proposed in Section 6 below . - To ensure the Regulation allows an accurate assessment of AI system s , the proposed mechanism for assessment must not be limited to the narrow notions of ‘ intended purpose ’ and ‘ reasonably foreseeable misuse ’ but should also allow the assessi ng body to examine ‘ potential use ’ or ‘ foreseeable use ’ of the given system . 4.6 . Biometric data The proposal uses the definition of ‘ biometric data ’ as “ personal data resulting from specific technical processing relating to the physical , physiological or behavioural characteristics of a natural person , which allow or confirm the unique identification of that natural person . ” 42 This is aligned with the definitions in other European Union legislation43 , but the way the definition is used here gives rise to concern . This is notable when contrasting the use of biometric data as discussed in the context of biometric identification against the definitions of an ‘ emotion recognition system ’ ( Art icle 2 ( 34 ) ) or a ‘ biometric categorisation system ’ ( Article 2 ( 35 ) ) where the proposal also refers to the definition of biometric data . Applications using emotion recognition or biometric categorisation can nevertheless also operate using less detailed data than would be needed to perform or confirm identi fication of an individual , while still holding potential harm for consumers if deployed in commercial contexts , such as in -store emotion recognition systems.44 In this vein , a system making inferences about consumer s ’ emotions could escape classification as such under Article 2 ( 34 ) by making a claim of not using biometric data – e.g . merely by virtue of using less data than would be required to establish or confirm a person ’ s identity . This risk is embedded in the current wording ’ s use of the term ‘ biometric data ’ which must be clarified . BEUC recommendations : - The definitions of ‘ emotion recognition system ’ ( Article 2 ( 34 ) ) and ‘ biometric categorisation system ’ ( Article 2 ( 35 ) ) should be amended to avoid being necessarily tied to the actual identification ( or confirmation of an identification ) of an individual . This way , a system making invasive ( but anonymous ) inferences about one ’ s own emotional state or social category will not escape classification under Article 2 ( 34 ) or Article 2 ( 35 ) . 4.7 . Remote biometric identification systems In the age of easily available and highly invasive facial recognition technology that can record and make inferences from a person ’ s presence and movements in varying contexts , regulating the use of remote biometric ident ification systems is importan t. Article 3 ( 36 ) defines a ‘ remote biometric identification system ’ as an AI system intended for the identification of natural persons at a distance , by comparing a person ’ s biomet ric data with the biometric data contained in a reference database , and without prior knowledge whether the targeted person will be present and can be identified . ‘ Distance ’ is 42 Article 3 ( 33 ) of the proposal . 43 Article 4 ( 14 ) of Regulation ( EU ) 2016/679 of the European Parliament and of the Council , Article 3 ( 18 ) of Regulation ( EU ) 2018/1725 of the European Parliament and of the Council , and Article 3 ( 13 ) of Directive ( EU ) 2016/680 of the European Parliament and of the Council . 44 Vzbv ( 2021 ) Artificial intelligence needs real world regulation , https : //www.vzbv.de/sites/default/files/2021 08/21 -08-03_vzbv_Position_Paper_AIA_ENG.pdf , p. 9 . 10 not explained and there is no mention regarding the targeted person ’ s knowledge about being subjected to identification . Two types of systems are distinguished : • ‘ real-time ’ remote biometric identification system s , where the capturing of biometric data , the comparison and the identification all occur without a significant delay , ruling out limited short delays in order to avoid circumvention ; • ‘ post ’ remote biometric identification system s , meaning remote biometric identification system s other than the above . Despite the attempt to rule out circumvention , the above definition still does not give a straight answer as to whether analysing a recording within hours of its creation is considered ‘ real -time ’ , creating a risk of circumvention . Recital 8 is of little help , introducing terms of ‘ live ’ or ‘ near -live ’ material as well as that of a ‘ significant delay ’ described as “ generated before the use of the system in respect of the natural persons concerned ” .45 BEUC recommendations : - For a clear distinction of what constitutes a ‘ remote identificati on ’ system , the meaning of identification ‘ at a distance ’ as mentioned in the definition of Article 3 ( 36 ) must be clarified . - What constitutes ‘ real-time ’ and ‘ post ’ recognition as defined in Article 3 ( 37 ) - ( 38 ) , along with ‘ live ’ and ‘ near -live ’ material as mentioned in Recital 8 must be clarified . 5 . List of prohibited practices ( Article 5 ) Article 5 establishes a list of four AI practices to be prohibited in the EU . BEUC strongly welcomes this regulatory approach . Certain AI practices represent such a n important risk for consumer rights , fundamental rights and our societal values that the most adequate regulatory solution is a clear prohibition . However , several elements of Article 5 need to be substantially improved to effectively take into account consumers ’ interests and ensure both a broad and clear application of these prohibitions . 5.1 . Subliminal techniques causing behavioural distortion and harm – Art . 5 ( 1 ) a ) The first prohibited AI practice consists of AI that deploys ‘ subliminal techniques beyond a person ’ s consciousness to materially distort a person ’ s behaviour in a manner that causes or is likely to cause that person or another person physical or psychological harm ’ . First , we regret that the application of this pr ovision is limited to AI that causes physical or psychological harm . AI that manipulates someone in a manner that causes him/her economic harm should also be included . This type of harm is a real risk in the age of dark patterns , even without the additional concerns brought by AI.46 For example , price 45 Recital 8 in fine of the proposal . 46 A recent example demonstrates how manipulated interface s have been found to push people into giving larger donations than they knew or planned to : https : //www.nytimes.com/2021/04/03/us/politics/trump donations.html 11 optimisation techniques , where insur ance firms target price increases to those perceived as less likely to switch and/or more likely to pay should not be permitted , i.e . there should be no loyalt y penalty for long -standing insurance consumers.47-48 Another example would be the use of smart meter data to establish personalised energy prices for consumers based on their consumption data . Suppliers could take advantage by adapting the price for elec tricity to the consumers ’ consumption pattern to make them pay more i.e . high electricity prices when your consumption is usually high . Our UK member Citizens Advice raised similar concern s in a recent report.49 Save for the narrowly defined case of social scoring conducted by or on behalf of authorities , societal harm has also been left out of Article 5 , despite the clear reference to the need for such prohibition s in Recital 15,50 and the clear indications of the harms cause d by algorithmic ecosystems feeding on user attention and weaponization of polarizing content.51 Second , the wording ‘ in order to ’ limits the application of this provision to AI whose ‘ intended purpose ’ is to cause physical or psychological harm , thus excl uding the ‘ potential use ’ or ‘ reasonably foreseeable misuse ’ of the AI.52 Aside from whether the proposal allows the examination of intended purpose for systems other than high -risk,53 we strongly reject the requirement to prove intent . This is not required in EU consumer law , in case of unfair commercial practices or product liability , and would be a significant and unacceptable step backwards regarding the level of protection that consu mers need and are entitled to expect under EU law . It also would mean that these provisions are in practice unenforceable as it would be very difficult , if not impossible , to prove the original malicious intent . Third , use of the ‘ subliminal ’ criterion , al though undefined in the proposal,54 means that a ban only applies to techniques which are barely noticed . As such , it is powerless in all cases where the individual has the faintest suspicion of being manipulated , leaving open situations where the individu al is aware of the presence of algorithms but not of their entire 47 The UK ’ s Financial Conduct Authority and the Central Bank of Ireland have recently published market investigations show how insurers often engaging in price optimisation practices , specifically targeting price increases on consumers who are perceived as less likely to switch ( a ‘ loyalty penalty ’ ) . These investigations were launched following research by our UK member Citizens Advice . 48 For more information , read BEUC ’ s position paper on Big Data and Insurance : https : //www.beuc.eu/publications/beuc -x-2020039_beuc_position_paper_big_data_and_ai_in_insurances.pdf 49 https : //www.citizensadvice.org.uk/a -price-of-ones-own-an-investigation -into-personalised -pricing -inessential -markets/ 50 Recital 15 states : ‘ Aside from the many beneficial uses of artificial intelligence , that technology can also be misused and provide novel and powerful tools for manipulative , exploitative and social control practices . Such practices are particularly harmful and should be prohibited bec ause they contradict Union values of respect for human dignity , freedom , equality , democracy and the rule of law and Union fundamental rights , including the right to non -discrimination , data protection and privacy and the rights of the child . ’ 51 See e.g . European Parliamentary Research Service , Polarisation and the use of technology in political campaigns and communication , PE 634.414 – March 2019 , https : //www.europarl.europa.eu/RegData/etudes/STUD/2019/634414/EPRS_STU ( 2019 ) 634414_EN.pdf , Reviglio U , Agosti C. Thinking Outside the Black -Box : The Case for “ Algorithmic Sovereignty ” in Social Media . Social Media + Society . April 2020. doi:10.1177/205 6305120915613 ; https : //journals.sagepub.com/doi/full/10.1177/2056305120915613 ; Bessi A , Zollo F , Del Vicario M , Puliga M , Scala A , Caldarelli G , et al . ( 2016 ) Users Polarization on Facebook and Youtube . PLoS ONE 11 ( 8 ) : e0159641 . doi:10.1371/journal.pone.0159641 https : //www.researchgate.net/publication/301926189_Users_Polarization_on_Facebook_and_Youtube ; on creation of echo chambers see Cinelli , De Francisci Morales , Galeazzi , Quattrociocchi , Starnini ( 2021 ) The echo chamber effect on social media , Proceedings of the N ational Academy of Sciences Mar 2021 , 118 ( 9 ) e2023301118 ; DOI : 10.1073/pnas.2023301118 https : //www.pnas.org/content/118/9/e2023301118 . 52 See Recital 16 of the proposal . 53 See section 4.5 on intended purpose and misuse . 54 See the discussion in Definitions above . 12 functionality . Similarly , it leaves out an entire body of aggressive algorithmic techniques for choice manipulation which are deployed against individuals without any intention of being sublim inal , such as choice architectures meant to evoke frustration and anger . Recent research shows that such techniques are highly successful despite the emotional price individuals pay for interacting with such systems.55 BEUC recommendations : - Art . 5 ( 1 ) a ) should be expanded to include AI practices that manipulates someone in a way t hat can cause them economic harm . - Art . 5 ( 1 ) a ) should be expanded to AI whose ‘ intended purpose ’ or ‘ reasonably foreseeable misuse ’ can manipulate someone and lead to physical , psychological or economic harm . This prohibition should also extend to uses w hich can be identified as ‘ potential use ’ or ‘ foreseeable use ’ and apply irrespectively of the risk classification of the AI system ( see section 4.5 ) . - The ‘ subliminal ’ criterion of Art . 5 ( 1 ) a ) should be removed as it is vague and unnecessary . The provision doesn ’ t protect the consumer against aggressive behavioural manipulation in the digital environment which can be performed openly , causing the consumer a powerful emotional backlash and still delivering high rates of success in getting consumer s to do something against their will.56 - A broad reversal of the burden of proof putting the onus of demonstrating compliance on the entity behind the AI system in all disputes involving individuals must be introduced in order to reinforce the prohibitions on manipulation.57 This recommendation must apply across all prohibited AI practices mentioned in sections 5.2 to 5.6 . - Societal harm caused by an AI practice should also be included . In this regard , AI systems that manipulate someone in a way that can have an impact on the functioning of democracy and the rule of law should also be covered . 5.2 . Vulnerability of groups – Art . 5 ( 1 ) b ) This provision focuses on AI that exploits vulnerabilities of specific groups such as children or mentally disabled persons , with the specific intention to materially distort their behaviour leading to physical or physiological harm . First , it does not apply to AI used to exploit oth er types of vulnerabilities such as financial vulnerabilities . For example , an AI that exploits someone ’ s gambling addiction in a manner that is likely to cause that person economic harm is allowed .58 55 Luguri , Jamie and Strahilevitz , Lior , Shining a Light on Dark Patterns ( March 29 , 2021 ) . 13 Journal of Legal Analysis 43 ( 2021 ) , University of Chicago Coase -Sandor Institute for Law & Economics Research Paper No . 879 , U of Chicago , Public Law Working Pape r No . 719 , Available at SSRN : https : //ssrn.com/abstract=3431205 or http : //dx.doi.org/10.2139/ssrn.3431205 at 65 . 56 id . 57 See also section 7.6 on acces s to justice . 58 Research shows how “ some in -game purchasing systems could be characterized as unfair or exploitative . These systems describe tactics that capitalize on info rmational advantages ( e.g. , behavioural tracking ) and data manipulation ( e.g. , price manipulation ) to optimize offers to incentivize continuous spending , while offering limited or no guarantees or protections ( e.g. , refund entitlement ) , with the potential to exploit vulnerable players ( e.g. , adolescents , problematic gamers ) . ” 13 Furthermore , while ensuring protection to the most vuln erable groups , this provision does not take into account the permanent state of vulnerability of all individuals created by exposure to ‘ black box ’ technology and economic practices that consumers can not grasp . The digital market is characterised by a pro found im balance of power , knowledge and agency , compared to other markets . This translates into a new position of vulnerability for consumer s that is both structural ( owing to the struct ure of digital markets which prevents consumers from interacti ng with market players on an equal footing ) and architectural ( due to the way interfaces are designed and operated ) . This imbalance and vulnerability are referred to in current academic debate s as ‘ digital asymmetry ’ .59 Digital as ymmetry is not addressed in the formulation of Article 5 ( 1 ) b ) , despite it being perpetuated and driven by algorithmic environments which use AI to maximise user monetisation and conversion rates . Current research suggests that the disempowerment of individuals in this context can o nly be addressed by a reversed burden of proof to prove a trader ’ s compliance which must become the standard for all data -driven services addressed to consumers.60 Thirdly , the proposal does not take in to consideration vulnerabilities of individuals which are not associated to their age , physical or mental disability . Consumers can become vulnerable for a temporary moment for different reasons ( e.g . emotional distress , grief , sorrow , etc. ) . An AI system that takes advantage of that temporary vulnerability should also be prohibited . Finally , similarly to Article 5 1 ( a ) , the wording ‘ in order to ’ ties the application of the provision to the construct of intended purpose of the AI system , as discussed earlier in this paper . This leaves out any considerations o f ‘ potential use ’ or ‘ reasonably foreseeable misuse ’ of the AI . It also as raises a question about its applicability to non-high-risk systems where risk management , including its mechanism for evaluation of intended purpose , is not envisaged .61 BEUC recommendations : - Art . 5 ( 1 ) b ) should also apply to AI used to exploit vulnerabilities other than those related to children or of physical ly- or mental ly-disabled people . For example , it should protect consumers made vulnerable through the use of persuasion profiles and personalisation practices ( digital asymmetry ) or through temporary vulnerabilities ( e.g . grief , sorrow , emotional distress ) who are not associated to group of people due to their age , physical or mental disability . - Art . 5 ( 1 ) b ) should be expanded to AI whose ‘ intended purpose ’ or ‘ reasonably foreseeable misuse ’ can manipulate someone and lead to physical , psychological or economic harm . This prohibition should also extend to uses which can be identified as ‘ potential use ’ or ‘ foreseeable use ’ and apply irrespectively of the risk classification of the AI system ( see section 4.5 ) . 59 Helberger N. Lynskey O. Micklitz H. -W. Rott P. ( 2021 ) Structural asymmetries in digital consumer markets , BEUC , https : //www.beuc.eu/publications/beuc -x-2021-018_eu_consumer_protection.0_0.pdf , at 46 . 60 Id , at 66 et seq . 61 See section 4.5 on intended purpose and misuse . 14 5.3 . Social scoring – Art . 5 ( 1 ) c ) While we welcome the prohibition of social scoring by public authorities , certain elements need to be clarified and the ban of this specific AI practice should be expanded to commercial use too . Firstly , the use of social scoring by private entities is not explicitly regulated by the AI Act since it is not included in Article 5 , nor is it in the list of high risk AI sys tems in Annex III.62 This highly problematic gap must be addresse d as social scoring by private entities can have a significant impact on our most basic fundamental rights and core democratic principles . It has been proven that people behave differently according to when they know that they are being observed or followed ( the panoptic effect ) .63 For example , Airbnb recently patented an AI system capable of generating s ocial scoring to determine consumer s ’ trustworthiness based on a variety of social media/online data .64 If operational , this AI is likely to lead to the discrimination of some social groups in the market for holiday homes – and yet , this may be just the tip of the iceberg.65 Under the proposed rules , such an AI system would not be regulated by the AI Act . Another controversial point is that the Commission ’ s proposal seems to accept a discriminatory behaviour by means of socia l scoring if the detrimental or unfavourable treatment is proportionate to the individual ’ s social behaviour ( Art . 5 ( 1 ) I Subpoint ( ii ) ) . Also , the prohibition of Article 5 I ( i ) is limited to social scoring that is based on data originally collected in a context that is unrelated to the context of the social scoring . In other words , scoring based on data originally collected in a context for social scoring would be lawful under the Commission ’ s proposal . For these reasons , and in line with the Opinion from European Data Protection Board ( EDPB ) and the European Data Protection Supervisor ( EDPS ) ,66 we support a general ban on the use of social scoring by both public and private entities . BEUC recommendations : Article 5 ( 1 ) c ) should be amended as follows : - It should include a general ban on AI used by both public and private bodies to evaluate the trustworthiness of an individual based on their social behaviour or other personal attributes , such as someone ’ s preferences , emotions , health or intelligence . - Social scoring should be prohibit ed regardless of the context under which the data used is collected . 62 Although not explicitly mentioned , one could envisage the use of social scoring in AI used for education purposes under Annex III , Point 3 . 63 https : //en.wikipedia.org/wiki/Panopticon 64 Clark Boyd , Would You Pass the Airbnb Psychopath Test ? Towards Data Sci ence , January 12 , 2020 , https : //towardsdat biometric ascience.com/would -you-pass-the-airbnb -psychopath -test-83e66cb55 65 Christopher Mims , The Secret Trust Scores Companies Use to Judge Us All , Wall Street Journal , April 6 , 2019 , https : //www.wsj.com/articles/the -secret -trust-scores -companies -use-to-judge -us-all-11554523206 66 EDPB-EDPS Joint Opinion 5/2021 on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence ( Artificial Intelligence Act ) , 18 June 2021 https : //edps.europa.eu/system/files/2021 -06/2021 -06-18-edpb-edps_joint_opinion_ai_regulation_en.pdf 15 - Discriminatory behaviour by means of soci al scoring should always be unlawful even if the detrimental or unfavourable treatment is proportionate in light of the individual ’ s social behaviour . 5.4 . Real-time remote biometric identification – Art . 5 ( 1 ) d ) Companies are increasingly using consumers ’ biometric data for different purposes . All over the world , facial recognition is used for ‘ tagging ’ people on social media platforms , to unlock smart phones or to authenticate/identify customers in the context of financial services . Biometrics a re particularly sensitive data , and their illegitimate processing can have very serious consequences for consumers and society . In a recent study67 commissioned by the Arbeiterkammer68 , the Institute for Technology Assessment ( Institut für Technikfolgen -Abschätzung ) looked at the impact of the wide scale use of biometric procedures on consumers . When it comes to facial recognition , the study underlined that it is a technology that , in today ’ s terms , poses the greatest threat to fundamental rights and democ racy . Due to t echnical shortcomings , such as extremely high error ratios , technically aggravated discrimination , racism , suppression , mass surveillance and the loss of privacy , anonymity and personal freedom , it should be regulated strictly . Article 5 ( 1 ) d ) forbids the use of ‘ real -time ’ remote biometric identification ( RBI ) systems in publicly accessible spaces69 for the purpose of law enforcement ( e.g . use of facial recognition AI on streets to scan passers -by ) . However , this prohibition is not absolute . Under the current proposal , in certain situations ( e.g . search for missing children ) and under certain conditions ( e.g . prior authorisation from judicial authority ) , the use of RBI systems in public spaces for law enforcement is permitted . The use of RBI by private entities is also allowed . BEUC strongly disagrees with the approach taken in the proposal on this issue , particularly on the exclusive focus on RBI used by public authorities . Due to its inherent intrusiveness and the risk it represent s for fundamental rights , core democratic principles and EU values , the use of remote biometric identification systems in public spaces by private entities must also be banned without exceptions . This is also the position of the European Data Protection Su pervisor and the European Data Protection Board which called for a general ban on any use of AI for automated recognition of human features in publicly accessible spaces , such as recognition of faces , gait , fingerprints , DNA , voice , keystrokes and other bi ometric or behavioural signals , in any context.70 As regards the use of remote biometric identification systems by public entities , the EU institutions should consider a full ban on such systems , in line with the recommendations of the EDPB and the EDPS . 67 https : //www.akeuropa.eu/policy -brief-body-access -key-biometric -methods -consumers 68 Arbeiterkammer is an associated member of BEUC . 69 Under the current proposal , ‘ publicly accessible spaces ’ means any physical place accessible to the public , regardless of whether certain conditions for access may apply ( Art . 3 ( 39 ) ) . 70 EDPB -EDPS Joint Opinion ( 2021 ) , section 32 https : //edpb.europa.eu/system/files/2021 -06/edpb edps_joint_opinion_ai_regulation_en.pdf . 16 BEUC recommendation : - The use of remote biometric identification systems by private entities in public spaces should be banned , without exceptions . 5.5 . Emotion recognition The use of AI for emotion recognition is very worrying for consumers ( e.g . real time facial recognition that analyses feelings and adapts what consumers see/or are offered accordingly ) as it can lead to serious infringements of consumers ’ privacy and to their manipulation . Furthermore , r esearchers have argued that ‘ it is not possible to co nfidently infer happiness from a smile , anger from a scowl , or sadness from a frown , as much of current technology tries to do when applying what are mistakenly believed to be the scientific facts ’ . 71 Under the current proposal , AI used for emotion recogn ition is not considered to be of high risk and is simply subject to a transparency obligation .72 This is not sufficient . The use of emotion recognition technology should only be allowed for strictly limited purposes and under very limited conditions , such as health or research purposes.73 BEUC recommendation : • Article 5 of the AI Act should ban the use of emotion recognition AI except in very specific circumstances related to health or research purposes , in line with the recommendations of the EDPB and EDPS . 5.6 . Scientifically unproven AI systems Due to risk of fraud , as highlighted by Consumer Reports74 , the AI Act should also prohibit the placing on the market , sale and use of AI -enabled technology for which are not scientifically substantiated or have been debunked by science . A similar view was expressed in the joint opinion of the EDPB and the EDPS , calling for a prohibition of ‘ AI systems whose scientific validity is not proven or which are in direct conflict with essential values of the EU ’ .75 Examples include companies performing physiognomy ( trying to use a person 's facial features or physical characteristics to determine their character , personality or sexual orientation76 ) which is widely considered as pseudo -science , and other types of sentiment analysis tools . BEUC recommendations : - The AI Act should prohibit the use of AI for which the scientific validity is unproven or the claimed benefits have been debunked by science . 71 Pag . 48 : Barrett , L. F. , Adolphs , R. , Marsella , S. , Martinez , A. M. , & Pollak , S. D. ( 2019 ) . Emotional Expressions Reconsidered : Challenges to Inferring Emotion From Human Facial Movements . Psychological Science in the Public Interest , 20 ( 1 ) , 1 –68 . https : //doi.org/10.1177/1529100619832930 72 Article 52 of the proposal . 73 EDPB -EDPS Joint Opinion ( 2021 ) , section 35 . 74 https : //ec.europa.eu/info/law/better -regulation/have -your-say/initiatives/12527 -Artificial -intelligence ethical -and-legal-requirements/F2660226_en 75 EDPB -EDPS Joint Opinion ( 2021 ) , section 3 3 . 76 https : //mashable.com/article/artificial -intelligence -ai-lgbtq-gay-straight 17 6 . Scope restriction and risk assessment 6.1 . Scope restriction 6.1.1 . High -risk systems One of the main shortcomings of the proposal is the limited scope of application of its obligations and requirements . These almost exclusively concern ’ high risk AI systems ’ , leaving a great majority of existing AI systems unregulated . For any other AI system , the European Commission relies on the concept of voluntarism . The proposal suggests that the requirements for ‘ high risk AI systems ’ could be applied on a voluntary basis via codes of conduct , and their drawing up must be encouraged by the Commission and the Member States ( Art . 69 ) . An AI system is considered high -risk if it is specified as such in Article 6 of the proposed Regulation , i.e . where it : • constitutes a product or a safety component of a product covered by European Union harmonisation legislation listen in Annex II and requiring third -party conformity assessment , or • is expressly listed in Annex III . A high -risk AI system can be a safety co mponent of a product which is not considered high risk under relevant harmonisation legislation.77 This approach has major implications . First , it leaves outside of the AI Act ’ s scope a vast majority of AI systems which affect the daily lives of citizens a nd consumers , such as online profiling and personalisation algorithms , content recommender systems that select what people see in their social media feeds . Connected devices with AI embedded ( e.g . smart meters , connected toys , virtual assistants ) are not classified as high-risk AI systems . In this regard , t here is a lack of clarity regarding the implications of Art . 43 ( 3 ) and the extent to which harmonised standards covering the requirements applicable to high -risk AI system s could also apply to what are considered ‘ lower -risk ’ connecte d devices . Importantly , this approach does not address a multitude of risks for consumers which arise from their everyday activities in the digital sphere . Risk of harms related to use of e.g . algorithmic recommender sys tems and personalisation systems , must also be included ( such as where an individual is only shown news items which aim to maximise engagement by evoking anger and despair ) . It should trigger an appropriate classification resulting in obligations such as duties of documentation ( maintaining an audit trail ) and reporting requirements . Second , the approach leaves no room for flexibility to address other risks that may manifest themselves after an AI system ’ s deployment . Systems which for example begin to pose a high level of risk to fundamental rights78 in the course of their operation such as general 77 Recital 31 of the proposal . 78 This applies to e.g . the general -purpose AI systems disc ussed in section on intended purpose and misuse .the general -purpose AI systems disc ussed in section 4.5 on intended purpose and misuse . 18 purpose AIs ,79 can not be included in its scope by any other means tha n by amending the Regulation or its Annexes . The AI Act runs the risk of becoming out of date very quickly . Third , the ‘ high -risk ’ category is itself narrowly defined , excluding from its scope AI applications that can cause serious harm if misused . For example , AI systems in tended to be used for underwriting and pricing purposes by insurance firms when offering insurance contracts to consumers are not considered ‘ high-risk ’ AI systems under Annex III . The Low Voltage Directive80 is not included in Annex II despite AI -powered consumer devices falling under its scope . 6.1.2 . Updating the list of high -risk systems The Commission intends to annually assess whether the list of high -risk applications in Annex III needs to be updated .81 Unfortun ately , such an update is subject to strict conditions that will make it very difficult to broaden its scope . • First , the proposal limits the possibility to expand the scope to the areas already listed in Annex III ( Art . 7 ( 1 ) a ) ) . As t he AI Act should be f orward -looking , it is unreasonable to expect that AI , a technology of significant complexity and challenges , is not going to represent a high risk in other sectors than those currently mentioned in Annex III in the years to come . • Second , AI can only be added to the scope of Annex III if it poses a risk of harm to the health and safety or has an impact on fundamental rights . This does not take into consideration the potential for economic harm or negative societal impacts . 6.1.3 . Systems other than high -risk BEUC is supportive of a ‘ risk -based approach ’ but one where all AI systems ( including non high-risk systems ) are adequately regulated . For consumers harmed by an AI system , it is irrelevant whether the damage is created by a ‘ high-risk AI ’ or a lower -risk AI . All AI systems , including medium - and low -risk , should be subject to a minimum set of common rules and principles . See Section 6 for a n analysis of these principles . BEUC recommendations : - The scope of the AI Act should be broadened to adequately regulate AI systems other than those it currently classifies as ‘ high-risk ’ , such as smart meters , AI powered connected toys , virtual assistants or AI that organises what people see on social media . - All AI systems employed in the EU , including medium - and low -risk systems , should respect a set of common principles established in the AI Act ( e.g . transparency , fairness , non -discrimination ) . - The existing list of ‘ high -risk ’ applications in Annex III sh ould be expanded to include additional AI applications . For example , AI used to assess insurance premiums and AI payment and debit collection services . Also , the Low Voltage Directive should be included in Annex II . 79 See footnote 41 for examples and further discussion on general -purpose AI . 80 Directive 2014/35/EU on the harmonisation of the laws of the Member States relating to the making available on the market of electrical equipment designe d for use within certain voltage limits 81 Articles 7 ( 1 ) , 73 and 84 ( 1 ) of the proposal . 19 6.2 . Assessment methodology to determine level of risk The Regulation does not envisage a risk assessment test , or other means of classif ying an AI system , that would complement the classification of ‘ high -risk ’ . A process which determines the level of risk of an AI system ( high , medium , low ) and potential breaches of Article 5 should be introduced . This process would take the form of a self -assessment by the provider of the system . This risk assessment should take into account numerous criteria , including the possible harm s arising throughout the whole life cycle of the system for both individuals and society , the type and nature of the data used ( e.g . personal / non -personal data ) ; the harm for the environment , the intended use of the AI system ; the type of algorithmic mod el , etc . The purpose of such a risk assessment is two -fold : - First , it will enable producer s to classify an AI system under a specific risk category ( high , medium or low ) and to apply the rules/obligations that go along with that AI category before placing the AI system on the market or putting it to use . - Second , this risk assessment should also determine whether the relevant AI system breaches any of the provisions of Article 5 . If that is the case , it should be prohibited . This risk assessment should be complemented with the conformity assessment procedure mentioned in Section 8 below . BEUC recommendations : - The risk framework must be broadened to also include categories of low and medium risk , to be assigned based on a risk classification system . - The risk classification system must include a general clause that will allow classification of AI systems to assign them an appropriate risk category , from high risk to low -risk . - A preliminary self -assessment must be made mandatory to all providers of AI systems to determine the category of risk and to rule out breach es of any of the express prohibitions of Article 5 before the system is put into use at regular intervals after its deployment . - The risk classification system should include risks in terms of health and safety , fundamental ri ghts , consume r protection , economic harm , but also societ al harms and the environment . - Tiers other than high risk must also be subject to appropriate requirements , such as complying with the horizontal princip les that the AI Act should establish ( such as transparency and fairness ) 82 and maintaining an audit trail ( documentation duty ) and reporting obligations for medium -risk systems to ensure a n adequate and verifiable level of transparency . 82 See section 7 below . 20 7 . Horizontal AI princ iples and rights for consumers Article 69 of the proposal encourage s Member States to draw up codes of conduct to foster the voluntary application of the requirements set out in Title III , Chapter 2 , to AI systems other than those considered high-risk . In addition to Article 52 on transparency and Article 5 on prohibited practices , only these voluntary commitment s would be applicable to non high-risk AI . Given the significant risk and potential to cause harm that AI systems can pose for individuals and society , i t is unacceptable to pin the protection of consumers on a set of non-enforceable rules . As mentioned above , a ll AI systems should be subject to a set of binding horizontal principles ( e.g . transparency , fairness , non -discrimination ) . L egal obligations and requirements should then gradually increase according to the level of risk . The greater the potential for an AI system to cause harm , the more stringent the legal requirements should be . The horizontal principles must be translated into enforceable rights for individuals and obligations for business users and providers of AI systems . 7.1 . Transparency , explanation , and objection The AI Act establishes certain transparency obligations for ‘ high-risk AI systems ’ , including transparency towards the users of the system83 ( i.e . for those who employ it , not consumers ) , supervisory authorities , documentation requirements . Article 52 also establishes limited transparency rules for certain AI . Howe ver , there is no rule obliging providers or business users of an AI to provide an individual explanation of the specific reasons for a decision to those affected . Rules on transparency should be applicable to all AI . Consumers should always have a right to receive an explanation about AI decision -making processes that may affect them individually or , on a larger scale , have the potential to cause harm to the society . Only with the right information can a decision be dul y contested . This right should encompass transparency about the fact that automation takes place84 , about how the AI system works , e.g . how information is processed and used , and what its purpose is , for example rank offers or tailor prices . Consumers should also have the possibility to request an explanation and learn how a machine has arrived at its result . Finally , consumers should have a right to object to algorithmic decision -making and to request human intervention whenever a decision ca n have a significant impact on them . Such a right should exist regardless of whether the processing of consumer ’ s personal data was involved in the algorithmic decision making . BEUC recommendation : - The AI Act should include a gen eral principle requiring all AI systems to be used in a transparent manner in relation to citizens and consumers . 83 Art . 13 of the proposal . 84 Art . 52 of the proposal addresses this concern to a certain extent . 21 - The AI Act should enable c onsumers to always have a right to receive an explanation about AI decision -making processes that may affect them individually . - The AI Act should gi ve consumers a right to object to algorithmic decision -making and to request human intervention whenever a decision can have a significant impact on them . 7.2 . Accountability and control All AI system s must feature certain organisational and technical measures to ensure legal compliance and regulatory oversight . Article 14 establishes a system based on human oversight but , as with the majority of the proposal ’ s requirements , it applies only to high risk AI systems . Whether an algorithm -based decision i s accurate , fair , or discriminative can only be assessed if an appropriate control system is in place . There must be a minimum requirement that due control is exercised by the entity which has access to the AI ’ s database and an understand ing of how the decision criteria are selected and applied . The entity which deploys the AI system must monitor it on an ongoing basis . The higher the potential risks , the greater the accountability measures which must be put in place . BEUC recommendation : - The AI Act should include an obligation for all AI providers to regularly monitor the functioning of their AI system and assess if its respect s the obligations and rights set out in the Act . Such control should always involve humans . - The AI Act must includ e a general principle of accountability which clearly sets out that t hose entities that develop and use AI systems are responsible and must be able to demonstrate compliance with the law . 7.3 . Fairness AI systems must be developed and used in a fair and responsible way . For example , decision -making processes must be fair from the perspective of the data that is processed , the means used in the decision process , and the intent behind the result . The outcome should be fair too , so as not to lead to unjust t reatment or behaviour . The last aspect is not fully addressed by EU data protection law , which focuses on the fair processing of personal data , but not on the consequences resulting from inferences and predicti ve analysis . Questions of fairness should also be seen under the aspect of general welfare considerations . A lack of fairness can lead to even greater societal asymmetries , unequal benefits for citizens /consumers , or could even lead to certain groups of people being exposed to higher risks of poverty . The deployment of AI systems must be accompanied by an assessment of their impact on the well -being of citizens . BEUC recommendation : - The AI Act should include a general principle that all AI systems and AI practices must be fair to individual citizens/consumers and society . 22 7.4 . Non-discrimination Consumers should be protected from illegal discrimination and unfair differentiation through the use of AI systems . Through use of biased datasets and algorithms , a profiling process may conduct erroneous inferences and produce incorrect prediction s , wrongly classif ying individuals by assuming certain characteristics . When aggregated , such errors could disproportionately harm certain groups . Many situations can not be properly tackled using existing anti-discrimination laws , as these traditionally focus on discrimination based on typical protected characteristics , such as race . Instead , AI system s can use categories to differentiate between groups that are not ( directly ) related to such characteristics . A prohibition on discrimination is included in Article 21 of the European Charter of Fundamental Rights ( ECFR ) , featuring an open -ended list of parameters such as sex , race , colour , ethnic or social origin , genetic features , language , religion or belief , political or any other opinion , membership of a national minority , property , birth , disability , age or sexual orientation . The risk of d iscrimination was also the reason the EPDB and EDPS recommended a prohib ition on AI systems conducting biometric categorisation of individuals .85 The proposal should prohibit AI systems from enabling or allowing discrimination of individuals on the basis of the characteristics liste d in the Charter , on the basis of biometrics or otherwise . It should also include other types of unfair discrimination such as that based on economic factors . Without including this principle , consumers will not be adequately protected against the risk of discriminatory AI systems . BEUC recommendation : - The AI Act should impose on all AI providers and business users a general obligation of non -discrimination . This should includ e a prohibition on AI systems which lead to discrimination on the basis of the characteristics specified in Article 21 E uropean Charter Fundamental Rights , based on biometric data or otherwise , but also unfair discrimination based on economic factors . 7.5 . Safety and security All AI-powered products and services must be safe and secure throughout their lifecycle . In addition to the necessary measures in relevant product -specific legislation , such as the EU directives on product safety and NLF ( New Legislative Framework ) -based legislation , it is important that the AI Act establishes a horizontal safety and security principle for all AI systems . This is particularly important given that AI systems are increasingly integrated in all types of products and services . It is also coherent to establish such a principle in the proposed Regulation , given its prominent focus on safety related issues . BEUC recommendation : - The AI Act must include a general principle requiring AI-powered products and services are safe and secure throughout their lifecycle ( ‘ security by design and by default ’ ) . 85 EDPB and EDPS Joint Opinion , Section 33 . 23 7.6 . Access to justice and right to redress , including collective redress Greater protection in terms of transparency , safety , non -discrimination , or fairness are vital before consumers can trust AI -powered products and services . However , it is equally important to ensure that consumers ha ve access to justice if AI -associated risks materialise . Victims should have a right to redress if harm occurs . The AI Act proposal completely fails to address this point . It does not include any mechanism allowing consumers to use private enforcement tools when an AI system or practice infringes their rights or causes them harm . The proposed Regulation cites the assurance of effective redress as one of the reasons to classify the use of certain AI systems by law e nforcement authorities as high -risk.86 The Explanatory Memorandum also makes a reference promising redress to be ‘ made possible ’ in the context of infringements of fundamental right s.87 Despite this , redress mechanisms do not exist in the proposed Regulat ion and individuals are not granted any instruments to protect their rights against infringements . There is no right to complain to a national supervisory authority or an obligation for companies to provide for a complaint mechanism . For example , if a con sumer is harmed by non -compliant high-risk AI system or by an AI practice prohibited under Art . 5 , the proposed rules do not foresee any rights or mechanisms to obtain redress . In consequence , the party which is the most vulnerable to harms caused by AI ( the individual ) is also the least protected . Also , t he proposal does not allow civil society organisations , including consumer organisations , to represent harmed consumers in the exercise of their rights . An article akin to Art 80 GDPR ( Representation of data subjects ) is missing .88 In addition to allowing consumers to mandate consumer organisations ( or other civil society organisations where relevant ) to represent them individually , the AI Act must allow representative actions to be used to defend consumers ’ rights collectively . This should apply in the case of illegal commercial practices , or in obtain ing compensation in case of harm suffered by a group of consumers . Consumers must be able via consumer organisations to jointly bring a court case to obtain compensation for damage s arising from the same source ( e.g . multiple consumers harmed by the same non -compliant AI system ) . They must also be able to ask the court to issue an injunction and stop the illegal practice . Consumer organisations must therefore be able to bring collective actions against AI systems and practices . The Representative Actions Directive89 must apply in this area . In the absence of adding the AI Act to the RAD Annex I , consu mers would have no way of exercising the ir rights collectively . Given the huge asymmetry of information and the vulnerability of consumers most AI cases will involve , it is unlikely they will ever be able to bring court cases individually . Representative a ctions are their only realistic possibility to seek justice . 86 Recital 38 of the proposal . 87 Explanatory Memorandum , Section 3.5 F undamental rights . 88 Article 80 GDPR enables consumers to mandate a not -for-profit body to lodge complaint s on their behalf . 89 Directive ( EU ) 2020/1828 on representative actions for the protection of the collective interests of consumers and repealing Directive 2009/22/EC 24 To enable collective actions , the AI Act needs to be included in the Annex of the Representative Actions Directive . Such a provision was included in the recent Commission ’ s proposal for a Digital Services Act . BEUC recommendations : The AI Act should include : - A right for consumers to complain to a national authority or launch a legal action in court when an AI system or practice which affects them infringes the Regulation . This should include a right to receive compensation for material or non -material damages s uffered . - An obligation for companies to make a complaint mechanism available to consumers . Companies must be obliged to react to those complaints within a short period of time . - An a rticle allowing consumer organisations or more broadly civil society organ isations to represent individual consumers in the exercise of their rights under this regulation . They should also be allowed to act in the ‘ general interest ’ ( i.e . be able to bring forward complaints without a mandate from an individual , when they conside r that an AI system or practice is infringing the rules ) . - A provision that adds the AI Act to the Annex of the Representative Action Directive ( RAD ) , which lists the laws where it is possible to file a representative action . It must be possible to file co llective redress or injunctive actions in case of noncompliant AI . 7.7 . Reliability and robustness AI-powered products must be technically reliable and robust by design . The more autonomous machines become , the more important it is for users to trust the system ’ s reliab ility regarding their performance , accuracy , and robustness throughout their life cycle . High quality data is essential for machine learning solutions . Requirements and guidelines on data quality are necessary to ensure that AI systems p erform the intended functions . In this respect , companies must use proper training data sets and put data quality mechanisms in place to avoid biases , errors , and other irregularities . Review and validation processes should become a common industry standar d. BEUC recommendation : - The AI Act should introduce a general principle that the performance of AI systems must be reliable , accura te , and robust throughout their life cycle . 8 . Conformity assessment procedure ( Art . 43 ) The proposal relies far too much on industry self -assessing it s compli ance . With t hird party assessment s carried out in only rare cases , this approach is inadequate as it does not take into consideration the complexity of the risks posed by AI . First , consumer trust on AI powered produc ts and services is likely to be stronger if the conformity assessment procedure is carried out by a third party . 25 Also , the proposed framework will lead to conflict of interest : the entity assessing whether a certain product is in compliance with the rules is the same wh ich is trying to place the AI on the EU market as quickly as possible.90 Self-assessment should not be the default rule when it comes to conformity assessment procedure s. While this conformity assessment procedure can be used for ‘ low -risk AI ’ , it must not be used for ‘ high-risk AI ’ . BEUC recommendations : - Third party assessment should be the rule to assess the conformity of ‘ high-risk AI systems ’ . Self -assessment should only be allowed in when AI systems are not considered to be high-risk . - For high -risk AI systems , t he results of the conformity a ssessment procedure and all relevant documentation ( including the results of the self -assessment foreseen in Section 6.2 ) must be notified to the relevant market surveillance authority before the product is placed on the market and published in a publicly accessed database . - For non -high-risk AI systems , the results of the conformity assessment procedure and all relevant docum entation ( including the results of the self -assessment foreseen in Section 6.2 ) should only be notified to public authorities when they have requested it . 9 . Standards ( This chapter is co -authored by ANEC91 and BEUC ) According to the concept of the proposed AI Act , the successful application of the AI Act will heavily depend on the development and application of harmonised ( technical ) standards by the manufacturers of an AI system . Although the New Approach of 1985 and the later New Legislative Framework , bot h making reference to harmonised standards , have proved key in building the Single Market for Goods , we are far from convinced that a regulatory tool designed to facilitate market access is the correct tool to protect the fundamental rights of consumers in the domain of AI . Article 40 establishes a strong incentive for the application of standards by manufacturers : high-risk AI systems which are in conformity with harmonised standards are presumed to be in conformity with the requirements of the AI Act , thus inversing the burden of proof about the compliance of the legal requirements . Another incentive for the use of standards lies in Article 43 , according to which service providers of an AI system who have applied harmonised standards will be able to car ry out a self -assessment.92 The heavy reliance of the proposed AI Act on harmonised standards and on the standardisation process to regulate AI , and therefore in using this tool to ensure that fundamental rights and the required level of consumer protectio n are granted , raises serious co ncerns and is an essential flaw of the proposed AI Act.93 90 A survey about independent third -party testing from International Federation of Inspection Agencies ( IFIA ) shows that the compliance and safety of independently -checked products is considerably higher than for products that rely simply on manufacturer ’ s self -declaration of conformity . 91 https : //www.anec.eu/ 92 See section 8 for more information on the conformity assessment procedure . 93 Several academics and organisations have explained in detail how the use of standardisation c an undermin e the EU ’ s democratic process . See Michael Veale , Frederik Zuiderveen Borgesius ( n 8 ) ; Martin Ebers , 26 First , it is not at all clear how fundamental rights and EU values and principles can be adequately transposed into technical standards . D espite the fact that in many international fora , standardisation of AI is high on the agenda and many initiatives have been undertaken already94 , for the EU , this must not be the only way to establish trustworthy and human -centric AI . It is not the role of standards to interpret legal requirements but that of a democratic legislative procedure . Technical standards must not go beyond the implementation of mere technical aspects and enter in areas of public policy and law which require a certain level of interpretation . 95 Concretely , under the proposed AI Act , it is unclear how technical standards will help service provid ers determine , for example , what types of biases are prohibited and how they should be mitigated ( Art . 10 ( 2 ) f ) ) . The way the AI Act envisages the role of standards amounts to a de facto regulation by private bodies ( ‘ regulatory capture ’ phenomenon ) and thus a serious lack of democratic accountability . 96 An Advocate -General from the Court of Justice of the European Union has already referred to standardisation as “ legislative delegation in favour of a private standardisation bo dy ” . As we are in the pres ence of a double delegation of powers ( from the co -legislators to the Commission and then from the Commission to the European Standardisation Organisations ) in the field of fundamental rights , it is essential that the AI Act clearly ringfences the conditions under which such delegations take place.97 Secondly , civil society and consumers ’ interests are not sufficiently represented at national , European and international standardisation bodies . These bodies are essentially driven by industry . The inf luence of consumer organisations is limited because of a lack of resources and expertise at the national level , and the setup of decision -making process ( based on the national delegation principle ) . Finally , f rom the European perspective , the increasing push of industry , reflected by the National Standardisation Bodies members of the European Standardisation Organisations , to set a single , globally -relevant standard is encouraging a convergence between European and international standards . Given the primacy of international standardisation over the regional , the consequence is that more standards for application within Europe are being drafted or revised at international level . However , the participation of consumers and civil society is even more limited in international standardisation discussions , while there is a Standardizing AI – The Case of the European Commission ’ s proposal for an Artificial Intelligence Act , in : Larry A. DiMatteo/Michel Cannarsa/Cris tina Poncibò ( eds . ) , The Cambridge Handbook of Artificial Intelligence : Global Perspectives on Law and Ethics , pending for publication , 22 pages , Cambridge University Press 2022 ; 94 https : //ethicsinact ion.ieee.org/p7000/ 95 As highlighted by Martin Ebers ( n 110 ) “ The standardization of AI systems is not a matter of purely technical decisions . Rather , a series of ethical and legal decisions must be made , which can not be outsourced to private SDOs [ Standa rd Developing Organisations ] , but which require a political debate involving society as a whole. ” ANEC comments on the European Commission proposal for an Artificial Intelligence Act ( ANEC -DIGITAL -2021G-071 ) 96 Ebers ( n 110 ) 97 Opinion of AG Campos Sanchez -Bordona in case C -613/14 James Elliott ECLI : EU : C:2016:63 [ 55 ] James Elliott ECLI : EU case C‑613/14 : ‘ it must , moreover , be noted that while the development of such a harmonised standard is indeed entrusted to an organisation governed by private law , it is nevertheless a necessary implementation measure which i s strictly governed by the essential requirements defined by that directive , initiated , managed and monitored by the Commission , and its legal effects are subject to prior publication by the Com mission of its references in the ‘ C ’ series of the Official Journal of the European Union . 27 strong participation of countries that do not share European values and principles , especially in AI standardisation . The use of standardisation as proposed in the AI Act is unacceptable in a fie ld as sensitive and fundamentally important to our society , and to the fundamental rights of individuals , as AI , particularly bearing in mind the EU ’ s objectives for the digital transition . More detailed and clearer requirements are needed in the AI Act fo r standards to function as ‘ technical specification s for repeated or continuous application ’ 98 and not to have the role of determining the level of protection of fundamental rights and thus possibly restricting such rights . BEUC recommendations : - Harmonised standards must not be used to define or apply fundamental rights , legal or ethical principles . Their use should be limited to implement technical aspects . In this regard , a standard should , for example , not be used to determine what types of biases are pr ohibited under Art . 10 ( 2 ) f ) . Instead , the AI Act must include more detailed rules about the requirements applicable to ‘ high -risk AI ’ , including rules on discrimination . - As standards will play an important role to detail the essential requirements listed in the AI Act , the governance system of the standardisation process must be changed significantly . Consumer organisations must be systematically involved in standardisation . Public authorities must also provide the political and financi al frameworks that permit the participation of all stakeholders – including consumers and broader societal interests . A general change of the standardisation process can not be done overnight , but new Articles should be added to the AI Act to cater for such participation . - Given that p ublic authorities have also withdrawn from many standardisation activities to the detriment of the public interest , we call on authorities to become more engaged in standardisation and support consumer participation in it . 10 . Enforcement 10.1 . Reporting ( only ) of serious incidents and of malfunctioning of high -risk systems ( Article 62 ) Under Article 62 , providers of high -risk AI systems placed on the European Union market are to report to market surveillance authorities a “ serious incide nt or any malfunctioning of those systems which constitutes a breach of obligations under Union law intended to protect fundamental rights ” . It should be clarified in a Recital that a breach of consumer rights would fall under the obligation to report under Article 62 . As such , a company should be obliged to report a faulty AI used in an energy distribution grid for example , which can have a serious impact on the finances and wellbeing of consumers by inadvertently cutting off their energy supply or overcharging them . An important aspe ct that i s also missing in Article 62 is the obligation to notify those subject to the AI system , i.e. , consumers , regarding a serious AI incident or malfunctioning AI . 98 Art 2.1 Regulation 1025/2012 on European Standardisation , 28 The establishment of a culture of information -sharing and cooperation is key to increas ing trust in the market and ensuring a high level of consumer protection . BEUC recommendations : Article 62 must include the following : - A Recital must clarify under Article 62 the obligation to report a serious incident includes breaches of consumer rights . - Article 62 must ensure that c onsumers affected by a serious AI incident or malfunctioning sho uld be immediately informed about it . 10.2 . Governance structure and enforcement ( Article 63 ) The proposed governance structure rests mainly at national level with market surveillance authorities . While this methodology has been regularly used in New Legislative Framework laws , its replication in the AI Act raises several concerns in relation to the obligations , competences and powers of the different actors involved and the different processes envisaged . First , there will be many different national authorities r esponsible for overseeing the application of the AI Act . For example , for AI systems listed in Annex II – Section A , the authorities responsible for that product safety legislation will be competent.99 For some AI systems listed in Annex III , Member States will have to designate a competent authority . This raises the following concerns : • The enforcement of the AI Act will greatly depend on the resources allocated to these different authorities . As it stands , data and consumer protection authorities may only play a marginal role due to the limited scope of Annex II and the competence under NLF based frameworks . The governance structure should clarif y that w here consumers interest ( other than safety ) are affected , or consumers are concerned as data subjects , these authorities should have access to relevant documentation and can require testing and investigations . • However , the myriad of competent authorities under the AI Act ( e.g . consumer protection authorit ies , telecommunications authorit ies , etc.100 ) will make it harder to ensure that each of these authorities has the adequate financial and human resources or the technical expertise to address the challenges of AI systems ( Article 59 ( 4 ) of the proposal ) . • To address possible shortcomings in terms of resources , enforcement at national level should be reinforced on a technical level by a highly specialised body of technical experts established by the Commission . This body should be tasked on a case-by-case basis with conducting expert evaluation s and assessment s of AI systems on request of a national authority conducting an investigation , in order to issue an opinion that is non -binding on the national authority . For cost efficiency , a single such body could be established at EU level and assist national authorities in the technical aspects of their investigations , to the extent indicated in their specific requests . 99 Article 63 ( 3 ) of the proposal . 100 As an example , these are the authorities competent for the application of the Radio Equipment Directive , which is one of the legal acts mentioned in Annex II – Section . 29 • The number of authorities involved in the application of the Regulation rais es concerns regarding the effectiveness and consistency of the enforcement and oversight system when it comes to the cooperation between the different competent authorities and supervisory authorities at national level and cross borders . This is why it is very important that enforcement cooperation between national authorities becomes an official task of the AI Board ( see section 10 ) . Second , while the Commission plays a central role if a national supervisory authority notifies its intention to adopt measur es concerning an AI system in its territory101 , the Commission has no powers to proactively take the lead in case of inaction by national authorities . In this regard , the procedure foreseen in Article 66 should not be limited to actions started by Member St ates ’ authorities . The European Commission should be able to start an evaluation procedure about an AI system according to Article 66 whenever ( i ) it has sufficient reasons to believe that that an AI system presents a risk , ( ii ) no market surveillance authority started an investigation under Article 65 ( 2 ) and ( iii ) the AI system affects consumers in more than one Member State . Thirdly , as already mentioned by other stakeholders102 , one point that has not been addressed in the proposal is the possible overlap between the competences of national supervisory authorities under the AI Act and the competences of other authorities , such as the data protection authorities under the GDPR . The AI Act must clarify to whom consumers can compla in in situations of overlap and who takes the lead in the enforcement action . There must be a clear and coherent oversight and enforcement structure to guarantee cooperation among all relevant authorities and the effective and consistent protection of consumers across the EU . Crucially , the precautionary principle103 allows market surveillance authorities to take temporary and preventive measures in the absence of proof of harm to consumers . As such , we deplore that this fundamental principle is absent from the AI Act . This principle should be applied where scientific evidence is insufficient , inconclusive or uncertain , and preliminary scientific evaluation s indicate that there are reasonable grounds for concern . We consider it essential to ensure that techn ologies that pose significant harms for individuals and society are not deployed until they are tested and certified . BEUC recommendations : - Enforcement at national level should be reinforced on a technical level with the creation of a highly specialised body of technical experts designated by the Commission . Such a body should assist national authorities and the Commission in the technical aspects of their investigations and have the competence to issue non binding opinions about specific cases brought up by the national authorities . - The procedure foreseen in Article 66 should not be limited to actions started by Member States ’ authorities . The Europea n Commission should be able to start an evaluation procedure about an AI system under this provision whenever ( i ) it has sufficient reasons to believe that that an AI system presents a risk , ( ii ) no market surveillance authority started an investigation un der Article 65 ( 2 ) and ( iii ) the AI system affects consumers in more than one Member State . 101 Articles 65 ( 5 ) and 66 of the proposal . 102 https : //www.accessnow.org/eu -minimal -steps -to-regulate -harmful -ai-systems/ 103 https : //eur -lex.europa.eu/legal -conte nt/EN/TXT/HTML/ ? uri=CELEX:52000DC0001 & from=EN 30 - The AI Act must clarify that market surveillance authorities carry out their activities based not only on proportionality but also on a precautionary approach . 10.3 . Acce ss to data and documentation ( Art . 64 ) It is important that third parties such as consumer organisations or other civil society organisations have access to relevant documentation created in the context of this proposal by the AI provider ( including the re sults of the conformity assessment s as well as all relevant documentation foreseen in section 8 ) . BEUC recommendation : - Article 64 should grant consumer organisations the right to access relevant documentation created in the context of the AI Act by the provider and request the authority to carry out ad -hoc audit s of a particular AI system . 10.4 . Procedure for dealing with AI pre senting a risk at national level ( Art . 65 ) When it comes to non -high-risk systems , the application of Article 65 lacks clarity and consistency . Article 65 ( 1 ) appears to establish a framework for assessing risk at national level , referencing Article 3 ( 19 ) of Regulation ( EU ) 2019/1020 on market surveillance , offering that a ‘ risk ’ is to be “ understood as a product presenting a risk defined in Article 3 , point 19 of [ Market Surveillance Regulation ] insofar as risks to the health or safety or [ … ] fundamental rights of persons are concerned ” . This formulation is unclear but it would appear its role is to enable a generally applicable risk assessment104 but limited to health , safety and fundamental right s.105 According to Article 65 ( 2 ) , when a market surveillance authority has ‘ sufficient reasons ’ to believe that any AI presents such a risk to health or safety or to the protection of fundamental rights , assessed in accordance with Article 65 ( 1 ) , it must carry out an evaluation of the AI in respect of its compliance with all the requirements and obligations of the AI Act . However , the reference to the compliance with the requirements and obligations of the proposal is misleading as the Regulation establis hes very few obligations for non -high risk AI systems . The only area where a non -high-risk AI system would be covered by this provision is a breach of the Article 5 ‘ black list ’ ( which will be extremely rare ) or transparency obligation of Article 52.106 In all other cases , the provisions of Article 65 ( 2 ) only allow the national authority to launch an investigation into high -risk AI systems . Also , to ensure that the evaluation carried out by the market surveillance authority does not significantly delay the enforcement process , Article 65 ( 2 ) should clarify that th is evaluation should be concluded without undue dela y and should never take longer than 6 months . 104 i.e . going beyond that considered reasonable and acceptable in relation to its intended purpose or under the normal or reasonably foreseeable conditions of use of the product concerned , including the duration of use and , where applicable , its putting into service , installation and maintenance requirements - Art . 3 ( 19 ) , Market Surveillance Regulation . 105 See also the comments on structure of risk in Section 4 106 This applies to AI systems intended to interact with natural persons , emotion recognition systems and biometric categorisation systems , and AI systems used to generate or manipulate image , audio or video content ( Article 1 ( c ) ) . 31 When it comes to the notification to other Member States of the application of corrective measures to an AI system , in addition to the rules foreseen in Article 65 ( 2 ) ,107 market surveillance authorities should be obliged to inform other Member States when , during the course of the ir own evaluation , there is cre dible evidence that the AI system under investigation may also harm consumers in other Member States . The proposal does not put in place a mechanism requiring early cooperation between national authorities from different MS when investigating suspicious A I systems . If a national authority starts an evaluation of an AI system under Article 65 ( 2 ) , the AI Act does not foresee any obligation for this authority to notify it to the national authorities from other Member States . There could then be a situation w here authorities from different Member States are investigating the same AI system in parallel without any cooperation or coordination . While in practice this issue may be addressed by the European Artificial Intelligence Board , it is important to ensure a coherent and consistent application of the AI Act by specify ing in Article 65 ( 2 ) that authorities which start an investigation into a suspicious AI system must inform their counterpa rts in the other Member States within the AI Board . Finally , c ivil society organisations should have the right to request an evaluation of AI systems by notified bodies when there are reasonable indications that the AI system causes significant harm . BEUC recommendations : - Civil society organisations must have the right to request an evaluation of AI systems by notified bodies when there are reasonable indications that the AI system causes significant harm . - The wording “ sufficient reasons to consider ” under Article 65 ( 2 ) should be clarified to avoid misunderstanding and uneven application at national level . In any case , a complain t from consumer s should be considered a s sufficient reasons to launch an investigation by the market authorities under Article 65 ( 2 ) . - To ensure that the evaluation carried out by the market surveillance authority does not significantly delay the enforcement process , Article 65 ( 2 ) should clarify that this evaluation be concluded without undue delay and never take longer than 6 months . - To ensure a coherent and consistent application of the AI Act , the Act must specify in Article 65 ( 2 ) that authorities which start an investigation into a suspicious AI system must inform their counterparts in the other Member States within the AI Board . - As indicated in section 6 , the requirements of Article 6 for high -risk systems must be complemented with a common framework of rules and obligatio ns applicable to AI systems presenting medium or low risk , along with a flexible system that would allow authorities and regulators to actively assign risk categories to AI systems . Once this is achieved , the wording of Article 65 ( 2 ) will be clearer as it will allow actual assessment s of risk presented by an AI system . 107 According to Article 65 ( 5 ) , market surveillance auth orities shall inform the Commission and other Member States about any corrective measures applicable to a non -compliant AI system . 32 - The risks mentioned in Article 65 ( 1 ) , as in the case of the entire Regulation , should not be limited to health , safety and fundamental rights108 but also include economic harm or societal impacts . - The wording of Article 65 ( 1 ) must leave no doubt regarding which parts of Article 3 ( 19 ) of Regulation ( EU ) 2019/1020 on market surveillance are included by reference . 10.5 . Union Safeguard Procedure ( Article 66 ) Under Article 66 , w hen objections are raised by a Member State against measures taken by another Member State or when the Commission conside rs that certain measures are contrary to EU law , the Commission shall enter into consultation with the relevant Member States and start an evaluation . BEUC supports the European Commission ’ s involvement in these types of cross -border procedures . We regret however that the Commission was only given the power to accept or refuse the measures taken by a Member State . The Commission should also be able to propose alternative measures to those taken by the authority of the Member State that initiated th e procedure under Article 65 , after seeking the advice of the AI Board . BEUC recommendation : - The Commission should be able to propose alternative measures to those taken by the authority of the Member State that initiated the procedure under Article 65 , after seeking the advice of the AI Board . 11 . AI Board ( Articles 56 – 58 ) The proposal establishes a European Artificial Intelligence Board ( ‘ AI Board ’ ) composed of representatives of the Member States , the European Data Protection Supervisor and the European Commission . The Board contribute s to effective cooperation between national author ities . The autonomy and powers of the European AI Board , comprised of high -level representatives of the national supervisory authorities and chaired by the Commission , seem limited regarding the application and enforcement of the AI Act . One of the AI Boa rd ’ s specific tasks should be to ensure that market surveillance authorities cooperate in the enforcement of the AI Act . The adequate functioning of Articles 65 ( Procedure for dealing with AI systems presenting a risk at national level ) and 66 ( Union Safeg uard Procedure ) , a crucial part of the enforcement mechanism of the AI Act , will heavily depend on a smooth cooperation between the different national supervisory authorities . It is therefore of the outmost importance that the AI Board discusses and assess es the functioning of these provisions and if there is a need to improve their application . 108 See comments in Section 4.3 . 33 BEUC recommendation : - The tasks of the AI Board must include ensur ing that market surveillance authorities actively cooperate in the enforcement of the AI Act , parti cularly in the application of Articles 65 and 66 . 12 . Interplay with other areas of EU laws 12.1 . General Data Protection Regulation ( GDPR ) Existing data protection legislation , in particular the GDPR , plays an important role in the regulation of AI . In this regard , a s pointed out by the EDPS and the EDPB , the Act must explicitly state that the GDPR applies to any processing of personal data falling within the scope of the AI Act , and that its provisions are without prejudice to the rights and obligations established under the GDPR . However , the GDPR does not provide sufficient protection in an AI context . Key rights for consumers , such as the right to contest an algorithmic decision109 , should not depend on the processing of personal data as this is the case under EU data protection rules . 12.2 . Product Liability Directive ( PLD ) In parallel and as a complement to the AI Act , the European Commission is considering updating civil liability rules to cope with the chall enges brought by AI , which include limited predictability , autonomous behaviour , continuous adaptation , complexity and opacity making difficult for claimants to claim compensation in case of harm . There is still uncertainty about the instrument that the Co mmission will propose . This may either be done through a revision of the Product Liability Directive and/or a specific instrument laying down civil liability rules for AI based on their risk profile . The European Commission published its inception impact a ssessment on the review of the Product Liability Directive on 30 June and the consultation period is now ongoing . The Commission is expected to publish its proposal in the fourth quarter of 2021 , or the first quarter of 2022 . BEUC strongly supports a revi sion of the Product Liability Directive . Adopted 36 years ago , this legislation is now outdated to cope with the problems arising out of ( in particular ) in the digital context.110 During this time , many ‘ offline ’ products have since been replaced by digit al goods . In order to fight the strong information asymmetries preventing consumers from claiming compensation , there should be a reversal of the burden of proof for plaintiffs . 12.3 . Medical Devices Regulation The proposal e xplains that the compliance of a high-risk AI system with its requirements will also be reviewed as part of the conformity assessment under the existing Medical Device Regulation ( MDR ) and will be ‘ CE ’ marked before being placed on the market . Therefore , the CE marking wi ll be an indication of the product ’ s compliance with the MDR as well as with the AI Act . 109 Article 22 General Data Protection Regulation . 110 https : //www.beuc.eu/publications/product -liability -20-how-make -eu-rules-fit-consumers -digital -age/html 34 However , the AI Act proposal does not spell out the interplay between its requirements and the regulatory obligations deriving from the MDR . On one hand , the MDR imposes detailed requirements regarding the development and marketing of medical devices at each stage – design and development , risk management , quality management , post marketing activities and so on . On the other hand , t he proposed AI Act specifies the provider ’ s and ( business ) user ’ s obligations at these stages as well . It remains unclear how these two sets of regulatory requirements will combine into one regulatory process without overlapping or creating interpretation issues . There needs to be f urther examination and perhaps a tailoring of regulations to implement the proposed AI regulatory requirements properly in the medical field . 12.4 . AI and consumer law – Addressing digital assymetry AI systems are instrumental in creating and perpetuating an on going state of asymmetry in the digital consumer -trader relationship . AI leads to further consumer disempowerment . The trader has a real -time influence on the environment the consumer finds himself , including its choice architecture . Traders also have acc ess to the consumer ’ s detailed personal profiles , including decision -making biases and pressure points . AI-driven algorithmic personalisation of interfaces and content adds a level of efficiency to user monetisation and conversion rates.111 This translates into a new position of vulnerability for consumer s that is both structural ( owing to the structure of digital markets which prevents consumers from interacti ng with market players on an equal footing ) and architectural ( due to the way interfaces are desig ned and operated ) . This imbalance of power and the embedded vulnerability are referred to in current academic debate as digital asymmetry112 and must be addressed through a review of the EU consumer law acquis . A review must introduce new measures such as a modernisation of the concepts of ‘ fairness ’ and ‘ vulnerability ’ , the expansion of blacklisted practices to include ‘ digital ’ practices and the introduction of a reversal of the burden of proof , placing the onus on the trader to prove their compliance wit h relevant legislation.113 12.5 . AI and sustainability Digitalisation and AI can help the urgently needed green transformation and the move towards more global sustainability . But they can also act as a ‘ fire accelerant ’ if not managed properly . To this end , the connection between the carbon footprint and computer processing is another of the essential considerations to be made when regulating AI . However , as explained previously ,114 one of the drawbacks of the risk classification adopted in the proposal is that no assessment framework is proposed that would allow authorities to evaluate the impact of an AI system on fundamental rights , including the right to a high level of environmental protection .115 Empirical findings have shown that digital technologie s contribute to 4 % of overall greenhouse gas emissions , a number expected to double by 2025 .116 Other studies show that training a single AI model emits carbon dioxide in amounts comparable to that of five cars over their lifetimes This problem must not be underestimated , particularly in the 111 Helberger N. Lynskey O. Micklitz H.-W. Rott P. ( 2021 ) Structural a symmetries in digital consumer markets , BEUC , https : //www.beuc.eu/publications/beuc -x-2021-018_eu_consumer_protection.0_0.pdf , at 106 . 112 Id , at 46 . 113 Id , at 48 , 75 . 114 See section 4.3 on risk . 115 Article 37 of the EU Charter of Fundamental Rights . 116 Maxime Efoui -Hess , Climate Crisis : The Unsustainable Use of Online Video , Shift Project ( 2019 ) . 35 context of the European Green Deal . In this sense , there needs to be a general rethink of political strategies to ensure coherence between sustainability and digital policy objectives . For example , it is contradictory to push for a massive use of IT systems that require infrastructures that are potentially very energy/carbon intensive without adequate measures to control their environmental impact . 12.6 . AI and financial services Insurers have always collected consumer data to inform their pricing and underwriting decisions when offering insurance contracts . However , the growing use of Artificial Intelligence and Big Data Analysis is transforming the sector , allowing insurers to process larger sets of data about consumers ( incl uding new , non-traditional data ) and to price and underwrite insurance contracts accordingly . Increasingly granular risk assessment capabilities will help insurers more easily identify the high-risk consumers most likely to make a claim on their insurance policy . Hyper personalised risk assessments , coupled with increasingly personalised prices in the insurance sector , could in due time leave certain groups of consumers uninsurable or be at risk of no longer being able to afford the premiums charged by ins urance firms . This could lead to their financial exclusion . Evidence is already beginning to emerge . BEUC ’ s Dutch member Consumentenbond , recently report ed a significant increase in home insurance premiums ( up to 30 % ) for groups of Dutch consumers , as insurers expanded their reliance on alternative data provided by big data firms . In some cases , there was clear evidence that the big data insurers relied on was false.117 As insurers expand their reliance on Big Data and AI , consumers could be at risk of biased and discriminatory outcomes.118 Insurers are currently prevented from basing pricing decisions based on certain protected characteristics , such as race and/or gender . However , other types of data could feasibly act as proxies that can be closely correlated with these protected characteristics ( e.g . postcodes signalling ethnicity or occupation categories signalling gender ) . In the UK , car insurance comparison websites reported higher premiums for consumers with names suggesting that the y are from ethnic minorities . As a result , ethnic minorities often significantly overpaid for their car insurance premiums.119 Likewise , in the US , evidence emerged of consumers in minority neighbourhoods paying higher insurance premiums compared to other n eighbourhoods with similar accident rates.120 Consumers should be protected from high risks , and insurers should ensure that the AI systems they rely on do not pose unacceptable risks to consumers . AI systems used for the purpose of pricing and underwriting decisions ( e.g . to assess the eligibility or set the premium for a consumer ’ s car or life insurance policy ) should be listed as a high -risk activity in the AI Regulation . 117 https : //www.consumentenbond.nl/nieuws/2018/premies -woonhuisverzekeringen -stijgen -door-gebruik -bigdata 118 BEUC addressed this topic in a re cent paper : https : //www.beuc.eu/publications/beuc -x-2020039_beuc_position_paper_big_data_and_ai_in_insurances.pdf 119 https : //www.bbc.com/news/business -43011882 120 https : //www.propublica.org/article/minority -neighborhoods -higher -car-insurance -premiums -white -areas same -risk 36 12.7 . AI and trade A recent study from vzbv121 , BEUC German member , found that current EU trade negotiations might significantly restrict the EU ’ s ability to regulate in the field of AI in the future , particularl y with regard to independent assessments and audits . The study finds that trade rules could impede future EU rules on transparency , certification and accountability . Also , p otential rules on the non -disclosure of source code currently under discussion in the World Trade O rganisation ( WTO ) would hinder effective transparency provisions within the AI Act . It is paramount that EU law makers must adopt trade rules that do not prevent future rules on trans parency and accountability . Also , p otential rules on the non -disclosure o f source code currently under discussion in the WTO must not impact an effective transparency , enforcement , monitoring and independent assessments of AI systems under the AI Act . 121 https : //www.vzbv.de/sites/default/files/downloads/2021/01/21/20 -01-19_vzb v_source_code_and_ai.pdf 3 This publication is part of an activity which has received funding under an operating grant from the European Union ’ s Consumer Programme ( 20 14-2020 ) . The content of this publication represents the views of the author only and it is his/her sole responsibility ; it can not be considered to reflect the views of the European Commission and/or the European Innovation Council and SMEs Executive Agency ( EISMEA ) or any other body of the European Union . The European Commission and the Agency do not accept any responsibility for use that may be made of the information it contains .
