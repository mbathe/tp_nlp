june governance artificial intelligence finance discussion document authors laurent dupont olivier fliche yang fintech hub acpr table contents executive summary introduction design development principles evaluation algorithms governance algorithms public consultation appendices technical scope detailed description exploratory works explainability interpretability technical aspects explainability review explanatory methods review potential attacks model bibliography thanks front page csirac one first five computers put service supervised trevor pearcey archival photograph november executive ummary discussion document follows upon work led acpr artificial intelligence since march initial report first public consultation acpr launch along actors financial ctor exploratory works aiming shed light issues explainability governance mainly understood machine learning composed meetings technical workshops covered three topics anti laundering combating financing terrorism aml internal models specifically credit scoring customer protection two focal areas emerged works namely evaluation governance algorithms evaluation four interdependent criteria evaluating algorithms tools finance identified appropriate data management fundamental issue every algorithm performance regulatory compliance conditional upon ethical considerations fairness processing absence discriminatory bias taken account regard performance algorithm addressed using variety metrics range metrics available sufficient assessing accuracy virtually algorithm used finance according technical functional criteria however sometimes necessary balance selected criteria desired degree explainability stability describes robust resilient algorithm behaviour turns lifecycle due care must taken guarantee generalizability production data continuously monitor risks model drift deployed production explainability close cousin algorithmic transparen inte rpretability put context order define actual purpose explanation specific result algorithm behaviour may prove necessary end users whether customers internal users cases serve tasked compliance governance algorithm provided explanation thus aims either inform customer ensure consistency workflows wherein humans make decisions facilitate validation monitoring model therefore introduce four levels explanation observation justification approximation replication order clarify expectations terms explainability finance depending targeted audience associated busine risk governance incorporating business processes finance inevitably impacts governance recommend particularly focus early algorithm design phase following aspects integration business processes component fulfil critical function dint operational role associated compliance risk engineering process follow well defined methodology throughout lifecycle algorithmic design monitoring production sense reproducibility quality assurance architectural design auditability automation interactions require particular kind explainability intended either internal operators need conf irm reject algorithm output customers entitled understand decisions impacting commercial offers made besides processes involving often leave room human intervention beneficial even cessary also bears new risks new risks include introduction biases explanation algorithm output stronger feeling engaging one responsibility contradicting algorithm confirming decisions security outsourcing models exposed new kinds attacks furthermore strategies development outsourcing skills outsourcing external hosting undergo careful risk assessment generally third risks aluated initial validation process process must often designing algorithm intended augmenting altering existing process instance governance framework applicable business line may cases maintained cases updated putting component production continuous validation process governance algorithm also presents challenges deployment production example cont inuous monitoring requires technical expertise tools order ensure aforementioned pri nciples followed time appropriate data management predictive accuracy stability availability valid explanations audit audit internal external systems finance exploratory works led acpr uggest adopting dual approach first facet analytical combines analysis source code data methods possible ased standards documenting algorithms predictive models datasets second facet empirical leverages methods providing explanations individual decision overall algorithm behaviour also relies two techniques testing algorithm black box challenger models comp model test benchmarking datasets curated auditor multi approach suitable internal auditors superviso authority however latter faces specific challenges due scope mission order effectively audit systems need build theoretical hands expertise data science developing toolkit specific purpose supervision public consultation analysis presented document subject public consultation objective submit financial actors concerned parties researchers service solution providers control auth orities etc guidelines sketched herein feedback broadly gather useful comment including supervisory authorities best practices introduction methodology following initial work december public consultation role artificial intelligence finance acpr fintech hub undertook exploratory works small number voluntary financial institutions order shed light issues explainability governance sector exploratory works resulted avenues reflection presented document technological spectrum considered detailed appendix financial actors evidenced apcr first public consultation particularly eager regulatory guidance pertaining indeed technology generates opportuni ties well risks operational otherwise one supervisory authority tasks provide guidance along practical implementation guidelines aim balancing freedom innovation regulatory compliance responsibl risk management exploratory works main goal exploratory works produce lines thought three topics related acpr main missions detailed follows topic fintech hub conducted deep analysis voluntary actors way case meetings present algorithms question along main explainability governance challenges case primary workshops technical phase involving ata scientists sides exchanging relevant methods tools including review sessions source code models developed actor workshops briefly described hereafter appendices provide detailed anonymized descrip tions topic anti laundering combating financing terrorism aml cft topic key issue whether improve financial transaction monitoring either complementing replacing traditional threshold mechanisms business rules tackle challenge workshop participants introduced algorithms able generate alerts addition traditional rule systems already place alerts directly sent review level streamlines secures alert review workflow resulting operational gain proven governance two key aml proces ses studied workshops namely declarations suspicion freezing assets needs ght human intervention requir processes continuous monitoring required algorithms see also cambridge judge business school level charge compliance see section explanations typical organization internal controls topic internal models banking insurance topic key issue determine conditions design internal models rather studying internal models whole workshops focused credit granting models related insofar scores produced models also used build risk classes rwas risk assets computed workshops involved two actors banking group designs implements credit scoring models internally consulting firm provides development platform hybrid models tested case computation probability default application scenarios demonstrated troducing impacts governance initial validation process becomes technically monitoring tools become requirement internal review explanatory methods integrated continuous oversight well audit processes topic customer protection topic key issue ensure algorithms used context non insurance product sales always take account client best interests model studied topic aimed producing prefilled quotes home insurance products use case involved two main compliance requirements fulfilling duty advise properly inform customer offering non insurance product consistent requirements needs expressed customer design development principles suggest four evaluation principles algorithms models appropriate data management performance stability explainability objectives represented principles need mutually balanced simultaneously maximizing usually impossible viewed cardina points compass guide design development algorithm appropriate data management principle evaluating compliance algorithm implementation requires covering large spectrum requirements core compliance requirements lies proper management data stage design implementation process algorithm described section input data defined data fed algorithm conception nput data comprises reference data training data test evaluation data proper management input data sometimes governed sector specific regulation example data completeness data quality requirements banking sector pre scribed prudential norm bcbs data governance dai essential function within financial organization number data business processes setting proper data governance algorithm work data sources fed inappropriately managed example fragmentary anecdotal insufficiently durable tampered organization control lifecycle basel committee banking supervision standard number international standard whose subject title principl effective risk data aggregation risk reporting evaluation system also needs take account operations performed input data prior machine learning phase may impact resulting model performance example training data well ethical acceptability example excluding protected variables training data finally evaluation also include operations performed output predictions decisions model produced algorithm post may significant impact well case methods aiming remove reduce discriminatory already trained models example cancelling dependency predictions made probabilistic model sen sitive variables kamishima regulatory compliance regulatory compliance often includes requirements two kinds compliance regulation pertaining data protection individual privacy starting gdpr europe taking account gulatory requirements specific domain use case example insurance prohibited steer sales process based customer capacity pay offer needs least consistent demands needs expressed customer driven maximization sales revenue insurance products compliance first category requirements assessed well methods undesired biases detected prevented suppressed operating aforementioned stages input data pre post dependency sensitive variables whether explicitly implicitly present data suppressed etc second category compliance requirements sector often goes beyond scope data management case obligation means performance obligation aml call suitable explanatory methods another example illustrate stakes sector regulatio detail system put production insurance organization aims target high prospective customers marketing campaign regarding multi insurance contract idd insurance distribution directive european directive introduced principles close equity principle described next section insurance product distributors always act honestly fairly professionally accordance best interests customers therefore allowed customer targeting condition criteria used based needs fulfilled polysemy term bias noted sometimes refers statistical objective characteristic predictor estimator times unfair unequal treatment whose olarity importance subjective ethical social nature presence statistical bias may lead fairness bias neither sufficient necessary condition issue discriminatory biases specific either risk exists statistical model documented literature redlining banking economy however risk amplified use algorithm addition detection mitigation techniques risk also product customer capacity challenge thus correctly appreciate prospective customers insurance needs much difficult evaluate algorithm human used requires using larger datasets breadth depth turn generates increases data risks implic correlations capacity subscribe difficult detect generally undesired biases often latent see next section short using implement customer targeting system marketing conditioned upon mastering risks deploying tools detect mitigate ethics fairness besides constraints stemming sector cross regulations ethical issues lie core ever usage business rocesses impact individuals groups people issues include social ethical concerns broadest sense particularly questions fairness raised automated computer decision process ethics guidelines published european commission european commission high expert group illustrate importance ethical issues blurred boundaries share principles described section human agency oversight technical robustness safety privacy data governance transparency diversity non fairness societal environmental well accountability guidelines underline broad spectrum challenges related ethics fair ness specifically analysis biases especially discriminatory nature active research domain schematically comprises following stages carefully defining constitutes problematic bias whether classifica tion bias prediction bias undesired statistical bias already present input data metrics enabling characterize quantify biases including via explanatory methods kamishima determining extent biases presen data reflected reinforced algorithms lastly mitigating biases whenever possible either data level algorithm level exploratory works conducted acpr along broader analysis financial sector showed bias detection mitigation early stage industry emphasis put internal validation systems regulatory compliance without pushing analysis algorithmic fairness case traditional methods particular risk reinforcing biases tends neglected blind spot however reflects relative lack maturity industry introduced primarily indeed conception insurance product start defining target market based characteristics group customers whose needs fulfilled product less business processes bear little ethics fairness risks thus anticipated progressive industrialization dditional use cases sector benefit currently active research topics performance principle performance algorithm typically assessed using two types metrics predictive performance metrics instance auc area curve alternatively score applied algorithms predict credit default risk physical mora person metrics categorized kri key risk indicators business performance metrics categorized kpi key performance indicators two points attention metrics consistency algorithm object ives compatibility compliance importantly algorithmic performance algorithm standalone objective needs particular weighed explainability principle subsequent sections show adequate explanation level depends given scenario multiple factors explanation recipients choice explanation level turn induces constraints technological choices notably simplicity selected algorithm overview fundamental trade driving choices given appendix stability principle stability principle consists ensuring algorithm performance main operational characteristics consistent time expectations terms stability important instance highly likely maximizing sales revenue insurance products inappropriate metric algorithm used part sales process might indeed introduce algorithmic process kind conflicts interest regulation precisely aims preventing appropriate data mana gement data processing oroughly documented design stages algorithm source code performa nce resulting model etc documentation enables risk assessment areas regulatory compliance ethics implementation tools detecting mitigating undesired biases need algorithmic performan performance metrics algorithm carefully selected evaluate technical efficacy algorithm alternatively business objectives inherent trade algorithm simplicity efficacy taken account case dimension data order make predictor variable tends much larger traditional predictive decisional three major sources instability herein identified algorithms production financial sector sel dom take account instability sources neither individually overall effect may due relative lack maturity engineering operational processes thus subject change future however instabili risks neglected since generate significant operational compliance risks hence mitigation methods suggested following temporal drift firstly stability model implies absence drift time essential since distribution input data might deviate sufficiently degrade model performance well characteristics compliance aspects absence bias especially periodicall temporal drift detected using somewhat classical monitoring alert generation mechanisms however built upon appropriate drift indicators well infrastructure key point regard tempo ral drift model often linked evolution training database hence first stage designing drift monitoring tool even taking account data processing consists detecting structural changes input generalization stability model also understood robustness sense generalization power confronted new lack generalization power may gone undetected model validation example test validation datasets though dissociated training data may testing testing almost inevitably differ real data fed model production lack generalization power detected least partially remedied model design parameterization stages however resulting model subjected continuous monitoring like temporal drift ultimately inevitable model performan never guaranteed generalize sufficiently well previously unseen data lastly model whether periodically quasi basis solve instability issues since results leas non decisions given input even one big data main characteristics situation techniques neural networks particularly shine generally speaking predictive power classification model shown increase number variables certain point degrades phenomenon called hughes peak koutroumbas associated curse dimensionality dimensionality reduction actually common concern shaw generalization power pred ictive bias two key criteria balance designing tuning predictive model generalization inversely proportional model variance hence arbitrage referred bias trade low bias usually associated high performance training test data whereas low variance implies model generalizes well new data subsequent versions model main consequence instability source course model lifecycle thus punctuated phases lack determinism system become problem particular decision must reproduced example comply gdpr right access opposition possibly accompanied explanation produced one explanatory methods describe hereafter instability source mitigated via low frequency least compensated properly archiving subsequent versions model used production explainability principle four principles exposed explainability one distinctive systems compared traditional business processes terminology notions algorithmic explainability transp arency interpretability auditability closely related transparency means albeit radical make decisions intelligible implies access algorithm source code resulting models extreme case com plete opacity algorithm said operate black box auditability means practical feasibility analytical empirical evaluation algorithm aims broadly collecting explanations predictions well evaluating according aforementioned criteria data management performance stability distinction explainability interpretability subject numerous debates summarized appendix term explainability often related technical objective understanding algorithm behaviour would thus suitable auditing whereas interpretability seems closely associated less technical discourse would thus primarily target consumers individuals impacted algorithm objectives explanations pertaining algorithm generally address following questions causes given decision prediction inherent uncertainty model carry errors made algorithm similar due human judgment beyond model prediction pieces information useful example assist human operator making final call stability potential instability sources may affect algorithms deployed organization time identified source associated risks operational compliance risks otherwise assessed proportionate detection mitigation methods implemented thus objectives explanation vary greatly especially depending stakeholders considered providing insights domain experts compliance teams facilitating model review engineering validation teams securing confidence individuals impacted model predictions decisions overview fundamental trade driving technical choice algorithm based types explanations required given appendix properties ideal explanation following properties accurate describe precisely possible case studied local explanation algorithm behavio global explanation comprehensive cover entirety motives characteristics considered decision prediction comprehensible require excessive effort order correctly understood intended recipients concise succinct enough grasped reasonable amount time accordance time productivity constraints encompassing process actionable enable one actions human opera tor overriding prediction decision robust remain valid useful even input data ever noisy reusable customizable according intended recipients practice qualities simultaneously achievable besides previously mentioned balanced principles notably performance thus properties rather serve comparison criteria explanations provided various methods select one appropriate specific use case explanation levels simplicity sake adopt hereafter term explainability rather interpretability describe broader concept section terminology algorithmic explainability aims demonstrate one hand algorithm operates roughly matches common meaning algorithmic transparency hand algorithm makes decision words interpretation decisions key challenge question auditability algorithm explainability associated challenges include human operators interact system understand behaviour individuals affected system predictions decisions customers sales context understand underlying motives designe system tasked check ing compliance assess social ethical acceptability order among things prove absence discriminatory bias decision process concept explanation level introduced attempts summarize single metric depth metric exists continuum along define four scale qualitatively distinct levels described following concept thus definition quality explanation aims facilitating choice rget explainability level without eliminating need multi analysis explanations provided level explanatio observation explanation answers technically question algorithm work functionally question algorithm purpose level achieved empirically observing algorithm outp individually whole function input data environment analytically via information sheet algorithm appendix model data used without requiring analysis code data level explanation approximation explanation provides often inductive answer question algorithm work level explanation achieved addition level explanations using explanatory methods operate model analysed appendix via structural analysis algorithm resulting model data used analysis fruitful algorithm designed composition multiple buil ding blocks hyper tuning auto ensemble methods boosting level explanation justification explanation answers question algorithm produce result general specific situatio level achieved either presenting simplified form explanatory elements higher levels possibly accompanied counterfactual explanations app endix model trained produce appendix noted level characterizes explanation type explanation rather algorithm model strictly speaking level intelligibility explanations provided system intrinsic explainability system thus highly explainable model decision tree might lend level explanation thoroughly detailing branches also level explanation simply stating decision predictor operating given set input variables latter would suffice case fine behaviour model need must disclosed ore technical perspective appendix examines detail technical feasibility higher explanations presents important hurdle overcome software dependencies along path reach level replication next sections describe two factors among number driving explanation level required algorithm especially financial sector one hand intended recipients explanation hand risk nature severity associated considered process thus algorithm might require higher explanation level inner behaviour also needs captured explanation provided particularly sensitive context recipients explanation first key influence factor expected explanation level type recipient targeted relevant form explanation proposed order effective depends technical business proficiency intrinsic motives demanding explanation hence different explanation levels could applied alg orithm depending whether explanations serve end user tries check treated unfairly system explanation intuitively intelligible auditor needs understand system echnical architecture detail subjected rigorous regulatory requirements hereafter describe three kinds recipients explanation suggest time appropriate form explanation looks like level explanation replication explanation provides demonstrable answer question prove algorithm works correctly level explanation achieved addition level methods detailed analysis algorithm model data practice feasible line review source code comprehensive analysis datasets used examination model parameters customer consumer imple explanations example explanation intended customer occurs context insurance product sales duty inform makes mandatory explain prospective customers offered given insurance product another one furthermore motives need cantered around consistency contract case non insurance adequacy case life insurance nature terms explanation must therefore intelligible sati sfactory regard consumer required master intricacies sales process implementation underlying algorithm continuous monitoring functional explanations internal review teams particularly taske continuous monitoring need verify model efficacy respect business objectives focus case put performance process involving rather internal mechanics thus explanation given functional nature auditor technical explanations thirdly auditor must ensure algorithm implementation consistent respect specifications including terms regulatory compliance technical requirements ntails example verifying model produced also check ing absence discriminatory bias model therefore explanation given must technically accurate representative possible audited model associated risks second factor influence required explanation level risk associated total partial replacement human process component nature severity risk highly variable shown following examples aml process freezing assets subjected performance obligation bears increased level risk introduced dint critical role also evaluation depends comparing human algorithmic efficiency precisely still risk particularly elevated continuous monitoring audit situation assess relative efficiency moderate daily user algorithm keeps performing controls using traditional transaction monitoring system internal models introducing computation solvency margins banking institution direct impact assessment solvability therefore team designs institution internal model expect satisfactory level explanation results computations insurance insurance contract sales process regulation imposes among thi ngs duty inform personalized presentation reasons justifications customer need conversely customer segmentation insurance sector relies mainly accuracy objectives without requirement terms explainability examples explanation levels use case attempt illustrate somewhat abstract definitions explanation levels driving factors concrete use cases deployed financial entities analysed exploratory workshops conducted acpr use case following table suggests explanation level function aforementioned criteria targeted recipients associated risk suggestions based initial market analysis whose observations present document aims validate correct see consultation section explainability use case impacted business processes determined roles filled component detailed types recipients targeted explanation described along nature associated risks entire context dictates level form appropriate explanation algorithm must agreed upon stakeholders algorithm governance domainbusines processai functionalityexplanation recipientscontext associated risk customercompensation processoperational risk customer satisfaction internal controldaily oversight operational risk compliance risk contract honoring financial auditorevaluation operational risk compliance risk contract honoring financial customeronline quote requestcompliance risk customer misinformation failure perform duty inform discriminatory biases internal control internal auditorcompliance checkingcompliance risk customer misinformation failure perform duty inform discriminatory biases validation teammodel model update policy solvency model risk compliance administrative management supervisory bodiesmodel solvency model risk compliance agent alert analysisnone analyst methodology modified algorithm internal controlcontinuous operational risk false negatives false positives compliance risk performance obligation auditorperiodic operational risk false negatives false positives compliance risk performance obligation modelsmodel designcomputation solvency margins assetsalertinguse case explainability criteria appropriate explanation level insurance contractscontract managementcompensation proposal sales proposalquote prefilling evaluation algorithms following diagram represents lifecycle algorithm resulting model design training phases use production possible iterations learning stage instance upon patching algorithm attempts put rspective implementation stages appropriate validation steps whether continuous periodic internal external also aims show stage lifecycle benefits suitable evaluation process based four principle previously mentioned namely data management performance stability explainability finally illustrates multifaceted approach evaluation detailed section combines analytical empirical evaluation evaluation lifecycle algorithms introduced business process detailed list stage design development principles data management performance stability explainability apply particular evaluation method appropriate stage governance algorithms introducing algorithms financial sector often aims via descriptive pre dictive methods automate improve customizing decision process used performed humans therefore governance algorithms requires careful consideration validation decision processes particular regulatory compliance objectives well performance objectives achievable certain level explainability traceability following governance concerns need taken account ear design phase integration traditional business processes impact integration internal controls specifically role assigned humans new processes relevance outsourcing partially fully design maintenance phases lastly internal external audit functions governance principles financial sector general governance principles applicable financial institution include description control culture policy implemented organization presentation ethical professional norms promotes along steps taken guarantee proper implementation norms process case failure addition principle procedures documented detect prevent conflicts interest context relevant elements governance introducing business processes appear operational procedures within processes extension segregation duties management algorithms management risks associated elements briefly described section operational procedures operational procedures adjusted different activities performed communicated periodically updated example via clear written documentation main goals describe various levels responsibility assigned resources devoted internal control mechani sms risk measurement mitigation monitoring systems implemented organization compliance monitoring procedures also list rules relative security business continuity planning segregation duties organi zational standards relative internal controls risk management methods tried tested implementing functions coso cobit risk document address another governance issue nevertheless precede decision adopt technical tool independently usage business application namely analysis words nly governance questions specific usage financial sector considered nevertheless internal control mechanisms conventionally comprise multiple vels control follow four principle classically levels level control within business units conduct activities perform duties controlled manner level control exercised unit managers directors complex organizations teams responsible internal controls also referred internal oversight level control exercised internal audit directorate aims guarantee proper implementation control mechanisms periodically reviewing operational accuracy clear segregation duties must exist business units commit operations record monitor operations ongoing basis risk recognition assessment organizations perform risk mapping must periodically updated evaluated develop coherent comprehensive view risks also define regularly promote solid consistent risk culture dealing risk awareness risk behaviour lastly implement systems procedures guarantee cross prospective risk analysis integration business processes one main challenges governance algorithms integration existing processes key factors take account role played algorithms within business process engineering methodology used end sers role roles played components business processes highly variable primary aml workshop section illustrates fun ction model operational even business case role route certain alerts triggered financial transactions particularly high estimated risk directly toward level compliance thus inducing operationa risk case compliance team becomes overloaded critical function component also elevated case constraint real operation suspicious transactions detected reported small lag feasible conversely incorporation prospective customer selection process purposes commercial canvassing cross truly disruptive incur significant change business process engineering meth odology definition appropriate engineering process varies greatly depending business process models used two examples shall illustrate variety situations crd european directive capital requirements directive defines basis organization within financial institutions used marketing teams common cas although covered exploratory works described herein significant room manoeuvre granted model building often iterative process since one model deployment used feed marketing campaign conversely cases studied acpr workshops require systematic engineering process closer best practices adopted software industry build automation reproducibility releases quality assurance process monitoring models ployed production including stability time engineering process thus meet stringent requirements latter case thus engineering process vary one build mode iterative build process way continuous process also fully automated typically using delivery mode system also variable manual delivery process final artefacts models retained put production extreme delivering entire datasets intermediate results algorithm execution model stages middle ground two approaches managed services approach offered consulting firm participated workshop probability default section composed two elements one hand model engineering workbench follows systematic albeit fully automated model approach controlled solution provider hand information sharing platform enables customer uses model perform complete review engineering process provides audit track independent execution process end users impact introduction business process primarily depends end users opposed personnel responsible internal control whose role examined next section end users may internal marketing teams business unit managers external clients prospects continuous integration continuous deployment refers general software engineering principles based automating entire design evelopment process enables frequent product releases traditional methods allow without trading quality methodology closely related agile methods well devops approach associates roles software development operations engineering methodo logy engineering process designed cover entire algorithm lifecycle depending use case systematic approach may necessary accordance principles model building automation build reproducibility quality assurance monitoring engineering workflow case full traceability design engine ering process guaranteed particular maintaining quality expected process requires examining whether particular form explana tion provided end users clarify motivate decisions predictions impacting types end users case integrating component aml workflow see details works appendix end users level level teams verifications performed compliance team need adequate new approach requires mastering underlying technology model validation needs performed much frequently case capital requirements models since drifts may occur real time example false positive rate deviates norm hence monitoring model must feasible quasi real time case workshop customer protection section prefilled quotes home insurance contract delivered customers requires explaining reasons offering specific product reasons must line customers requirements needs human interactions essential end users algorithm insofar internal users tasked ensuring accuracy quality business process remain independent machine human experts able spot manifest errors made algorithm also offers benefit contributing performance stability two four design princi ples presented document also provides additional leverage check absence systematic biases temporal drift decisions made advice given automated process finance situation introducing ocesses enables decrease operational risk human intervention decision process delegated software inconsequential introduces new kind risk downside enabling human operator validate decisions may become liable especially cases contradict algorithmic result rather confirm besides humans sometimes modify behaviour interacting machine may tend systematically follow algorithmic esults including erroneous ones rather engaging liability rebutting issue independence algorithm responsibility towards decisions course related explainability principle since human operat needs understand mainspring given decision order need counter appropriate one lastly human intervention might introduce bias desired explanations associated algorithm output whenev explanations provided amended operator become disconnected actual underlying factors led output explanations become distorted overridden furthermore transparency longer guaranteed algorithm ich may hide deficiencies basic recommendation appears relevant regard namely allowing human intervention define formulate algorithm explanations internal control system major impact introducing algorithms pertains continuous lidation algorithms specifically internal control procedures organization internal controls monitoring algorithmic performance detecting potential drift time requires different design human validation process example aml workshop section illustrated partial replacement level operators algorithm may decrease capacity evalua process efficiency future least terms false negatives alerts raised system thus corresponding transactions analysed human eye operators assigned anual labelling parallel algorithm execution thus continuously yielding new training data organizational structure internal controls algorithm often aims replace partially fully tasks performed level team reviewed hierarchy level team charge compliance checks probably level charge internal controls although nothing precludes automation stage achieved distant future users algo rithm output thus part team tasked monitoring behaviour designers initial functional validation case workshop credit scoring models section pre model validation process defined involvement technical teams charge building validating end users scope conditions human intervention business processes well defined particular integration processes planned according end users needs end users include internal external indiv iduals respective forms algorithmic explanations appropriate articulated algorithmic results may also submitted human validation process validation governed rules documented part internal control procedures human responsibility becomes engaged algorithm may modify human behavior judgment organization inter nal controls internal control procedures algorithms extent possible nvolve technical specialists domain experts indeed monitoring algorithms requires initial technical validation components involved continuous monitoring adequate management compliance risks generated reinforced models locally globally within banking grou question compliance risk management department particular model deployment model creation patch issue affecting already model requires validation group risk committee among oth things approve chosen risk management strategy use within models thus considered evaluated stakeholders across organization technical experts domain specialists within high committees approach appears beneficial applicable use cases possibly variations according critical impacted business function ongoing functional validation exploratory works aml topic wherein algorithm detects anomalies analysed teams levels shed light algorithm requires sophisticated methods ongoing review traditional methods incl udes continuously monitoring proper calibration algorithm volume alerts raised level rate false positives filtered downstream etc upgrade ongoing validation process thus requires teams charge mini mum deploying mastering tools monitoring operational behaviour real time need algorithms building appropriate expertise tradecraft detect incidents upfront ideally diagnose remedy well case internal risk model updates whenever used construction internal models essential consideration validation process define triggering events model revalidation regard internal models differ expert systems based rules various predefined configuration parameters revalidated time parameters proactively updated deemed obsolete internal mod els become invalid following major change input data however noted models always devoid predefined configuration initial functional lidation impact algorithm initial validation process defined early design phase stakeholders involved include technical staff design validation teams domain experts transverse committees concerned business processes question ongoing functional val idation ongoing functional validation algorithms requires dedicated tools dashboards enabling teams charge monitor overall behavior closely interacting technical experts designed performed initial validation parameters learning algorithm hyper case must subject treatment traditional rule models particular case internal risk models referred basel models banking sector model update policy defined banking institution clearly documents number criter specific threshold exceeded upon parameter adjustment trigger requirement report model update supervisory authority kind parameter adjustment typically decided performed human experts charge risk model therefore one may ask become triggering events model based fact classical internal models assume parameters calibrated data unlike training phase model besides current regulation requires governance framework comprises processes back model parameter updating parameter updates often consequence back results thus caused changes observed data internal model update must reported soon induced change deemed material institution therefore define within governance framework process evaluating materiality change whether model ses lastly actors banking sector opt scheduled calibration strategy however classical internal models update parameters periodic systematic basis example volatility parameters case mark model also similarly models therefore model update policy standpoint appears models treated like traditional internal models technical validation technical expertise required validation typically thr oughout data science spectrum data owner data steward respectively responsible governance quality data used algorithms data engineers data scientists tasked ensuring proper operational behaviour software components implement algorithms lastly context data analysts perform initial ongoing validation algorithms output engineering stages covered adequate technical validation process include model selection training tuning continuous monitoring model non algorithm absence model drift higher task detecting new data sources variables feed algorithm management risks internal control procedures inevitably impacted use evolution closely related generated risks depending type integration business pro cesses automated computer decision nature risks regulatory legal operational financial let consider example used assisting insurance claim processing downstream process within value chain insurance product distribution part exploratory works presented documented recently experienced increased usage typical use case context perform algorithmic filtering incoming dama reports detect likely fraud cases apply exclusion criteria cases gone filters result compensation offer automatically generated cases whereas rejected cases routed toward operators charg insurance claim files main challenge accuracy claim management process insurance organization exposed financial risk rate incoming claims resulting compensation offer unduly increases operational risk exists case volume cases rejected algorithms increases point overloading level teams need review lastly compliance risk appears overload rate disputed claims increases significan tly conversely main stake workshop customer protection section explainability advice given order consumer offered insurance product informed prior making decision besides various risks cross concern necessity dedicated safety mechanism part plan going way business continuity planning designed remedy incident major malfunction failure component integration initial business processes sufficiently modular robust safety mechanism may simply consist falling back onto initial process time period necessary fix failure process structurally impacted integration safety mechanism require sophistication often proves complex imp lement also needs officially validated technical validation technica validation algorithms introduced business processe requires developing broad data science experti spread across departments within organizational structure built around multi profiles gathered within pecialized business unit technical expertise span data science spectrum data engineering state techniques multi generalist skills financial sector specialization deep knowledge business processes specific organization security outsourcing security solutions relying algorithms requires taking account least two types risks rarely encountered traditional solutions specific algorithmic risk terms availability integrity data processing risk additional consideration potential outsourcing design impl ementation exploitation solutions bears security risks security security challenges similar traditional systems typically pertain confidentiality integrity availability treatmen although tailored exact usage way unique financial sector even attack surface finance narrower sectors security finance usually well mature area furthermore exposure things like open source code use public data thus far tended limited way make model safe different way web service exposed rest underlying data sources matter secured three potential targets lie three different architectural layers mutually interwoven respectively model layer application layer data layer comprehensiv description potential flaws model means remedy beyond scope document categorization main possible attacks however given appendix reversion trend however already visible many actors rely heavily sometimes exclusively open libraries products implementing functionality avoid wheel use altern ative data collected web publically available sources becomes widespread among data systems finance rest representational state transfer architectural method commonly used build applications exposed rest api application programming interface thus simple standard easily secured method design deploy web service management risks nature risks pertaining role process carefully identified operational risk financial risk compliance risk etc risks included risk map required governance principles including incident remediation business continuity plans third outsourcing risks financial actors rely different types third providers develop design implementation may outsourced software products services common offering lastly hosting administration applications often delegated cloud hosting services provider outsourcing risks risks classically generated outsourcing software skills design implementation particularly acute risks difficult mitigate practice sufficiently anticipated hence good practice prior outsourcing decision perform risk analysis taking account following risks reversibility reported outsourcing guidelines published european control authorities eba eiopa reversibility outsourced solutions constitutes significant non source vulnerability within financial institutions today using may even exacerbate concerns contro lling entire engineering workflow outsourced requires mastering variety skills including data science skills pertaining advanced techniques software design architecture tradecraft related complex systems multiple integration points among components whose source code always open well documented devops expertise order manage heterogeneous infrastructure often combines dedicated hosting servers cloud services even entire skillset available often case large banking institutions skills may scattered across departments whose technical teams siloed able internalize delivered often monolith third outsourced development described workshop probability default section outsourcing development component induces many changes business process resulting challenges among others security assessing security processes involving take account classical security evaluation analysis risks mitigation techniques particular impact assessment incident remediation plan security audit mission consider potential attacks models underlying data predictive decision systems whole developing nurturing adequate resources human technological validate code written outside organization ensuring delivered software thoroughly documented pos sible periodically updated meet criteria internal control procedures planning audit deployed solution early design phase requires thinking ahead terms software architecture integration pabilities deploying sophisticated well explanatory methods order explain results produced system whose development outsourced software risks induced software acquisition strate similar resulting outsourcing development dependency risk non results lacking security software provider product support deficiencies audit capability assuming audit operations relevant recurrently buying provider workshop probability default section raised issue dependency risk towards solution provider specific case risk controlled insofar provider enables customer review stages ading delivered model remains nonetheless customer responsibility master technologies involved order mitigate dependency vendor lock risks furthermore caution advised acquiring software sufficiently limit dependency risk instance resulting model constitutes deliverable exclusion upstream stages engineering workflow allowing rebuild model alternatively workflow adequ ately documented lack information model designed expected behave may lead operational risk difficulties internal control audit missions lastly noted risk non results neither new traditional models aims replace often share characteristic outsourced solutions especially rely stochastic methods monte carlo cloud hosting service application hosting public cloud become outsourcing scenario commonly encountered financial sector accompany trend initial set outsourcing guidelines published european control author ities banking eba insurance eiopa guidelines cover less ground domains namely assessing critical business processes impact analysis documentation requirements duty inform supervisor access audit rights financial institution also supervisor security risks associated data management subcontracting contingency planning including business continuity plans exit strategy tsourcing agreement monte carlo simulations class optimization methods relies randomness precisely repeated com putationally random sampling emulate behaviour often deterministic process model audit algorithms evaluation principles previously exposed context initial ongoing alidation remain valid audit operations performed internal teams part periodic review control authority part supervisory missions thus auditor need consider precise context algorithm developed particular business processes integrated impacted one way another based context objectives auditors need consider aforementioned trade offs different evaluation criteria algorithms evaluation methods need suitable self systems audit team skills must sufficient meet requirements teams charge ongoi review multi approach variety situations encountered evaluating algorithm due previously mentioned parameters combination algorithm type end users application scenario circumstances validation process access source code underlying data may may possible technical resources may available necessity handle different situations encourages adoption multidimensional approach evaluation algorithms one described following associates analytical empirical methods analytical evaluation algorithm characterized maximum explainability level according four levels identified section levels observation justification make algorithm internal behaviour intelligible explanation rely model architecture piecewise anal ysis algorithm various levels granularity levels approximation replication hand rely structural detailed analysis algorithm precisely analysis source code resulting model third outsourcin risks decision outsource design implementation hosting operations system use third products services must preceded risk analysis take account results especially regard reversibility governance principles pertaining third relation also observed ensuring proper documentation deliverables traceability process enable auditing necessary guaranteeing financial institution access technical practical also legal regard intellectual property rights source code models even development hosting latter outsourced offering guarantee supervisor enable audit missions cover systems source code data use organizational risk policy adequately determined explainability level required use cases algorithm audit missions pertaining algorithms focus high stake algorithms logically assigned higher explainability level case analytical evaluation feasible assuming prerequisites met important one accessibility source code including documentation cases internal external audit mission addresses algorithm assigned lower explainability level one first stages audit consist validating level compatible types risks compliance require ments business process algorithm integrated audit may evaluate algorithm impact efficiency process via empirical methods described following section empirical evaluation system case treated black box evaluated outside observing behaviour based various input data several approaches feasible three described following post explanatory methods methods operate pre models categorized global local depending whether aim explain specific decision algorithm overall behaviour number methods called black explanatory methods remain valid even impossible access algorithm documentation source code refore particularly suitable algorithms whose maximum required explainability level auditor also use post explanatory methods complement explanatory method implemented algorithm designer described appendix besides counterfactual explanations constitute particularly interesting case post explanatory method insofar contribute assessing appropriate data management principle described document followed terms regulatory compliance specifically respect gdpr terms ethics fairness non list post explanatory methods provided appendix workshops led acpr financial actors also detailed appendix shown explanatory methods already widespread use within internal validation processes algorithms mostly way assess proper behaviour means efficiency metrics seems logical methods also mad available external actors tasked evaluating systems analytical evaluation stakes warrant ppropriate analytical evaluation techniques tools implemented early algorithm design stage methods may rely information sheet describing algorithm model data used whenever ossible analysis code data benchmark datasets method consists providing test data designed stress algorithm dataset may either synthetically generated composed real data nonymized need even hybrid typically via generative model enables augment initial bootstrap data sample technical standpoint empirical evaluation kind requires dedicated data science resources specifically data engineering profiles build benchmark datasets frameworks challenger models method consists providing challenger model whose predictions decisions compared produced model eva luation point attention regarding method practical feasibility indeed development challenger models requires allocating significant resources human material time task constraints also hardly compatible audit missions currently performed supervisor consist analysing properties model place checking consistency respect regulatory requirements thus aim building alternative mod next section suggests ways implement type empirical evaluation method ambitious complex task lastly noted multiple empirical evaluation metrics available choice depen objectives using benchmark datasets challenger models metrics focus efficacy order assess algorithmic performance whereas others analyse particular segments population treated order detect discriminatory biases others still investigate decisions made algorithm particular data point etc put perspective effort required building alternative model implementing credit model banking institution typically invo lves tens employees timespan several years even though scope limited organization data challenges supervisor previously described multidimensional approach evaluating algorithms requires supervisory authority adapt tools methods data indeed analysis component differs procedural algorithm even process performed human operators requires certain level expertise significant resources dedicated building tools challenger models maintain datasets enable efficient control missions besides variety use cases variety possible models given use require supervisor find balance implementation evaluation methods one hand adaptability necessary support inevitab diversity models encountered different organizations hand sufficient level formalism enables systematic approach able plug challenger model organization data conversely test deployed model benchmark dataset work tools line work consists building software data science components order facilitate accelerate supervisory missions tools enable producing challenger models previousl described order compare provided supervised organizations specific obstacle supervisor dependency heterogeneous data models across financial actors evaluation method based challenger models plies models able ingest data according structure specific organization challenge analogous output example transaction case internal risk models instance diversity models seen factor prevention herd behavi hence systemic risk see discussion paper artificial intelligence challenges financial sector published acpr december empirical evaluation empirical evaluation methods implemented early algorithm design stage included quality assurance process resulting models part functional integration tests explanatory methods viewed essential tool evaluating model may implemented design stage perate previously trained models besides methods apply models need evaluated black boxes choice appropriate explanatory methods take account algorithm type intended audience explanations risk associated process internal audit supervisory missions may also use empirical evaluation methods benchmarking using test scenarios datasets comparison challenger models models deployed within organization order facilitate missions recommended design data models algorithms modular well possible constitutes good software engineering practice anyway monitoring odels described workshop aml section produce categorical output risk level whereas others produce numeric suspicion score work data line work consists enabling supervisory authority access process data various open closed sources public data regulatory data supervisory mission reports etc different levels national european international control authorities data allow build maintain datasets benchmarking models deployed industry goals measure models performance assess explainability new kinds data detect temporal drifts main challenge prese nted benchmark dataset approach closely related described challenger models absence standardization efforts datasets must produced according format semantics aligned place within supervised ganizations also incurs additional cost technological methodological adaptation training order enable accompany adaptation methods tools data available supervisor appropriate training clear req uirement field data science training may find place supervision authority considered within acpr specialized institutions example pertaining compliance banking sector public consult ation respondents invited illustrate answers following questions using use cases particularly implemented organization context explainability principle basis exploratory works conducted three topics along ith broader investigation finance state domains document outlined number expectations pertaining explainability algorithms relevance guidelines needs confirmed several points object following questions question experience kind knowledge experience possess regarding general particular data science operational tradecraft answer ing behalf financial institution level familiarity within personnel technical busi ness roles question implementation question solely financial corporatio algorithms implemented organization algorithm type specify use cases type environment development production use case according criteria evaluation methods algorithm selected raw performance trade respective roles teams involved design implementation algorithms organization data scientist software architects project management business experts compliance officers question definition exp lanation levels four explanation levels emerging analysis observation justification approximation replication clearly defined indicate points misunderstanding performance principle stability principle question adequacy expla nation levels explanation levels appear represent adequate scale following senses span entire spectrum current future applications finance full transparency way algorithms operating black boxes choice four levels seem appropriate fewer levels question practical examples explanation levels table presented section suggests appropriate explanation level use cases financial sector suitable suggested levels insufficiently reason suggestions adapted usage scenarios specify scenarios sense question technical performance metrics view technical performance metrics commonly used auc score gini score etc specifically adequacy respect various algorithms availability methods choose metrics metrics used model validation selection operating point model drift detection question functional performanc metrics functional metrics kpi seem relevant evaluating component metrics account compliance requirements specific processes considered responsible defining functional metrics technical domain experts without input risk management compliance teams question temporal drift mod els risks according generated potential drift models time methods used remedy risks least circumscribe testing alert triggering based model drift detection etc appropriate data management principle question model generalisation limits generalization power models identified whether relation overfitting intrinsic limits model limits handled testing question retraining sourc instability based experience model retraining phases whether periodic continuous basis source model instability techniques could used limit source instability testing appropriate datasets etc question regulatory compliance data management experience methods techniques appear advisable order ensure compliance vario regulatory requirements relative data management gdpr cross regulations sector regulations european insurance distribution directive specify stage development process design training prediction involve methods techniques question bias detection tigation methods appear advisable order analyze biases systems following types bias iases input data fed models biases present algorithms biases models produced algorithms decision prediction precisely fairness metrics enable identification ases example discriminatory nature methods used mitigate undesired biases thusly identified integration business processes internal control system question role according outlines method assess integration components business processes method enable evaluate critical function components disruptive wit respect traditional process human interactions possible thoughts maintaining parallel processes assigned human operators continuously evaluate correct algorithm results question engineering methodo logy engineering methodology used differ used traditional models generally standard software engineering practices way according odel process take account integration models business processe question risk management introduction business processes impact risk management generate new risks magnify pre risks specify nature risks operational financial legal new risk management methods called example calibration models order limit exposure given type risk question functional validation initial functional validation process model prior deployment production functional validation deploying new version specify answer depends type update patch improvement components continuously monitored business risks question internal model update policy internal risk models conditions may according algorithms used within basel models banking sector within internal models insurance sector organization internal model update policy take account internal models security outsourcing multi approach evaluation document suggests implementing multidimensional approach auditing processes using follo wing questions aim define approach question technical validation initial technical validation process model prior deployment production technical indicators methods used continuously monitor components deployed production question outsourcing use generate specific challenges risks development hosting administration outsourced ones question security impact using security types attack models causative attacks surrogate model attacks adversarial attacks etc appear important terms occurrence likelihood terms damage inflicted case success specify according type model use case environment dedicated hosting servers cloud services etc question analytical evaluation following elements available evaluating algorithm relevant organizations source code documentation resulting models training validation data specify answer depends algorithm type use case involved context evaluation internal validation external audit use standardized documentation frameworks information sheets describing algorithm model data used question explanatory methods explanatory methods appendix currently implement among use cases knowledge know explanatory method described document ones already implemented deployed appropriate explanatory method use depend algor ithm type depend intended recipients explanation way depend level risk associated business process way question empirical evaluation empirical evaluation methods suggested section benchmark datasets challenger models seems appropriate opinion architecture data processing workflows systems within relevant organizations sufficient modular robust enable kind functional testing data model level data format schema sufficiently standardized flexible support data benchmarking method without incurring data integration costs supervisor analogously sufficiently documented transparent support integration challenger models developed supervisor without appro ach rendered unrealistic information asymmetry appendices technical scope extremely broad field whose definition based academic work industry practices evolves quickly time within discussion document considered solely embodiments relevant financial sector current form likely evolutions near medium horizon scope document restricted machine learning happens probably intensely studied field within forms taken nsideration robotics game theory optimization constraints multi systems knowledge representation reasoning planning automation among methods used financial sector considered document following catego ries mentioned without comprehensiveness unsupervised learning methods particular clustering techniques commonly used fraud anomaly detection scenarios predictive models may called traditional cision trees logistic linear regressions sophisticated yet also commonly implemented models decision based ensemble methods random forests gradient tree boosting nlp natural language processing used classify analyse kinds text data deep learning deep neural networks used various use cases including computer vision particularly shine although less prominent use case finance sectors models algorithms anothe key point terminology distinction algorithm model produced algorithm algorithm indicated field considered document executable procedure represented software ode like algorithm specificity respect types algorithms operate input data training data also validation data produce model output model generally speaking composed predictive algorithm model data predictive algorithm typically optimization procedure minimizes error metric model training data examples shall illustrate relations models algorithms linear regression algorithm produces model composed vector weights decision construction algorithm produces model tree whose internal nodes logical conditions involving predictor variable whose leaves predicted alues neural network algorithm based back method gradient descent algorithm produces model graph structure whose nodes weight vectors terms model algorithm sometimes used interchangeably within present document context unambiguous meaning refers model building process realized algorithm prediction process realized already model detailed description exploratory works appendix presents exploratory work three topics selected description purpose relevance exercise objectives algorithm presented financial actor involved technical details method implementation validation process adopted actor governance issues raised introduction business process evaluation methods used implications according four evaluation principles exposed document appropriate data management performance stability explainability engineering methodology used develop system question following sections way constitute evaluation algorithms studied exploratory works business processes goal provide contextual factual information reader shed light lessons drawn acpr discussion document topic anti laun dering combating financing terrorism aml regulatory context current aml regulation requires financial institutions implement risk management procedures enabling detect peps politically exposed persons transactions inv olving individuals tied high country listed fatf financial action task force european commission well transactions incoherent anomalous respect organization knowledge customers result sar suspicious activity report equivalently suspicious transaction report european national regulations pertaining freezing assets also require financial institutions set unit dedicated implementing relevant asures include addition asset freezing prohibition making funds available regulations require using computer system practice organizations use software processes due size activ ity volume lastly regulations contain provision specific use purpose exercise objectives primary aml workshop augmented secondary workshop following understanding potential use cases aml gaining familiarity underlying techniques call applications published march stated works envisioned way related acpr supervisory procedures thinking possible adjustments supervisory processes view controlling aml processes objectives algorithm main project studied within topic consists introducing models aid filtering transactional messages words design algorithms assist agents tasked distinguishing among list alerts raised rule third monitoring tool false positives transactions concerning individuals actually embargo sanction lists process prior introducing operators review alerts issued screening mechanism order determine whether physical moral persons targeted restrictive measures operators organized according two levels level team charge initial alert processing basis decision matrix alerts resolved level escalated level teams authorized release payment reject file homonymy case administrative authority responsible freezing assets role model developed assist decision pro cess route transactional messages appropriate level based relevance sensitive messages directly processed level aims streamlining securing overall process level teams longer charge initial processing alerts able absorb volume increase model developed participant workshop dubbed tpa true positive acceleration technical details algorithm based neural network fed features varying levels complexity message characteristics phonetic distances strings address components using ner named entity recognition semantic analysis free text variables extracted transactional messages filtering tool contain personal data unlike original messages contributes rationalizing filtering process indeed quickly efficiently discriminating heaps vol uminous messages frees analysts focus tasks higher added analysis results produced also gives higher accuracy daily job since risk forecasting process gains precision volu data analysed grows time reduced amount routine repetitive tasks along opportunity partake engaging strategic works also contribute employee retention lastly situation also considered direc tly contributes improving decision making human analysts performing post analysis abandoned escalated alerts give means adjust decisions future alerts validation process starting point participant workshop build existing validation methods used risk management models could relevant internal control procedures usual frameworks risk management models organized around model validation eam model update team two teams mutually independent independent review tends increase algorithm efficacy reduce operational risk goal perform formal validation year time model unde rgoes significant change meanwhile machine human expert systems might use rule procedures order build reference dataset operate benchmark compare model development identify cases decisions deviate expected norms peculiarity validation processes lifecycle models one hand integration component business process performed according validation methods line organization governance framework hand statistical validation model consistent first kind validation repeated time ideally continuous basis words notion priori validation since shorter validation cycles necessary makes dichotomy initial validation ongoing review less relevant case algorithms rate validation rocess proportional risks particular terms regulatory compliance governance issues governance schema chosen actor ensure two human role monitoring algorithm level analysts tasked authorize reject transactions also guarantee algorithm proper behaviour level analysts also annotate transactions parallel algorithm increases amount additional training data available latter pproach retained actors introduced aml filtering workflows section however enables validate performan stability algorithm time even presence major changes transaction profiles terms operational risk point attention significant decrease level workload order due introduction model necessary anticipate operational risk would result either interruption algorithm operation general system failure risk critical potential ramifications given component contributes performance obligation particular organization needs ensure level validation teams remain capable absorbing necessary entirety incoming transactions without degrading quality service provided evaluation methods implications explainability explainability requirements algorithm different workshops pertain credit granting models construction insurance product indeed requirement motivate decisions made algorithm impact individual checking relevance alert raised algorithm also relatively simple analyst order efficient job comparing alert sanction list operator need know reasons alert triggered important benefit explainability case business value facilitates analysis patterns filtering behaviour captured algorithm also constitute training data assistance understanding operations performed human analyst additional help ongoing review algorithm efficacy key asset domain subjec ted performance obligation otherwise put false negatives costly reduced strict minimum performance statistical performance predictive model along operational impact alert processing workflow evaluated following observations statistically speaking model exhibits slight overfitting however appear induce functional risk given algorithm integrated overall process worst process automatically accelerated nevertheless adequately processed level teams need algorithm impact business process manifests significant decrease volume alerts proc essed level teams marginal increase volume alerts processed level teams due improved recall model stability model behaviour appears stable time insofar relative impact accel eration message processing workload managed level level teams stable time however usage scenario data quality comprehensiveness essential freshness necessary ensuring model relies operating properly two approaches used aim making temporality explicit algorithm since plays key role semantics data particular datasets used aml periodically reviewed order take account new methods used malicious individuals building generic time variables instance instead using country variable use country belongs given sanction list time feature relat issue considered appropriate data management project directly stems compliance department however previously indicated specificity introduced use responsibility validation process additio compliance team also lies domain experts technical experts engineering methodology project undertaken according agile methodology time workshop still experimental stage suggested ction appears sensible stage demand even sensitive topic aml excessively broad cumbersome validation process would involve departments hinder deployment production secondary workshop secondary workshop aml topic conducted another banking section summarizes noteworthy differences primary worksh objectives algorithm business process integrated case filtering transactional messages screen sanction lists primary workshop led potential rejection payment assets frozen detect suspicious transactions appropriate yield suspicious activity report sar function performed enterprise software specialized filtering financial transactions uses preconfigured siness rules rules executed transaction produce suspicion score used route transactions threshold toward teams tasked analysis alerts following standard teams broken vels alerts first threshold directed level level branch offices second higher threshold directed level correspondents banking group new approach del trained training dataset composed manually issued alerts validated alerts generated rule software noted significant portion manually issued alerts suspicion score produced business rules zero integration process differs primary aml workshop model introduced complement enterprise software following features function model primary workshop escalate alerts trigged business rules level level case model produces additional alerts sent directly level thus follows parallel workflow serial one execution business rules would followed prediction thus rather classifier previously alerts banking group deployed detection tool validated alerts applicable entire trans action flow also filter introduced transaction assigned high suspicion score model alert generated alert raised business rule engine customer within three preceding months words alert triggered corresponds customer given high score stayed rule engine detection threshold lastly contrary business rule engine mod takes account information beyond transactional data statistical features transactions combined static variables either direct measures duration customer relationship asset value constructed variables types products contracts sliding time window workshop presented secondary study conducted belatedly furthermore use case technical implementation relatively similar first workshop tracfin traitement renseignement action contre les circuits financiers clandestins service french ministry finances charge enforce aml regulation coordinating application governance issues contrary primary aml workshop level teams annotate transactions parallel model detect false negatives accordi organization team relevant sample sufficient number false negatives would large two methods could considered analysing false negatives namely either lowering alert triggering threshold systemat ically sending suspicious cases review would likely induce excessive additional workload operators besides false positives simply due non variables noted way introducing business process complement enterprise software along routing alerts level result additional workload level teams new project initiated goal routing certain alerts raised enterprise software level level order reduce extra workload also relation changes business process banking group decided structure organization aml expertise around dual skillsets employees master including data management issues possess business experience including risk management different governance choices aml workshops particularly interesting option probably suitable particular context feedbacks gathered around projects likely provide valuable know regarding possible trade predictiv power model temporal stability workload dedicated manual annotation data explainability explainability requirements aimed different types users joint effort within banking group involving technical teams compliance department people led proposing explainability forms adapted user type wishes observe context line approach described section recipients explanation technical teams particular data scientists rely explanations model construction phase continuous monitoring shap shapley additive explanations values xplanation form used case understand decision made particular transaction compliance experts use explanations support decision abandon validate alert workshops organized users order better defin needs simple tabular representations shap values quickly deemed inadequate led development gui graphical user interface showing explanations still based shap values easier interpret acti onable lastly banking group also aims provide relevant explanations internal external auditors including complement previous explanation forms documentation ensuring proper intelligibility algorithm performance main performance indicator rate alerts generated detection system result sar introduction according aforementioned architecture enabled doubling indicator stability monitoring tool imple mented early initial deployment model detect operational anomaly model drift tool periodically checks several indicators characterizing model input data output score distribution etc technical tea indicated workshop still early determine whether drifts model less frequent need reconfigure enterprise software updating model would nevertheless simpler updating parameters business rule engine several reasons simple retraining phase without addition new features also fully automatable entirety model parameters adjusted without manual intervention besides odel update retraining deployment production would take longer days significantly less reconfiguration enterprise software topic internal models banking insurance second topic ploratory works conducted acpr pertained internal risk capital requirements models fact candidates topic suggested study use cases slightly different domain consequence topic pivoted toward risk credit modelli considering granted individuals businesses consisted two distinct workshops workshop focusing credit granting models models usually compute credit score participant workshop banking group another workshop relative behavioural credit models models aim estimate probability default given time horizon current credit participant workshop large consulting firm provides banking orga nizations model construction platform regulatory context workshops shared following initial observations classical internal models generally relatively easy audit perform poorly advanced complex models provide performance improvement albeit cost explainability regulatory requirements identified hindrances implementation innovative algorithms especially based requirements pertain stability result ing models auditability also transparency explainability algorithms additional challenges related personal data protection along limitations inherent data terms access completeness example make challenging analyse correlations among multiple variables characterizing customers behaviour workshop credit scoring purpose exercise banking group question implemented methodological guidelines credit scoring models workshop aimed explain credit modelling teams took account guidelines defined refined course many years build models accordance objectives algorithm workshop involve analysis several credit scoring models answered dual objective reduce dependency toward third data providers credit bureau integrating additional internal data sources algorithms instance behavioural data addition credit bureau scores traditional internal data credit history classical objective improve discriminating power credit scoring models three models studies respectively credi enterprises credit purchase used vehicles credit household equipment purchases household equipment model described detail section two models present similar issues functional technical level business objective household equipment model make decision credit request within minutes technical details project relies following data sources data credit applications individual data applicant applicant appropriate information product amount credit terms etc data contractual risk data used computing default states data used computing behavioural variables external data credit bureau scores data rom central banks data scientists met workshop insist specifically importance enriching internal data typically kind used projects using external data latter various types text time series etc sometim collected open data sources obtained via web scraping strength lies using novel algorithms also leveraging data sources often called alternative data sources models implemented teams decided use gradient tree boosting algorithm variants thereof comparing algorithms commonly used organization particular svms demanding computing resources neural networks deemed unsuitable use ase validation process validation process within banking group credit granting model developed using prior deployment production whether new model patch already model follows credit teams signed model usually located country centralized teams cases sufficient data science resources available local entities send validation team dossier comprising technical documentation along entire source code validation team inspects documentation conceptual validation model generation code training test validation order verify results bring critical look methods used possible cause validation team possesses necessary skills evaluate model according principles described document data management performance stability explainability certain entities banking group credit granting mode used basel models internal risk models banking sector cases validation team presents model group risk committee order get strategy choice approved constant risk decreased risk hybrid stra tegy appropriate dossier validated group level sent ecb validation prudential models validation process thus comprises conceptual phases also applied phases governance issues workshop described scenario component introduced computer aid decision process fully automated process indeed component part multi process execution business rules related age filters ess previously defined domain experts jointly validation team automatic computation credit score given lower weight business rules overall decision process possible intervention human agent override decision cases high score credit granted system cases score threshold evaluation methods implications explainability multiple objectives explanations use case mod designers need guarantee proper behaviour algorithm facilitate validation process explanations also aimed teams responsible continuously monitoring system lastly future useful agent need understand negative result produced algorithm making decision either confirming credit denial granting credit manual override shap method retained three situations lime also valuated following reasons enables global explainability type information weighs model decisions local explainability values taken specific data point impact decision positively negati vely form explanation provided shap deemed users analogous traditional logistic regression model method easy implement three situations counterfactual explanatory method section however also considered would likely require significant amount work especially large amount information needs presented users besides explanation intuitive possible straightforward cases underlying decision tree split criteria quite logical age years performance main methods metrics retained evaluate model performance confusion matrix score assess recall precision gini score evaluate discriminating power kappa coefficient comparing old new scorin models particular gini threshold defined guidelines implemented throughout organization credit models current status threshold achievable designed models except certain population segments younger age groups regulatory models higher threshold case gini gain obtained going traditional scoring models model produced gradient tree boosting rather small percenta points case examined workshop household equipment model nevertheless reach percentage points som models developed team namely initially low discriminating power furthermore ven seemingly marginal gini gain generally represents significant decrease key business metric case namely expected credit loss stability main stability metric retained project based cross results name checks standard deviation different folds several indicators also monitored mutation rate population using population stability index evolution portfolio profile credit application rate acceptance rate number defaults previous three months accordance monitoring practices described section evolution business performance metrics case alert raised indicators analysis performed order find probable causes corresponding statistical anomalies remediation plan produced may cases include model redesign due lack ndsight operation new model thus far done parallel traditional model still used production teams able estimate stability appropriate update frequency engineering methodology credi granting models developed teams yet production method analyse corporate credit risk however implemented deployed leverages mostly open data order estimate company default risk workshop probability default purpose exercise workshop conducted credit department consulting firm offers clients financial sector engineering solution applicable building models estimate probabil ities default workshop quite complementary previous one focused credit scoring models insofar relates generic fully externalized solution thus represents interesting example adoption financial actor product developed third solution offered consulting firm product operating black box toolbox enables design build model maintain ing constant interaction betwe solution provider customer practice resulting model hybrid one partly based advanced algorithms design phase translated simple explainable algorithms deployment phase choice app ears motivated necessity deliver well model along audit track solution currently available designed support credit scoring probability default models however solution provider workin applying similar approach internal risk models namely leveraging yield corrections improvements currently used models form business rules objectives algorithm main objectives project following increasing performance models used decision particular improved risk discrimination identification non effects risk factors improved classification individuals faster identification changes underlying risk portfolio improving data quality use quality assessment improvement techniques refining estimation regulatory capital requirements use accurate models increasing transp arency auditability models data availability essential issue case since volume data exploited varies greatly use case data points consumer credit far housing credit technical details main stages nominal behaviour solution commonly encountered adopting advanced models exception last one makes approach original stages data quality control data preparation prior modelling construction reference model traditional type practice logistic regression construction challenger model advanced type sophisticated supervised algorithms used typically random forests neural networks identification margin improvement reference model use case considered prediction error attributed population thus goal identify population segments incorrectly classified reference model visual explanation decisions made model methods used classical ones shap lime extraction simple auditable business rules explain performance gap odel ref erence model aim population segments incorrectly classified reference model automatically identified business rules extracted domain expert typically risk management reduce much possible hat performance gap definition final hybrid model combination reference model business rules solution offered manage services besides hybrid workflow described information sharing platfor enables customer review entire design process independently execution workflow extent model building approach adopted relies challenger models mentioned document possible audit method section several hundreds model exemplars compared one another best one retained following system minimize performance gap reference model top challenger nutshell strategy try replicate performance best challenger models remaining inside controlled operational framework guaranteed combining intrinsically explainable model logistic regression limited number business rules creators platform examined insist choice avoiding pure models made early project firstly notoriously difficult implement type scenario secondly model would hide behaviour inherent population considered transition individuals across population segments time essentially observed credit model validation process initial functional validation relies documentation algorithm presentation results performed solution provider support customer iterative mode specifically focused aforementioned stages identification margins improvement definition resulting hybrid model continuous functional validation similar back usually performed credit models except freque monitoring population segments impacted business rules required goal anticipate detection model biases interestingly back results presented risk committee order assess relevance model adj ustment governance issues solution design ultimately consists tuning reference model via business rules stages aims make compatible governance frameworks common traditional models particular hybrid del designed consulting firm assimilated classical behaviour credit granting models follows analogous business process regression model comparable irb internal ratings approach models first executed hen override similar notching practiced credit rating agencies applied human agent weakness identified model output besides compatibility tried governance framework benefit exp ected approach model explainability next section choice hybrid model also made various operational reasons easier implementation stability robustness another governance issue raised use case however relatively common namely outsourcing model design implementation also maintenance evaluation methods implications explainability choosing hybrid model based decision rules solution provider generation convincing explanations local global intended users governance bodies instance essential explainability criterion aforementioned overrides model decisions must motivated particular user model typically account manager must understand model produced given score besides explained section intervention human agent introduces risk explanatory bias respect objective result provided model hybrid model business rules pre algorithm essence model first opti mized logistic regression addition business rules aims optimize resulting hybrid model cases terms global performance local explainability use shap enables provide reasons particular sco given logistic regression model explanation decision made overall hybrid model turn consists augmenting shap values motives overrides made business rules motives quite simply membe rship individual considered one population segments model predictive performance optimized algorithm performance following metrics combine predictive performance business efficacy section used assess relevance overall model construction workflow predictive performance metric gini score gain used typically order cases studied two business performance indicators computed gain terms returns measured keeping risk appetite constant around reduction expected loss standard computation int ernal risk models concern also evident certain technical hoices example genetic algorithm picked hyper parameter tuning rather bayesian optimization method deemed easier explain even laypeople offering comparable performance furthermore replicability model studied initial runs experienced problematic lack reproducibility later solved stability firstly workshop illustrated observation made section namely temporal drift predictive model may cases due significant change input data without even considering impact algo rithm thus case credit models structural modifications population considered may introduce model biases nevertheless evolution client database banking group instance rarely taken account irb models consulting firm participated workshop advocates adoption portfolio monitoring solution banking institutions wherein customer portfolios well credit asset portfolios regularly analysed detect tructural changes regarding stability predictive model sub undertaken solution provider order provide kpis basis monitoring back protocol hybrid models order identify deviations model stability hybrid model fact differs logistic regression model choice business rules embedded choice made customer interaction consulting firm discuss technical implications together customer may also choose model review suppress rule example aligned risk appetite due data qualit problems identified variable involved rule furthermore analysis model stability shown introducing business rules make model less robust provided rules guaranteed apply population segments identified also enable specific monitoring population segments lastly initial studies suggest periodicity months model updates would adequate credit scoring probability default models appr opriate data management kind outsourced model solution validation resulting models well adequacy data management ultimately responsibility customer compliance risk departments ther thus delegation responsibility nevertheless regulatory requirements imposed end customer particularly comes explaining model predictions reported onto solution provided third interestingly rkshop participant indicated project undertaken set ethics committee involving large banking institutions among customers ultimate goal producing mrm model risk management framework engineering methodo logy choice iterative model building workflow rather fully automated single process deliberate indeed solution designed workshop participant involves human intervention hybrid model optimization phase app roach makes end automation build process impossible providing according solution creators benefits terms explainability stability resulting model see previous section besides lack end automa tion engineering methodology relies two foundations one hand hybrid model building workflow follows systematic approach developed according industry standards hand tooling handed customer take form information model data results sharing platform enables customer loop decisions made results obtained model construction objective platform still constru ction time writing provide automated audit track exchanges customer goal architectural choice make stage model building traceable even model deployed production whether stage automated performed human agent risks induced outsourcing section call following comm ents aforementioned model building method enables reproducibility auditability models produced quality service customer responsibility customer ultimately decides deploy models charge operational maintenance continuity service reversibility raise major difficulties either since customer able revert regression model time furthermore evolution business rules monitored independently fro model construction outsourced lastly risk dependency towards solution provider remains specifically fundamental aspect technical knowledge kind situation end customer responsible develo ping maintaining expertise know order control risk topic customer protection workshop conducted insurance institution around project pertaining sales proposals project aims produce prefilled quotes home insurance regulatory context mentioned section duty advise defined idd insurance distribution directive imposes insurance product accordance client best interests therefore goal technological innovation domain make offer consistent customer needs requirements contribute creation demand purpose exercise main challenge workshop shed light focusing specific use case regulatory issues raised use distribution insurance products objectives algorithm cus tomer already subscribed contract example automobile insurance system implemented attempts prefill home insurance quote including starting price technical details specificity use case reliance eographical data directly linked real sociology gridded data provided insee french national institute statistics economic studies including information ratio houses apartments rate home ownership average surface average household income neighbourhood commune levels data buildings acquired data provider gives building surface perimeter building shape determined probability house apartment estimated average number rooms commune level postal address field text analysis performed order extract discriminating features prediction email field used sam prediction quote prefilled following target variables predicted iteratively one predicted using predicted value variable using first predictions home type house apartment customer status owner tenant number rooms optional insurance valuables year construction validation process validation involved mainly compliance department performs consistency checks needs expressed ustomer one hand risks declared prefilled possibly amended quote hand governance issues prefilled quote produced algorithm examined leveraged insurance institution several use cases sending email hyperlink quote processing incoming calls order perform portfolio cross supporting outgoing telephone marketing campaigns main governance issue respect compliance requirements related insurance product distribution notably duty advise imposes motives offering particular product exposed prospective customer well consistency product characteristics customer needs requirements particular attention focused human interactions subscription process based prefilled information discourage customer express verify accuracy declared ame time regulation requires amendments checking unchecking option changes indicated customer prefilled quote faithfully reflected appropriate documents formalizing gathering customer needs requirements insurance product offer governance issues illustrated certain measures adopted system design development order ensure proper information customer upon opening prefilled quote popup window explicitly enjoins prospective customer verify information correct need insurance valuables option particularly telling initially checked quotes produced system turned customers unchecked box therefore decided purpose providing quote closely adjusted customer intentions check uncheck box based model prediction valuables variable evaluation meth ods implications explainability explaining individual prediction customer represent according insurance institution major issue case predictive models intended marketing present case rather tha explanation explicit validation request provided customer contrary seems important provide explanation model predictions specifically prediction errors teams tasked monitoring system ensuring indeed prediction error strong impact subscription process process must correctly understood customer indeed failure advice even liability invoked erroneous prediction corrected insured thus becomes false declaration albeit unintentional besides prediction errors benefit insured generate accumulate additional risk one financial oper ational nature performance predictive performance model trivial assess measured classification accuracy according aforementioned target variables retaining first three variables error margin one unit number rooms model produces correct predictions stability use case present stability challenge since input data relatively static predictive power minor business impact particular algorithm emed efficient users may endowed prescriptive power even though designed aim stake future tential disputes case false statement whose origin would lie quote prefilling however possibility provide explanations algorithmic decisions purposes internal control external audit explored workshop appropriate data management terms data management domain insurance pricing defines forbidden variables absence variables resulting models therefore guaranteed well practical infeasibility nference predictor variable used model engineering methodology predictive models described deployed production although run production environment use automated decision pro cess predictions provided continuously instead manual collection stage algorithm output necessary thus predictive model validated functionally technically executed periodic basis resu lts used three situations previously described email campaigns incoming calls outgoing telephone campaigns explainability interpretability distinction two concepts frequent topic within scientific literatu however consensus definition without distinction burrell insists issue interpretability algorithmic results without defining terms question doshi kim fail distinguish two terms define relation nevertheless article strives justify necessity categorizing various forms interpretability similarly biran cotton use circular reasoning around concepts explanation closely related concept interpretability systems interpretable operations understood human pointing lack formal definition bogroff guéguan define interpretability ability explain present stages using humanly understandable terms part tim miller offers comprehensive analysis concepts introduction notion degree allows define interpretabi lity degree observer understand cause decision unfortunately explainability defined according notion way obtain human agent understanding miller emphasiz necessity reade observe similarities differences two stating five lines prior would used interchangeably definition distinction molnar lifts miller definition interpretability attempt distinguishing two terms defines explainability explanations predictions provided individuals introduces question good explanation book bryce goodman seth flaxman european union regulations algorit hmic decision right explanation implicitly distinguish two concepts reading gdpr articles mention particular algorithm operates correlation association performs pred ictions without providing explanatory element correlations associations difficulty arises thus interpretation becomes difficult insofar algorithm works without explain inner workings authors iden tify tension right access personal information collected articles right collect data article giving article disproportionate weight would lead development black society pasquale intervention institut recherche informatique toulouse laurent serrurier links explainability technical characteristics algorithm whereas interpretability related ethical dimension explainability thus technical feature algorithm complex nature interpretability refers social acceptability likewise talk given cpr louis abraham tackles biran cottonn definition mixes concepts explanation clo sely related concept interpretability systems interpretable operati ons understood huma either introspection produced relates interpretability question explainabilit question aurélien garivier article toward responsible artificial intelligence offers explicit distinction defining two terms per article berkeley vie systems challenges decision rule said interpretable one understand associates response observations said explainable one understand elements decision grounded possibly using counterfactual reasoning within interp retability lipton article gives satisfying meaning concepts interpretability explainability rejecting understanding interpretability monolithic concept lipton introduces continuum based number logical riteria trust algorithm results causality transferability knowledge information contained decision fairness decision framework enables propose concrete representation continuum intelligibility explanation technical aspects explainability trade appendix describes technical choices arise appropriate level explainability selected upon introduction business process description generic scope since elements considered restricted financial sector two trade presented simplicity efficacy algorithm one hand sobriety fidelity chosen explanatory method hand efficacy trade given type algorithm may less complex sense lending inspection inner workings also vary efficacy measured previously indicated using predictive performance business performance metrics following diagram attempts illustrate ficacy trade among common algorithms among numerous simpli fications approximations operated diagram following points underlined simplicity efficacy metrics one hand ordering algorithm types terms simplicity highly subjective indeed size structure model significant impact explainability model type understanding part model useless thus random forest compri sing thousands trees typically much difficult understand single neural network composed dozen neurons hand deterministic stochastic nature algorithm another essential criterion take int account assessing efficacy instance results fundamentally stochastic algorithm depend random sampling building training evaluation datasets also within procedure example bootstrap method contain stages lastly noted efficacy given algorithm type evaluated single dimension scale either since depends use case considered nature volume data choice parameters etc axonom also emphasized representation algorithms previous diagram pretend comprehensive particular categories reinforcement learning excluded upfront best current knowledge absent solutions deployed today market hand unsupervised learning algorithms ignored example graph analysis based factoring company features allows del interdependency network generated smbs partaking lending platform factoring technique used may svd singular value decomposition latent factor model case type modelling demonstrated desc riptive value also value predictor credit default risk platforms ahelegbey furthermore credit default risk easily amenable traditional non model decoupling design modelling lastly des ign algorithm generally decoupled structure resulting mode strength innovation brought hybrid models one described workshop probability default section approach consists building simple intuitive model iterative optimization comparing efficient often complex model resulting model combin best worlds namely performance terms recall precision complex algorithm explainability terms interpretability limited size final predictive model trade exp lainability requirement induced introduction business process limited trade pertaining algorithm explanation intelligible convincing intended recipients suitab use case considered proportionate risk associated business process trade play well one hand explanation fidelity respect algorithm produced given prediction imperfec since algorithm behaviour necessarily simplified output explained terms certain characteristics individual transaction considered hand sobriety explanation intuitiveness inte lligibility layperson subjective constrained practice following diagram attempts represent trade explanation according type algorithm type explanatory method corridors drawn show given algorithm type explanatory methods deviate slightly general trends reaching high explanation level feasibility replication noted explanation level replication aims identically reproduce model behaviour understand inner workings full est detail may prove impossible certa models typically deep neural networks interestingly financial actors met exploratory works led acpr implemented replication method early design initial validation phases algorithms hence upstream internal control audit procedures specifically opted implementing algorithms multiple som cases three languages software engineering technique classically used particularly critical components system problem software dependencies besides problem arises whenever code review order levels problem specific algorithms comes virtually well software audit mission multiple external software libraries tools components invoked code analysed review ranges difficult case open source software impossible case closed source code even simple logistic linear regression algorithm uses several third libraries problem amplified sophisticated algorithms incidentally also require higher explanation level conclusion reaching level level explanation challenging situations challenge made difficult certain circumstances algorithm relies third libraries pro ducts audit mission needs cover entire model building workflow resulting model approach sometimes mentioned facilitate kind analysis consists setting certification process components similarly software security components must tried officially approved prior embedded critical applications rate detailed code analysis suggested level explanations replication also focus use libraries crucial point hyper parameter optimization stage insofar significantly impacts algorithm accuracy review explanatory methods review pretend comprehensive limited use financial sector besides aims paint picture use cases deployed practice whether currently production simply experimental stage frame reference algorithm explanatory algorithms classically grouped three categories according come within model lifecycle explanatory methods aim describe data used building model explainable modelling contributes production intrinsically explainable models lastly post explanatory methods attempt yield satisfactory explanations previously built trained models explanatory metho upstream learning stage model somewhat limited form explainability provided whose goal illustrate data used algorithm common methods used purpose following exploratory data analysis exploratory data analysis often relies data visualization enables reveal characteristics data even potentially hidden descriptive statistics methods model domain within nancial sector particularly useful detecting mitigating undesired biases section potential sources problematic biases numer ous kamishima direct indirect latent dependency sensitive variables sampling biases difficult case detec labelling biases training data imperfect convergence training stage dataset ocumentat ion severa dataset documentation standards suggested either applicable models mitchell associated services hind kind approach based thorough concise formal documentation datasets services suitable level explanations described document section however technical focus standards proposed thus far makes ill customers end users algorithms instead intended creators tools even individuals charge monitoring operational behaviour standardization effort nevertheless recent like evolve near future dataset summarization methods order facilitate mental representation interpretation datasets particularly voluminous heterogeneous ones certain dataset summarization methods may used ideally complement aforementioned exploratory analysis documentation methods examples dataset summarization methods textual data automatic document summarization classification images visual scene synthesis data type extraction representative typical exemplars dataset particularly atypical exemplars well respectively called prototypes criticisms kim explainable feature engineering last type pre explana tory method stems observation explanation predictive model good predictive features relies therefore particular care must taken feature engineering stage designing system nstruction predictor variable original variables order adequately training data algorithm two methods mentioned murdoch intervention domain experts sufficiently knowledgeable source data extract variables combination variables intermediate computation results etc increase model predictive accuracy maintaining interpretability results words human expertise enables certain cases sidestep usually inevitable trade efficacy explainability model section modelling automated approach usual data analysis techniques used dimensionality reduction clustering extract predictor variable compact repr esentative possible explainable modelling methods enable simultaneously training predictive model building associated explanatory model category explanatory method referred explainable modelling methods howev far less frequently implemented pre even post explanatory approaches several reasons explainable modelling requires access source code produces predictive model possibility modify algorithm contrary access model sufficient post explanatory methods makes much widely applic able explainable modelling useful explanations necessary early design phase algorithm demands mature engineering methodology adequate planning introduction business process lastly explainable modelling suitable audit predictive model available black box without documentation algorithm primary highly ambitious goal explainable modelling avoid uch possible already mentioned trade efficacy explainability strive provide additional explainability without necessarily sacrificing predictive accuracy methods explainable modelling described foll ows intrinsically explainable models intrinsically explainable model chosen outset example linear models decision models trivial kind explainable modelling approach assuming fficacy trade kept mind specific model produced algorithm actually explainable latter point always guaranteed cases adopting explainable family models sufficient since may lead del many dimensions remain intelligible hybrid explainable models hybrid explainable models applicable specific model type namely neural networks following types models belong category deep papernot mcdaniel extracts internal representation neural network within layers order illustrate final result obtained last layer variant approach deep weighted averaging classifier senn self neural networks alvarez uses neural networks simultaneously train predictor variable weights aggregation method linear model variant contextual explanation network joint ediction explanation approach consists training model produce prediction explanation prediction recently received considerable attention despite two major limitations firstly algorit need modified explanations must provided entire training dataset often unrealistic secondly explanations produced accurate relevant information provided human agents training hybrid model necessarily justify genuine internal workings predictive model following methods fall joint approach ted teaching explanations decisions hind associated training data point motive behind resulting prediction variant generation multimod explanations park data methods include visual explanations hendricks object recognition images generation concise explanations natural language english predictive model using textual source data lei architectural adjustment methods methods relying architectural adjustments mostly specific deep learning toda relatively infrequent financial sector nonetheless worth mentioning attention models aim identify important feature groups within input data images textual data relevantly financial sector time series studies jain however illustrate limits approach terms perfo rmance resulting model regularization methods regularization methods typically used enhance performance model however kinds regularization enable improve model explainability example decision boundary model may constrained training approachable decision tree makes future predictions easily comprehensible human another example methods orient model training assign weight predictor variable labelled important domain expert ross modelling decoupling particular note specialized approaches decouple training stage algorithm structure resulting model example approach hybrid method described workshop probability default section advanced model low explainability natu trained achieve high predictive accuracy domain experts extract number business rules augment intrinsically explainable model decision tree number overrides resulting system thus benefits fro accuracy complex algorithm explainability simple predictive model post explanatory methods methods operating previously models facto commonly intended meaning explanatory methods general goal provide post explanation justify illustrate given result set results produced model model thus considered object studied changes made contrary explainable modelling approaches section whose data manipulated contrary pre approaches section two main criteria used distinguish post explanatory methods firstly local global scope local explanatory methods provide explanation decision made particular input data point instance given credit application granted applicant global explanatory methods attempt simultaneously explain entirety possible decisions case general characteristics respective outcomes acceptance denial credit applications second criterion whether method applicable type model model methods specific type model algorithm model methods local explanatory methods black methods black methods also called model applicable type model may consist simple classifier example bayesian classifier trained parzen windows sophisticated number operate perturbing model observing influence predictor variables following techniques among common model explanatory methods naive bayes models often crude comparison next ones lime locally interpretable model explanations works constructing intermediate representation domain model real model find optimal trade fidelity model explanations mplicity explanations whose purpose intelligible domain experts necessarily technically savvy shap combines game theory shapley values optimization credit allocation order explain influence predictor variable predicted values also model manner lundberg variants shap method example adapted data structured network chen causal interpretation methods compute marginal influence predictor variable joint influence variable pairs datta slim supersparse linear integer models selects decision rules optimize accuracy binary classifier constraints number variables relative weights noted even commonly used local explanatory methods lime shap based model perturbations encounter practical limitations terms security dylan particular hey vulnerable adversarial attacks appendix produce models including discriminatory biases explanations generated reassuring even indistinguishable explanations produced unbiased model black explanatory methods also specific models operating nlp generally provide either numeric explanations explanations form textual example adaptation lime method nlp ribeiro provides explanations degree importance predictor variable generative method liu provides explanations simple textual example model methods number local explanatory methods specific type model first noted models directly interpretable logistic regressions linear regressions variants glm generalized linear models provided density limited additive models gam generalized additive models decision trees random forests least limited depth volume number explanatory methods specific deep learning models explanations form surrogate models particularly decision trees approximate neural network craven explanations based attention mechanisms choi explanations attribute decisions neural network certain predictor variables shrikumar lastly following methods domain explanations nlp algorithms based recurrent neural networks strobelt explanations algorithms example interpretable units bau uncertainty maps kendall aliency maps adebayo counterfactual explanation counterfactual explanations place among methods aiming explain algorithm insofar ones involving causal explanations grounded statistics inferences generalized large data lumes precisely counterfactual explanation prediction generated model input data given input data close possible would resulted prediction different general unfavourable outcome prediction decision example low credit score computed resulting credit application denied relevant explanation creator system auditor individual impacted outcome case applicant requested credit answer question change minimal possible credit application would led acceptance thus rather local explanation quantifies influence rious predictor variables age income credit history etc negative outcome far useful practical simple explanation obtained example household income much instead much credit would granted certain methods generating counterfactual explanations even goes beyond definition mcgrath methods produce positive counterfactual explanations apply cases original decision favourable individual considered using previous example correspond credit application denied thus counterfactual explanation indicates safety margin favourable outcome kind explanation may useful make inf ormed decision request another credit future given initial application accepted another enhancement achieved weighing explanatory factors based variability using yet credit scoring example individual proven better able reduce personal expenditures increase revenue enhancement method would produce explanation monthly expenses cut half credit would gran specific explanation indeed useful one involving features household income directly actionable ideally counterfactual explanatory methods applicable algorithms studies black xes certain methods fact satisfy condition well situations wachter global explanatory methods global explanatory methods provide explanation entirety decisions made model example contribution age variable decisions accept deny credit applications set applications global explanatory methods may useful internal control teams auditor order obtain understanding gene ral behaviour algorithm however usually show ability tackle causality promising deployment general within financial sector particular example explainability internal risk models implemented banking institutions would reinforced models enabled assess causal relations causal inference facto core concerns empirical economy since least years nevertheless missing commonly models well classical models currently deployed banks limitations compared study single concrete use case using local explanation several concrete use cases example compare algorithmic results two individual detect potential inequality treatment global explanatory methods also difficult materialize practice methods exist specific model types example possible extract deep neural networks set decision rules easy interpret according situation relatively faithful deep learning model considered deepred zilke addition methods able provide global explanation independently type model studied however case partial dependence plots pdp show marginal effect given variable model prediction friedman review potential attacks model security recent field study important enough object taxonomy taxonomy nevertheless constant evolution given changing nature field papernot among notorious attacks models following categories distinguished example scenario given type causative attacks data poisoning training data altered modified feature values new features created etc causative integrity attacks subcategory causative attacks used grant generous loans low insurance premiums malicious individuals causative availability attacks another subcat egory causative attacks used discriminate population group denying benefits rest population watermark attacks malicious actor modifies code algorithm watermark integrity attacks example conditions introduced certain input data features trigger advantageous outcomes chosen cases watermark availability attacks example rules injected code order suppress favourable outcomes target population group surrogate models attacks inversion attacks equivalent reverse engineering model membership tests another type surrogate model attacks operate inferring whether input data points belong original training set adversarial attacks construct synthetic examples order avoid detrimental outcome alternatively obtain favourable outcome impersonation attacks work injecting data corresponding real identity composi several real identities order usurp identity particularly interesting aspect security defences tend beneficial side effect hall namely bring model closer satisfying four design development principles mentioned document section appropriate data management performance stability explainability give one example defence data poisoning attacks roni method reject negative impact works rejecting training data decreases model accuracy barreno hence also protects degrading model performance due training data drift illustration facial recognition algorithm secured roni exclude training set series pictures associated document would significantly lower precision contributes ensuring integrity also performance model could example used banking ins titution remotely identify new customers use case commonly known kyc distance bibliograph louis abraham algorithms trust acpr mars peter addo dominique guégan bertrand hassani credit risk analysis using machine deep learning models ffhalshs julius adebayo justin gilmer michael muelly ian goodfellow moritz hardt kim sanity checks saliency maps neurips aeapp final report public consultation guidelines outsourcing cloud service providers eiopa daniel felix ahelegbey paolo giudici branka hadji latent factor models credit scoring systems physica statistical mechanics applica tions february maruan avinava dubey eric xing contextual explanation networks david alvarez tommi jaakkola towards robust interpretability self neural networks david baehrens timon schroeter stefan harmeling motoaki kawanabe katja hansen klaus robert muller explain individual classification decisions mach learn res marco barreno blaine nelson anthony joseph tygar security machine learning mach learn doi robert bartlett adair morse richard stanton nancy wallace consumer discrimination fintech era national bureau economic research david bau bolei zhou aditya khosla aude oliva antonio torralba network dissection quantifying interpretability deep visual representations cvpr roland berger road investment dynamics european ecosystem global index umang bhatt alice xiang shubham sharma adrian weller ankur taly yunhan jia joydeep ghosh ruchir puri josé moura peter ecke rsley explainable machine learning deployment biran ourtenay cotton explanation justification machine learning survey alexis bogroff dominique guégan artificial intelligence data ethics holistic approach risks regulation hal jenna burrell machine thinks nderstanding opacity machine learning algorithms big data society cambridge judge business school global regtech industry benchmark eport cambridge judge business school world economic forum transforming paradigms global financial services survey jianbo chen song martin wainwright michael jordan efficient model interpret ation structured data iclr edward choi mohammad taha bahadori jimeng sun joshua kulas andy schuetz walter stewart retain interpretable predictive model healthcare using reverse time attention mechanism nips mark craven jude shavlik extracting tree representations trained networks nips wei dai isaac wardlaw data profiling technology data governance regarding big data review rethinking information technology new generations advances intelligent systems computing isbn anupam datta shayak sen yair zick algorithmic transparency via quantitative input influence theory experiment learning systems security privacy ieee symposium ieee doshi kim towards rigorous science interpretable machine learning eba draft recommendations outsourcing cloud servic providers article regulation european commission high expert group ethics guidelines trustworthy artificial intelligence jerome friedman greedy function approximation gradient boosting machine annals statistics donna fuscaldo zestfinance using bring fairness mortgage lending aurélien garivier toward responsible artificial intelligence institut mathématique toulouse mars bryce goodman seth flaxman european union regulations algorithmic decision right explanation dominique guégan bertrand hassani regulatory learning supervise machine learning models application credit scoring ffhalshs patrick hall proposals model vulnerability security reilly lisa anne hendricks zeynep akata marcus rohrbach jeff donahue bernt schiele trevor darrell generating visual explanations michael hind sameep mehta aleksandra mojsilovic ravi nair karthikeyan natesan ramamurthy alexandra olteanu kush varshney increasing trust services supplier declarations conformity corr michael hind dennis wei murray campbell noel codella amit dhurandhar aleksandra mojsilović karthikeyan natesan ramamurthy kush varshney ted teaching explain decisions sartha jain byron wallace attention explanation konstantinos koutroumbas sergios theodoridis pattern recognition burlington isbn jung mueller pedemonte plances thew machine learning financial services bank england financial conduct authority toshihiro kamishima shotaro akaho hideki asoh jun sakuma fairness classifier prejudice remover regularizer flach eds ecml pkdd part lncs alex kendall yarin gal uncertainties need bayesian deep learning computer vision nips faye kilburn blackrock use machine learning gauge liquidity risk kim rajiv khanna oluwasanmi koyejo examples enough learn criticize criticism interpretability conference neural information processing systems nips barcelona spain kpmg compliance control tao lei regina barzilay tommi jaakkola rationalizing neural predictions emnlp zachary lipton mythos model interpretability hui liu qingyu yin william yang wang towards explainable nlp generative explanation framework text classification corr lloyd taking control artificial intelligence insurance emerging risk report scott lundberg lee unified approach interpreting model predictions nips scott lundberg gabriel erion hugh chen alex degrave jordan prutkin bala nair ronit katz jonathan himmelfarb nisha bansal lee explainable trees loca explanations global understanding mas monetary authority singapore principles promote fairness ethics accountability transparency use artificial intelligence data analytics singapore financial sector rory grath luca costabello chan van paul sweeney farbod kamia zhao shen freddy lécué interpretable credit application predictions counterfactual explanations tim miller explanation artificial intelligence insights social sciences margaret mitchell simone andrew zaldivar parker barnes lucy vasserman benhutchinson elena spitzer inioluwa deborah raji timnit gebru model cards model eporting proceedings conference fairness accountability transparency fat christoph molnar interpretable machine learning guide making black box models explainable james murdoch chandan singh karl kumbier reza abbasi bin yua interpretable machine learning definitions methods applications nicolas papernot patrick mcdaniel deep neighbors towards con fident interpretable robust deep learning nicolas papernot marauder map security privacy machine learning overview current future research directions making machine learning secure private proceedings acm workshop artificial intelligence security dong huk park lisa anne hendricks zeynep akata anna rohrbach bernt schiele trevor darrell marcus rohrbach multimodal explanations justifying decisions pointing evidence keyur patel marshall lincoln magic weighing risks financial services centre study financial innovation james proudman cyborg supervision app lication advanced analytics prudential supervision bank england pwc opportunities await insurtech reshaping insurance global fintech survey marco tulio ribeiro sameer singh carlos guestrin model interpretab ility machine learning icml workshop human interpretability machine learning marco túlio ribeiro sameer singh carlos guestrin trust explaining predictions classifier explainable nlp kdd andrew ross michael hughes finale doshi right right reasons training differentiable models constraining explanations lukas ryll sebastian seidens evaluating performance machine learning algorithms financial market forecasting comprehensive survey laurent serrurier point sur explicabilité interprétabilité machine learning irit novembre blake shaw tony jeb ara structure preserving embedding proceedings international conference machine learning montreal canada reza shokri marco stronati congzheng song vitaly shmatikov membership inference attacks machine learning models ieee symposium security privacy avanti shrikumar peyton greenside anshul kundaje learning important features propagating activation differences icml justin sirignano paar sadwhani kay giesecke deep learning mortgage risk dylan slack sophie hilgard emily jia sameer singh himabindu lakkaraju fooling lime shap adversarial attacks post hoc explanation methods hendrik strobelt sebastian gehrmann hanspeter pfister alexander rush lstmvis tool visual analysis hidden state dynamics recurrent neural networks ieee trans vis comput graph erik štrumbelj igor kononenko efficient explanation individual classifications using game theory journal machine learning research tapestry networks banks delay upgrading core legacy banking platforms berk ustun cynthia rudin supersparse linear integer models optimized medical scoring systems machine learning sandra wachter brent mittelstadt chris russell counterfactual explanations without opening black box automated decisions gdpr harvard journal law technology volume number spring kelvin jimmy ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel yoshua bengio show attend tell neural image caption generation visual attenti proceedings international conference machine learning pmlr jan ruben zilke eneldo loza mencía frederik janssen deepred rule extraction deep neural networks calders ceci malerba eds discovery science lecture notes computer science vol springer cham thanks authors document wish thank participants exploratory works described herein domain experts data scientists validation compliance teams well managers responding call applications active contribution discussions presentations workshops authors also wish thank acpr colleagues contributed brai nstorming analyses resulted document specifically jean barjon emmanuelle boucher nicolas carta laurent clerc aml unit richard diani thierry frigout cyril gruffat gauthier jacquemin boris jaros matthias laport farid oukaci jérôme schmidt pierre walckenaer
