© 05-2020 | Defender of Rights When it comes to the law , we are all equalAlgorithms : preventing automated discrimination In partnership with the CNIL Algorithms : preventing automated discrimination 3 Algorithms : preventing automated discrimination | 2020During the current global health crisis , the use of digital tools has increased and diversified as never before , resulting in major debates . These digital tools are often based on algorithms , although users are not always aware or informed . Recourse to algorithms as a basis for public or private decision-making is not a new phenomenon : the automated calculation of financial risk performed by banks ( `` scoring `` ) and which involves combining various criteria drawn from information provided by loan applicants has become more widespread over the last decades . Yet , as noted by the Conseil d ’ Etat , intensive use of algorithms as a result of computers ’ new calculation power and the mass processing of what is now a large amount of data marks an `` unprecedented turning point '' 1 . In just a few years , the use of algorithms has expanded to the private sector and to administrations2 . Today , such processes can be found in fields that are as essential to individuals as access to social benefits3 , policing and justice4 , the running of organisations such as hospitals , access to public services and recruitment procedures5.Since 2006 , machine learning technologies have taken off . Once rolled out , these learning systems continue to evolve , striving for perfection . These technological evolutions , which are still in progress , are undeniable sources of progress for individuals and society , allowing for quicker , more reliable and personalised results as well as new analyses in many fields . However , the Data Protection CommissionCNIL and the Defender of Rights have both , in their own area of expertise , voiced their concerns regarding the impact of these algorithmic systems on fundamental rights6 . It is with this mindset that the Defender of Rights is acting , in partnership with the CNIL , in the hope of highlighting the considerable risk of discrimination that each and every one of us is exposed to by the exponential use of algorithms in all aspects of our life . This topic has long been a blind spot in public debate . This must change . 1 Conseil d ’ Etat , Puissance publique et plateformes numériques : accompagner « l ’ ubérisation » , La documentation française , 2017 , p. 59 . 2 See , for example : The State ’ s Inter-ministerial Directorate for Digital , Information and Communications Systems ( Direction interministérielle du numérique et du système d'information et de communication de l ’ Etat ) , DINSIC , Guide des algorithmes publics 2019 . 3 National Delegation to Combat Fraud ( Délégation Nationale à la Lutte contre la Fraude ) , Le « data mining » , une démarche pour améliorer le ciblage des contrôles , Paris , 14 January 2014 . 4 Soraya Amrani Mekki , `` Justice prédictive et accès au juge '' , La Justice Prédictive , Actes du Colloque of 12 February 2018 organised by Conseil d ’ Etat and Cour de cassation Lawyers Council for its bicentenary in partnership with the Paris-Dauphine PSL University , Paris , Dalloz , 2018 . 5 Christine Bargain , Marie Beaurepaire , Dorothée Prud ’ homme , Recruter avec des algorithmes ? Usages , opportunités et risques , AFMD , 2019 . 6 CNIL , Travaux sur le système APB ( decision no . 2017-053 of 30 August 2017 ) ; Comment permettre à l'Homme de garder la main ? Rapport sur les enjeux éthiques des algorithmes et de l'intelligence artificielle , 15 December 2017 . Defender of Rights , Guide - Recruter avec des outils numériques sans discriminer published in 2015 , Opinion no . 15-25 of 1 December 2015 on security in stations ; Report titled `` Lutte contre la fraude aux prestations sociales : à quel prix pour les droits des usagers ? '' , September 2017 , Parcoursup decisions ( 2018-323 of 21 December 2018 and 2019-21 of 18 January 2019 ) , Opinion 18-26 of 31 October 2018 on the Draft Programming and Reform Act for Justice , opinion 19-11 of 5 September 2019 on the Draft Act on Bioethics.Introductory remarks 4 Algorithms : preventing automated discrimination | 2020At first glance , algorithms sort , categorise and organise information by eliminating any prejudice and bias specific to human beings . Thus , they should be able to ensure the equal treatment expected by applying the same criteria and weighting regardless of the requester ’ s original or sexual orientation for example . In reality though , there is no technological magic or mathematical neutrality : algorithms are designed by humans using data that mirror human practices . As such , bias can be introduced into every stage of the development and deployment of systems : as from the intention that initially governs the algorithm ’ s development , during the creation of the computer code , the executable code , during execution , in the context of execution and maintenance7 . Some completely intentional bias can also result from the inclusion of prohibited grounds for discrimination in an algorithm . Some reasons can be taken into account to justify the criteria used by an algorithm in some specific cases such as state of health for insurance , age for bank loans or the place of residence to adjust premiums , if their use is considered proportionate to a legitimate purpose8 . However , criteria such as gender or origin can not constitute lawful criteria , regardless of context . Nevertheless , the discriminatory effects of algorithms are often based on mechanisms that are less visible than the inclusion of easily-identifiable prohibited grounds for discrimination in the algorithm.Biased data Discriminatory mechanisms are frequently based on the bias of the data selected and used by a traditional algorithm or fed into a learning algorithm during its learning phase and after . One of the most frequent biases is based on a lack of representativity in the data used . For example , in 2018 , a study explained why some facial recognition systems , which are based on learning techniques9 , found it harder to identify women , people who are not white and more so women of colour , by generating a high error rate for these populations : the datasets that this model was based on was characterised by a large predominance of male white faces10 . The issue is similar for voice identification technologies : having not been designed with women and their voices in mind , and not having been built ( and therefore fed with `` female '' data ) and tested in this regard , the system does not work as well for women11 . The data integrated into algorithmic systems or used to teach a machine learning system can also be biased when they are the mathematical result of past oftendiscriminatory practices and behaviour and of systemic discrimination present in society . 7 Barocas S. , Selbst and Andrew D. `` Big data ’ s disparate impact '' , California Law Review , June 2016 Vol . 104 , no . 3 , pp.671-732 . 8 C.E. , 30 October 2001 , no . 204909 , association française des Stés financières . See the article `` Testing , scoring , ranking ... '' , Revue trimestrielle de droit civil , July-September 2002 , no . 3 , p. 498 . 9 CNIL , Reconnaissance faciale . Pour un débat à la hauteur des enjeux , 15 November 2019 . 10 According to MIT researcher Joy Buolamwini ’ s study , the error rates of Amazon ’ s Rekognition software were 1 % for lighter-skinned men , 7 % for lighter-skinned women , 12 % for darker skinned men and 35 % for darker skinned women . See Hardesty , Larry . `` Study Finds Gender and Skin-Type Bias in Commercial Artificial-Intelligence Systems . '' MIT News , 11 February 2018 . 11 During an internship focused on voice recognition for helicopter pilots , when a woman was put at the commands , the system did not work as well , representing a serious safety issue . ( TUAL M. , `` La diversité humaine est un enjeu central pour le développement de l ’ intelligence artificielle '' , Le Monde , 30/07/2018 ) .How can algorithms be discriminatory ? 5 Algorithms : preventing automated discrimination | 202012 EEOC , Conference - Big Data in the Workplace : Examining Implications for Equal Employment Opportunity Law , 13 October 2016 . 13 Decision 2019-021 of 18 January 2019 on the operation of the national platform for pre-registration for the first year of higher education ( Parcoursup ) . 14 See KIM PT , `` Data-driven discrimination at work '' , 58 Wm . and Mary Law Review 857 , 2016 . 15 `` The algorithms that detect hate speech online are biased against black people '' , 15 August 2019 , vox.com.In available employment data , women are less well represented and tend to occupy certain business sectors and lower positions with lower pay . Based on such data , an algorithm might deduce that women are not as productive as men and do not reach positions of responsibility . As a result , an algorithm used for recruitment based on biased data will reproduce such biases , and even exacerbate them12 . False neutrality of algorithms and true discriminatory effects The use of apparently neutral criteria , i.e . criteria that does not include prohibited grounds for discrimination , can have discriminatory effects as highlighted by the Defender of Rights in its Parcoursup decision13 . In this case , university algorithms which take into account the seemingly neutral criteria of institution of origin could , indirectly , result in discriminating against youths of foreign origin , given the strong residential and educational segregation observed in Ile-de-France in particular . Most often , these discriminatory effects are caused by the combination of several neutral criteria . The criteria and data in question may even seem far removed from prohibited reasons , but their correlation provides similar results to those which would have been obtained had the protected characteristic been applied . Learning algorithms , and the many correlations they make between massive amounts of data , can easily generate such effects . In this case , belonging to a protected category is encoded in `` neutral '' data . Designed to maximise its ability to find similar characteristics among massive amounts of data , the programme recreates a whole matching the protected category , and applies specific processing to it . In order to target its advertising , the American supermarket company Target developed a predictive model to identify pregnant clients based on their purchase habits concerning 25 products14 . Such models could be used for discriminatory purposes or could have discriminatory effects . Algorithms may combine several sources of bias and ruin even the best intentions . Several American studies have recently demonstrated the discriminatory nature of the main `` smart '' system models used to automatically detect hate speech for moderation purposes : the probability of having one ’ s message reported as offensive or hate speech by the system was 1.5 times higher among Afro-American internet users . Such biases come from learning data : the panel of data was created by humans who first classed the messages containing abusive language as offensive or hate speech . These biases are also exacerbated by the technical limits of the system which struggles to identify the nuances of a language and put slang or sarcastic statements in their context for example15 . 6 Algorithms : preventing automated discrimination | 2020The discriminatory effects of algorithms can often only be measured by researchers at group level . They risk remaining completely invisible for victims . Furthermore , while the cognitive biases of one human being vary depending on circumstances and contingently translate into discriminatory practices , the discriminatory biases integrated by an algorithm are applied automatically and could systematise discrimination . There is a significant risk of reinforcing `` essentialization '' and `` stereotypes '' as the algorithm ’ s predictive nature is based on the behaviour or homogenised characteristics of groups of people . These systems therefore could `` reinforce discrimination and prejudices by giving them an appearance of objectivity '' 16 . While the discriminatory effects of the algorithm are not always identifiable at individual level , the seemingly neutral algorithmic system may result in discrimination against protected social groups , which could translate , for example , into lesser access to the goods sought or a higher error rate produced by the system in their regard . This risk of discrimination is even greater for social groups having already been the victim of major systemic discrimination in society , for example women , people with a disability or immigrants . By integrating former discriminatory practices as part of a dataset used for its learning phase , the bias of `` smart '' systems tends to increase as they are rolled out . Predpol software enables many police forces to direct their action and `` rationalise '' their activity by identifying `` hot points '' where there is a higher risk of offences being committed , in order to increase patrols . This model also takes accounts of influence factors such as population density , the proximity of bars or means of transport . However , the predominance of information on the places where past offences and crimes have been committed is problematic . In the United States as in other countries , police controls , arrests and places where they decide to patrol target minorities and certain areas much more than others . Based on Predpol ’ s suggestions , police forces would be mainly directed to these districts and would observe new offences , thereby feeding the learning base with new biased data . Algorithms could thus cause feedback loops in which stereotypes , discrimination and inequality mutually reinforce one another and contribute towards the long-term crystallisation of situations of inequality17 . Only by precisely and regularly checking the learning algorithm ’ s results can it be ensured that the algorithm does not become discriminatory over the course of its successive encoding . Lastly , it should be added that these systems tend to target and control , and therefore stigmatise , members of alreadyunderprivileged and dominated social groups more than others18 . In 2019 , several associations brought legal action against the Dutch State to have an algorithm developed by the Ministry of Social Affairs and Employment to predict the likelihood of an individual committing benefit and tax fraud declared unlawful . 16 Dunja Mijatovic , Commissioner for Human Rights , `` Safeguarding human rights in the era of artificial intelligence '' , Commissioner for Human Rights Comment , Strasbourg , 3 July 2018 . 17 Hiring by Algorithm : predicting and Preventing disparate impact - Ifeoma Ajunwa , Sorelle Freidler , Carlos Scheidegger , Suresh Venkatasubramanian ; Draft of January 2016 . 18 Virginia Eubanks , Automating inequalities . How High-tech tools profiles , police , and punish the Poor ; St. Martin 's Press , January 2018.Invisible and potentially massive discrimination 7 Algorithms : preventing automated discrimination | 2020During the hearing , the government acknowledged that this algorithm targeted districts containing a higher number of social benefit recipients , despite the lack of evidence that these districts showed a higher benefit fraud rates19 . 19 Open Democracy , `` Welfare surveillance on trial in the Netherlands '' , 8 November 2019 . The Hague Court issued a decision on 5 February 2020 acknowledging that the government had breached the right to privacy and family life set out in Article 8 of the ECHR and ordered that it cease using this algorithm . The judges based their decision on the fact that the algorithm Syri lacked transparency . The court did not address a possible breach of Article 22 of the GDPR which bans automated decision-making in some cases . 20 Cathy O ’ Neil , Algorithmes . La bombe à retardement , Les Arènes , 2018 ( USA , 2016 ) . 21 Cédric Villani , Donner un sens à l ’ intelligence artificielle : pour une stratégie nationale et européenne , Report to the Government , 8 March 2018 . 22 Telecom Paris Tech , Algorithmes : biais , discrimination et équité , February 2019 ; Aude Bernheim , Flora Vincent , L ’ intelligence artificielle , pas sans elles ! , Laboratoire de l ’ égalité , Belin editions , 2019 ; Institut Montaigne , Rapport Algorithmes : contrôle des biais SVP , March 2020 ; Collective report ordered by the Etalab mission , Ethique et responsabilité des algorithmes publics , ENA , Class of 2018-2019 `` Molière '' , June 2019 . 23 40th International Conference of Data Protection & Privacy Commissioners , Declaration on Ethics and Data Protection , 23 October 2018 . 24 Equinet , Regulating for an equal AI : A New Role for Equality Bodies . Meeting the new challenges to equality and non-discrimination from increased digitisation and the use of Artificial Intelligence , June 2020.The right to non-discrimination must be effectively respected under all circumstances , including when a decision involves recourse to an algorithm . The extensive use of algorithms is - in the words of Cathy O ’ Neil - a `` weapon of Math destruction '' as regards equality issues20 . Nevertheless , despite the first alarm bells rang by the Villani report21 and a few initiatives22 , awareness is slow to emerge in France : algorithm designers , like the organisations buying and using these types of systems , do not demonstrate the necessary vigilance to avoid a type of invisible automated discrimination . Yet , the fairness principle , which poses the notion of `` users ’ interests '' as an obligation for the person responsible for the algorithm , like the principle of vigilance and reflexivity which involves regular , methodical and deliberative checks on learning objects , should guide reflection and action23 . It should be reminded that non-discrimination is not an option but is part of a legal framework which sets out an analysis grid to identify situations of unequal treatment in order to implement a fundamental right : that to not be discriminated.Organisations using algorithms can not escape their responsibilities under the cover of ignorance , technological incompetence or opaque systems . Algorithmic biases must be able to be identified and corrected and those responsible for discriminatory decisions as a result of algorithmic processing must be sanctionable . As highlighted by existing literature , the lack of transparency of the systems implemented and the data correlations enabled by algorithms , often entirely invisibly , render the protection offered by law uncertain and even ineffective . Thus , how can one exercise a right to recourse when one is not even aware of being the victim of discrimination as a result of an algorithm , when the organisation using the algorithm itself is not aware of it , when the designer of the algorithm will not or can not explain how such a tool works ? How can one find out whether an algorithm is discriminating a given social group ? And , if such is the case , how can these breaches to rights be sanctioned ? The work carried out alongside our European counterparts as members of the Equinet network24 , such as the cross-disciplinary seminar on `` Algorithms , bias and combatting discrimination '' organised on 28 and 29 May 2020 in partnership with the CNIL , Recommendations 8 Algorithms : preventing automated discrimination | 202025 These guidelines are interdependent and reinforce one another : Diversity , non-discrimination and fairness ( no . 5 ) , Accountability and Transparency ( n°7 ) , human agency and oversight , guideline ( no . 1 ) for example . 26 Following the deployment of the GDPR , in February 2020 , the European Commission published a white paper promoting an AI approach based on trust , the recommendations in which are largely drawn from works of a European expert group . 27 Sarah Myers West , Meredith Whittaker and Kate Crawford , Discriminating systems : Gender , Race and Power in AI , AI Now , New York University , April 2019.has highlighted the lack of legal and technical expertise and the need to devise effective countermeasures . The guidelines published by the European Commission in April 2019 provide indications25 and European experts ’ first conclusions26 call for mobilisation to match the stakes . Without in-depth reflection and mobilisation by public authorities , there is a significant risk in France that the right to non-discrimination will not be able to fulfil its purpose and protect the population . As part of its mission to combat discrimination and promote equality , the Defender of Rights therefore wishes to raise awareness , in partnership with the CNIL , of the need to mobilise as from today to prevent and correct such discrimination . While awaiting such a mobilisation , which the Defender of Rights intends to fully participate in over the coming months , the following aims should contribute towards launching and structuring a necessary collective reflection . Inform and raise awareness amongst professionals The social reality of discrimination and the framework of anti-discrimination law are still little known and rarely considered by data and algorithm experts in Europe . There are significant acculturation and training issues , with IT and data analysis professions - which are often criticised for lacking diversity27 - being still too unaware of the risks to fundamental rights caused by algorithms . Reciprocally , professionals who purchase and use such processes within organisations should be trained to `` keep a handle '' and a critical eye on algorithms .Support research to develop studies to measure and methods to prevent bias Available studies and analyses have started to show the magnitude of algorithm ’ s discriminatory bias , but these still widely relate to systems implemented in the United States . In order to take account of the significantly more regulated and limited deployment of algorithms in Europe and of our specific demographic and social contexts , these analyses must be developed in the European Union and in France . Public research organisations and public procurement could support these approaches and experiments which require real statistical expertise and a cross-disciplinary approach combining computer engineering ( to understand and handle issues ) , economy ( to measure any potential discrimination ) and the law ( to qualify the discrimination ) . Exploring `` fair learning '' perspectives , i.e . the design of algorithms meeting equality and explainability objectives and not merely performance objectives , is another major research challenge . 9 Algorithms : preventing automated discrimination | 202028 Article R. 311-3-1-2 of the Code on relations between the public and the administration 29 Collective report ordered by the Etalab mission , Ethique et responsabilité des algorithmes publics , ENA , Class of 2018-2019 `` Molière '' , June 2019 . 30 Decision no . 2020-834 Priority preliminary ruling on the issue of constitutionality of 3 April 2020 . The judges considered that the limits imposed by law to the exercise of the right to access administrative documents were justified by legitimate interest grounds and proportionate to this objective , i.e . the secrecy of deliberations protecting the independence of educational teams and the authority of their decisions . However , it makes one important reservation : each higher education establishment `` must account for - to use the terms of Article 15 of the declaration - of the criteria that it has used , where applicable using algorithmic means of processing , to study the applications sent on Parcoursup . `` ( French Constitutional Council commentary , p. 26 ) . See also Defender of Rights , Decision 2019-099 of 8 April 2019 on the operation of the Parcoursup platform , particularly the lack of transparency of the allocation procedure and the rejection of the request for communication of the algorithmic procedures used by the association made by the French Conseil d ’ Etat ( 12 June 2019 , no . 427916 ) .Reinforce algorithms ’ information , transparency and explainability requirements The user ’ s right to information on the one hand , and transparency and explainability on the other , are clear pre-conditions to measure discrimination , monitor systems and ensure the effectiveness of the right to recourse . However , the opacity of systems and their secret nature are an obstacle to the discovery of potential biases and to recourse ; this obstacle is all the more troublesome when the algorithm ’ s result condition access to fundamental rights and public services . The GDPR provides the first substantial solutions to these issues . For example , for reasons of transparency , its Article 13 sets out the obligation of providing `` meaningful information about the logic involved '' in any automated decision-making with a significant impact on the data subject . Furthermore the CRPA ( Code on Relations between the Public and the Administration ) - completed by the Digital Republic Act of 2018 , specifies which information must be provided to the recipient of the individual decision regarding the `` degree and method of contribution of the algorithmic processing to the decision-making process `` , `` the data processed and its source `` , and `` the processing parameters and [ ... ] weighting applied to the data subject `` 28.To combat discriminatory bias , the legal requirements of information , transparency and explainability should be further developed . Firstly , all of these requirements should not only be restricted to decision-making algorithms and those involving personal data processing29 . Furthermore , they should be applied to both private and public sector algorithms . Lastly , the different requirements based on the level of automation of decisions should be reviewed : human intervention - which is sometimes formally provided for in many algorithmic processing operations - should not be merely symbolic and only actually provide artificial protection . When they exist , transparency requirements in respect of third parties are still insufficient as noted by the Constitutional Council in its decision of 3 April 2020 on Parcoursup30 . Third parties and not only the recipients of individual decisions should be able to access the criteria used by the algorithm to allow them to detect potential cases of bias . The general information published on algorithmic processing and the individual explanations regarding a given decision must , in all cases , be provided to the public and to users in an accessible and intelligible language . Professionals involved in algorithmic processes - whether employees or public servants - must be informed so that they are able to understand the tool ’ s general operation , increase their vigilance as regards the risk of bias and ensure that they have effective control over the processing . 10 Algorithms : preventing automated discrimination | 202031 AIA is part of a broader framework of the Act on public finance management and its Directive on automated decision-making , having entered into force on 26 November 2018 . This test sets out the responsibilities of federal institutions as regards the use of automated decision-making systems for administrative decisions . 32 See on the CNIL ’ s website , the infographic on algorithms for which DPIAs are required which repeats the positions adopted by the European Data Protection Committee ( WP29 ) . 33 White Paper on Artificial Intelligence , A European approach to excellence and trust , European Commission , COM ( 2020 ) 65 final . 34 Reco 1.4 ( B ) Recommendation CM/Rec ( 2020 ) 1 of the Committee of Ministers to member States on the human rights impacts of algorithmic systems.Perform impact assessments to anticipate algorithms ’ discriminatory effects The principle of explainability and the identification of potential bias seem to clash with the `` black boxes '' that many algorithms become when the secret regarding the code is not revealed or when a learning algorithm is opaque . The issue of monitoring the effects of these systems must therefore be resolved from the algorithm ’ s design phase or during their learning phase . In Canada , audits including discrimination issues are required of public institutions since 1 April 2020 and the Federal Government has set up a platform - the AIA ( algorithmic impact assessment ) - to assist administrations with these impact assessments31 . Such a requirement could be introduced in France based on the Data protection impact assessment ( DPIA ) model already provided for by Article 35 of the GDPR . This prior analysis , which is mandatory for some algorithms , must include an assessment of risks to the rights and freedoms of individuals and is therefore already a means of anticipating such discriminatory effects . However , expressly providing for the assessment of these biases as part of impact assessments or rendering these mandatory for all algorithmic processing operations32 would ensure effective compliance with the principle of nondiscrimination . In addition to prior assessment , the regular monitoring of algorithms ’ effects after deployment should be required based on the model applied to monitor the side-effects of medicines . Many questions still remain and some answers must be clarified . In any case , the methods and means to be implemented must ensure respect for our fundamental rights and freedoms in the face of the technological and economic frenzy surrounding what is commonly referred to as `` artificial intelligence '' . For example , should we - as suggested by the European Commission - adopt a risk-based approach , which would increase the level of requirement and monitoring based on an algorithm ’ s use and expected impact33 ? Are audit or accreditation procedures enough to ensure our rights are respected ? How can we prove discrimination ? Should we set up - as recommended by the Council of Europe - an institutional or regulatory framework and algorithm standards according to main sectors34 ? The Defender of Rights will continue its reflection on this topic and will contribute to the reflection carried out by public decisionmakers , notably in partnership with the CNIL , but also with Etalab , CNNum , the CNCDH , academics having participated in the seminar and the European network Equinet . In this perspective , guaranteeing respect for the rights of every individual , and in particular the right to not be discriminated against , will be its only compass . © 05-2020 | Defender of Rights— Defender of Rights TSA 90716 - 75334 Paris Cedex 07 Tel . : 09 69 39 00 00 www.defenseurdesdroits.fr — www.defenseurdesdroits.frFind all our news at :
