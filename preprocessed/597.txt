white paper prevent discriminatory outcomes machine learning march future council human rights contents world economic rights reserved part publication may reproduced transmitted form means including photocopying recording information storage retrieval system ref case paper written world economic forum global future council human rights findings interpretations conclusions expressed herein result collaborative process facilitated endorsed world economic forum whose results necessarily represent views world economic forum entirety members partners stakeholders individual global future council members listed contributors foreword executive summary introduction section challenges issues around data data used train machine learning applications sources risk around training data machine learning applications use case unequal access loans rural farmers kenya use case unequal access education indonesia concerns around algorithm design risk discrimination algorithm design deployment use case exclusionary health insurance systems mexico scenario china social credit scores section responsibilities businesses principles combating discrimination machine learning bringing principles life human rights due diligence machine learning making human rights due diligence machine learning effective conclusion appendix appendix challenges companies appendix principles ethical design use autonomous systems appendix areas action matrix human rights machine learning acknowledgements prevent discriminatory outcomes machine learningforeword world economic forum global future council human rights works promote practical solutions human rights challenges context unfolding fourth industrial revolution paper evolved conversations among members council dozens experts fields human rights machine learning using machines find patterns large quantities data make predictions patterns unlocking many new kinds value better ways diagnose cancer enabling cars creating new opportunities individuals machine translation example break linguistic barriers voice recognition empower illiterate people council chosen focus companies designing implementing technology maximize potential benefits work heeds call forum founder executive chairman klaus schwab ethical standards apply emerging technologies rightly says urgently needed establish common ethical guidelines embed society white paper offers framework understanding potential risks machine learning applications discriminatory outcomes order arrive roadmap preventing different applications require different actions combat discrimination encourage dignity assurance white paper offer set transferable guiding principles particularly relevant field machine learning base approach rights enshrined universal declaration human rights elaborated dozen binding international treaties provide substantive legal standards protection respect human rights safeguarding emphasis risks meant undersell promise machine learning halt use concern around discriminatory outcomes machine learning upholding human rights also maintaining trust protecting social contract founded idea person best interests served technology using used absent trust opportunity use machine learning advance humanity set back many companies begun explore ideas fairness inclusion accountability transparency machine learning including microsoft google deepmind alphabet pervasive justifiable concerns remain efforts promote transparency accountability might undermine companies rights trade secrets security cases right privacy however systems continuing influence people socially sensitive spaces housing credit employment education healthcare etc mostly absence adequate government regulation whether due technology outpacing regulatory mechanisms lack government capacity political turmoil unfavorable conditions need active private companies erica kochi global future council human rights unicef innovation schwab fourth industrial revolution geneva world economic forum res june accessed september prevent discriminatory outcomes machine learningexecutive summary machine learning systems already used make decisions job applicants hired mortgage applicants given loan prisoners released parole decisions affect human rights often vulnerable people society designed used well machine learning systems help eliminate kind human bias society working hard stamp however also possible machine learning systems reinforce systemic bias discrimination prevent dignity assurance example historical data employment may show women getting promoted less men machine learning system trained data concludes women worse hires perpetuate discrimination discriminatory outcomes violate human rights also undermine public trust machine learning public opinion becomes negative likely lead reactive regulations thwart development machine learning positive social economic potential challenges algorithmic aids used decades machine learning posing new challenges due greater complexity opaqueness ubiquity exclusiveness challenges related data used machine learning systems large datasets needed train systems expensive either collect purchase effectively excludes many companies public civil society bodies machine learning market training data may exclude classes individual generate much data living rural areas countries opted sharing data data may biased even machine learning algorithms trained good data sets design deployment could encode discrimination ways choosing wrong model wrong data building model inadvertently discriminatory features absence human oversight involvement unpredictable inscrutable systems unchecked intentional already examples systems disproportionately identify people color higher risk committing crime systematically exclude people mental disabilities hired risks especially high countries existing inequalities often deeper training data less available government regulation oversight weaker implications many human rights least right privacy focus discrimination growing evidence salience wide range entities globally including involved data collection algorithm design employ systems developed third party principle nondiscrimination critical human rights whether civil political like rights privacy freedom expression economic social like rights adequate health housing responsibilities business governments international organizations role play regulations tend keep pace technological development white paper makes case businesses need integrate principles nondiscrimination empathy human rights due diligence process businesses take ongoing proactive reactive steps ensure cause contribute human rights abuses international human rights law companies respect human rights according guiding principles business human rights responsibility respect human rights exists compliance national laws regulations protecting human even lack regulation specifically machine learning human rights principles obligations still apply prevent discriminatory outcomes machine learningdrawing existing work propose four central principles combat bias machine learning uphold human rights dignity active inclusion fairness right understanding access remedy active inclusion development design applications must actively seek diversity input especially norms values specific populations affected output systems fairness people involved conceptualizing developing implementing machine learning systems consider definition fairness best applies context application prioritize architecture machine learning system evaluation metrics right understanding involvement systems affects individual rights must disclosed systems must able provide explanation understandable end users reviewable competent human authority impossible rights stake leaders design deployment regulation technology must question whether access redress leaders designers developers systems responsible identifying potential negative human rights impacts systems must make visible avenues redress affected disparate impacts establish processes timely redress discriminatory outputs recommend three steps companies identifying human rights risks linked business operations propose common standards assessing adequacy training data potential bias established adopted multistakeholder approach taking effective action prevent mitigate risks propose companies work concrete ways enhance company governance establishing augmenting existing mechanisms models ethical compliance transparent efforts identify prevent mitigate human rights risks propose companies monitor machine learning applications report findings working certified auditing bodies ways analogous industries rare mineral extraction large multinational companies set example taking lead results audits made public together responses company recognize much work still speculative given nascent nature applications particularly global south incredible rate change complexity scale issues hope report advance internal corporate discussions topics contribute larger public debate following release white paper hope actively work members forum see recommendations fit business practices variety players working build engage machine learning applications compared prior waves technological change unprecedented opportunity prevent negative implications early stage maximize benefits millions active inclusion fairness right understanding access remedyactive inclusion fairness right understanding access remedyactive inclusion fairness right understanding access remedyactive inclusion fairness right understanding access remedy prevent discriminatory outcomes machine learningintroduction contrast traditional programming people solution problem machine learning system sifts data recognizes patterns automates based discoveries machine learning kind artificial intelligence glossary terms see appendix nuances works may difficult digest promise plain increased efficiency accuracy scale speed making decisions finding best answers questions ranging type illness next systems could potentially increase fairness making decisions humans biased system sifting job applications might example ensure women ethnic minority candidates fairly considered however systems also opposite reinforcing kinds systemic bias discrimination society working hard stamp systems still nascent even developed economies already examples weapons math destruction cathy neil cites systems disproportionately identify people color higher risk committing crime systematically exclude people mental disabilities hired applications already used make many decisions qualifies loan whether someone given parole type care child receive social service programs decisions affect human rights especially society vulnerable framed universal declaration human rights pillar international legal system since idea human rights simple powerful people free equal right treated machine learning disproportionately harmful countries existing inequalities often deeper training data less available government regulation oversight weaker many current applications might seem relevant human rights image recognition systems used tag photos social media however easy conceive scenarios become image recognition systems example identify person sexual orientation reasonable accuracy consider might used governments countries homosexuality illegal potential bias discrimination goes well beyond sectors lending insurance hiring employment education cathy neil says predictive models increasingly tools relying run institutions deploy resources manage discriminatory outcomes violate human rights also undermine public trust machine learning public opinion machine learning becomes negative likely lead reactive regulations poorly informed unimplementable costly thwarts development machine learning close myriad opportunities use good augmenting capabilities individuals opening new ways apply talents new model needed machine learning developers deployers address human rights implications products accelerating innovation responsible pwc derek hawkins researchers use facial recognition tools predict sexual orientation lgbt groups happy washington post prevent discriminatory outcomes machine learningsection challenges algorithmic aids used decades banks instance automating mathematical functions score eligibility credit applicants experiences show algorithms discriminate unexpected ways example race included data sets credit applicants use proxy indicators still result racial minorities access credit unfairly limited regulations adopted prevent accidental discrimination machine learning posing new challenges due greater complexity opaqueness ubiquity exclusiveness complexity past algorithmic systems relied reasoning systems create complex models difficult trace decisions back ask questions made offer logical flow human understand different traditional software explains guy katz postdoctoral research fellow computer science opaqueness past systems one could easily determine source discriminatory decision put place ways prevent machine learning systems opaque due complexity also proprietary nature algorithms lack transparency auditability contributes popular understanding systems black ubiquity many people particularly europe already interact machine systems daily examples come new york times algorithms decide kids school often garbage picked police precincts get officers building code inspections targeted even metrics used rate teacher exclusiveness systems require massive data sets learn programmers technical education exclude huge subsets people systems today almost entirely developed small homogenous teams often data sets often proprietary require largescale resources collect purchase great strides made sharing datasets transfer learning technology minimizes data needed develop systems companies massive proprietary datasets still definite advantages challenges manifest two categories related data related way algorithms designed developed appendix summarizes companies tackle issues explored section marina krakovsky finally peek inside black box machine learning systems machine learning power promise computers royal society jim dwyer showing algorithms behind new york city services new york times kate crawford artificial intelligence white guy problem new york times important new challenges around machine learning technologies worth bearing mind past recommendations eliminate bias computer systems hold value relevance friedman nissenbaum identified three categories bias computer systems including preexisting roots social institutions practices attitudes technical arises technical constraints considerations emergent arises context use read bias computer systems prevent discriminatory outcomes machine learningissues around data training data foundation machine learning data used train machine learning applications machine learning requires data train example applications determine eligibility lending housing insurance traditionally draw factors historical salary ranges payment debt histories family situations residence status education employment opportunities historical data used include grades time spent prior jobs number promotions received countries regulations prevent use factors gender race religion marital status determine access housing insurance education employment others regulations exist enough resources political enforce increasingly social media mobile usage data inform decisions likely credible loan recipient good enable lenders employers assess applicant spending habits risky behavior work education histories professional networks little regulation exists area implications less well understood example application training data demonstrates people influential social networks active social networks good employees application might filter people backgrounds attended less prestigious schools cautious posting social media sources risk around training data machine learning applications data availability many cases data belong mobile network operator internet platform service provider collecting often companies generate purchase data choose keep private companies public civil society bodies lack resources purchase collect data effectively excluding participating machine learning market often groups generate smaller digital footprint include traditionally discriminated people countries example household one home automation product generate data point every six seconds mozambique population lack internet access average household generates zero digital data points south asia fewer women men phone women countries report using phones less frequently intensively men especially mobile live rural areas thinner digital footprint global population lack access basic voice text services mobile broadband internet mostly rural asia time consumer lending social strengthen underwriting grow customer base social media data pwc internet things privacy security connected world ftc pdf bridging gender gap mobile access usage middleincome countries gsma rural coverage strategies sustainability gsma ownload prevent discriminatory outcomes machine use case unequal access loans rural farmers kenya kenya microloan company tala smartphone app collects data loan applicants including number people contact daily movements routine habits like whether call mother every day pay bills time tala suggests using inputs gauge credit risk offers alternative pathway credit lack credit however risks rural kenyans less digital infrastructure fewer opportunities develop digital footprint might unfairly excluded algorithms trained data points captured urban populations kathleen siminyu artificial intelligence income countries east african experience solon barocas big data disparate impact biased data computing law garbage garbage dictates training systems limited biased errorstrewn data lead biased models discriminatory outcomes example historical data employment often show women getting promoted less men women worse jobs workplaces historically biased systems trained understand women worse hires men continue favor men continue generate discriminatory baseline data data mining automated process extracting useful patterns large data sets particular patterns serve basis subsequent especially sensitive statistical bias helps discover patterns organizations tend treat generalizable even though analyzed data includes partial sample circumscribed period ensure data mining reveals patterns hold true widely sample must proportionally representative population use case unequal access education indonesia indonesia economic development unfolded disparately across geographical subsequently ethnic lines example access higher education relatively uniform across country top universities island java large majority students attend universities java firms hiring sectors train systems screen applicants based factors like educational attainment status may systematically exclude poorer islands papua concerns around algorithm design modeling fairness risk discrimination algorithm design deployment even machine learning algorithms trained good data sets design deployment could encode discrimination five main ways prevent discriminatory outcomes machine choosing wrong model algorithms often designed based algorithms proven successful ostensibly similar contexts algorithms work well one context may discriminate systems used predictive policing example based earthquake modeling earthquakes recorded consistently crimes predictive policing models may skew towards overpredicting crime areas reporting rates higher similarly algorithm successfully assesses relative risk applicants loans may overlook relevant data points deployed countries building model inadvertently discriminatory features humans define algorithms success looks like usually means maximizing profits accuracy efficiency rather maximizing example one model tasked predicting likelihood similar error rate black white defendants likely err wrongly predicting black defendants would white defendants would humans specify weight algorithms give variables create bias example algorithm assess loan applicants may consider income levels reliability past repayments human decision give weight former may unfairly discriminate members groups tend women teams tendency develop conforming approaches hinder ability innovate spot incorrect absence human oversight involvement machine learning becomes sophisticated includes less human supervision however human loop necessary notice important factors unexpectedly overlooked example university pittsburgh medical center used predict pneumonia patients low risk developing complications could sent home model recommended doctors send home patients asthma seen data developed complications doctors however knew routinely placed patients intensive care precaution impossible define advance discrimination may happen given context humans need kept involved systems made interpretable unpredictable inscrutable systems human makes decision whether hire someone inquire decided one way systems lack transparency traceability sometimes matter may need understand behind google maps determines suggested route decisions impact rights however imperative instance system makes decisions parole identifying remedying possible discrimination depends able understand steps taken reach decisions criminal justice public housing welfare health provision examples areas black box systems developed used unchecked intentional discrimination cases bias intentionally built algorithms instance employers want avoid hiring women likely become pregnant might employ systems identify filter subset women absence adequate regulation burden lies company leadership designers data scientists engineers others involved creating systems build ways predict prevent monitor bias calders zliobaite unbiased computational processes lead discriminative decision procedures custers calders schermer zarsky eds discrimination privacy information society cited algorithmic accountability interview cathy neil input svetlana sicular gartner useful overview interpretability means machine learning please read lipton mythos model interpretability aaron bornstein artificial intelligence permanently inscrutable nautilus september use case exclusionary health insurance systems mexico mexico among countries quality healthcare available private insurance least two private multinational insurance companies operating mexico using maximize efficiency profitability potential implications human right fair access adequate healthcare imagine scenario insurance companies use mine data shopping history recognize patterns associated customers charge poorest sickest would least able afford access health services scenario china social credit scores details publicly available reports suggest china creating model score citizens analyzing wide range data banking tax professional performance records smartphones social aim speculated use data enforce moral authority designed communist one open question mean governments act scores computed using data incomplete historically biased using models built fairness julie makinen china prepares rank citizens social los angeles times november julie makinen china prepares rank citizens social los angeles times november prevent discriminatory outcomes machine ungps endorsed human rights council office high commissioner human rights guiding principles business human rights implementing united nations protect respect remedy framework online guiding principles business human rights ungps commentary principle ungps commentary principle germany adopts vehicles law reuters online may responsibilities businesses international human rights law states primary obligation uphold human rights companies respect human rights according guiding principles business human responsibility global standard expected conduct business enterprises wherever responsibility respect human rights exists independently states abilities willingness fulfil human rights obligations exists compliance national laws regulations protecting human given complex nature rapid pace technical development governments unlikely able develop legal regulatory frameworks protect human rights deployment use timely effective manner occasionally regulators get ahead widespread deployment new technologies example germany introduced laws many governments regulators still struggling today questions first arose rise internet role intermediaries relation content limits privacy even lack regulation specifically machine learning human rights principles obligations still apply context include ensuring respect principle including using representative training data particular use case addressing bias training data designing algorithms way favor particular groups others ensuring applications could manifestly violate human rights developed used example systems could used predict person sexual orientation thus used persecute lgbti people ensuring applications prevent people enjoying human rights actively put risk human rights violations used black box systems provision public services deny people access effective redress principles combating discrimination machine learning emerging communities researchers businesses developers thinking machine learning social economic ethical implications everyday lives might design systems maximize human benefit notable recent initiatives define principles ethical accountable use summarized appendix include asilomar principles safe ethical beneficial use developed future life institute endorsed leading figures including elon musk stephen hawking fatml fairness accountability transparency machine learning principles accountable algorithms developed large network scientists researchers industry professionals global initiative ethical considerations artificial intelligence autonomous systems developed institute electrical electronics engineers ieee world largest technical professional organization advancement technology prevent discriminatory outcomes machine learningin focusing human right synthesize existing body work four critical principles combat reinforcement bias machine learning active inclusion development design applications must involve diversity input especially norms values specific populations affected output systems individuals must give explicit consent system use protected sensitive race religion gender personal data make decisions guiding questions diverse pool designers involved creation system evaluated veracity data considered alternative sources mapped understood particular groups may advantage disadvantage context system deployed sufficiently researched taken account norms context system deployed calculated error rates types different assessed potential differential impacts fairness many different ways defining fairness people involved conceptualizing developing implementing machine learning systems consider definition best applies context application every case fairness dignity affected people prioritized architecture machine learning system evaluation metrics issues bias guiding questions identified definition fairness suits context application product aligns international declaration human rights included relevant domain experts whose interdisciplinary insights allow understand potential sources bias unfairness design ways counteract mapped understood particular groups may advantage disadvantage context system deployed applied rigorous trials ensure system amplify biases error due issues training data algorithms elements system design outlined ongoing system evaluating fairness throughout life cycle product procedure correct unforeseen cases unfairness uncover right understanding systems involved affects individual rights must disclosed systems must able provide explanation decisionmaking understandable end users reviewable competent human authority guiding questions logged sources potential error uncertainty openly disclosed aspects decisionmaking algorithmic much data sources made transparent much system explain end users much code procedures made provided detailed documentation technically suitable apis permissive use terms allow third parties provide review behavior system access redress designers developers systems responsible use actions systems must make visible avenues redress affected disparate impacts establish processes timely redress discriminatory outputs guiding questions confident output algorithm intended understand probabilistic nature algorithms recognizing outputs correct amending errors method checking output algorithm decorrelated protected sensitive features tested series counterfactual propositions track results algorithm would different end user different race age lived elsewhere reporting processes recourse place process place make necessary fixes design system based reported issues simply removing sensitive categories datasets sufficient undesirable biases often preserved unobserved remaining attributes one solution use sensitive attributes test system behavior find correct bias fairness criminal justice risk assessments state art berk provide review technical pathways towards promoting fairness machine learning berk fairness criminal justice risk assessments state art institute report prevent discriminatory outcomes machine learningbringing principles life human rights due diligence machine learning companies developing using systems must integrate principles human rights due diligence process businesses take ongoing proactive reactive steps ensure uphold people dignity cause contribute human rights abuses responsibility lies engineers building models goal leadership steer technology uphold human rights organizations existing discrimination policies industries standards protect human rights must updated reflect new considerations pertaining policies industry standards absent must developed implemented nature human rights due diligence differ depending whether company developer user particular use cases potential impact human rights circumstances businesses must take three core steps step identifying human rights risks linked business operations companies engineering implementing systems responsibility map human rights risks life cycle product development deployment use developers leadership take account risks inherent defined section deployment nature use identity end user developer human rights record lead different assessments example system might human rights risks used airline customer service could impact human right housing used mortgage company step taking effective action prevent mitigate risks leadership step requires establishing framework incentives development teams prioritize positive human rights outcomes developers step requires detecting correcting data bias ensuring data sets including training data represent populations application affect example software sifting job applications use training data embeds existing discriminatory practices women minorities often requires developers consult external domain experts well end users clients instance developer application determine eligibility mortgages would important consult public nonprofit bodies work housing issues use machine learning systems potentially significant impact human rights companies seek independent auditing algorithms based industry standards human rights framework businesses using ongoing checks identify amend bias transparent efforts identify prevent mitigate human rights risks leadership step involves explicitly encouraging transparency developers transparency would include explaining process identifying human rights risks risks identified steps taken prevent mitigate possible use software improve transparency possible companies publish technical papers explain design workings applications transparency also requires people know used make decision impacts prevent discriminatory outcomes machine learningthe promise human rights due diligence process often realized due weaknesses implementation current due diligence models resulted inconsistencies identifying mitigating human rights risks companies industry similar operating environments example response risks right privacy mass surveillance programs cybercrime apple applied default encryption imessage facetime platforms google applied duo service option blackberry apply regular bbm service human rights due diligence involves independent assessment well company except independent research undertaken media civil society shortcomings must addressed robust independent trusted process adopted relation identifying human rights risks linked business operations propose common standards assessing adequacy training data potential bias established adopted along common minimum standards procedures identifying human rights risks system design standards already exist strengthened adapted account new challenges related approach key initiatives like good summit partnership could focal points include diverse range companies civil society academics existing initiatives ieee fatml others provide solid basis participation agencies human rights council office high commissioner human rights critical legitimacy international applicability relation taking effective action prevent mitigate risks propose company leadership work concrete ways enhance company governance activities require augmenting existing mechanisms models ethical compliance tools already established instance credit industry existing standards evaluating enforcing fair lending expanded address existing internal codes conduct accountability schemes developed taking inclusive approach making human rights due diligence machine learning effective relation transparent efforts identify prevent mitigate human rights risks propose companies monitor applications report findings suggest working certified auditing bodies evaluate effects policies practices human rights analogous industries rare mineral extraction large multinational companies set example taking lead submitting independent audits future may opportunity establish international independent auditing body carry evaluations global scale results audits made public together responses company appendix contains matrices detail areas companies regarding steps see amnesty international private favourite messaging apps october online prevent discriminatory outcomes machine learningthe application human rights standards machine learning recent topic inquiry recommendations paper among first developed published area expect developed elaborated others recommendations meant function universal manual useful starting point companies leadership development teams building existing mechanisms sector encourage readers choose elements recommendations relevant integrate best fits individual needs white paper sought move human rights issue center discussion potential social impacts machine learning expand focus concerns include parts world currently absent conversation exploration emerging complex subject sought identify areas geographic technical discrimination machine learning likely impact human rights evaluate businesses responsibilities lie addressing algorithmic discrimination present realistic ways forward overcoming challenges identifying eliminating bias discrimination result machine learning applications easy task following recommendations companies work together domain experts stakeholders relevant partners public private sectors leverage machine learning way includes benefits people prevents discrimination cultivate huge value society also build public trust reduce risks reactive poorly informed regulation confusing unimplementable times economically costly solution eliminate risk discrimination machine learning systems recognize many recommendations require tailoring recognize much work still speculative given nascent nature applications particularly global south undertaken research last eight months mindful incredible rate change complexity scale issues companies face integrating machine learning business models conclusion technically rigorous review available tools creating accountable algorithms recommend kroll impressive article accountable algorithms university pennsylvania law review october hope report advance internal corporate discussions topics contribute larger public debate following release white paper hope actively work members forum see recommendations fit business practices variety players working build engage machine learning applications prevent discriminatory outcomes machine learningalgorithm algorithm formally specified sequence logical operations provides instructions computers act data thus automate decisions algorithms play role automating discovery useful patterns data sets automating relies simpler terms set rules computer follows solve algorithmic accountability responsibility algorithm designers provide evidence potential realised artificial intelligence science making machines auditability ability third parties probe understand review behavior algorithm disclosure information enables monitoring checking criticism including provision detailed documentation technically suitable apis permissive terms big data large heterogeneous forms data collected without strict experimental design big data becoming common due proliferation digital storage greater ease acquisition data mobile phones higher degree interconnection devices internet discrimination distinction exclusion preference made basis race colour sex religion political opinion national extraction social origin effect nullifying impairing equality opportunity human rights human rights rights inherent human beings whatever nationality place residence sex national ethnic origin color religion language status equally entitled human rights without discrimination rights interrelated interdependent indivisible universal human rights often expressed guaranteed law forms treaties customary international law general principles sources international law international human rights law lays obligations governments act certain ways refrain certain acts order promote protect human rights fundamental freedoms individuals businesses responsibility respect human rights means avoid infringing human rights others address adverse human rights impacts machine learning machine learning model one leverages computer programs automatically improve experience three main subsets machine learning supervised learning unsupervised learning reinforcement learning deep learning powerful class learning techniques models used settings mechanics implications deep learning outside scope paper definitions subsets found training data data used input machine learning algorithm process populating training machine learning model trained model represents patterns contained training data transparency ability know machine learning system performs well big data disparate impact machine learning power promise computers learn example royal society algorithmic accountability machine learning power promise computers learn example royal society human rights committee general comment iccpr provide definition discrimination united nations human rights guiding principles business human rights implementing united nations protect respect remedy framework office high commissioner human rights online principle tom mitchell machine learning royal society prevent discriminatory outcomes machine learningappendix challenges companies companies avoid issues incomplete data action impact organize research determine whether certain data sets fit internally agreed upon standards adequate representative data looking quantitative qualitative metrics identify opportunities expand data collection efforts contextually appropriate viable possible without violating privacy ensure diversity development teams bring different perspectives together afford insights whether certain populations adequately included represented training data participate open source data algorithm sharingcollect data diverse sources format free accessible order get representative far reaching spread data build harmonized standards data labellingall companies benefit greater transparency requirements around licensed datasets particularly important smaller companies resourced undergo extensive testing prior release develop standards track provenance development use training data sets throughout life understand monitor issues potential bias biases may already map risks sense could wrong order able course correct either expanding data set changing way machine learning system designed deployed necessary engage stakeholders domain experts participatory mannerbetter identify entire range data types necessary adequately train system given context better understand appropriately source data needed train leaders human rights responsibilitiesequip technical teams leadership knowledge ability translate human rights responsibilities code making easier avoid discriminatory outcomes promote transparency understandability applicationsallow domain experts say data sets might inadequate invasive use provide mechanism safe feedback audience delivered companies avoid issues biased data action impact organize research uncover whether certain data sets fit widely accepted standards fair representative data looking quantitative qualitative metrics ensure diversity development teams bring different perspectives together afford insights whether certain populations adequately included represented training data map risks sense could wrong order able course correct either expanding data set changing way machine learning system designed deployed necessary develop standards track provenance development use training data sets throughout life understand monitor issues potential bias biases may already engage stakeholders domain experts participatory mannerbetter identify entire range data types necessary adequately train system given context better understand appropriately source data needed train leaders human rights responsibilitiesequip technical teams knowledge ability translate human rights responsibilities code making easier avoid discriminatory outcomes promote transparency understandability applicationsallow domain experts say data sets might inadequate invasive pull provide mechanism safe feedback audience delivered language concept institute report prevent discriminatory outcomes machine learningwhat companies avoid choosing wrong model data action impact organize research understanding certain models performed similar contexts guide model data selection ensure diversity development teams bring different perspectives together afford insights model types data types may need considered order design system accurate monitor model use across different contexts applications introduce errors bias cultural assumptions domains keep models date contextually relevantreduce chances bias error result static applications longer reflect realities needs given context map risks sense could wrong order identify stages require checks leverage dynamic testing include dynamic testing determine algorithms performing according chosen set indicators reflect order course correct either changing training data target variables parameters cost functions elements application necessary engage stakeholders domain experts participatory mannerbest identify types considerations made model applied particular domain industry geography population etc design fair contextually appropriate model train leaders human rights responsibilitieshave leaders data scientists able translate ethics code runs systems minimize risk inadvertent blatant discrimination companies avoid building model discriminatory features action impact organize research understanding certain models performed similar contexts guide model data selection ensure diversity development teamsbring different perspectives together afford insights model types data types may need considered order design system accurate keep models date contextually relevantreduce chances bias error result static applications longer reflect realities needs given context map risks sense could wrong order identify stages require checks leverage dynamic testing determine set indicators could used detect discrimination might helpful dynamic testing include dynamic testing determine algorithms performing according chosen set indicators reflect order course correct either changing training data target variables parameters cost functions elements application necessary calibrate models include fairness criteria appropriatebalance model success according accuracy also fairness nondiscrimination engage stakeholders domain experts participatory mannerbest identify types considerations made model applied particular domain industry geography population etc design fair contextually appropriate model train leaders human rights responsibilitieshave leaders data scientists able translate ethics code runs systems minimize risk inadvertent blatant discrimination top recommendations field medium institute report recommend actions companies working designing implementing systems take minimize risks discrimination resulting algorithms technically rigorous specific set guidelines highly recommend kroll report accountable algorithms university pennsylvania law review prevent discriminatory outcomes machine learningwhat companies avoid choosing wrong model data action impact organize research understanding certain models performed similar contexts guide model data selection ensure diversity development teams bring different perspectives together afford insights model types data types may need considered order design system accurate monitor model use across different contexts applications introduce errors bias cultural assumptions domains keep models date contextually relevantreduce chances bias error result static applications longer reflect realities needs given context map risks sense could wrong order identify stages require checks leverage dynamic testing include dynamic testing determine algorithms performing according chosen set indicators reflect order course correct either changing training data target variables parameters cost functions elements application necessary engage stakeholders domain experts participatory mannerbest identify types considerations made model applied particular domain industry geography population etc design fair contextually appropriate model train leaders human rights responsibilitieshave leaders data scientists able translate ethics code runs systems minimize risk inadvertent blatant discrimination companies avoid building model discriminatory features action impact organize research understanding certain models performed similar contexts guide model data selection ensure diversity development teamsbring different perspectives together afford insights model types data types may need considered order design system accurate keep models date contextually relevantreduce chances bias error result static applications longer reflect realities needs given context map risks sense could wrong order identify stages require checks leverage dynamic testing determine set indicators could used detect discrimination might helpful dynamic testing include dynamic testing determine algorithms performing according chosen set indicators reflect order course correct either changing training data target variables parameters cost functions elements application necessary calibrate models include fairness criteria appropriatebalance model success according accuracy also fairness nondiscrimination engage stakeholders domain experts participatory mannerbest identify types considerations made model applied particular domain industry geography population etc design fair contextually appropriate model train leaders human rights responsibilitieshave leaders data scientists able translate ethics code runs systems minimize risk inadvertent blatant discrimination companies ensure human involvement oversight action impact organize research understanding certain models performed similar contexts guide model data selection contribute shared body knowledge inform standards auditing understanding ensure diversity development teamsbring different perspectives together afford insights model types data types may need considered order design system accurate map risks sense could wrong order identify stages require checks leverage dynamic testing determine set indicators could used detect discrimination might helpful dynamic testing engage stakeholders domain experts participatory mannerbest identify types considerations made model applied particular domain industry geography population etc design fair contextually appropriate model train leaders human rights responsibilitieshave leaders data scientists able translate ethics code runs systems minimize risk inadvertent blatant discrimination companies avoid issues unpredictable inscrutable systems action impact promote transparency understandability applicationsbuild ability ask decision made thereby promoting accountability ability act accordingly put limits sources risk provide mechanism safe feedback audience delivered map risks provide sense could wrong order identify stages require checks leverage dynamic testing determine set indicators could used detect discrimination might helpful dynamic testing include dynamic testing provide accountability applications inscrutable better understand systems treating certain subgroups within population identify discrimination engage stakeholders domain experts participatory mannerbest identify types transparency scrutability particularly critical certain domain instance systems used score applicants hireability important able trace identify variables taken account understand weighted order sure calculated nondiscriminating ways one biggest advantages applications ability compute pace human could ever hope keep would unreasonable human checking every single computation application executes would also hindrance technology benefits suggest instead companies need keep human oversight vital moments design monitoring deployment stages moments different across different applications rigorous needs others report good overview different definitions interpretable explainable machine learning entails recommend doran article explainable really mean new conceptualization perspectives doran explainable really mean new conceptualization perspectives prevent discriminatory outcomes machine learningwhat companies avoid intentional discrimination using action impact ensure diversity development teams bring different perspectives together afford insights model types data types may need considered order design system accurate create internal code conduct based human rights frameworkguide involved designing interpreting decisions understand violation code conduct constitutes discrimination create incentive model adherence human rights guidelinesencourage people avoid discrimination create company culture promotes human rights seeks eliminate intentional inadvertent discrimination promote transparency understand ability allow users monitoring systems understand model built discrimination desired outcome hold relevant parties accountable provide mechanism safe feedback audience delivered map risks sense could wrong order identify stages require checks leverage dynamic testing determine set indicators could used detect discrimination might helpful dynamic testing include dynamic testing determine algorithms performing according chosen set indicators reflect order course correct either changing training data target variables parameters cost functions elements application necessary calibrate models include fairness crite ria appropriatecreate automatic checks balances system might able prevent discrimination even intended balance model success according accuracy also fairness train leaders human rights responsibilitieshave leaders data scientists able translate ethics code runs systems minimize risk inadvertent blatant discrimination restrict deployment cases judged incongruous human rightsprotect people discriminatory outcomes sensitive application contexts limit human rights abuses prevent discriminatory outcomes machine learningappendix principles ethical design use autonomous systems asilomar principles ethics values safe ethical beneficial use fatml principles accountable algorithmsieee principles ethically aligned design accuracy verifiability safety systems safe secure throughout operational lifetime verifiably applicable feasible accuracy identify log articulate sources error uncertainty throughout algorithm data sources expected implications understood inform mitigation procedureshuman benefit safety must verifiably safe secure throughout operational lifetime auditabilityfailure transparency systems cause harm possible ascertain judicial transparency systems involved key judicial decisionmaking explanation auditable competent human authority made availableexplainability ensure algorithmic decisions well data driving decisions explained end users stakeholders nontechnical terms auditability enable interested third parties probe understand review behavior algorithm disclosure information enables monitoring checking criticism including provision detailed documentation technically suitable apis permissive use must possible discover system made particular decision acted certain way system causes harm discover root cause responsibility responsibility designers builders systems stakeholders moral implications use misuse actions responsibility make available externally visible avenues redress adverse individual societal effects designate internal role person responsible timely remedy issues responsibility designers developers systems remain aware take account diversity existing relevant cultural norms manufacturers must able provide accountability proving system operates certain ways fairness values alignmentshared benefit technologies benefit empower many people possible shared prosperity economic prosperity created shared broadly benefit humanity power conferred control highly advanced systems respect improve rather subvert social civic processesfairness ensure algorithmic decisions create discriminatory unjust impacts comparing across different demographics embedding values identify norms elicit values specific community affected particular ensure norms values included compatible relevant community human benefit human rights design operate way respects human rights freedoms human dignity cultural diversity privacy personal privacy people right access manage control data generate given systems power analyze utilize data liberty privacy use personal data must unreasonably curtail people real perceived liberty personal data individual access control people must able define access manage personal data curators unique identity notes asilomar ieee guidelines include additional principles relate human control avoidance lethal autonomous weapons arms race capability questions regarding beneficence artificial general intelligence superintelligence prevent discriminatory outcomes machine learningappendix areas action matrix human rights machine learning identifying human rights risks linked business operations need action tasks actors assess wider impactsorganize research hire designate people oversee research ways maximize benefits preventing human rights violations assign taskforce build strategic approach preventing negative outcomes mlbusinesses involved developing deploying machine learning systems starting company leadership makes strategic decisions deployed companies like google microsoft amazon facebook already involved dedicated research efforts space entities involved deploying machine learning systems map risks releasing system companies run rigorous trials ensure amplify biases errors due issues training data algorithms elements system design rapidly changing field methods assumptions testing conducted along results openly documented publicly available clear versioning accommodate updates new map human rights risks throughout life cycle machine learning products development deployment use mapping take account risks inherent machine learning including data bias inadequate data must include intended uses potential human rights abuses case update human rights risks new use case applicationbusinesses involved developing deploying machine learning systems entities involved deploying machine learning systems done new york city initial mapping risks led city council consider bill ensure transparency testing algorithmic decisionmaking systems top recommendations field ainow institute active inclusion right understanding access redress prevent discriminatory outcomes machine learningdevelop enhance industry stan dardsdevelop augment standards evaluate fairness inclusion accountability machine learningcompanies capacity partake efforts arrive common understanding set standards fairness dignity assurance machine involved developing deploying machine learning systems might look similar approaches taken develop standards fairness trade fair trade movement entities involved deploying machine learning systems build harmonized standards data labellingcompanies work together develop use widely understood transparency taxonomy label data accurately allow companies licensing data know getting businesses whose commercial model involves licensing datasets develop standards track provenance development use training data sets throughout life appropriate employees data scientists data anthropologists etc develop better records training data set created maintained continue examine existing training data sets work understand potential blind spots biases may already also include evaluation ethical outcomes delivery gartner research predicted hires customer service mostly write scripts bot interactions input data also output potential scientists measurement researchers within bias research field like partnership ieee fatml language concept ainow institute report report prevent discriminatory outcomes machine learningtaking effective action prevent mitigate risks need action tasks actors enhance company gover nancecreate internal code conduct based human rights frameworkbusinesses work internally define promote use code conduct responsible use data algorithms explicit priority around preventing human rights may opportunities integrate code conduct existing workplace conduct guidelines sexual harassment protocol model new principles would drafted outline qualifies discrimination relevant products developed within company person involved discriminatory model would considered held possible consortia businesses work together verify whether internal codes conducts align wider vision applications machine learning hope work many organizations seeking establish standards soon yield shared understanding internal codes conduct look like businesses involved developing deploying machine learning systems starting company leadership working together governments viable multisector oversight bodies consortia already work space partnership etc think ngos focused good ethical applications entities involved deploying machine learning systems create incentive model adherence human rights guidelineswork management teams across business verticals integrate internal code conduct employee incentive models training programs management models include employing causal exert influence relevant industry embrace openness accountability human rights machine learning applicationsbusinesses involved developing deploying machine learning systems starting company leadership makes strategic decisions deployed entities involved deploying machine learning systems develop augment standards evaluate fairness inclusion accountability machine learningcompanies capacity partake efforts arrive common understanding set standards fairness machine learning businesses involved developing deploying machine learning systems starting company leadership makes strategic decisions deployed might look similar approaches taken develop standards fairness trade fair trade movement entities involved deploying machine learning systems develop standards track provenance development use training data sets throughout life appropriate employees data scientists data anthropologists etc develop better records training data set created maintained continue examine existing training data sets work understand potential blind spots biases may already leadership makes strategic decisions deployed social scientists measurement researchers within bias research field like partnership ieee fatml citing algorithmic accountability applying concept different country concepts web foundation idea put forward richard socher interview see kilbertus language concept ainow institute report report prevent discriminatory outcomes machine learningtake inclusive approach designengage stakeholders domain experts participatory mannerdesign engagement strategies include users stakeholders defining algorithmic features parameters detect correct data bias ensure data sets including training data adequate consulting end users clients external domain experts intensify diversity equity inclusivity efforts beyond human resources allowed effectively influence approach towards product development services provision corporations designated task forces applicable relevant stakeholders users platform ensure diversity development teamscreate explicit internal commitment inclusionary hiring across company within development teams build diversity inclusion principles human resources practices guidelines set goals appropriate company context size design carry sporadic internal audits evaluate company diversity goals outline steps taken promote diversity needed periodically add new team members rotate temporary team members areas bring fresh perspectives team problem teams grasping new ideas technologies established approaches difficult change companies universities conferences stakeholders field release data participation women minorities marginalized groups within research development many recognize current lack diversity serious issue yet insufficiently granular data scope problem needed measure progress beyond need deeper assessment workplace cultures technology industry requires going beyond simply hiring women minorities toward building genuinely inclusive companies developing deploying systems train developers leaders human rights responsibilitiesrequire training human rights expand curriculum designers include coursework human rights data science ethics develop modular design programs human rights adapted integrated existing ethics nondiscrimination curricula invest education engineers global south promote sustained participation form low middle income countries examples initiatives include data science africa codatarda school research data science deep learning indab higher learning institutions civil society organizations corporations hiring example blue sky agenda education collection ideas ethics education seeks democratization education emphasizes inclusiveness development toward goal respecting values rights diverse report prevent discriminatory outcomes machine learningoptimize mod els fairness account ability transpar ency editabilitycalibrate models include fairness criteria general calibrating false positive false negative rates group population algorithm making decisions help equalize impacts words individuals responsible designing algorithms weighting variables ask question system fails fail prevent failure johndrow lum article algorithm removing sensitive information application recidivism prediction provides thorough analysis machine learning algorithms employed augment fairness decision developing starting company leadership makes strategic decisions deploye include dynamic testing create integrate quality assessment indicators include fairness appropriate integrate dynamic testing procedures provide accountability either employing cryptographic commitments equivalents sealed documents held third party safe place fair random choices technique allowing software make fully reproducible random choices zero knowledge proofs cryptographic tools allow prove decision policy actually used certain property without revealing either property known decision policy development teams teams tasked monitoring evaluating applications implemented interviews cathy neil joshua cohen johndrow lum algorithm removing sensitive information application recidivism prediction datta sen zick algorithmic transparency via quantitiative input influence theory experiments learning systems pulled algorithmic accountability citing kroll february prevent discriminatory outcomes machine learningbeing transparent efforts identify prevent mitigate human rights risk need action tasks actors monitor refine algorithmsorganize human oversightallocate increased resources human monitoring evaluation methods outcomes monitoring defined open academically rigorous processes accountable public particularly high stakes decisionmaking contexts views experiences traditionally marginalized communities possible designate internal partnered task force oversight human rights concerns machine learning applications design conduct audits hire external firm audit evaluate potential risks discrimination related input data decision factors output invest quality controls oversee data collection processes including verification involving human operators within decision making systems undertake checks ongoing basis ensure produced systems biased correct bias entities businesses involved deploying machine learning systems independent auditing bodies appointed businesses developing using might resemble similar auditing practices industries flo certification model fair trade standards msc chain custody surveillance audits sustainable products monitor model use across different contexts technical experts domain experts managers involved implementation model ensure applications introduce errors bias cultural assumptions domains create process monitoring systems throughout life cycle technical experts domain experts working given application keep models date contextually relevantdepending context models need updated whether new training data new parameters new target variables technical components updates prioritized scheduled based contextdata scientists engineers algorithm designers work within companies implementing machine learning systems top recommendations field citing algorithmic accountability applying concept different country concepts web foundation top recommendations field campolo alex sanfilippo madelyn whittaker meredith crawford kate report institute prevent discriminatory outcomes machine learningestablish channels share impact transparently provide mechanism safe feedback audience delivered share publicly information process adopted identify human rights risks risks identified concrete steps taken prevent mitigate risks could include publishing technical papers explain design machine learning applications work information plain language aimed communities impacted use machine learning applications establish open communication channel representative group people application could affect could involve focus groups consultation processes identify ways constrain use deep networks inscrutable functions relate people rightsbusinesses involved developing deploying machine learning systems companies like microsoft google already deeply involved partnerships begin understand best promote transparency public sector entities govern regulate example european union new policies enshrining right understand big data technologies measure evaluate reportwhere machine learning used make decision may directly impact enjoyment human rights clearly disclose use people impacted decisions provide mechanisms recourse machine learning used circumstances interacts public makes decisions affect individuals legally would significant impact ensure appropriate notices provided consent may needed cases certain jurisdictions responsible research innovation rri model measure report evidence positive social impacts system societybusinesses involved developing deploying machine learning systems working together governments viable oversight bodies consortia already work fair space partnership etc think focused good ethical applications entities involved deploying machine learning systems prevent discriminatory outcomes machine learningacknowledgements global future council human rights dapo akande professor public international law faculty law university oxford allgrove head global information technology baker mckenzie communications industry michelle chief executive officer impaqto daniel bross senior adviser haas school business university california berkeley amal clooney barrister doughty street chambers steven crown deputy general counsel microsoft eileen donahoe executive director global digital policy incubator sherif head technology human rights amnesty international isabelle president commission nationale information des liberts cnil damiano felice director strategy access medicine foundation samuel gregory programme director witness erica kochi unicef innovation united nations children fund unicef lim executive director asia cloud computing association katherine maher executive director wikimedia foundation marcela manubens global integrated social unilever sustainability andrew mclaughlin partner higher ground labs mayur patel executive strategy econet media corporate development michael posner jerome kohlberg professor ethics stern school business new york finance director center business university human rights esra shafei founder executive director mideast youth hilary sutcliffe director societyinside manuela veloso herbert simon university professor school carnegie mellon university computer science global future council fellow miles jackson associate professor law university oxford world economic forum silvia magnoni head civil society communities society world economic forum innovation lisa ventura project specialist global leadership fellow world economic forum society innovation key contributors jennie bernstein urban innovation specialist united nations children fund unicef sherif head technology human rights amnesty international erica kochi unicef innovation united nations children fund unicef mayur patel executive strategy econet media corporate development world economic forum route capite switzerland tel fax contact world economic forum committed improving state world international organization cooperation forum engages foremost political business leaders society shape global regional industry agendas
