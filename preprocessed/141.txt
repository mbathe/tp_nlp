AI​ ​Now​ ​2017​ ​Report Authors Alex​ ​Campolo , ​ ​New​ ​York​ ​University Madelyn​ ​Sanfilippo , ​ ​New​ ​York​ ​University Meredith​ ​Whittaker , ​ ​Google​ ​Open​ ​Research , ​ ​New​ ​York​ ​University , ​ ​and​ ​AI​ ​Now ​ ​Kate ​ ​Crawford , ​ ​Microsoft​ ​Research , ​ ​New​ ​York​ ​University , ​ ​and​ ​AI​ ​Now Editors Andrew​ ​Selbst , ​ ​Yale​ ​Information​ ​Society​ ​Project​ ​and​ ​Data​ ​ & ​ ​Society Solon​ ​Barocas , ​ ​Cornell​ ​University Table​ ​of​ ​Contents Recommendations 1 Executive​ ​Summary 3 Introduction 6 Labor​ ​and​ ​Automation 7 Research ​ ​by​ ​Sector​ ​and​ ​Task 7 AI​ ​and​ ​the​ ​Nature​ ​of​ ​Work 9 Inequality​ ​and​ ​Redistribution 1​3 Bias​ ​and​ ​Inclusion 1​3 Where​ ​Bias​ ​Comes ​ ​From 1​4 The​ ​AI​ ​Field​ ​is​ ​Not​ ​Diverse 1​6 Recent​ ​Developments ​ ​in​ ​Bias​ ​Research 1​8 Emerging ​ ​Strategies​ ​to​ ​Address​ ​Bias 20 Rights​ ​and​ ​Liberties 21 Population ​ ​Registries ​ ​and​ ​Computing​ ​Power 2​2 Corporate ​ ​and​ ​Government​ ​Entanglements 2​3 AI​ ​and​ ​the​ ​Legal​ ​System 2​6 AI​ ​and​ ​Privacy 2​8 Ethics​ ​and​ ​Governance 30 Ethical​ ​Concerns ​ ​in​ ​AI 30 AI​ ​Reflects​ ​Its​ ​Origins 31 Ethical​ ​Codes 3​2 Challenges ​ ​and​ ​Concerns ​ ​Going​ ​Forward 3​4 Conclusion 3​6 This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License AI​ ​Now​ ​2017​ ​Report 1 Recommendations These​ ​recommendations​ ​reflect​ ​the ​ ​views​ ​and​ ​research​ ​of​ ​the ​ ​ ​​AI​ ​Now​ ​Institute​ ​at​ ​New​ ​York University.​ ​We ​ ​thank​ ​the​ ​experts​ ​who​ ​contributed​ ​to​ ​the​ ​​AI ​ ​Now ​ ​2017 ​ ​Symposium ​ ​and Workshop ​​ ​for​ ​informing​ ​these​ ​perspectives , ​ ​and​ ​our​ ​research​ ​team​ ​for​ ​helping​ ​shape​ ​the​ ​ ​AI Now ​ ​2017 ​ ​Report . 1.Core​ ​public​ ​agencies , ​ ​such​ ​as​ ​those​ ​responsible​ ​for​ ​criminal​ ​justice , ​ ​healthcare , welfare , ​ ​and ​ ​education ​ ​ ( e.g​ ​ “ high ​ ​stakes ” ​ ​domains ) ​ ​should ​ ​no ​ ​longer​ ​use​ ​ “ black​ ​box ” AI​ ​and ​ ​algorithmic​ ​systems.​​ ​This​ ​includes​ ​the​ ​unreviewed​ ​or​ ​unvalidated​ ​use ​ ​of pre-trained​ ​models , ​ ​AI​ ​systems​ ​licensed​ ​from​ ​third​ ​party​ ​vendors , ​ ​and​ ​algorithmic processes​ ​created​ ​in-house.​ ​The ​ ​use​ ​of​ ​such​ ​systems​ ​by​ ​public​ ​agencies​ ​raises​ ​serious due​ ​process​ ​concerns , ​ ​and​ ​at​ ​a​ ​minimum​ ​they​ ​should​ ​be ​ ​available​ ​for​ ​public​ ​auditing , testing , ​ ​and​ ​review , ​ ​and​ ​subject​ ​to​ ​accountability​ ​standards . 2.Before​ ​releasing​ ​an​ ​AI​ ​system , ​ ​companies​ ​should ​ ​run​ ​rigorous​ ​pre-release​ ​trials​ ​to ensure​ ​that​ ​they​ ​will​ ​not​ ​amplify​ ​biases​ ​and ​ ​errors​​ ​​due​ ​to ​ ​any​ ​issues​ ​with ​ ​the​ ​training data , ​ ​algorithms , ​ ​or​ ​other​ ​elements​ ​of​ ​system​ ​design.​​ ​As​ ​this​ ​is​ ​a​ ​rapidly​ ​changing​ ​field , the​ ​methods​ ​and​ ​assumptions​ ​by​ ​which​ ​such​ ​testing​ ​is​ ​conducted , ​ ​along​ ​with​ ​the results , ​ ​should​ ​be ​ ​openly​ ​documented​ ​and​ ​publicly​ ​available , ​ ​with​ ​clear​ ​versioning​ ​to accommodate ​ ​updates​ ​and​ ​new​ ​findings . 3.After​ ​releasing​ ​an ​ ​AI​ ​system , ​ ​companies​ ​should ​ ​continue​ ​to ​ ​monitor​ ​its​ ​use​ ​across different​ ​contexts​ ​and​ ​communities.​​ ​The​ ​methods​ ​and​ ​outcomes​ ​of​ ​monitoring​ ​should be​ ​defined​ ​through​ ​open , ​ ​academically​ ​rigorous​ ​processes , ​ ​and​ ​should​ ​be ​ ​accountable to​ ​the​ ​public.​ ​Particularly​ ​in​ ​high​ ​stakes​ ​decision-making​ ​contexts , ​ ​the​ ​views​ ​and experiences​ ​of​ ​traditionally​ ​marginalized​ ​communities​ ​should​ ​be ​ ​prioritized . 4.More​ ​research ​ ​and ​ ​policy​ ​making​ ​is​ ​needed ​ ​on ​ ​the​ ​use​ ​of​ ​AI​ ​systems​ ​in ​ ​workplace management​ ​and ​ ​monitoring , ​ ​including​ ​hiring​ ​and ​ ​HR.​​ ​This​ ​research​ ​will​ ​complement the​ ​existing​ ​focus​ ​on​ ​worker​ ​replacement​ ​via​ ​automation.​ ​Specific​ ​attention​ ​should​ ​be given​ ​to​ ​the ​ ​potential​ ​impact​ ​on​ ​labor​ ​rights​ ​and​ ​practices , ​ ​and​ ​should​ ​focus​ ​especially on​ ​the​ ​potential​ ​for​ ​behavioral​ ​manipulation​ ​and​ ​the ​ ​unintended​ ​reinforcement​ ​of​ ​bias in​ ​hiring​ ​and​ ​promotion . 5.Develop​ ​standards​ ​to ​ ​track​ ​the​ ​provenance , ​ ​development , ​ ​and ​ ​use​ ​of​ ​training​ ​datasets throughout​ ​their​ ​life​ ​cycle.​ ​​This​ ​is​ ​necessary​ ​to​ ​better​ ​understand​ ​and​ ​monitor​ ​issues​ ​of bias​ ​and​ ​representational​ ​skews.​ ​In​ ​addition​ ​to​ ​developing​ ​better​ ​records​ ​for​ ​how​ ​a training​ ​dataset​ ​was​ ​created​ ​and​ ​maintained , ​ ​social​ ​scientists​ ​and​ ​measurement researchers​ ​within​ ​the ​ ​AI​ ​bias​ ​research​ ​field​ ​should​ ​continue ​ ​to​ ​examine​ ​existing​ ​training datasets , ​ ​and​ ​work ​ ​to​ ​understand​ ​potential​ ​blind​ ​spots​ ​and​ ​biases​ ​that​ ​may​ ​already​ ​be at​ ​work . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 2 6 . Expand ​ ​ AI ​ ​ bias ​ ​ research ​ ​ and ​ ​ mitigation ​ ​ strategies ​ ​ beyond ​ ​ a ​ ​ narrowly ​ ​ technical approach . ​ ​ ​ Bias ​ ​ issues ​ ​ are ​ ​ long ​ ​ term ​ ​ and ​ ​ structural , ​ ​ and ​ ​ contending ​ ​ with ​ ​ them necessitates ​ ​ deep ​ ​ interdisciplinary ​ ​ research . ​ ​ Technical ​ ​ approaches ​ ​ that ​ ​ look ​ ​ for ​ ​ a one-time ​ ​ “ fix ” ​ ​ for ​ ​ fairness ​ ​ risk ​ ​ oversimplifying ​ ​ the ​ ​ complexity ​ ​ of ​ ​ social ​ ​ systems . ​ ​ Within each ​ ​ domain ​ ​ – ​ ​ such ​ ​ as ​ ​ education , ​ ​ healthcare ​ ​ or ​ ​ criminal ​ ​ justice ​ ​ – ​ ​ legacies ​ ​ of ​ ​ bias ​ ​ and movements ​ ​ toward ​ ​ equality ​ ​ have ​ ​ their ​ ​ own ​ ​ histories ​ ​ and ​ ​ practices . ​ ​ Legacies ​ ​ of ​ ​ bias can not ​ ​ be ​ ​ “ solved ” ​ ​ without ​ ​ drawing ​ ​ on ​ ​ domain ​ ​ expertise . ​ ​ Addressing ​ ​ fairness meaningfully ​ ​ will ​ ​ require ​ ​ interdisciplinary ​ ​ collaboration ​ ​ and ​ ​ methods ​ ​ of ​ ​ listening ​ ​ across different ​ ​ disciplines . 7 . Strong ​ ​ standards ​ ​ for ​ ​ auditing ​ ​ and ​ ​ understanding ​ ​ the ​ ​ use ​ ​ of ​ ​ AI ​ ​ systems ​ ​ “ in ​ ​ the ​ ​ wild ” are ​ ​ urgently ​ ​ needed . ​ ​ ​ Creating ​ ​ such ​ ​ standards ​ ​ will ​ ​ require ​ ​ the ​ ​ perspectives ​ ​ of ​ ​ diverse disciplines ​ ​ and ​ ​ coalitions . ​ ​ The ​ ​ process ​ ​ by ​ ​ which ​ ​ such ​ ​ standards ​ ​ are ​ ​ developed ​ ​ should ​ ​ be publicly ​ ​ accountable , ​ ​ academically ​ ​ rigorous ​ ​ and ​ ​ subject ​ ​ to ​ ​ periodic ​ ​ review ​ ​ and ​ ​ revision . 8 . Companies , ​ ​ universities , ​ ​ conferences ​ ​ and ​ ​ other ​ ​ stakeholders ​ ​ in ​ ​ the ​ ​ AI ​ ​ field ​ ​ should release ​ ​ data ​ ​ on ​ ​ the ​ ​ participation ​ ​ of ​ ​ women , ​ ​ minorities ​ ​ and ​ ​ other ​ ​ marginalized ​ ​ groups within ​ ​ AI ​ ​ research ​ ​ and ​ ​ development . ​ ​ ​ Many ​ ​ now ​ ​ recognize ​ ​ that ​ ​ the ​ ​ current ​ ​ lack ​ ​ of diversity ​ ​ in ​ ​ AI ​ ​ is ​ ​ a ​ ​ serious ​ ​ issue , ​ ​ yet ​ ​ there ​ ​ is ​ ​ insufficiently ​ ​ granular ​ ​ data ​ ​ on ​ ​ the ​ ​ scope ​ ​ of the ​ ​ problem , ​ ​ which ​ ​ is ​ ​ needed ​ ​ to ​ ​ measure ​ ​ progress . ​ ​ Beyond ​ ​ this , ​ ​ we ​ ​ need ​ ​ a ​ ​ deeper assessment ​ ​ of ​ ​ workplace ​ ​ cultures ​ ​ in ​ ​ the ​ ​ technology ​ ​ industry , ​ ​ which ​ ​ requires ​ ​ going beyond ​ ​ simply ​ ​ hiring ​ ​ more ​ ​ women ​ ​ and ​ ​ minorities , ​ ​ toward ​ ​ building ​ ​ more ​ ​ genuinely inclusive ​ ​ workplaces . 9 . The ​ ​ AI ​ ​ industry ​ ​ should ​ ​ hire ​ ​ experts ​ ​ from ​ ​ disciplines ​ ​ beyond ​ ​ computer ​ ​ science ​ ​ and engineering ​ ​ and ​ ​ ensure ​ ​ they ​ ​ have ​ ​ decision ​ ​ making ​ ​ power . ​ ​ ​​ ​ As ​ ​ AI ​ ​ moves ​ ​ into ​ ​ diverse social ​ ​ and ​ ​ institutional ​ ​ domains , ​ ​ influencing ​ ​ increasingly ​ ​ high ​ ​ stakes ​ ​ decisions , ​ ​ efforts must ​ ​ be ​ ​ made ​ ​ to ​ ​ integrate ​ ​ social ​ ​ scientists , ​ ​ legal ​ ​ scholars , ​ ​ and ​ ​ others ​ ​ with ​ ​ domain expertise ​ ​ that ​ ​ can ​ ​ guide ​ ​ the ​ ​ creation ​ ​ and ​ ​ integration ​ ​ of ​ ​ AI ​ ​ into ​ ​ long-standing ​ ​ systems with ​ ​ established ​ ​ practices ​ ​ and ​ ​ norms . 10 . Ethical ​ ​ codes ​ ​ meant ​ ​ to ​ ​ steer ​ ​ the ​ ​ AI ​ ​ field ​ ​ should ​ ​ be ​ ​ accompanied ​ ​ by ​ ​ strong ​ ​ oversight and ​ ​ accountability ​ ​ mechanisms . ​ ​ ​ More ​ ​ work ​ ​ is ​ ​ needed ​ ​ on ​ ​ how ​ ​ to ​ ​ substantively ​ ​ connect high ​ ​ level ​ ​ ethical ​ ​ principles ​ ​ and ​ ​ guidelines ​ ​ for ​ ​ best ​ ​ practices ​ ​ to ​ ​ everyday ​ ​ development processes , ​ ​ promotion ​ ​ and ​ ​ product ​ ​ release ​ ​ cycles . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 3 Executive ​ ​ Summary Artificial ​ ​ intelligence ​ ​ ( AI ) ​ ​ technologies ​ ​ are ​ ​ in ​ ​ a ​ ​ phase ​ ​ of ​ ​ rapid ​ ​ development , ​ ​ and ​ ​ are ​ ​ being adopted ​ ​ widely . ​ ​ While ​ ​ the ​ ​ concept ​ ​ of ​ ​ artificial ​ ​ intelligence ​ ​ has ​ ​ existed ​ ​ for ​ ​ over ​ ​ sixty ​ ​ years , real-world ​ ​ applications ​ ​ have ​ ​ only ​ ​ accelerated ​ ​ in ​ ​ the ​ ​ last ​ ​ decade ​ ​ due ​ ​ to ​ ​ three ​ ​ concurrent developments : ​ ​ better ​ ​ algorithms , ​ ​ increases ​ ​ in ​ ​ networked ​ ​ computing ​ ​ power ​ ​ and ​ ​ the ​ ​ tech industry ’ s ​ ​ ability ​ ​ to ​ ​ capture ​ ​ and ​ ​ store ​ ​ massive ​ ​ amounts ​ ​ of ​ ​​ ​ data . AI ​ ​ systems ​ ​ are ​ ​ already ​ ​ integrated ​ ​ in ​ ​ everyday ​ ​ technologies ​ ​ like ​ ​ smartphones ​ ​ and ​ ​ personal assistants , ​ ​ making ​ ​ predictions ​ ​ and ​ ​ determinations ​ ​ that ​ ​ help ​ ​ personalize ​ ​ experiences ​ ​ and advertise ​ ​ products . ​ ​ Beyond ​ ​ the ​ ​ familiar , ​ ​ these ​ ​ systems ​ ​ are ​ ​ also ​ ​ being ​ ​ introduced ​ ​ in ​ ​ critical areas ​ ​ like ​ ​ law , ​ ​ finance , ​ ​ policing ​ ​ and ​ ​ the ​ ​ workplace , ​ ​ where ​ ​ they ​ ​ are ​ ​ increasingly ​ ​ used ​ ​ to predict ​ ​ everything ​ ​ from ​ ​ our ​ ​ taste ​ ​ in ​ ​ music ​ ​ to ​ ​ our ​ ​ likelihood ​ ​ of ​ ​ committing ​ ​ a ​ ​ crime ​ ​ to ​ ​ our fitness ​ ​ for ​ ​ a ​ ​ job ​ ​ or ​ ​ an ​ ​ educational ​ ​ opportunity . AI ​ ​ companies ​ ​ promise ​ ​ that ​ ​ the ​ ​ technologies ​ ​ they ​ ​ create ​ ​ can ​ ​ automate ​ ​ the ​ ​ toil ​ ​ of ​ ​ repetitive work , ​ ​ identify ​ ​ subtle ​ ​ behavioral ​ ​ patterns ​ ​ and ​ ​ much ​ ​ more . ​ ​ However , ​ ​ the ​ ​ analysis ​ ​ and understanding ​ ​ of ​ ​ artificial ​ ​ intelligence ​ ​ should ​ ​ not ​ ​ be ​ ​ limited ​ ​ to ​ ​ its ​ ​ technical ​ ​ capabilities . ​ ​ The design ​ ​ and ​ ​ implementation ​ ​ of ​ ​ this ​ ​ next ​ ​ generation ​ ​ of ​ ​ computational ​ ​ tools ​ ​ presents ​ ​ deep normative ​ ​ and ​ ​ ethical ​ ​ challenges ​ ​ for ​ ​ our ​ ​ existing ​ ​ social , ​ ​ economic ​ ​ and ​ ​ political relationships ​ ​ and ​ ​ institutions , ​ ​ and ​ ​ these ​ ​ changes ​ ​ are ​ ​ already ​ ​ underway . ​ ​ Simply ​ ​ put , ​ ​ AI ​ ​ does not ​ ​ exist ​ ​ in ​ ​ a ​ ​ vacuum . ​ ​ We ​ ​ must ​ ​ also ​ ​ ask ​ ​ how ​ ​ broader ​ ​ phenomena ​ ​ like ​ ​ widening ​ ​ inequality , an ​ ​ intensification ​ ​ of ​ ​ concentrated ​ ​ geopolitical ​ ​ power ​ ​ and ​ ​ populist ​ ​ political ​ ​ movements ​ ​ will shape ​ ​ and ​ ​ be ​ ​ shaped ​ ​ by ​ ​ the ​ ​ development ​ ​ and ​ ​ application ​ ​ of ​ ​ AI ​ ​ technologies . Building ​ ​ on ​ ​ the ​ ​ inaugural ​ ​ 2016 ​ ​ report , ​ ​​ The ​ ​ AI ​ ​ Now ​ ​ 2017 ​ ​ Report ​ ​ ​ addresses ​ ​ the ​ ​ most ​ ​ recent scholarly ​ ​ literature ​ ​ in ​ ​ order ​ ​ to ​ ​ raise ​ ​ critical ​ ​ social ​ ​ questions ​ ​ that ​ ​ will ​ ​ shape ​ ​ our ​ ​ present ​ ​ and near ​ ​ future . ​ ​ A ​ ​ year ​ ​ is ​ ​ a ​ ​ long ​ ​ time ​ ​ in ​ ​ AI ​ ​ research , ​ ​ and ​ ​ this ​ ​ report ​ ​ focuses ​ ​ on ​ ​ new developments ​ ​ in ​ ​ four ​ ​ areas : ​ ​ labor ​ ​ and ​ ​ automation , ​ ​ bias ​ ​ and ​ ​ inclusion , ​ ​ rights ​ ​ and ​ ​ liberties , and ​ ​ ethics ​ ​ and ​ ​ governance . ​ ​ We ​ ​ identify ​ ​ emerging ​ ​ challenges ​ ​ in ​ ​ each ​ ​ of ​ ​ these ​ ​ areas ​ ​ and make ​ ​ recommendations ​ ​ to ​ ​ ensure ​ ​ that ​ ​ the ​ ​ benefits ​ ​ of ​ ​ AI ​ ​ will ​ ​ be ​ ​ shared ​ ​ broadly , ​ ​ and ​ ​ that risks ​ ​ can ​ ​ be ​ ​ identified ​ ​ and ​ ​ mitigated . Labor ​ ​ and ​ ​ automation ​ : ​ ​ Popular ​ ​ media ​ ​ narratives ​ ​ have ​ ​ emphasized ​ ​ the ​ ​ prospect ​ ​ of mass ​ ​ job ​ ​ loss ​ ​ due ​ ​ to ​ ​ automation ​ ​ and ​ ​ the ​ ​ widescale ​ ​ adoption ​ ​ of ​ ​ robots . ​ ​ Such ​ ​ serious scenarios ​ ​ deserve ​ ​ sustained ​ ​ empirical ​ ​ attention , ​ ​ but ​ ​ some ​ ​ of ​ ​ the ​ ​ best ​ ​ recent ​ ​ work on ​ ​ AI ​ ​ and ​ ​ labor ​ ​ has ​ ​ focused ​ ​ instead ​ ​ on ​ ​ specific ​ ​ sectors ​ ​ and ​ ​ tasks . ​ ​ While ​ ​ few ​ ​ jobs ​ ​ will be ​ ​ completely ​ ​ automated ​ ​ in ​ ​ the ​ ​ near ​ ​ term , ​ ​ researchers ​ ​ estimate ​ ​ that ​ ​ about ​ ​ a ​ ​ third of ​ ​ workplace ​ ​ tasks ​ ​ can ​ ​ be ​ ​ automated ​ ​ for ​ ​ the ​ ​ majority ​ ​ of ​ ​ workers . ​ ​ New ​ ​ policies ​ ​ such as ​ ​ the ​ ​ Universal ​ ​ Basic ​ ​ Income ​ ​ ( UBI ) ​ ​ are ​ ​ being ​ ​ designed ​ ​ to ​ ​ address ​ ​ concerns ​ ​ about job ​ ​ loss , ​ ​ but ​ ​ these ​ ​ need ​ ​ much ​ ​ more ​ ​ study . An ​ ​ underexplored ​ ​ area ​ ​ that ​ ​ needs ​ ​ urgent ​ ​ attention ​ ​ is ​ ​ how ​ ​ AI ​ ​ and ​ ​ related algorithmic ​ ​ systems ​ ​ are ​ ​ already ​ ​ changing ​ ​ the ​ ​ balance ​ ​ of ​ ​ workplace ​ ​ power . ​ ​ Machine learning ​ ​ techniques ​ ​ are ​ ​ quickly ​ ​ being ​ ​ integrated ​ ​ into ​ ​ management ​ ​ and ​ ​ hiring AI ​ ​ Now ​ ​ 2017 ​ ​ Report 4 decisions , ​ ​ including ​ ​ in ​ ​ the ​ ​ so-called ​ ​ gig ​ ​ economy ​ ​ where ​ ​ technical ​ ​ systems ​ ​ match workers ​ ​ with ​ ​ jobs , ​ ​ but ​ ​ also ​ ​ across ​ ​ more ​ ​ traditional ​ ​ white ​ ​ collar ​ ​ industries . ​ ​ New systems ​ ​ make ​ ​ promises ​ ​ of ​ ​​ ​ flexibility ​ ​ and ​ ​ efficiency , ​ ​ but ​ ​ they ​ ​ also ​ ​ intensify ​ ​ the surveillance ​ ​ of ​ ​ workers , ​ ​ who ​ ​ often ​ ​ do ​ ​ not ​ ​ know ​ ​ when ​ ​ and ​ ​ how ​ ​ they ​ ​ are ​ ​ being tracked ​ ​ and ​ ​ evaluated , ​ ​ or ​ ​ why ​ ​ they ​ ​ are ​ ​ hired ​ ​ or ​ ​ fired . ​ ​ Furthermore , ​ ​ AI-assisted forms ​ ​ of ​ ​ management ​ ​ may ​ ​ replace ​ ​ more ​ ​ democratic ​ ​ forms ​ ​ of ​ ​ bargaining ​ ​ between workers ​ ​ and ​ ​ employers , ​ ​ increasing ​ ​ owner ​ ​ power ​ ​ under ​ ​ the ​ ​ guise ​ ​ of ​ ​ technical neutrality . Bias ​ ​ and ​ ​ inclusion ​ : ​ ​ One ​ ​ of ​ ​ the ​ ​ most ​ ​ active ​ ​ areas ​ ​ of ​ ​ critical ​ ​ AI ​ ​ research ​ ​ in ​ ​ the ​ ​ past year ​ ​ has ​ ​ been ​ ​ the ​ ​ study ​ ​ of ​ ​ bias , ​ ​ both ​ ​ in ​ ​ its ​ ​ more ​ ​ formal ​ ​ statistical ​ ​ sense ​ ​ and ​ ​ in ​ ​ the wider ​ ​ legal ​ ​ and ​ ​ normative ​ ​ senses . ​ ​ At ​ ​ their ​ ​ best , ​ ​ AI ​ ​ systems ​ ​ can ​ ​ be ​ ​ used ​ ​ to ​ ​ augment human ​ ​ judgement ​ ​ and ​ ​ reduce ​ ​ both ​ ​ our ​ ​ conscious ​ ​ and ​ ​ unconscious ​ ​ biases . ​ ​ However , training ​ ​ data , ​ ​ algorithms , ​ ​ and ​ ​ other ​ ​ design ​ ​ choices ​ ​ that ​ ​ shape ​ ​ AI ​ ​ systems ​ ​ may reflect ​ ​ and ​ ​ amplify ​ ​ existing ​ ​ cultural ​ ​ assumptions ​ ​ and ​ ​ inequalities . ​ ​ For ​ ​ example , natural ​ ​ language ​ ​ processing ​ ​ techniques ​ ​ trained ​ ​ on ​ ​ a ​ ​ corpus ​ ​ of ​ ​ internet ​ ​ writing ​ ​ from the ​ ​ 1990s ​ ​ may ​ ​ reflect ​ ​ stereotypical ​ ​ and ​ ​ dated ​ ​ word ​ ​ associations—the ​ ​ word “ female ” ​ ​ might ​ ​ be ​ ​ associated ​ ​ with ​ ​ “ receptionist. ” ​ ​ If ​ ​ these ​ ​ models ​ ​ are ​ ​ used ​ ​ to ​ ​ make educational ​ ​ or ​ ​ hiring ​ ​ decisions , ​ ​ they ​ ​ may ​ ​ reinforce ​ ​ existing ​ ​ inequalities , ​ ​ regardless of ​ ​ the ​ ​ intentions ​ ​ or ​ ​ even ​ ​ knowledge ​ ​ of ​ ​ system ’ s ​ ​ designers . Those ​ ​ researching , ​ ​ designing ​ ​ and ​ ​ developing ​ ​ AI ​ ​ systems ​ ​ tend ​ ​ to ​ ​ be ​ ​ male , ​ ​ highly educated ​ ​ and ​ ​ very ​ ​ well ​ ​ paid . ​ ​ Yet ​ ​ their ​ ​ systems ​ ​ are ​ ​ working ​ ​ to ​ ​ predict ​ ​ and understand ​ ​ the ​ ​ behaviors ​ ​ and ​ ​ preferences ​ ​ of ​ ​ diverse ​ ​ populations ​ ​ with ​ ​ very ​ ​ different life ​ ​ experiences . ​ ​ More ​ ​ diversity ​ ​ within ​ ​ the ​ ​ fields ​ ​ building ​ ​ these ​ ​ systems ​ ​ will ​ ​ help ensure ​ ​ that ​ ​ they ​ ​ reflect ​ ​ a ​ ​ broader ​ ​ variety ​ ​ of ​ ​ viewpoints . Rights ​ ​ and ​ ​ liberties ​ : ​ ​ The ​ ​ application ​ ​ of ​ ​ AI ​ ​ systems ​ ​ in ​ ​ public ​ ​ and ​ ​ civil ​ ​ institutions ​ ​ is challenging ​ ​ existing ​ ​ political ​ ​ arrangements , ​ ​ especially ​ ​ in ​ ​ a ​ ​ global ​ ​ political ​ ​ context shaped ​ ​ by ​ ​ events ​ ​ such ​ ​ as ​ ​ the ​ ​ election ​ ​ of ​ ​ Donald ​ ​ Trump ​ ​ in ​ ​ the ​ ​ United ​ ​ States . ​ ​ A number ​ ​ of ​ ​ governmental ​ ​ agencies ​ ​ are ​ ​ already ​ ​ partnering ​ ​ with ​ ​ private ​ ​ corporations to ​ ​ deploy ​ ​ AI ​ ​ systems ​ ​ in ​ ​ ways ​ ​ that ​ ​ challenge ​ ​​ ​ civil ​ ​ rights ​ ​ and ​ ​ liberties . ​ ​ For ​ ​ example , police ​ ​ body ​ ​ camera ​ ​ footage ​ ​ is ​ ​ being ​ ​ used ​ ​ to ​ ​ train ​ ​ machine ​ ​ vision ​ ​ algorithms ​ ​ for ​ ​ law enforcement , ​ ​ raising ​ ​ privacy ​ ​ and ​ ​ accountability ​ ​ concerns . ​ ​ AI ​ ​ technologies ​ ​ are ​ ​ also being ​ ​ deployed ​ ​ in ​ ​ the ​ ​ very ​ ​ legal ​ ​ institutions ​ ​ designed ​ ​ to ​ ​ safeguard ​ ​ our ​ ​ rights ​ ​ and liberties , ​ ​ with ​ ​ proprietary ​ ​ risk ​ ​ assessment ​ ​ algorithms ​ ​ already ​ ​ being ​ ​ used ​ ​ to ​ ​ help judges ​ ​ make ​ ​ sentencing ​ ​ and ​ ​ bail ​ ​ decisions , ​ ​ potentially ​ ​ amplifying ​ ​ and ​ ​ naturalizing longstanding ​ ​ biases , ​ ​ and ​ ​ rendering ​ ​ them ​ ​ more ​ ​ opaque ​ ​ to ​ ​ oversight ​ ​ and ​ ​ scrutiny . Privacy ​ ​ rights ​ ​ represent ​ ​ a ​ ​ particularly ​ ​ sensitive ​ ​ challenge ​ ​ for ​ ​ current ​ ​ AI ​ ​ applications , especially ​ ​ in ​ ​ domains ​ ​ like ​ ​ healthcare , ​ ​ where ​ ​ AI ​ ​ is ​ ​ being ​ ​ used ​ ​ to ​ ​ help ​ ​ make diagnoses . ​ ​ For ​ ​ AI ​ ​ to ​ ​ deliver ​ ​ on ​ ​ its ​ ​ promises , ​ ​ it ​ ​ requires ​ ​ large ​ ​ amounts ​ ​ of ​ ​ data , ​ ​ which likely ​ ​ means ​ ​ an ​ ​ increase ​ ​ in ​ ​ data ​ ​ collection , ​ ​ both ​ ​ its ​ ​ scale ​ ​ and ​ ​ granularity . ​ ​ Without contextual ​ ​ knowledge , ​ ​ informed ​ ​ consent , ​ ​ and ​ ​ due ​ ​ processes ​ ​ mechanisms , ​ ​ these systems ​ ​ can ​ ​ create ​ ​ risks ​ ​ that ​ ​ threaten ​ ​ and ​ ​ expose ​ ​ already ​ ​ vulnerable ​ ​ populations . Ethics ​ ​ and ​ ​ governance ​ : ​ ​ The ​ ​ areas ​ ​ of ​ ​ ethics ​ ​ and ​ ​ governance ​ ​ attempt ​ ​ to ​ ​ address many ​ ​ of ​ ​ the ​ ​ challenges ​ ​ and ​ ​ opportunities ​ ​ identified ​ ​ above . ​ ​ We ​ ​ track ​ ​ the ​ ​ growing AI ​ ​ Now ​ ​ 2017 ​ ​ Report 5 interest ​ ​ in ​ ​ ethical ​ ​ codes ​ ​ of ​ ​ conduct ​ ​ and ​ ​ principles , ​ ​ while ​ ​ noting ​ ​ that ​ ​ these ​ ​ need ​ ​ to be ​ ​ tied ​ ​ more ​ ​ closely ​ ​ to ​ ​ everyday ​ ​ AI ​ ​ design ​ ​ and ​ ​ development . ​ ​ The ​ ​ military ​ ​ use ​ ​ of artificial ​ ​ intelligence ​ ​ takes ​ ​ on ​ ​ a ​ ​ special ​ ​ urgency ​ ​ in ​ ​ the ​ ​ case ​ ​ of ​ ​ lethal ​ ​ autonomous weapons ​ ​ systems . There ​ ​ are ​ ​ multiple ​ ​ signs ​ ​ of ​ ​ progress ​ ​ in ​ ​ the ​ ​ development ​ ​ of ​ ​ professional ​ ​ and ​ ​ legal ethical ​ ​ codes ​ ​ to ​ ​ govern ​ ​ the ​ ​ design ​ ​ and ​ ​ application ​ ​ of ​ ​ AI ​ ​ technologies . ​ ​ However , ​ ​ in the ​ ​ face ​ ​ of ​ ​ rapid , ​ ​ distributed , ​ ​ and ​ ​ often ​ ​ proprietary ​ ​ AI ​ ​ development ​ ​ and implementation , ​ ​ such ​ ​ forms ​ ​ of ​ ​ soft ​ ​ governance ​ ​ face ​ ​ real ​ ​ challenges . ​ ​ Among ​ ​ these are ​ ​ problems ​ ​ of ​ ​ coordination ​ ​ among ​ ​ different ​ ​ ethical ​ ​ codes , ​ ​ as ​ ​ well ​ ​ as ​ ​ questions around ​ ​ enforcement ​ ​ mechanisms ​ ​ that ​ ​ would ​ ​ go ​ ​ beyond ​ ​ voluntary ​ ​ cooperation ​ ​ by individuals ​ ​ working ​ ​ in ​ ​ research ​ ​ and ​ ​ industry . ​ ​ New ​ ​ ethical ​ ​ frameworks ​ ​ for ​ ​ AI ​ ​ need ​ ​ to move ​ ​ beyond ​ ​ individual ​ ​ responsibility ​ ​ to ​ ​ hold ​ ​ powerful ​ ​ industrial , ​ ​ governmental ​ ​ and military ​ ​ interests ​ ​ accountable ​ ​ as ​ ​ they ​ ​ design ​ ​ and ​ ​ employ ​ ​ AI . The ​ ​ following ​ ​ report ​ ​ develops ​ ​ these ​ ​ themes ​ ​ in ​ ​ detail , ​ ​ and ​ ​ reflects ​ ​ on ​ ​ the ​ ​ latest ​ ​ academic research . ​ ​ AI ​ ​ is ​ ​ already ​ ​ with ​ ​ us , ​ ​ and ​ ​ we ​ ​ are ​ ​ now ​ ​ faced ​ ​ with ​ ​ important ​ ​ choices ​ ​ on ​ ​ how ​ ​ it ​ ​ will be ​ ​ designed ​ ​ and ​ ​ applied . ​ ​ Most ​ ​ promisingly , ​ ​ the ​ ​ approaches ​ ​ described ​ ​ in ​ ​ this ​ ​ report demonstrate ​ ​ that ​ ​ there ​ ​ is ​ ​ growing ​ ​ interest ​ ​ in ​ ​ developing ​ ​ AI ​ ​ that ​ ​ is ​ ​ attuned ​ ​ to ​ ​ underlying issues ​ ​ of ​ ​ fairness ​ ​ and ​ ​ equality . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 6 Introduction In ​ ​ July ​ ​ of ​ ​ 2016 , ​ ​ Kate ​ ​ Crawford ​ ​ and ​ ​ Meredith ​ ​ Whittaker ​ ​ co-chaired ​ ​ the ​ ​ first ​ ​​ AI ​ ​ Now Symposium ​ ​ ​ in ​ ​ collaboration ​ ​ with ​ ​ the ​ ​ Obama ​ ​ White ​ ​ House ’ s ​ ​ Office ​ ​ of ​ ​ Science ​ ​ and Technology ​ ​ Policy ​ ​ and ​ ​ the ​ ​ National ​ ​ Economic ​ ​ Council . ​ ​ The ​ ​ event ​ ​ brought ​ ​ together ​ ​ experts and ​ ​ members ​ ​ of ​ ​ the ​ ​ public ​ ​ to ​ ​ discuss ​ ​ the ​ ​ near-term ​ ​ social ​ ​ and ​ ​ economic ​ ​ impacts ​ ​ of artificial ​ ​ intelligence ​ ​ ( AI ) . ​ ​ AI ​ ​ systems ​ ​ are ​ ​ already ​ ​ being ​ ​ integrated ​ ​ in ​ ​ social , ​ ​ political ​ ​ and 1 economic ​ ​ domains , ​ ​ and ​ ​ the ​ ​ implications ​ ​ can ​ ​ be ​ ​ complex ​ ​ and ​ ​ unpredictable . ​ ​ The now-annual ​ ​​ AI ​ ​ Now ​ ​ Symposium ​ ​ ​ focuses ​ ​ on ​ ​ AI ’ s ​ ​ core ​ ​ social ​ ​ implications , ​ ​ bringing ​ ​ together leading ​ ​ experts ​ ​ from ​ ​ across ​ ​ sectors ​ ​ and ​ ​ disciplines ​ ​ with ​ ​ the ​ ​ aim ​ ​ of ​ ​ better ​ ​ understanding how ​ ​ AI ​ ​ systems ​ ​ are ​ ​ already ​ ​ working ​ ​ in ​ ​ the ​ ​ world . The ​ ​​ AI ​ ​ Now ​ ​ 2016 ​ ​ Symposium ​ ​ ​ identified ​ ​ instances ​ ​ where ​ ​ AI ​ ​ challenged ​ ​ current ​ ​ thinking about ​ ​ professional ​ ​ responsibilities , ​ ​ decision-making ​ ​ and ​ ​ accountability . ​ ​ Following ​ ​ this , ​ ​​ The AI ​ ​ Now ​ ​ 2016 ​ ​ Report ​ ​ ​ reflected ​ ​​ ​ expert ​ ​ discussion ​ ​ and ​ ​ provided ​ ​ recommendations ​ ​ for ​ ​ future research ​ ​ and ​ ​ policy ​ ​ interventions . 2 The ​ ​​ AI ​ ​ Now ​ ​ 2017 ​ ​ Symposium ​ ​ ​ deepened ​ ​ this ​ ​ examination ​ ​ of ​ ​ the ​ ​ near-term ​ ​ social ​ ​ and economic ​ ​ implications ​ ​ of ​ ​ AI , ​ ​ and ​ ​ the ​ ​ accompanying ​ ​ report ​ ​ provides ​ ​ an ​ ​ overview ​ ​ of ​ ​ the ​ ​ key issues ​ ​ that ​ ​ the ​ ​​ 2017 ​ ​ Symposium ​ ​​ addressed . ​ ​ These ​ ​ are : ​ ​ 1 ) ​ ​​ Labor ​ ​ and ​ ​ Automation ​ , ​ ​ 2 ) ​ ​​ Bias and ​ ​ Inclusion ​ , ​ ​ ​ 3 ) ​ ​ Rights ​ ​ and ​ ​ Liberties ​ ​ ​ and ​ ​ 4 ) ​ ​​ Ethics ​ ​ and ​ ​ Governance ​ . ​ ​ In ​ ​ selecting ​ ​ these ​ ​ four themes , ​ ​ we ​ ​ are ​ ​ building ​ ​ on ​ ​ the ​ ​ 2016 ​ ​ report ​ ​ and ​ ​ introducing ​ ​ new ​ ​ areas ​ ​ of ​ ​ concern , ​ ​ with 3 close ​ ​ attention ​ ​ to ​ ​ developments ​ ​ that ​ ​ have ​ ​ occurred ​ ​ in ​ ​ the ​ ​ last ​ ​ 12 ​ ​ months . The ​ ​ first ​ ​ section ​ ​ on ​ ​​ Labor ​ ​ and ​ ​ Automation ​ ​ ​ considers ​ ​ the ​ ​ need ​ ​ for ​ ​ a ​ ​ more ​ ​ granular , skills-based , ​ ​ and ​ ​ sectoral ​ ​ approach ​ ​ to ​ ​ understanding ​ ​ AI ​ ​ and ​ ​ automation ’ s ​ ​ impacts ​ ​ on ​ ​ labor practices . ​ ​ While ​ ​ big ​ ​ questions ​ ​ about ​ ​ what ​ ​ implications ​ ​ automation ​ ​ and ​ ​ AI ​ ​ have ​ ​ for ​ ​ labor overall ​ ​ are ​ ​ still ​ ​ wide ​ ​ open , ​ ​ there ​ ​ are ​ ​ also ​ ​ important ​ ​ questions ​ ​ about ​ ​ the ​ ​ distinct ​ ​ roles ​ ​ that automation ​ ​ and ​ ​ AI ​ ​ will ​ ​ play ​ ​ within ​ ​ specific ​ ​ industries , ​ ​ sectors ​ ​ and ​ ​ tasks ​ ​ - ​ ​ particularly ​ ​ how ​ ​ it will ​ ​ be ​ ​ used ​ ​ as ​ ​ a ​ ​ tool ​ ​ of ​ ​ employee ​ ​ hiring , ​ ​ firing ​ ​ and ​ ​ management . ​ ​ The ​ ​ second ​ ​ section focuses ​ ​ on ​ ​​ Bias ​ ​ and ​ ​ Inclusion ​ , ​ ​ a ​ ​ growing ​ ​ concern ​ ​ among ​ ​ those ​ ​ looking ​ ​ at ​ ​ the ​ ​ design ​ ​ and social ​ ​ implications ​ ​ of ​ ​ AI ​ ​ decision-making ​ ​ systems . ​ ​ Here , ​ ​ we ​ ​ address ​ ​ the ​ ​ problem ​ ​ of diversity ​ ​ and ​ ​ inclusion ​ ​ within ​ ​ the ​ ​ AI ​ ​ industry ​ ​ itself . ​ ​ We ​ ​ also ​ ​ share ​ ​ new ​ ​ technical ​ ​ advances 1 As ​ ​ AI ​ ​ pioneers ​ ​ Stuart ​ ​ Russell ​ ​ and ​ ​ Peter ​ ​ Norvig ​ ​ point ​ ​ out , ​ ​ the ​ ​ history ​ ​ of ​ ​ artificial ​ ​ intelligence ​ ​ has ​ ​ not ​ ​ produced ​ ​ a ​ ​ clear definition ​ ​ of ​ ​ AI , ​ ​ but ​ ​ can ​ ​ be ​ ​ seen ​ ​ as ​ ​ variously ​ ​ emphasizing ​ ​ four ​ ​ possible ​ ​ goals : ​ ​ “ systems ​ ​ that ​ ​ think ​ ​ like ​ ​ humans , ​ ​ systems ​ ​ that act ​ ​ like ​ ​ humans , ​ ​ systems ​ ​ that ​ ​ think ​ ​ rationally , ​ ​ systems ​ ​ that ​ ​ act ​ ​ rationally. ” ​ ​ In ​ ​ this ​ ​ report ​ ​ we ​ ​ use ​ ​ the ​ ​ term ​ ​ AI ​ ​ to ​ ​ refer ​ ​ to ​ ​ a broad ​ ​ assemblage ​ ​ of ​ ​ technologies , ​ ​ from ​ ​ early ​ ​ rule-based ​ ​ algorithmic ​ ​ systems ​ ​ to ​ ​ deep ​ ​ neural ​ ​ networks , ​ ​ all ​ ​ of ​ ​ which ​ ​ rely ​ ​ on an ​ ​ array ​ ​ of ​ ​ data ​ ​ and ​ ​ computational ​ ​ infrastructures . ​ ​ These ​ ​ technologies ​ ​ span ​ ​ speech ​ ​ recognition , ​ ​ language ​ ​ translation , image ​ ​ recognition , ​ ​ predictions ​ ​ and ​ ​ determinations ​ ​ - ​ ​ tasks ​ ​ that ​ ​ have ​ ​ traditionally ​ ​ relied ​ ​ on ​ ​ human ​ ​ capacities ​ ​ across ​ ​ the ​ ​ four goals ​ ​ Russell ​ ​ and ​ ​ Norvig ​ ​ identify . ​ ​ While ​ ​ AI ​ ​ is ​ ​ not ​ ​ new , ​ ​ recent ​ ​ developments ​ ​ in ​ ​ the ​ ​ ability ​ ​ to ​ ​ collect ​ ​ and ​ ​ store ​ ​ large ​ ​ quantities of ​ ​ data , ​ ​ combined ​ ​ with ​ ​ advances ​ ​ in ​ ​ computational ​ ​ power ​ ​ have ​ ​ led ​ ​ to ​ ​ significant ​ ​ breakthroughs ​ ​ in ​ ​ the ​ ​ field ​ ​ over ​ ​ the ​ ​ last ​ ​ ten years . ​ ​ Stuart ​ ​ J . ​ ​ Russell ​ ​ and ​ ​ Peter ​ ​ Norvig , ​ ​ Artificial ​ ​ Intelligence : ​ ​ A ​ ​ Modern ​ ​ Approach , ​ ​ Englewood ​ ​ Cliffs , ​ ​ NJ : ​ ​ Prentice ​ ​ Hall , 1995 : ​ ​ 27 2 AI ​ ​ Now , ​ ​ “ The ​ ​ AI ​ ​ Now ​ ​ Report : ​ ​ The ​ ​ Social ​ ​ and ​ ​ Economic ​ ​ Implications ​ ​ of ​ ​ Artificial ​ ​ Intelligence ​ ​ Technologies ​ ​ in ​ ​ the ​ ​ Near-Term , ” ( 2016 ) ​ ​​ https : //ainowinstitute.org/AI_Now_2016_Report.pdf ​ . 3 ​ ​​ ​​ ​​ ​​ ​​ Ibid . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 7 that ​ ​ help ​ ​ to ​ ​ better ​ ​ understand ​ ​ and ​ ​ mitigate ​ ​ biases ​ ​ that ​ ​ AI ​ ​ systems ​ ​ may ​ ​ perpetuate ​ ​ and even ​ ​ amplify ​ ​ due ​ ​ to ​ ​ biased ​ ​ training ​ ​ data , ​ ​ faulty ​ ​ algorithms ​ ​ or ​ ​ other ​ ​ factors . ​ ​ The ​ ​ third section , ​ ​ on ​ ​​ Rights ​ ​ and ​ ​ Liberties ​ , ​ ​ begins ​ ​ by ​ ​ recognizing ​ ​ the ​ ​ recent ​ ​ rise ​ ​ of ​ ​ political authoritarianism , ​ ​ and ​ ​ asks ​ ​ about ​ ​ the ​ ​ role ​ ​ of ​ ​ AI ​ ​ systems ​ ​ in ​ ​ either ​ ​ supporting ​ ​ or ​ ​ eroding citizens ’ ​ ​ rights ​ ​ and ​ ​ liberties ​ ​ in ​ ​ areas ​ ​ like ​ ​ criminal ​ ​ justice , ​ ​ law ​ ​ enforcement , ​ ​ housing , ​ ​ hiring , lending ​ ​ and ​ ​ other ​ ​ domains . ​ ​ The ​ ​ last ​ ​ section , ​ ​ on ​ ​​ Ethics ​ ​ and ​ ​ Governance ​ , ​ ​ connects ​ ​ AI ​ ​ as ​ ​ we see ​ ​ it ​ ​ today ​ ​ with ​ ​ the ​ ​ history ​ ​ of ​ ​ AI ​ ​ research ​ ​ and ​ ​ development . ​ ​ It ​ ​ also ​ ​ looks ​ ​ at ​ ​​ whose concerns ​ ​ are ​ ​ ultimately ​ ​ reflected ​ ​ in ​ ​ the ​ ​ ethics ​ ​ of ​ ​ AI , ​ ​ and ​ ​ how ​ ​ ethical ​ ​ codes ​ ​ and ​ ​ other strategies ​ ​ could ​ ​ be ​ ​ developed ​ ​ in ​ ​ a ​ ​ time ​ ​ of ​ ​ political ​ ​ volatility . We ​ ​ are ​ ​ in ​ ​ the ​ ​ early ​ ​ stages ​ ​ of ​ ​ a ​ ​ long-term ​ ​ discussion , ​ ​ and ​ ​ accordingly , ​ ​ there ​ ​ are ​ ​ as ​ ​ many new ​ ​ questions ​ ​ as ​ ​ there ​ ​ are ​ ​ answers ​ ​ to ​ ​ the ​ ​ old ​ ​ ones . ​ ​ We ​ ​ hope ​ ​ this ​ ​ report ​ ​ provides ​ ​ a productive ​ ​ grounding ​ ​ in ​ ​ the ​ ​ extraordinary ​ ​ challenges ​ ​ and ​ ​ opportunities ​ ​ of ​ ​ the ​ ​ current moment , ​ ​ and ​ ​ helps ​ ​ spur ​ ​ research ​ ​ and ​ ​ inquiry ​ ​ into ​ ​ the ​ ​ social ​ ​ and ​ ​ economic ​ ​ implications ​ ​ of the ​ ​ turn ​ ​ to ​ ​ AI . ​ ​ . Labor ​ ​ and ​ ​ Automation The ​ ​ editors ​ ​ of ​ ​​ Nature ​ ​​ have ​ ​ argued ​ ​ that ​ ​ we ​ ​ need ​ ​ to ​ ​ match ​ ​ technical ​ ​ AI ​ ​ research ​ ​ funding with ​ ​ “ solid , ​ ​ well-funded ​ ​ research ​ ​ to ​ ​ anticipate ​ ​ the ​ ​ scenarios ​ ​ [ AI ] ​ ​ could ​ ​ bring ​ ​ about , ​ ​ and ​ ​ to study ​ ​ possible ​ ​ political ​ ​ and ​ ​ economic ​ ​ reforms ​ ​ that ​ ​ will ​ ​ allow ​ ​ those ​ ​ usurped ​ ​ by ​ ​ machinery to ​ ​ contribute ​ ​ to ​ ​ society. ” ​ ​ The ​ ​ ​ AI ​ ​ Now ​ ​ Labor ​ ​ Primer ​ ​ ​ described ​ ​ how ​ ​ forms ​ ​ of ​ ​ automation 4 based ​ ​ on ​ ​ machine ​ ​ learning ​ ​ and ​ ​ robotics ​ ​ have ​ ​ the ​ ​ potential ​ ​ to ​ ​ both ​ ​ increase ​ ​ the productivity ​ ​ of ​ ​ labor ​ ​ and ​ ​ to ​ ​ exacerbate ​ ​ existing ​ ​ inequalities ​ ​ in ​ ​ the ​ ​ distribution ​ ​ of ​ ​ wealth . 5 In ​ ​ an ​ ​ economic ​ ​ context ​ ​ characterized ​ ​ by ​ ​ both ​ ​ low ​ ​ productivity ​ ​ growth ​ ​ and ​ ​ historically ​ ​ high levels ​ ​ of ​ ​ inequality , ​ ​ it ​ ​ will ​ ​ be ​ ​ important ​ ​ to ​ ​ find ​ ​ ways ​ ​ to ​ ​ use ​ ​ AI ​ ​ to ​ ​ promote ​ ​ equality ​ ​ and shared ​ ​ prosperity . 6 While ​ ​ there ​ ​ is ​ ​ still ​ ​ considerable ​ ​ attention ​ ​ focused ​ ​ on ​ ​ large , ​ ​ structural ​ ​ changes ​ ​ in ​ ​ labor markets ​ ​ and ​ ​ on ​ ​ the ​ ​ economy ​ ​ as ​ ​ a ​ ​ whole , ​ ​ new ​ ​ research ​ ​ has ​ ​ been ​ ​ focusing ​ ​ on ​ ​ specific industries ​ ​ and ​ ​ the ​ ​ impact ​ ​ of ​ ​ AI ​ ​ systems ​ ​ on ​ ​ particular ​ ​ tasks ​ ​ within ​ ​ a ​ ​ profession . ​ ​ This ​ ​ section describes ​ ​ new ​ ​ developments ​ ​ in ​ ​ AI ’ s ​ ​ application ​ ​ within ​ ​ various ​ ​ labor ​ ​ sectors , ​ ​ and ​ ​ suggests directions ​ ​ that ​ ​ research ​ ​ could ​ ​ productively ​ ​ explore ​ ​ in ​ ​ the ​ ​ future . Research ​ ​ by ​ ​ Sector ​ ​ and ​ ​ Task At ​ ​ the ​ ​ beginning ​ ​ of ​ ​ 2017 , ​ ​ the ​ ​ McKinsey ​ ​ Global ​ ​ Institute ​ ​ ( MGI ) ​ ​ released ​ ​ a ​ ​ report ​ ​ looking ​ ​ at specific ​ ​ workplace ​ ​​ tasks ​ ​ ​ and ​ ​ whether ​ ​ they ​ ​ were ​ ​ more ​ ​ or ​ ​ less ​ ​ susceptible ​ ​ to ​ ​ automation , specifically ​ ​ those ​ ​ involving ​ ​ “ predictable ​ ​ physical ” ​ ​ activities ​ ​ and ​ ​ those ​ ​ involving ​ ​ data collection ​ ​ or ​ ​ processing . ​ ​ While ​ ​ relatively ​ ​ few ​ ​ current ​ ​ jobs ​ ​ can ​ ​ be ​ ​ totally ​ ​ automated ​ ​ with 4 “ Anticipating ​ ​ Artificial ​ ​ Intelligence , ” ​ ​​ Nature ​ ​ ​ 532 , ​ ​ no . ​ ​ 7600 ​ ​ ( April ​ ​ 28 , ​ ​ 2016 ) : ​ ​ 413 , ​ ​ doi:10.1038/532413a . 5 “ Labor ​ ​ and ​ ​ AI ” ​ ​ ( New ​ ​ York , ​ ​ NY : ​ ​​ AI ​ ​ Now ​ , ​ ​ July ​ ​ 7 , ​ ​ 2016 ) , ​ ​​ https : //ainowinstitute.org/AI_Now_2016_Primers.pdf ​ . 6 Jason ​ ​ Furman , ​ ​ “ Is ​ ​ This ​ ​ Time ​ ​ Different ? ​ ​ The ​ ​ Opportunities ​ ​ and ​ ​ Challenges ​ ​ of ​ ​ Artificial ​ ​ Intelligence , ” ​ ​ expanded ​ ​ remarks ​ ​ from the ​ ​​ AI ​ ​ Now ​ ​ ​ expert ​ ​ workshop , ​ ​ July ​ ​ 7 , ​ ​ 2016 , ​ ​ New ​ ​ York ​ ​ University , https : //obamawhitehouse.archives.gov/sites/default/files/page/files/20160707_cea_ai_furman.pdf ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 8 today ’ s ​ ​ technology , ​ ​ MGI ​ ​ estimates ​ ​ that ​ ​ 60 ​ ​ percent ​ ​ of ​ ​ all ​ ​ occupations ​ ​ have ​ ​ the ​ ​ potential ​ ​ for about ​ ​ a ​ ​ third ​ ​ of ​ ​ their ​ ​ activities ​ ​ to ​ ​ be ​ ​ automated . ​ ​ In ​ ​ a ​ ​ similar ​ ​ vein , ​ ​ analysts ​ ​ in ​ ​ Deloitte ’ s 7 Human ​ ​ Capital ​ ​ division ​ ​ predict ​ ​ a ​ ​ future ​ ​ where ​ ​ human ​ ​ skills ​ ​ will ​ ​ be ​ ​ “ augmented ” ​ ​ through “ collaboration ” ​ ​ with ​ ​ machines ​ ​ capable ​ ​ of ​ ​ performing ​ ​ routine ​ ​ tasks . 8 To ​ ​ prepare ​ ​ for ​ ​ these ​ ​ changes , ​ ​ it ​ ​ will ​ ​ be ​ ​ essential ​ ​ that ​ ​ policymakers ​ ​ have ​ ​ access ​ ​ to ​ ​ robust data ​ ​ on ​ ​ how ​ ​ advances ​ ​ in ​ ​ machine ​ ​ learning , ​ ​ robotics ​ ​ and ​ ​ the ​ ​ automation ​ ​ of ​ ​ perceptual tasks ​ ​ are ​ ​ changing ​ ​ the ​ ​ nature ​ ​ and ​ ​ organization ​ ​ of ​ ​ work , ​ ​ and ​ ​ how ​ ​ these ​ ​ changes ​ ​ manifest across ​ ​ different ​ ​ roles ​ ​ and ​ ​ different ​ ​ sectors . ​ ​ This ​ ​ data ​ ​ will ​ ​ be ​ ​ necessary ​ ​ for ​ ​ any ​ ​ robust ​ ​ policy proposal . ​ ​ However , ​ ​ a ​ ​ recent ​ ​ report ​ ​ from ​ ​ the ​ ​ National ​ ​ Academies ​ ​ of ​ ​ Sciences , ​ ​ Engineering , and ​ ​ Medicine ​ ​ identifies ​ ​ a ​ ​ lack ​ ​ of ​ ​ such ​ ​ data , ​ ​ finding ​ ​ existing ​ ​ federal ​ ​ statistical ​ ​ data ​ ​ limited ​ ​ in its ​ ​ capacity ​ ​ to ​ ​ answer ​ ​ these ​ ​ questions . ​ ​ The ​ ​ report ​ ​ recommends ​ ​ new ​ ​ multidisciplinary ​ ​ and qualitative ​ ​ research ​ ​ methods ​ ​ to ​ ​ capture ​ ​ present ​ ​ and ​ ​ future ​ ​ transformations ​ ​ in ​ ​ work . 9 A ​ ​ series ​ ​ of ​ ​ economic ​ ​ studies ​ ​ have ​ ​ begun ​ ​ to ​ ​ investigate ​ ​ the ​ ​ effects ​ ​ of ​ ​ robots ​ ​ on ​ ​ labor markets ​ ​ from ​ ​ an ​ ​ empirical ​ ​ perspective . ​ ​​ ​ A ​ ​ 2015 ​ ​ paper ​ ​ by ​ ​ George ​ ​ Graetz ​ ​ and ​ ​ Guy ​ ​ Michaels used ​ ​ new ​ ​ data ​ ​ from ​ ​ the ​ ​ International ​ ​ Federation ​ ​ of ​ ​ Robots ​ ​ to ​ ​ estimate ​ ​ changes ​ ​ in productivity ​ ​ and ​ ​ employment ​ ​ due ​ ​ to ​ ​ robot ​ ​ adoption , ​ ​ finding ​ ​ increases ​ ​ in ​ ​ productivity ​ ​ and slightly ​ ​ lowered ​ ​ working ​ ​ hours ​ ​ for ​ ​ low ​ ​ and ​ ​ middle-skilled ​ ​ workers . ​ ​ Using ​ ​ the ​ ​ same ​ ​ data , 10 Daron ​ ​ Acemoglu ​ ​ and ​ ​ Pascual ​ ​ Restrepo ​ ​ analyzed ​ ​ developments ​ ​ in ​ ​ labor ​ ​ markets ​ ​ across ​ ​ the United ​ ​ States ​ ​ from ​ ​ 1990 ​ ​ to ​ ​ 2007 . ​ ​ They ​ ​ estimated ​ ​ that ​ ​ the ​ ​ number ​ ​ of ​ ​ jobs ​ ​ lost ​ ​ due ​ ​ to robots ​ ​ during ​ ​ this ​ ​ period ​ ​ ranged ​ ​ from ​ ​ 360,000 ​ ​ to ​ ​ 670,000 , ​ ​ and ​ ​ that ​ ​ this ​ ​ trend ​ ​ could accelerate ​ ​ with ​ ​ a ​ ​ more ​ ​ intensive ​ ​ adoption ​ ​ of ​ ​ automation ​ ​ across ​ ​ sectors . ​ ​ ​ Model 11 assumptions ​ ​ play ​ ​ an ​ ​ important ​ ​ role ​ ​ in ​ ​ these ​ ​ empirical ​ ​ analyses ​ ​ and ​ ​ will ​ ​ need ​ ​ to ​ ​ be 12 continually ​ ​ tested ​ ​ against ​ ​ employment ​ ​ data . ​ ​ To ​ ​ this ​ ​ end , ​ ​ Management ​ ​ Professor ​ ​ and former ​ ​ Senior ​ ​ Economist ​ ​ at ​ ​ the ​ ​ White ​ ​ House ​ ​ Council ​ ​ of ​ ​ Economic ​ ​ Advisers ​ ​ Robert ​ ​ Seamans argues ​ ​ that ​ ​ even ​ ​ more ​ ​ fine-grained , ​ ​ company-level ​ ​ data ​ ​ will ​ ​ be ​ ​ necessary ​ ​ to ​ ​ understand whether ​ ​ AI ​ ​ and ​ ​ automation ​ ​ systems ​ ​ are ​ ​ replacing ​ ​ or ​ ​ complementing ​ ​ human ​ ​ workers . 13 7 Ibid. , ​ ​ 5-6 . 8 Jeff ​ ​ Schwartz , ​ ​ Laurence ​ ​ Collins , ​ ​ Heather ​ ​ Stockton , ​ ​ Darryl ​ ​ Wagner ​ ​ and ​ ​ Brett ​ ​ Walsh , ​ ​ “ The ​ ​ Future ​ ​ of ​ ​ Work : ​ ​ The ​ ​ Augmented Workforce , ” ​ ​ ( Deloitte ​ ​ Human ​ ​ Capital , ​ ​ February ​ ​ 28 , ​ ​ 2017 ) , https : //dupress.deloitte.com/dup-us-en/focus/human-capital-trends/2017/future-workforce-changing-nature-of-work.html 9 National ​ ​ Academies ​ ​ of ​ ​ Sciences , ​ ​ Engineering ​ ​ and ​ ​ Medicine , ​ ​ “ Information ​ ​ Technology ​ ​ and ​ ​ the ​ ​ U.S. ​ ​ Workforce : ​ ​ Where ​ ​ Are We ​ ​ and ​ ​ Where ​ ​ Do ​ ​ We ​ ​ Go ​ ​ from ​ ​ Here ? , ” ​ ​ ( Washington , ​ ​ DC : ​ ​ The ​ ​ National ​ ​ Academies ​ ​ Press , ​ ​ 2017 ) , https : //www.nap.edu/read/24649/ ​ . 10 ​ ​​ ​​ ​​ ​ ​ Georg ​ ​ Graetz ​ ​ and ​ ​ Guy ​ ​ Michaels , ​ ​ “ Robots ​ ​ at ​ ​ Work , ” ​ ​ IZA ​ ​ Discussion ​ ​ Paper ​ ​ ( Institute ​ ​ for ​ ​ the ​ ​ Study ​ ​ of ​ ​ Labor ​ ​ ( IZA ) , ​ ​ March ​ ​ 2015 ) , http : //econpapers.repec.org/paper/izaizadps/dp8938.htm . 11 Daron ​ ​ Acemoglu ​ ​ and ​ ​ Pascual ​ ​ Restrepo , ​ ​ “ Robots ​ ​ and ​ ​ Jobs : ​ ​ Evidence ​ ​ from ​ ​ US ​ ​ Labor ​ ​ Markets , ” ​ ​ Working ​ ​ Paper ​ ​ ( Cambridge MA : ​ ​ National ​ ​ Bureau ​ ​ of ​ ​ Economic ​ ​ Research , ​ ​ March ​ ​ 2017 ) , ​ ​ doi:10.3386/w23285 . 12 ​ ​​ ​​ ​ ​​ ​ For ​ ​ instance , ​ ​ economists ​ ​ at ​ ​ the ​ ​ Economic ​ ​ Policy ​ ​ Institute ​ ​ argue ​ ​ that ​ ​ Restrepo ​ ​ and ​ ​ Acemoglu ’ s ​ ​ estimates ​ ​ of ​ ​ unemployment were ​ ​ localized ​ ​ and ​ ​ that ​ ​ the ​ ​ media ​ ​ distorted ​ ​ their ​ ​ conclusions ​ ​ regarding ​ ​ job ​ ​ loss ​ ​ while ​ ​ also ​ ​ ignoring ​ ​ productivity ​ ​ increases . See : ​ ​ Lawrence ​ ​ Mishel ​ ​ and ​ ​ Bivens , ​ ​ “ The ​ ​ Zombie ​ ​ Robot ​ ​ Argument ​ ​ Lurches ​ ​ on : ​ ​ There ​ ​ Is ​ ​ No ​ ​ Evidence ​ ​ That ​ ​ Automation ​ ​ Leads ​ ​ to Joblessness ​ ​ or ​ ​ Inequality ” ​ ​ ( Washington , ​ ​ DC : ​ ​ Economic ​ ​ Policy ​ ​ Institute , ​ ​ May ​ ​ 24 , ​ ​ 2017 ) , http : //www.epi.org/publication/the-zombie-robot-argument-lurches-on-there-is-no-evidence-that-automation-leads-to-job lessness-or-inequality/ . 13 Robert ​ ​ Seamans , ​ ​ “ We ​ ​ Won ’ t ​ ​ Even ​ ​ Know ​ ​ If ​ ​ A ​ ​ Robot ​ ​ Takes ​ ​ Your ​ ​ Job , ” ​ ​​ Forbes ​ , ​ ​ January ​ ​ 11 , ​ ​ 2017 , http : //www.forbes.com/sites/washingtonbytes/2017/01/11/we-wont-even-know-if-a-robot-takes-your-job/ ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 9 AI ​ ​ and ​ ​ the ​ ​ Nature ​ ​ of ​ ​ Work While ​ ​ the ​ ​ displacement ​ ​ of ​ ​ entire ​ ​ occupations , ​ ​ such ​ ​ as ​ ​ taxi ​ ​ or ​ ​ truck ​ ​ drivers , ​ ​ is ​ ​ clearly ​ ​ an 14 important ​ ​ concern , ​ ​ AI ​ ​ is ​ ​ also ​ ​ transforming ​ ​ a ​ ​ wide ​ ​ range ​ ​ of ​ ​ occupations ​ ​ and ​ ​ roles . ​ ​ Across sectors , ​ ​ automated ​ ​ management ​ ​ and ​ ​ hiring ​ ​ technologies ​ ​ are ​ ​ being ​ ​ introduced , ​ ​ promising to ​ ​ increase ​ ​ worker ​ ​ productivity ​ ​ and ​ ​ flexibility , ​ ​ but ​ ​ also ​ ​ exposing ​ ​ workers ​ ​ to ​ ​ new ​ ​ forms ​ ​ of monitoring , ​ ​ manipulation ​ ​ and ​ ​ control . ​ ​ This ​ ​ changes ​ ​ labor ​ ​ processes ​ ​ and ​ ​ power ​ ​ relations . Further ​ ​ research ​ ​ on ​ ​ this ​ ​ topic ​ ​ is ​ ​ needed ​ ​ to ​ ​ address ​ ​ how ​ ​ AI ​ ​ is ​ ​ transforming ​ ​ the ​ ​ nature ​ ​ of work ​ ​ itself , ​ ​ and ​ ​ how ​ ​ these ​ ​ transformations ​ ​ are ​ ​ manifesting ​ ​ for ​ ​ specific ​ ​ occupations ​ ​ within specific ​ ​ sectors . Luke ​ ​ Stark ​ ​ and ​ ​ Alex ​ ​ Rosenblat ’ s ​ ​ research ​ ​ with ​ ​ Uber ​ ​ drivers ​ ​ suggests ​ ​ one ​ ​ model ​ ​ for ​ ​ this approach . ​ ​ By ​ ​ listening ​ ​ to ​ ​ drivers , ​ ​ they ​ ​ identified ​ ​ algorithmic ​ ​ forms ​ ​ of ​ ​ management ​ ​ used ​ ​ by the ​ ​ company . ​ ​ While ​ ​ its ​ ​ driver ​ ​ platform , ​ ​ which ​ ​ acts ​ ​ as ​ ​ a ​ ​ kind ​ ​ of ​ ​ remote ​ ​ management 15 console , ​ ​ helps ​ ​ make ​ ​ more ​ ​ efficient ​ ​ use ​ ​ of ​ ​ driver ​ ​ time ​ ​ in ​ ​ this ​ ​ digital ​ ​ “ matching ​ ​ market , ” 16 the ​ ​ platform ​ ​ also ​ ​ exposes ​ ​ fundamental ​ ​ informational ​ ​ asymmetries ​ ​ between ​ ​ worker ​ ​ and platform ​ ​ owner . ​ ​ For ​ ​ example , ​ ​ drivers ​ ​ have ​ ​ about ​ ​ 15 ​ ​ seconds ​ ​ to ​ ​ accept ​ ​ ride ​ ​ requests ​ ​ via ​ ​ the platform , ​ ​ and ​ ​ are ​ ​ not ​ ​ shown ​ ​ the ​ ​ rider ’ s ​ ​ destination . ​ ​ With ​ ​ drivers ​ ​ in ​ ​ the ​ ​ dark , ​ ​ they ​ ​ don ’ t know ​ ​ when ​ ​ they ​ ​ will ​ ​ accept ​ ​ short , ​ ​ unprofitable ​ ​ fares . ​ ​ Meanwhile , ​ ​ Uber ​ ​ furthers ​ ​ its ​ ​ own goal ​ ​ of ​ ​ providing ​ ​ near-instantaneous ​ ​ service ​ ​ to ​ ​ all ​ ​ prospective ​ ​ riders . ​ ​ Because ​ ​ Uber 17 designs ​ ​ the ​ ​ platform ​ ​ and ​ ​ can ​ ​ change ​ ​ it ​ ​ at ​ ​ will , ​ ​ conflicts ​ ​ of ​ ​ interest ​ ​ between ​ ​ worker ​ ​ and platform ​ ​ owner ​ ​ are ​ ​ systematically ​ ​ settled ​ ​ in ​ ​ favor ​ ​ of ​ ​ Uber ​ ​ via ​ ​ the ​ ​ platform ​ ​ itself , ​ ​ not collective ​ ​ bargaining ​ ​ or ​ ​ other ​ ​ processes ​ ​ that ​ ​ allow ​ ​ for ​ ​ worker ​ ​ participation . ​ ​ This ​ ​ flatly contradicts ​ ​ any ​ ​ argument ​ ​ that ​ ​ the ​ ​ platform ​ ​ is ​ ​ “ neutral. ” ​ ​ It ​ ​ will ​ ​ be ​ ​ interesting ​ ​ to ​ ​ see ​ ​ what comes ​ ​ of ​ ​ the ​ ​ recent ​ ​ New ​ ​ York ​ ​ administrative ​ ​ law ​ ​ judge ’ s ​ ​ ruling , ​ ​ which ​ ​ classified ​ ​ Uber drivers ​ ​ as ​ ​ “ employees ” ​ ​ under ​ ​ New ​ ​ York ​ ​ law , ​ ​ contrary ​ ​ to ​ ​ Uber ’ s ​ ​ claims ​ ​ otherwise . 18 Of ​ ​ course , ​ ​ asymmetrical ​ ​ forms ​ ​ of ​ ​ workplace ​ ​ management ​ ​ and ​ ​ control ​ ​ long ​ ​ predate ​ ​ AI . 19 The ​ ​ task ​ ​ for ​ ​ researchers ​ ​ is ​ ​ to ​ ​ determine ​ ​ specifically ​ ​ what ​ ​ makes ​ ​ AI-powered ​ ​ asymmetries different ​ ​ from ​ ​ other ​ ​ forms ​ ​ of ​ ​ monitoring , ​ ​ such ​ ​ as ​ ​ Taylorist ​ ​ scientific ​ ​ management ​ ​ and ​ ​ the 20 audit ​ ​ culture ​ ​ of ​ ​ total ​ ​ quality ​ ​ control . ​ ​​ One ​ ​ clear ​ ​ difference ​ ​ is ​ ​ AI ’ s ​ ​ reliance ​ ​ on ​ ​ workplace 21 surveillance ​ ​ and ​ ​ the ​ ​ data ​ ​ it ​ ​ produces , ​ ​ and ​ ​ thus ​ ​ the ​ ​ normalization ​ ​ of ​ ​ workplace ​ ​ surveillance 14 Truckers , ​ ​ like ​ ​ ride-sharing ​ ​ drivers , ​ ​ are ​ ​ also ​ ​ subject ​ ​ to ​ ​ data-driven ​ ​ forms ​ ​ of ​ ​ surveillance ​ ​ and ​ ​ control . ​ ​ e.g . ​ ​ Karen ​ ​ E. ​ ​ C. ​ ​ Levy , “ The ​ ​ Contexts ​ ​ of ​ ​ Control : ​ ​ Information , ​ ​ Power , ​ ​ and ​ ​ Truck-Driving ​ ​ Work , ” ​ ​​ The ​ ​ Information ​ ​ Society ​ ​ ​ 31 , ​ ​ No . ​ ​ 2 ​ ​ ( March ​ ​ 15 , ​ ​ 2015 ) : 160–74 , ​ ​ doi:10.1080/01972243.2015.998105 . 15 Alex ​ ​ Rosenblat ​ ​ and ​ ​ Luke ​ ​ Stark , ​ ​ “ Algorithmic ​ ​ Labor ​ ​ and ​ ​ Information ​ ​ Asymmetries : ​ ​ A ​ ​ Case ​ ​ Study ​ ​ of ​ ​ Uber ’ s ​ ​ Drivers , ” International ​ ​ Journal ​ ​ of ​ ​ Communication ​ ​ 10 ​ ​ ( July ​ ​ 27 , ​ ​ 2016 ) : ​ ​ 3758-3784 , http : //ijoc.org/index.php/ijoc/article/view/4892/1739 ​ . 16 Eduardo ​ ​ M. ​ ​ Azevedo ​ ​ and ​ ​ E. ​ ​ Glen ​ ​ Weyl , ​ ​ “ Matching ​ ​ Markets ​ ​ in ​ ​ the ​ ​ Digital ​ ​ Age , ” ​ ​ Science ​ ​ 352 , ​ ​ no . ​ ​ 6289 ​ ​ ( May ​ ​ 27 , ​ ​ 2016 ) : 1056–57 , ​ ​​ http : //science.sciencemag.org/content/352/6289/1056 ​ . 17 Rosenblat ​ ​ and ​ ​ Stark , ​ ​ “ Algorithmic ​ ​ Labor ​ ​ and ​ ​ Information ​ ​ Asymmetries , ” ​ ​ 3762 . 18 Dana ​ ​ Rubenstein , ​ ​ “ State ​ ​ Labor ​ ​ Judge ​ ​ Finds ​ ​ Uber ​ ​ an ​ ​ ‘ employer ’ , ” ​ ​​ Politico ​ , ​ ​ May ​ ​ 13 , ​ ​ 2017 , http : //www.politico.com/states/new-york/albany/story/2017/06/13/state-labor-court-finds-uber-an-employer-112733 ​ . 19 ​ ​​ ​​ ​​ Ifeoma ​ ​ Ajunwa , ​ ​ Kate ​ ​ Crawford ​ ​ and ​ ​ Jason ​ ​ Schultz , ​ ​ “ Limitless ​ ​ Worker ​ ​ Surveillance , ” ​ California ​ ​ Law ​ ​ Review ​ ​ ​ 105 , ​ ​ No . ​ ​ 3 ​ ​ , ​ ​ 2017 . 20 Hugh ​ ​ G. ​ ​ J ​ ​ Aitken , ​ ​​ Taylorism ​ ​ at ​ ​ Watertown ​ ​ Arsenal ; ​ ​ Scientific ​ ​ Management ​ ​ in ​ ​ Action ​ , ​ ​ 1908-1915 . ​ ​ ( Cambridge : ​ ​ Harvard University ​ ​ Press , ​ ​ 1960 ) . 21 Marilyn ​ ​ Strathern , ​ ​​ Audit ​ ​ Cultures : ​ ​ Anthropological ​ ​ Studies ​ ​ in ​ ​ Accountability , ​ ​ Ethics ​ ​ and ​ ​ the ​ ​ Academy ​ ​ ​ ( London : ​ ​ Routledge , 2000 ) . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 10 practices . ​ ​ Such ​ ​ systems ​ ​ provide ​ ​ employers ​ ​ with ​ ​ expansive ​ ​ and ​ ​ often ​ ​ invasive ​ ​ data ​ ​ about the ​ ​ workplace ​ ​ behaviors ​ ​ of ​ ​ their ​ ​ employees . ​ ​ It ​ ​ is ​ ​ this ​ ​ data ​ ​ that ​ ​ AI-powered ​ ​ management systems ​ ​ rely ​ ​ on ​ ​ to ​ ​ generate ​ ​ insights . ​ ​ As ​ ​ AI-driven ​ ​ management ​ ​ becomes ​ ​ more ​ ​ common , ​ ​ so will ​ ​ the ​ ​ data ​ ​ collection ​ ​ and ​ ​ worker ​ ​ surveillance ​ ​ practices ​ ​ on ​ ​ which ​ ​ it ​ ​ relies . ​ ​ Worryingly , ​ ​ this employee ​ ​ monitoring ​ ​ is ​ ​ not ​ ​ necessarily ​ ​ limited ​ ​ to ​ ​ the ​ ​ workplace , ​ ​ and ​ ​ can ​ ​ spill ​ ​ into ​ ​ private life , ​ ​ such ​ ​ as ​ ​ with ​ ​ fitness ​ ​ trackers , ​ ​ ubiquitous ​ ​ productivity ​ ​ apps , ​ ​ or ​ ​ company-issued smartphones ​ ​ equipped ​ ​ with ​ ​ monitoring ​ ​ features . While ​ ​ we ​ ​ might ​ ​ assume ​ ​ this ​ ​ would ​ ​ be ​ ​ held ​ ​ in ​ ​ check ​ ​ by ​ ​ privacy ​ ​ laws ​ ​ and ​ ​ existing ​ ​ policy , Ifeoma ​ ​ Ajunwa , ​ ​ Kate ​ ​ Crawford ​ ​ and ​ ​ Jason ​ ​ Schultz ​ ​ published ​ ​ a ​ ​ study ​ ​ of ​ ​ existing ​ ​ legal frameworks , ​ ​ assessing ​ ​ if ​ ​ there ​ ​ are ​ ​ any ​ ​ meaningful ​ ​ limits ​ ​ on ​ ​ workplace ​ ​ surveillance . ​ ​ They found ​ ​ very ​ ​ few , ​ ​ some ​ ​ of ​ ​ which ​ ​ are ​ ​ already ​ ​ under ​ ​ threat ​ ​ from ​ ​ the ​ ​ Trump ​ ​ administration . 22 This ​ ​ degree ​ ​ of ​ ​ 24/7 ​ ​ surveillance ​ ​ has ​ ​ the ​ ​ potential ​ ​ to ​ ​ transform ​ ​ key ​ ​ features ​ ​ of ​ ​ prior management ​ ​ systems , ​ ​ potentially ​ ​ in ​ ​ ways ​ ​ workers ​ ​ won ’ t ​ ​ be ​ ​ aware ​ ​ of ​ ​ or ​ ​ have ​ ​ a ​ ​ say ​ ​ in . Employers ​ ​ could ​ ​ easily ​ ​ use ​ ​ machine ​ ​ learning ​ ​ techniques ​ ​ to ​ ​ identify ​ ​ behavioral ​ ​ patterns both ​ ​ during ​ ​ and ​ ​ outside ​ ​ of ​ ​ work ​ ​ hours , ​ ​ and ​ ​ then ​ ​ exploit ​ ​ these ​ ​ one-sided ​ ​ insights ​ ​ to increase ​ ​ profits ​ ​ and ​ ​ manipulate ​ ​ behaviors , ​ ​ with ​ ​ potentially ​ ​ negative ​ ​ effects ​ ​ for ​ ​ workers . Uber ’ s ​ ​ platform ​ ​ demonstrates ​ ​ how ​ ​ workers ​ ​ are ​ ​ directly ​ ​ and ​ ​ indirectly ​ ​ manipulated ​ ​ in service ​ ​ of ​ ​ instant ​ ​ customer ​ ​ gratification . ​ ​ The ​ ​ company ​ ​ wants ​ ​ to ​ ​ keep ​ ​ up ​ ​ the ​ ​ number ​ ​ of available ​ ​ cars , ​ ​ even ​ ​ during ​ ​ times ​ ​ of ​ ​ low ​ ​ demand ​ ​ when ​ ​ drivers ​ ​ make ​ ​ less ​ ​ money . ​ ​ To ​ ​ address this , ​ ​ the ​ ​ ride-sharing ​ ​ company ​ ​ drew ​ ​ on ​ ​ behavioral ​ ​ economic ​ ​ research ​ ​ about ​ ​ the psychological ​ ​ tendency ​ ​ of ​ ​ taxi ​ ​ workers ​ ​ to ​ ​ set ​ ​ round ​ ​ earnings ​ ​ goals ​ ​ and ​ ​ stop ​ ​ working ​ ​ when they ​ ​ reach ​ ​ them . ​ ​ Uber , ​ ​ with ​ ​ access ​ ​ to ​ ​ vast ​ ​ real-time ​ ​ data ​ ​ about ​ ​ driver ​ ​ activities , ​ ​ can 23 quickly ​ ​ test ​ ​ such ​ ​ theories , ​ ​ using ​ ​ machine ​ ​ learning ​ ​ to ​ ​ identify ​ ​ exploitable ​ ​ behavioral patterns , ​ ​ even ​ ​ at ​ ​ an ​ ​ individual ​ ​ level . ​ ​ Uber ​ ​ discovered ​ ​ that ​ ​ drivers ​ ​ quickly ​ ​ abandon ​ ​ mental income ​ ​ targets ​ ​ in ​ ​ favor ​ ​ of ​ ​ working ​ ​ at ​ ​ times ​ ​ of ​ ​ high ​ ​ demand . ​ ​ To ​ ​ combat ​ ​ this ​ ​ tendency , ​ ​ Uber sent ​ ​ tailored ​ ​ nudge ​ ​ messages ​ ​ to ​ ​ drivers ​ ​ indicating ​ ​ when ​ ​ they ​ ​ are ​ ​ close ​ ​ to ​ ​ revenue ​ ​ target 24 during ​ ​ times ​ ​ when ​ ​ it ​ ​ was ​ ​ advantageous ​ ​ for ​ ​ Uber ​ ​ to ​ ​ keep ​ ​ its ​ ​ drivers ​ ​ on ​ ​ the ​ ​ road . ​ ​ Until ​ ​ a 25 recent ​ ​ feature ​ ​ in ​ ​​ The ​ ​ New ​ ​ York ​ ​ Times ​ , ​ ​ drivers ​ ​ were ​ ​ unaware ​ ​ that ​ ​ they ​ ​ were ​ ​ subjects ​ ​ in ​ ​ a large ​ ​ behavioral ​ ​ experiment ​ ​ that ​ ​ sought ​ ​ to ​ ​ modify ​ ​ their ​ ​ actions ​ ​ to ​ ​ benefit ​ ​ the ​ ​ company ’ s goals . ​ ​ Given ​ ​ the ​ ​ opacity ​ ​ of ​ ​ these ​ ​ systems , ​ ​ there ​ ​ may ​ ​ be ​ ​ many ​ ​ more ​ ​ such ​ ​ experiments ​ ​ that 22 Ifeoma ​ ​ Ajunwa , ​ ​ Kate ​ ​ Crawford ​ ​ and ​ ​ Jason ​ ​ Schultz , ​ ​ “ Limitless ​ ​ Worker ​ ​ Surveillance , ” ​ California ​ ​ Law ​ ​ Review ​ ​ ​ 105 , ​ ​ No . ​ ​ 3 ​ ​ ( June ​ ​ 1 , 2017 ) . 23 Colin ​ ​ Camerer , ​ ​ Linda ​ ​ Babcock , ​ ​ George ​ ​ Loewenstein ​ ​ and ​ ​ Richard ​ ​ Thaler , ​ ​ “ Labor ​ ​ Supply ​ ​ of ​ ​ New ​ ​ York ​ ​ City ​ ​ Cab ​ ​ Drivers : ​ ​ One Day ​ ​ at ​ ​ a ​ ​ Time , ” ​ ​ The ​ ​ Quarterly ​ ​ Journal ​ ​ of ​ ​ Economics ​ ​ 112 , ​ ​ No . ​ ​ 2 ​ ​ ( May ​ ​ 1 , ​ ​ 1997 ) : ​ ​ 407–41 , ​ ​ doi:10.1162/003355397555244 . 24 The ​ ​ use ​ ​ of ​ ​ “ nudge ” ​ ​ as ​ ​ a ​ ​ more ​ ​ technical , ​ ​ policy-oriented ​ ​ term ​ ​ has ​ ​ emerged ​ ​ out ​ ​ of ​ ​ work ​ ​ in ​ ​ the ​ ​ decision ​ ​ and ​ ​ choice ​ ​ sciences , most ​ ​ influentially ​ ​ that ​ ​ of ​ ​ behavioral ​ ​ economist ​ ​ Richard ​ ​ Thaler ​ ​ and ​ ​ the ​ ​ legal ​ ​ scholar ​ ​ Cass ​ ​ Sunstein , ​ ​ who ​ ​ headed ​ ​ the ​ ​ Obama administration ’ s ​ ​ Office ​ ​ of ​ ​ Information ​ ​ and ​ ​ Regulatory ​ ​ Affairs . ​ ​ They , ​ ​ in ​ ​ turn , ​ ​ draw ​ ​ on ​ ​ psychological ​ ​ studies ​ ​ of ​ ​ how ​ ​ people make ​ ​ decisions ​ ​ under ​ ​ conditions ​ ​ of ​ ​ uncertainty ​ ​ and ​ ​ avoid ​ ​ errors ​ ​ due ​ ​ to ​ ​ heuristics—like ​ ​ an ​ ​ earnings ​ ​ goal—and ​ ​ biases . ​ ​ These were ​ ​ first ​ ​ identified ​ ​ by ​ ​ the ​ ​ influential ​ ​ psychologists ​ ​ Amos ​ ​ Tversky ​ ​ and ​ ​ Daniel ​ ​ Kahneman . ​ ​ V. : Richard ​ ​ H. ​ ​ Thaler ​ ​ and ​ ​ Cass ​ ​ R. Sunstein , ​ ​​ Nudge : ​ ​ Improving ​ ​ Decisions ​ ​ About ​ ​ Health , ​ ​ Wealth , ​ ​ and ​ ​ Happiness ​ ​ ​ ( New ​ ​ York : ​ ​ Penguin ​ ​ Books , ​ ​ 2009 ) ; ​ ​ Amos ​ ​ Tversky and ​ ​ Daniel ​ ​ Kahneman , ​ ​ “ Judgment ​ ​ under ​ ​ Uncertainty : ​ ​ Heuristics ​ ​ and ​ ​ Biases , ” ​ ​​ Science ​ ​ ​ 185 , ​ ​ No . ​ ​ 4157 ​ ​ ( September ​ ​ 27 , ​ ​ 1974 ) : 1124–31 , ​ ​ doi:10.1126/science.185.4157.1124 . 25 Noam ​ ​ Scheiber , ​ ​ “ How ​ ​ Uber ​ ​ Uses ​ ​ Psychological ​ ​ Tricks ​ ​ to ​ ​ Push ​ ​ Its ​ ​ Drivers ’ ​ ​ Buttons , ” ​ ​​ The ​ ​ New ​ ​ York ​ ​ Times ​ , ​ ​ April ​ ​ 2 , ​ ​ 2017 , https : //www.nytimes.com/interactive/2017/04/02/technology/uber-drivers-psychological-tricks.html ? _r=0 ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 11 workers ​ ​ and ​ ​ the ​ ​ public ​ ​ will ​ ​ never ​ ​ know ​ ​ about . This ​ ​ case ​ ​ illustrates ​ ​ how ​ ​ AI ​ ​ management ​ ​ might ​ ​ differ ​ ​ from ​ ​ past ​ ​ forms ​ ​ of ​ ​ incentive-based control . ​ ​ As ​ ​ companies ​ ​ gather ​ ​ more ​ ​ data ​ ​ on ​ ​ their ​ ​ workers , ​ ​ they ​ ​ no ​ ​ longer ​ ​ need ​ ​ to ​ ​ rely ​ ​ on generalized ​ ​ psychological ​ ​ theories ​ ​ or ​ ​ human-to-human ​ ​ assessments ​ ​ of ​ ​ merit . ​ ​ They ​ ​ can instead ​ ​ exploit ​ ​ information ​ ​ asymmetries ​ ​ to ​ ​ identify ​ ​ behavioral ​ ​ patterns ​ ​ at ​ ​ the ​ ​ ​ individual level ​ ​​ and ​ ​ nudge ​ ​ people ​ ​ toward ​ ​ the ​ ​ most ​ ​ profitable ​ ​ activities ​ ​ for ​ ​ the ​ ​ platform ​ ​ owners , ​ ​ even when ​ ​ these ​ ​ operate ​ ​ against ​ ​ the ​ ​ best ​ ​ interests ​ ​ of ​ ​ workers ​ ​ themselves . ​ ​ By ​ ​ selectively exploiting ​ ​ workers ’ ​ ​ behavior , ​ ​ often ​ ​ without ​ ​ workers ’ ​ ​ consent ​ ​ or ​ ​ even ​ ​ knowledge , ​ ​ these technologies ​ ​ have ​ ​ the ​ ​ potential ​ ​ to ​ ​ make ​ ​ workers ​ ​ complicit ​ ​ in ​ ​ their ​ ​ own ​ ​ exploitation . ​ ​ To address ​ ​ these ​ ​ emerging ​ ​ imbalances ​ ​ of ​ ​ workplace ​ ​ power , ​ ​ it ​ ​ will ​ ​ likely ​ ​ be ​ ​ necessary ​ ​ for unions , ​ ​ labor ​ ​ rights ​ ​ advocates ​ ​ and ​ ​ individual ​ ​ workers ​ ​ to ​ ​ participate ​ ​ in ​ ​ the ​ ​ design ​ ​ of ​ ​ worker platforms . ​ ​ It ​ ​ will ​ ​ also ​ ​ likely ​ ​ be ​ ​ necessary ​ ​ to ​ ​ give ​ ​ workers ​ ​ a ​ ​ democratic ​ ​ voice ​ ​ in ​ ​ shaping ​ ​ both whether ​ ​ and ​ ​ how ​ ​ they ​ ​ are ​ ​ monitored ​ ​ and ​ ​ how ​ ​ machine ​ ​ learning ​ ​ techniques ​ ​ will ​ ​ be ​ ​ used ​ ​ to process ​ ​ such ​ ​ data . ​ ​ This ​ ​ is ​ ​ a ​ ​ rich ​ ​ area ​ ​ of ​ ​ research ​ ​ and ​ ​ design ​ ​ for ​ ​ the ​ ​ technical ​ ​ architects ​ ​ of AI ​ ​ management ​ ​ systems , ​ ​ labor ​ ​ organizers ​ ​ and ​ ​ advocates ​ ​ to ​ ​ explore . AI ​ ​ management ​ ​ systems ​ ​ also ​ ​ provide ​ ​ new ​ ​ and ​ ​ invasive ​ ​ methods ​ ​ for ​ ​ evaluating ​ ​ employees and ​ ​ making ​ ​ retention ​ ​ decisions . ​ ​ For ​ ​ example , ​ ​ the ​ ​ employee ​ ​ monitoring ​ ​ firm ​ ​ Veriato captures ​ ​ information ​ ​ from ​ ​ nearly ​ ​ any ​ ​ task ​ ​ a ​ ​ worker ​ ​ performs ​ ​ on ​ ​ a ​ ​ computer , ​ ​ from browsing ​ ​ history ​ ​ to ​ ​ email ​ ​ and ​ ​ chat , ​ ​ even ​ ​ taking ​ ​ periodic ​ ​ screenshots ​ ​ of ​ ​ workers ’ ​ ​ monitor displays . ​ ​ The ​ ​ firm ’ s ​ ​ software ​ ​ aggregates ​ ​ this ​ ​ information , ​ ​ then ​ ​ uses ​ ​ machine ​ ​ learning ​ ​ to detect ​ ​ anomalous ​ ​ behaviors . ​ ​ The ​ ​ program ​ ​ can ​ ​ then ​ ​ send ​ ​ warning ​ ​ messages ​ ​ to ​ ​ employees who ​ ​ deviate ​ ​ from ​ ​ the ​ ​ norm . ​ ​ What ​ ​ the ​ ​ consequences ​ ​ of ​ ​ such ​ ​ deviance ​ ​ are ​ ​ for ​ ​ workers ​ ​ is 26 up ​ ​ to ​ ​ the ​ ​ employer . ​ ​ And ​ ​ this ​ ​ isn ’ t ​ ​ all . ​ ​ Veriato ’ s ​ ​ software ​ ​ also ​ ​ offers ​ ​ features ​ ​ to ​ ​ score ​ ​ email and ​ ​ chats ​ ​ for ​ ​ sentiment ​ ​ using ​ ​ natural ​ ​ language ​ ​ processing . ​ ​ Language ​ ​ that ​ ​ their ​ ​ program determines ​ ​ to ​ ​ be ​ ​ “ negative ” ​ ​ is ​ ​ interpreted ​ ​ by ​ ​ the ​ ​ company ​ ​ as ​ ​ an ​ ​ indication ​ ​ of ​ ​ a productivity ​ ​ risk , ​ ​ or ​ ​ of ​ ​ an ​ ​ employee ​ ​ who ​ ​ is ​ ​ getting ​ ​ ready ​ ​ to ​ ​ leave ​ ​ the ​ ​ company . ​ ​ Similarly , another ​ ​ company , ​ ​ Workday , ​ ​ assigns ​ ​ employees ​ ​ individualized ​ ​ risk ​ ​ score ​ ​ based ​ ​ on ​ ​ 60 factors . ​ ​ Many ​ ​ employees ​ ​ who ​ ​ use ​ ​ a ​ ​ work-issued ​ ​ computer ​ ​ or ​ ​ mobile ​ ​ are ​ ​ already ​ ​ subject 27 to ​ ​ this ​ ​ type ​ ​ of ​ ​ monitoring ​ ​ and ​ ​ software-driven ​ ​ ranking ​ ​ and ​ ​ assessment . ​ ​ Additionally , ​ ​ many of ​ ​ them ​ ​ likely ​ ​ have ​ ​ no ​ ​ idea ​ ​ that ​ ​ their ​ ​ value ​ ​ as ​ ​ an ​ ​ employee ​ ​ is ​ ​ being ​ ​ determined ​ ​ in ​ ​ part ​ ​ by software ​ ​ systems ​ ​ scoring ​ ​ everything ​ ​ from ​ ​ the ​ ​ emotional ​ ​ content ​ ​ of ​ ​ their ​ ​ emails ​ ​ to ​ ​ their frequency ​ ​ of ​ ​ accepting ​ ​ meeting ​ ​ requests . Beyond ​ ​ employee ​ ​ surveillance , ​ ​ the ​ ​ combination ​ ​ of ​ ​ customer ​ ​ surveillance ​ ​ and ​ ​ AI ​ ​ has ​ ​ the potential ​ ​ to ​ ​ turn ​ ​ previously ​ ​ stable ​ ​ employment ​ ​ in ​ ​ sectors ​ ​ like ​ ​ food ​ ​ service ​ ​ and ​ ​ retail ​ ​ into ​ ​ a form ​ ​ of ​ ​ gig ​ ​ work . ​ ​ So-called ​ ​ scheduling ​ ​ software ​ ​ has ​ ​ allowed ​ ​ retailers ​ ​ to ​ ​ switch ​ ​ from standard ​ ​ shifts ​ ​ to ​ ​ a ​ ​ more ​ ​ “ on ​ ​ call ” ​ ​ model , ​ ​ based ​ ​ on ​ ​ algorithmic ​ ​ predictions ​ ​ about ​ ​ whether customers ​ ​ will ​ ​ be ​ ​ in ​ ​ a ​ ​ store ​ ​ at ​ ​ a ​ ​ given ​ ​ time . ​ ​ While ​ ​ the ​ ​ use ​ ​ of ​ ​ such ​ ​ software ​ ​ can ​ ​ cut ​ ​ an employer ’ s ​ ​ costs ​ ​ by ​ ​ reducing ​ ​ staff ​ ​ during ​ ​ off-peak ​ ​ customer ​ ​ hours , ​ ​ as ​ ​ Solon ​ ​ Barocas ​ ​ and 26 Ted ​ ​ Greenwald , ​ ​ “ How ​ ​ AI ​ ​ Is ​ ​ Transforming ​ ​ the ​ ​ Workplace , ” ​ ​ Wall ​ ​ Street ​ ​ Journal , ​ ​ March ​ ​ 10 , ​ ​ 2017 , ​ ​ sec . ​ ​ Business , https : //www.wsj.com/articles/how-ai-is-transforming-the-workplace-1489371060 ​ . 27 Ibid . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 12 Karen ​ ​ Levy ​ ​ have ​ ​ observed , ​ ​ it ​ ​ is ​ ​ “ highly ​ ​ destabilizing ” ​ ​ for ​ ​ workers ​ ​ who ​ ​ never ​ ​ know ​ ​ ahead ​ ​ of time ​ ​ whether ​ ​ or ​ ​ not ​ ​ they ​ ​ will ​ ​ be ​ ​ called ​ ​ in ​ ​ for ​ ​ work . ​ ​ The ​ ​ use ​ ​ of ​ ​ predictive ​ ​ scheduling 28 software , ​ ​ whether ​ ​ by ​ ​ gig ​ ​ employers ​ ​ like ​ ​ Uber ​ ​ or ​ ​ more ​ ​ traditional ​ ​ employers , ​ ​ collapses work-life ​ ​ boundaries . ​ ​ It ​ ​ also ​ ​ puts ​ ​ workers ​ ​ at ​ ​ risk ​ ​ of ​ ​ over- ​ ​ or ​ ​ underwork , ​ ​ gives ​ ​ workers ​ ​ little to ​ ​ no ​ ​ control ​ ​ over ​ ​ shift ​ ​ times , ​ ​ and ​ ​ provides ​ ​ them ​ ​ with ​ ​ little ​ ​ ability ​ ​ to ​ ​ predict ​ ​ income ​ ​ flows or ​ ​ to ​ ​ plan ​ ​ ahead ​ ​ for ​ ​ things ​ ​ like ​ ​ child ​ ​ care ​ ​ or ​ ​ a ​ ​ second ​ ​ job . ​ ​ Recognizing ​ ​ the ​ ​ negative ​ ​ impacts that ​ ​ such ​ ​ precarious ​ ​ schedules ​ ​ can ​ ​ have ​ ​ on ​ ​ workers , ​ ​ the ​ ​ Oregon ​ ​ state ​ ​ Senate ​ ​ and ​ ​ House recently ​ ​ passed ​ ​ a ​ ​ bill ​ ​ mandating ​ ​ that ​ ​ large ​ ​ employers ​ ​ in ​ ​ retail , ​ ​ manufacturing ​ ​ and hospitality ​ ​ provide ​ ​ workers ​ ​ a ​ ​ written ​ ​ estimate ​ ​ of ​ ​ their ​ ​ schedule ​ ​ at ​ ​ least ​ ​ 7 ​ ​ days ​ ​ before ​ ​ the start ​ ​ of ​ ​ the ​ ​ work ​ ​ week . ​ ​ Barring ​ ​ a ​ ​ veto ​ ​ from ​ ​ the ​ ​ state ’ s ​ ​ Governor , ​ ​ Oregon ​ ​ will ​ ​ join ​ ​ New 29 York , ​ ​ San ​ ​ Francisco ​ ​ and ​ ​ Seattle , ​ ​ who ​ ​ have ​ ​ also ​ ​ passed ​ ​ laws ​ ​ mandating ​ ​ predictable scheduling . The ​ ​ increasing ​ ​ role ​ ​ of ​ ​ AI ​ ​ and ​ ​ automation ​ ​ within ​ ​ various ​ ​ labor ​ ​ sectors ​ ​ has ​ ​ the ​ ​ potential ​ ​ to revise ​ ​ our ​ ​ understanding ​ ​ of ​ ​ labor ​ ​ and ​ ​ our ​ ​ expectations ​ ​ of ​ ​ goods ​ ​ and ​ ​ services . ​ ​ As consumers ​ ​ grow ​ ​ accustomed ​ ​ to ​ ​ dealing ​ ​ with ​ ​ automated ​ ​ systems , ​ ​ there ​ ​ is ​ ​ a ​ ​ potential ​ ​ to ignore ​ ​ or ​ ​ devalue ​ ​ the ​ ​ human ​ ​ labor ​ ​ that ​ ​ remains ​ ​ essential ​ ​ in ​ ​ many ​ ​ instances . ​ ​ The ​ ​​ AI ​ ​ Now 2016 ​ ​ Labor ​ ​ Primer ​ ​ ​ emphasized ​ ​ that ​ ​ AI ​ ​ often ​ ​ demands ​ ​ “ human ​ ​ caretakers ” — ​ ​ these ​ ​ vary , 30 from ​ ​ workers ​ ​ who ​ ​ maintain ​ ​ and ​ ​ repair ​ ​ data ​ ​ centers ​ ​ to ​ ​ moderators ​ ​ who ​ ​ check ​ ​ the ​ ​ results ​ ​ of even ​ ​ the ​ ​ most ​ ​ sophisticated ​ ​ computer ​ ​ vision ​ ​ algorithms . ​ ​ Since ​ ​ the ​ ​​ AI ​ ​ Now ​ ​ 2016 ​ ​ Labor 31 Primer ​ , ​ ​ Facebook ​ ​ has ​ ​ announced ​ ​ the ​ ​ hiring ​ ​ of ​ ​ 3,000 ​ ​ workers ​ ​ to ​ ​ monitor ​ ​ its ​ ​ live ​ ​ video streaming ​ ​ services ​ ​ for ​ ​ violence , ​ ​ exploitation ​ ​ and ​ ​ hate ​ ​ speech . ​ ​ This ​ ​ is ​ ​ both ​ ​ an 32 acknowledgement ​ ​ that ​ ​ AI ​ ​ systems ​ ​ don ’ t ​ ​ always ​ ​ do ​ ​ the ​ ​ work ​ ​ as ​ ​ intended , ​ ​ and ​ ​ an ​ ​ example of ​ ​ how ​ ​ essential ​ ​ human ​ ​ work ​ ​ happening ​ ​ behind ​ ​ the ​ ​ scenes ​ ​ of ​ ​ complex ​ ​ systems ​ ​ is ​ ​ often invisible . ​ ​ Not ​ ​ surprisingly , ​ ​ this ​ ​ work ​ ​ tends ​ ​ to ​ ​ be ​ ​ outsourced ​ ​ to ​ ​ countries ​ ​ where ​ ​ wages ​ ​ are very ​ ​ low . ​ ​ How ​ ​ will ​ ​ such ​ ​ maintenance ​ ​ and ​ ​ repair ​ ​ work ​ ​ be ​ ​ valued ​ ​ by ​ ​ consumers ​ ​ who ​ ​ have been ​ ​ led ​ ​ to ​ ​ believe ​ ​ that ​ ​ such ​ ​ services ​ ​ are ​ ​ entirely ​ ​ automated ? ​ ​ How ​ ​ will ​ ​ companies ​ ​ that promote ​ ​ themselves ​ ​ as ​ ​ fully ​ ​ automated ​ ​ “ AI ​ ​ magic ” ​ ​ treat ​ ​ and ​ ​ recognize ​ ​ workers ​ ​ within these ​ ​ systems ? ​ ​ Additionally , ​ ​ how ​ ​ will ​ ​ this ​ ​ lack ​ ​ of ​ ​ visibility ​ ​ impact ​ ​ workers ’ ​ ​ ability ​ ​ to organize ​ ​ and ​ ​ shape ​ ​ their ​ ​ own ​ ​ working ​ ​ conditions ? Managers ​ ​ too , ​ ​ will ​ ​ need ​ ​ to ​ ​ rethink ​ ​ how ​ ​ they ​ ​ formulate ​ ​ goals ​ ​ and ​ ​ use ​ ​ data , ​ ​ while acknowledging ​ ​ the ​ ​ limits ​ ​ and ​ ​ risks ​ ​ of ​ ​ automated ​ ​ systems . ​ ​ Michael ​ ​ Luca , ​ ​ Jon ​ ​ Kleinberg , ​ ​ and Sendhil ​ ​ Mullainathan ​ ​ argue ​ ​ that ​ ​ these ​ ​ systems ​ ​ can ​ ​ miss ​ ​ contextual ​ ​ details ​ ​ and ​ ​ may ​ ​ not 28 Solon ​ ​ Barocas ​ ​ and ​ ​ Karen ​ ​ Levy , ​ ​ “ What ​ ​ Customer ​ ​ Data ​ ​ Collection ​ ​ Could ​ ​ Mean ​ ​ for ​ ​ Workers , ” ​ ​​ Harvard ​ ​ Business ​ ​ Review ​ , ​ ​ August 31 , ​ ​ 2016 , ​ ​​ https : //hbr.org/2016/08/the-unintended-consequence-of-customer-data-collection ​ . 29 ​ ​​ ​​ ​​ Hillary ​ ​ Borrud , ​ ​ “ Oregon ​ ​ on ​ ​ way ​ ​ to ​ ​ become ​ ​ first ​ ​ state ​ ​ to ​ ​ guarantee ​ ​ predictable ​ ​ work ​ ​ schedules , ” ​ ​ Oregonian , ​ ​ June ​ ​ 29 , ​ ​ 2017 , sec . ​ ​ Oregon ​ ​ Live , ​ ​​ http : //www.oregonlive.com/politics/index.ssf/2017/06/oregon_on_way_to_become_first.html 30 “ AI ’ s ​ ​ human ​ ​ caretakers ” ​ ​ in ​ ​ the ​ ​​ 2016 ​ ​ ​​ AI ​ ​ Now ​ ​ ​​ Labor ​ ​ and ​ ​ Automation ​ ​ Prime ​ r , ​ ​ “ Labor ​ ​ and ​ ​ AI , ” https : //ainowinstitute.org/AI_Now_2016_Primers.pdf ​ . 31 Sarah ​ ​ T. ​ ​ Roberts , ​ ​ “ Commercial ​ ​ Content ​ ​ Moderation : ​ ​ Digital ​ ​ Laborers ’ ​ ​ Dirty ​ ​ Work , ” ​ ​ in ​ ​​ The ​ ​ Intersectional ​ ​ Internet : ​ ​ Race , ​ ​ Sex , Class ​ ​ and ​ ​ Culture ​ ​ Online ​ , ​ ​ ed . ​ ​ Safiya ​ ​ Umoja ​ ​ Noble ​ ​ and ​ ​ Brendesha ​ ​ M. ​ ​ Tynes ​ ​ ( New ​ ​ York : ​ ​ Peter ​ ​ Lang , ​ ​ 2016 ) , ​ ​ 147–60 . 32 Kathleen ​ ​ Chaykowski , ​ ​ “ Facebook ​ ​ Is ​ ​ Hiring ​ ​ 3,000 ​ ​ Moderators ​ ​ In ​ ​ Push ​ ​ To ​ ​ Curb ​ ​ Violent ​ ​ Videos , ” ​ ​​ Forbes ​ , ​ ​ accessed ​ ​ May ​ ​ 10 , 2017 , ​ http : //www.forbes.com/sites/kathleenchaykowski/2017/05/03/facebook-is-hiring-3000-moderators-in-push-to-curb violent-videos/ ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 13 provide ​ ​ clear ​ ​ reasoning ​ ​ for ​ ​ decisions . ​ ​ They ​ ​ advise ​ ​ managers ​ ​ to ​ ​ ask ​ ​ employees ​ ​ and stakeholders ​ ​ to ​ ​ articulate ​ ​ concerns ​ ​ with ​ ​ such ​ ​ systems ; ​ ​ more ​ ​ democratic ​ ​ input ​ ​ can ​ ​ often improve ​ ​ performance . ​ ​ Similarly ​ ​ they ​ ​ recommend ​ ​ that ​ ​ diverse ​ ​ data-inputs ​ ​ be ​ ​ used ​ ​ in pursuit ​ ​ of ​ ​ long-term ​ ​ goals ​ ​ and ​ ​ values , ​ ​ instead ​ ​ of ​ ​ focusing ​ ​ too ​ ​ narrowly ​ ​ on ​ ​ low-hanging fruit , ​ ​ which ​ ​ can ​ ​ often ​ ​ produce ​ ​ unintended ​ ​ consequences , ​ ​ like ​ ​ clickbait ​ ​ in ​ ​ search ​ ​ of ​ ​ social media ​ ​ engagement . 33 Inequality ​ ​ and ​ ​ Redistribution What ​ ​ happens ​ ​ to ​ ​ workers ​ ​ after ​ ​ their ​ ​ jobs ​ ​ have ​ ​ been ​ ​ automated ? ​ ​ The ​ ​ potential ​ ​ for ​ ​ AI systems ​ ​ to ​ ​ exacerbate ​ ​ inequality ​ ​ has ​ ​ been ​ ​ widely ​ ​ acknowledged . ​ ​ To ​ ​ address ​ ​ what ​ ​ to ​ ​ do about ​ ​ it , ​ ​ some ​ ​ are ​ ​ turning ​ ​ to ​ ​ models ​ ​ of ​ ​ resource ​ ​ redistribution , ​ ​ and ​ ​ to ​ ​ the ​ ​ idea ​ ​ of ​ ​ a universal ​ ​ basic ​ ​ income ​ ​ ( UBI ) . ​ ​ The ​ ​ past ​ ​ year ​ ​ has ​ ​ seen ​ ​ a ​ ​ number ​ ​ of ​ ​ high-profile ​ ​ experiments in ​ ​ redistributive ​ ​ social ​ ​ welfare , ​ ​ based ​ ​ on ​ ​ assumptions ​ ​ that ​ ​ AI ​ ​ and ​ ​ automation ​ ​ will ​ ​ require resource ​ ​ distribution ​ ​ not ​ ​ explicitly ​ ​ tied ​ ​ to ​ ​ the ​ ​ sale ​ ​ of ​ ​ individual ​ ​ labor . ​ ​ Some ​ ​ of ​ ​ the ​ ​ most visible ​ ​ efforts ​ ​ have ​ ​ come ​ ​ from ​ ​ governments ​ ​ and ​ ​ private ​ ​ actors ​ ​ running ​ ​ small ​ ​ trials ​ ​ where people ​ ​ receive ​ ​ direct ​ ​ cash ​ ​ transfers ​ ​ in ​ ​ the ​ ​ form ​ ​ of ​ ​ a ​ ​ basic ​ ​ income ​ ​ stipend . ​ ​ It ​ ​ bears ​ ​ noting that ​ ​ payments ​ ​ made ​ ​ as ​ ​ a ​ ​ part ​ ​ of ​ ​ these ​ ​ experiments ​ ​ can not ​ ​ be ​ ​ considered ​ ​ “ universal ” insofar ​ ​ as ​ ​ they ​ ​ are ​ ​ provided ​ ​ to ​ ​ a ​ ​ limited ​ ​ number ​ ​ of ​ ​ people . ​ ​ Thus , ​ ​ while ​ ​ these ​ ​ experiments can ​ ​ gather ​ ​ informative ​ ​ data ​ ​ that ​ ​ tells ​ ​ us ​ ​ about ​ ​ individual ​ ​ reactions ​ ​ to ​ ​ the ​ ​ receipt ​ ​ of ​ ​ such funds , ​ ​ they ​ ​ can not ​ ​ account ​ ​ for ​ ​ the ​ ​ society-wide ​ ​ impact ​ ​ of ​ ​ a ​ ​ universal ​ ​ payment . ​ ​ For example , ​ ​ in ​ ​ April ​ ​ of ​ ​ 2017 , ​ ​ the ​ ​ government ​ ​ of ​ ​ Ontario ​ ​ began ​ ​ a ​ ​ UBI ​ ​ pilot ​ ​ research ​ ​ program with ​ ​ 4,000 ​ ​ participants ​ ​ that ​ ​ will ​ ​ provide ​ ​ up ​ ​ to ​ ​ C $ 16,989 ​ ​ per ​ ​ year ​ ​ for ​ ​ a ​ ​ single ​ ​ person ​ ​ and C $ 24,027 ​ ​ per ​ ​ year ​ ​ for ​ ​ a ​ ​ couple , ​ ​ less ​ ​ 50 ​ ​ percent ​ ​ of ​ ​ any ​ ​ earned ​ ​ income . ​ ​ Y ​ ​ Combinator , ​ ​ a 34 Silicon ​ ​ Valley-based ​ ​ startup ​ ​ incubator , ​ ​ began ​ ​ a ​ ​ one ​ ​ year ​ ​ UBI ​ ​ pilot ​ ​ study ​ ​ in ​ ​ Oakland ​ ​ in ​ ​ which one ​ ​ hundred ​ ​ families ​ ​ will ​ ​ receive ​ ​ $ 1,000 ​ ​ to ​ ​ $ 2,000 ​ ​ per ​ ​ month ​ ​ over ​ ​ the ​ ​ course ​ ​ of ​ ​ a ​ ​ year . ​ ​ Y 35 Combinator ​ ​ president ​ ​ ( and ​ ​ OpenAI ​ ​ co-chairman ) ​ ​ Sam ​ ​ Altman ​ ​ explicitly ​ ​ references ​ ​ job displacement ​ ​ due ​ ​ to ​ ​ technology ​ ​ as ​ ​ a ​ ​ motivating ​ ​ factor ​ ​ for ​ ​ UBI ​ ​ research . ​ ​ ​ While ​ ​ UBI 36 remains ​ ​ a ​ ​ politically ​ ​ contentious ​ ​ idea ​ ​ with ​ ​ significant ​ ​ variations ​ ​ in ​ ​ approach ​ ​ and implementation , ​ ​ it ​ ​ is ​ ​ currently ​ ​ one ​ ​ of ​ ​ the ​ ​ most ​ ​ commonly ​ ​ proposed ​ ​ policy ​ ​ responses ​ ​ to AI-driven ​ ​ job ​ ​ losses , ​ ​ and ​ ​ as ​ ​ such ​ ​ deserves ​ ​ close ​ ​ assessment . Bias ​ ​ and ​ ​ Inclusion The ​ ​ word ​ ​ “ bias ” ​ ​ has ​ ​ multiple ​ ​ meanings ​ ​ that ​ ​ intersect ​ ​ with ​ ​ AI ​ ​ applications ​ ​ in ​ ​ ways ​ ​ that ​ ​ can overlap ​ ​ and ​ ​ occasionally ​ ​ contradict ​ ​ each ​ ​ other . ​ ​ This ​ ​ can ​ ​ add ​ ​ unnecessary ​ ​ confusion ​ ​ to what ​ ​ is ​ ​ a ​ ​ critically ​ ​ needed ​ ​ domain ​ ​ of ​ ​ research . ​ ​ In ​ ​ statistics—used ​ ​ in ​ ​ many ​ ​ machine ​ ​ learning 33 ​ ​​ ​​ ​​ Michael ​ ​ Luca , ​ ​ Jon ​ ​ Kleinberg , ​ ​ and ​ ​ Sendhil ​ ​ Mullainathan , ​ ​ “ Algorithms ​ ​ Need ​ ​ Manage rs , ​ ​ Too , ” ​ ​​ Harvard ​ ​ Business ​ ​ Review ​ , January ​ ​ 1 , ​ ​ 2016 , ​ ​ https : //hbr.org/2016/01/algorithms-need-managers-too . 34 Ministry ​ ​ of ​ ​ Community ​ ​ and ​ ​ Social ​ ​ Services , ​ ​ “ Ontario ’ s ​ ​ Basic ​ ​ Income ​ ​ Pilot , ” ​ ​​ News.ontario.ca ​ , ​ ​ April ​ ​ 24 , ​ ​ 2017 , https : //news.ontario.ca/mcss/en/2017/04/ontarios-basic-income-pilot.html ​ . 35 Michael ​ ​ J . ​ ​ Coren , ​ ​ “ Y ​ ​ Combinator ​ ​ Is ​ ​ Running ​ ​ a ​ ​ Basic ​ ​ Income ​ ​ Experiment ​ ​ with ​ ​ 100 ​ ​ Oakland ​ ​ Families , ” ​ ​​ Quartz ​ , ​ ​ June ​ ​ 1 , ​ ​ 2017 , https : //qz.com/696377/y-combinator-is-running-a-basic-income-experiment-with-100-oakland-families/ ​ . 36 Sam ​ ​ Altman , ​ ​ “ Moving ​ ​ Forward ​ ​ on ​ ​ Basic ​ ​ Income , ” ​ ​​ Y ​ ​ Combinator ​ , ​ ​ May ​ ​ 31 , ​ ​ 2016 , https : //blog.ycombinator.com/moving-forward-on-basic-income/ ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 14 applications— “ bias ” ​ ​ has ​ ​ a ​ ​ specific ​ ​ meaning ​ ​ that ​ ​ differs ​ ​ from ​ ​ the ​ ​ popular ​ ​ and ​ ​ social scientific ​ ​ definitions . ​ ​ For ​ ​ example , ​ ​ the ​ ​ idea ​ ​ of ​ ​ “ selection ​ ​ bias ” ​ ​ refers ​ ​ to ​ ​ errors ​ ​ in ​ ​ estimation that ​ ​ result ​ ​ when ​ ​ some ​ ​ members ​ ​ of ​ ​ a ​ ​ population ​ ​ are ​ ​ more ​ ​ likely ​ ​ to ​ ​ be ​ ​ sampled ​ ​ than ​ ​ others . So ​ ​ when ​ ​ a ​ ​ machine ​ ​ learning ​ ​ program ​ ​ trained ​ ​ to ​ ​ recognize , ​ ​ say , ​ ​ faces ​ ​ of ​ ​ a ​ ​ particular ​ ​ racial group ​ ​ is ​ ​ applied ​ ​ to ​ ​ larger ​ ​ or ​ ​ more ​ ​ diverse ​ ​ populations , ​ ​ it ​ ​ may ​ ​ produce ​ ​ biased ​ ​ results ​ ​ in ​ ​ the sense ​ ​ of ​ ​ having ​ ​ a ​ ​ lower ​ ​ measure ​ ​ of ​ ​ accuracy . The ​ ​ word ​ ​ “ bias ” ​ ​ also ​ ​ has ​ ​ normative ​ ​ meanings ​ ​ in ​ ​ both ​ ​ colloquial ​ ​ and ​ ​ legal ​ ​ language , ​ ​ where it ​ ​ refers ​ ​ to ​ ​ judgement ​ ​ based ​ ​ on ​ ​ preconceived ​ ​ notions ​ ​ or ​ ​ prejudices , ​ ​ as ​ ​ opposed ​ ​ to ​ ​​ ​ the impartial ​ ​ evaluation ​ ​ of ​ ​ facts . ​ ​ Impartiality ​ ​ is ​ ​ a ​ ​ core ​ ​ value ​ ​ of ​ ​ many ​ ​ legal ​ ​ systems ​ ​ and ​ ​ governs many ​ ​ legal ​ ​ processes , ​ ​ from ​ ​ juror ​ ​ selection ​ ​ to ​ ​ the ​ ​ limitations ​ ​ placed ​ ​ on ​ ​ judges . ​ ​ For ​ ​ example , in ​ ​ the ​ ​ United ​ ​ States ​ ​ the ​ ​ Sixth ​ ​ Amendment ​ ​ to ​ ​ the ​ ​ Constitution ​ ​ mandates ​ ​ a ​ ​ right ​ ​ to ​ ​ an impartial ​ ​ jury ​ ​ and ​ ​ the ​ ​ Fourteenth ​ ​ mandates ​ ​ equal ​ ​ protection ​ ​ under ​ ​ the ​ ​ law . ​ ​ This ​ ​ sense ​ ​ of the ​ ​ word ​ ​ bias ​ ​ is ​ ​ closely ​ ​ linked ​ ​ to ​ ​ normative ​ ​ and ​ ​ ethical ​ ​ perspectives ​ ​ on ​ ​ fairness , ​ ​ and ​ ​ the idea ​ ​ that ​ ​ different ​ ​ groups ​ ​ should ​ ​ be ​ ​ treated ​ ​ equally . When ​ ​ examining ​ ​ technical ​ ​ systems , ​ ​ there ​ ​ can ​ ​ be ​ ​ a ​ ​ temptation ​ ​ to , ​ ​ or ​ ​ vested ​ ​ interest ​ ​ in , limiting ​ ​ discussion ​ ​ of ​ ​ bias ​ ​ to ​ ​ the ​ ​ first ​ ​ more ​ ​ ‘ neutral ’ ​ ​ statistical ​ ​ sense ​ ​ of ​ ​ the ​ ​ term . ​ ​ However , in ​ ​ practice ​ ​ there ​ ​ is ​ ​ rarely ​ ​ a ​ ​ clear ​ ​ demarcation ​ ​ between ​ ​ the ​ ​ statistical ​ ​ and ​ ​ the ​ ​ normative definitions : ​ ​ biased ​ ​ models ​ ​ or ​ ​ learning ​ ​ algorithms , ​ ​ as ​ ​ defined ​ ​ statistically , ​ ​ can ​ ​ lead ​ ​ to unequal ​ ​ and ​ ​ unfair ​ ​ treatments ​ ​ and ​ ​ outcomes ​ ​ for ​ ​ different ​ ​ social ​ ​ or ​ ​ racial ​ ​ groups . The ​ ​ danger ​ ​ of ​ ​ bias ​ ​ increases ​ ​ when ​ ​ these ​ ​ systems ​ ​ are ​ ​ applied , ​ ​ often ​ ​ in ​ ​ non-transparent ways , ​ ​ to ​ ​ critical ​ ​ institutions ​ ​ like ​ ​ criminal ​ ​ justice ​ ​ and ​ ​ healthcare . ​ ​ The ​ ​ social ​ ​ sciences ​ ​ and critical ​ ​ humanities ​ ​ have ​ ​ decades ​ ​ of ​ ​ research ​ ​ on ​ ​ bias ​ ​ within ​ ​ social ​ ​ systems ​ ​ that ​ ​ have ​ ​ much to ​ ​ offer ​ ​ the ​ ​ current ​ ​ debate ​ ​ on ​ ​ bias ​ ​ in ​ ​ AI ​ ​ and ​ ​ algorithmic ​ ​ systems . ​ ​ Since ​ ​​ AI ​ ​ Now ​ ​ ​ is ​ ​ deeply 37 interested ​ ​ in ​ ​ the ​ ​ social ​ ​ and ​ ​ political ​ ​ implications ​ ​ of ​ ​ AI , ​ ​ this ​ ​ report ​ ​ will ​ ​ use ​ ​ the ​ ​ word ​ ​ “ bias ” in ​ ​ its ​ ​ broader , ​ ​ normative ​ ​ sense ​ ​ in ​ ​ the ​ ​ following ​ ​ section , ​ ​ while ​ ​ acknowledging ​ ​ its ​ ​ close relationship ​ ​ with ​ ​ statistical ​ ​ usages . While ​ ​ the ​ ​ potential ​ ​ impact ​ ​ of ​ ​ such ​ ​ biases ​ ​ are ​ ​ extremely ​ ​ worrying , ​ ​ solutions ​ ​ are complicated . ​ ​ This ​ ​ is ​ ​ in ​ ​ part ​ ​ because ​ ​ biased ​ ​ AI ​ ​ can ​ ​ result ​ ​ from ​ ​ a ​ ​ number ​ ​ of ​ ​ factors , ​ ​ alone ​ ​ or in ​ ​ combination , ​ ​ such ​ ​ as ​ ​ who ​ ​ develops ​ ​ systems , ​ ​ what ​ ​ goals ​ ​ system ​ ​ developers ​ ​ have ​ ​ in ​ ​ mind during ​ ​ development , ​ ​ what ​ ​ training ​ ​ data ​ ​ they ​ ​ use , ​ ​ and ​ ​ whether ​ ​ the ​ ​ the ​ ​ systems ​ ​ work ​ ​ well for ​ ​ different ​ ​ parts ​ ​ of ​ ​ the ​ ​ population . ​ ​ This ​ ​ section ​ ​ addresses ​ ​ the ​ ​ latest ​ ​ research ​ ​ on ​ ​ bias ​ ​ in 38 AI ​ ​ and ​ ​ discusses ​ ​ some ​ ​ of ​ ​ the ​ ​ emerging ​ ​ strategies ​ ​ being ​ ​ used ​ ​ to ​ ​ address ​ ​ it . Where ​ ​ Bias ​ ​ Comes ​ ​ From AI ​ ​ systems ​ ​ are ​ ​ taught ​ ​ what ​ ​ they ​ ​ “ know ” ​ ​ from ​ ​ training ​ ​ data . ​ ​ Training ​ ​ data ​ ​ can ​ ​ be 37 ​ ​​ ​​ ​​ Barocas , ​ ​ Crawford , ​ ​ Shapiro ​ ​ and ​ ​ Wallach , ​ ​ “ The ​ ​ Problem ​ ​ with ​ ​ Bias : ​ ​ Allocative ​ ​ versus ​ ​ Representational ​ ​ Harms ​ ​ in ​ ​ Machine Learning , ” ​ ​ SIGCIS ​ ​ conference , ​ ​ October ​ ​ 2017 . 38 Solon ​ ​ Barocas ​ ​ and ​ ​ Andrew ​ ​ D. ​ ​ Selbst , ​ ​ “ Big ​ ​ Data ’ s ​ ​ Disparate ​ ​ Impact , ” ​ ​ California ​ ​ Law ​ ​ Review ​ ​ 104 , ​ ​ No . ​ ​ 3 ​ ​ ( June ​ ​ 1 , ​ ​ 2016 ) : ​ ​ 671 , doi:10.15779/Z38BG31 ; ​ ​ Sarah ​ ​ Bird , ​ ​ Solon ​ ​ Barocas , ​ ​ Kate ​ ​ Crawford , ​ ​ Fernando ​ ​ Diaz ​ ​ and ​ ​ Hanna ​ ​ Wallach , ​ ​ `` Exploring ​ ​ or Exploiting ? ​ ​ Social ​ ​ and ​ ​ Ethical ​ ​ Implications ​ ​ of ​ ​ Autonomous ​ ​ Experimentation ​ ​ in ​ ​ AI , '' ​ ​ ( 2016 ) . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 15 incomplete , ​ ​ biased ​ ​ or ​ ​ otherwise ​ ​ skewed , often ​ ​ drawing ​ ​ on ​ ​ limited ​ ​ and 39 40 non-representative ​ ​ samples ​ ​ that ​ ​ are ​ ​ poorly ​ ​ defined ​ ​ before ​ ​ use . ​ ​ Such ​ ​ problems ​ ​ with 41 training ​ ​ data ​ ​ may ​ ​ not ​ ​ be ​ ​ obvious , ​ ​ as ​ ​ datasets ​ ​ may ​ ​ be ​ ​ constructed ​ ​ in ​ ​ non-transparent ​ ​ ways . ​ ​ Additionally , ​ ​ given ​ ​ that ​ ​ humans ​ ​ must ​ ​ label ​ ​ much ​ ​ of ​ ​ the ​ ​ training ​ ​ data ​ ​ by ​ ​ hand , ​ ​ human 42 biases ​ ​ and ​ ​ cultural ​ ​ assumptions ​ ​ are ​ ​ transmitted ​ ​ by ​ ​ classification ​ ​ choices . ​ ​ Exclusion ​ ​ of 43 certain ​ ​ data ​ ​ can , ​ ​ in ​ ​ turn , ​ ​ mean ​ ​ exclusion ​ ​ of ​ ​ sub-populations ​ ​ from ​ ​ what ​ ​ AI ​ ​ is ​ ​ able ​ ​ to ​ ​ “ see ” and ​ ​ “ know. ” ​ ​ While ​ ​ pernicious , ​ ​ these ​ ​ biases ​ ​ are ​ ​ difficult ​ ​ to ​ ​ find ​ ​ and ​ ​ understand , ​ ​ especially 44 when ​ ​ systems ​ ​ are ​ ​ proprietary , ​ ​ treated ​ ​ as ​ ​ black ​ ​ boxes ​ ​ or ​ ​ taken ​ ​ at ​ ​ face ​ ​ value . ​ ​ Computer 45 scientists ​ ​ have ​ ​ noted ​ ​ that ​ ​ the ​ ​ complexity ​ ​ of ​ ​ machine ​ ​ learning ​ ​ systems ​ ​ not ​ ​ only ​ ​ must ​ ​ face difficulties ​ ​ in ​ ​ interpreting ​ ​ opaque , ​ ​ unsupervised ​ ​ models , ​ ​ but ​ ​ may ​ ​ also ​ ​ take ​ ​ on ​ ​ “ technical debt ” ​ ​ that ​ ​ makes ​ ​ maintenance ​ ​ and ​ ​ improvement ​ ​ costly—leading ​ ​ to ​ ​ situations ​ ​ where ​ ​ bias may ​ ​ be ​ ​ difficult ​ ​ to ​ ​ identify ​ ​​ and ​ ​​ mitigate . 46 Non-representative ​ ​ collection ​ ​ of ​ ​ data ​ ​ can ​ ​ also ​ ​ produce ​ ​ bias . ​ ​ Data ​ ​ is ​ ​ expensive , ​ ​ and ​ ​ data ​ ​ at scale ​ ​ is ​ ​ hard ​ ​ to ​ ​ come ​ ​ by . ​ ​ Thus , ​ ​ those ​ ​ who ​ ​ want ​ ​ to ​ ​ train ​ ​ an ​ ​ AI ​ ​ system ​ ​ are ​ ​ drawn ​ ​ to ​ ​ the ​ ​ use of ​ ​ easily ​ ​ available ​ ​ data , ​ ​ often ​ ​ crowd-sourced , ​ ​ scraped , ​ ​ or ​ ​ otherwise ​ ​ gathered ​ ​ from 47 existing ​ ​ user-facing ​ ​ apps ​ ​ and ​ ​ properties . ​ ​ This ​ ​ type ​ ​ of ​ ​ data ​ ​ can ​ ​ easily ​ ​ privilege socioeconomically ​ ​ advantaged ​ ​ populations , ​ ​ those ​ ​ with ​ ​ greater ​ ​ access ​ ​ to ​ ​ connected ​ ​ devices and ​ ​ online ​ ​ services . ​ ​ These ​ ​ same ​ ​ types ​ ​ of ​ ​ bias ​ ​ can ​ ​ also ​ ​ exist ​ ​ when ​ ​ data ​ ​ is ​ ​ collected ​ ​ from particular ​ ​ groups ​ ​ and ​ ​ not ​ ​ others . ​ ​​ A ​ ​ recent ​ ​ example ​ ​ comes ​ ​ from ​ ​ an ​ ​ experiment ​ ​ by ​ ​ OpenAI 48 in ​ ​ which ​ ​ a ​ ​ year ’ s ​ ​ worth ​ ​ of ​ ​ messages ​ ​ from ​ ​ the ​ ​ discussion ​ ​ forum ​ ​ Reddit ​ ​ were ​ ​ used ​ ​ as ​ ​ data ​ ​ to train ​ ​ an ​ ​ AI ​ ​ model ​ ​ to ​ ​ “ speak. ” ​ ​ Reddit ​ ​ is ​ ​ itself ​ ​ a ​ ​ skewed ​ ​ sub-population ​ ​ of ​ ​ internet ​ ​ users , 49 and ​ ​ this ​ ​ experiment ​ ​ can ​ ​ give ​ ​ us ​ ​ a ​ ​ sense ​ ​ of ​ ​ the ​ ​ types ​ ​ of ​ ​ bias ​ ​ that ​ ​ can ​ ​ occur ​ ​ when ​ ​ a ​ ​ small , 39 David ​ ​ J . ​ ​ Beymer , ​ ​ Karen ​ ​ W. ​ ​ Brannon , ​ ​ Ting ​ ​ Chen , ​ ​ Moritz ​ ​ AW ​ ​ Hardt , ​ ​ Ritwik ​ ​ K. ​ ​ Kumar ​ ​ and ​ ​ Tanveer ​ ​ F. ​ ​ Syeda-Mahmoo , ​ ​ `` Machine learning ​ ​ with ​ ​ incomplete ​ ​ data ​ ​ sets , '' ​ ​ U.S. ​ ​ Patent ​ ​ 9,349,105 , ​ ​ issued ​ ​ May ​ ​ 24 , ​ ​ 2016 . 40 Lisa ​ ​ Gitelman , ​ ​​ Raw ​ ​ data ​ ​ is ​ ​ an ​ ​ oxymoron , ​ ​ ​ ( MIT ​ ​ Press : ​ ​ 2013 ) . 41 Ishan ​ ​ Misra , ​ ​ C. ​ ​ Lawrence ​ ​ Zitnick , ​ ​ Margaret ​ ​ Mitchell ​ ​ and ​ ​ Ross ​ ​ Girshick , ​ ​ `` Seeing ​ ​ through ​ ​ the ​ ​ Human ​ ​ Reporting ​ ​ Bias : ​ ​ Visual Classifiers ​ ​ from ​ ​ Noisy ​ ​ Human-Centric ​ ​ Labels , '' ​ ​ ( In ​ ​ Proceedings ​ ​ of ​ ​ the ​ ​ IEEE ​ ​ Conference ​ ​ on ​ ​ Computer ​ ​ Vision ​ ​ and ​ ​ Pattern Recognition , ​ ​ pp . ​ ​ 2930-2939 , ​ ​ 2016 ) . 42 Josh ​ ​ Attenberg , ​ ​ Prem ​ ​ Melville , ​ ​ Foster ​ ​ Provost ​ ​ and ​ ​ Maytal ​ ​ Saar-Tsechansky , ​ ​ `` Selective ​ ​ data ​ ​ acquisition ​ ​ for ​ ​ machine learning , '' ​ ​ In ​ ​​ Cost-sensitive ​ ​ machine ​ ​ learning ​ . ​ ​ ( CRC ​ ​ Press : ​ ​ 2011 ) , ​ ​ pp . ​ ​ 101-155 ; ​ ​ Christian ​ ​ Beyer , ​ ​ Georg ​ ​ Krempl ​ ​ and ​ ​ Vincent Lemaire , ​ ​ `` How ​ ​ to ​ ​ select ​ ​ information ​ ​ that ​ ​ matters : ​ ​ a ​ ​ comparative ​ ​ study ​ ​ on ​ ​ active ​ ​ learning ​ ​ strategies ​ ​ for ​ ​ classification , '' ​ ​ In Proceedings ​ ​ of ​ ​ the ​ ​ 15th ​ ​ International ​ ​ Conference ​ ​ on ​ ​ Knowledge ​ ​ Technologies ​ ​ and ​ ​ Data-driven ​ ​ Business ​ , ​ ​ p. ​ ​ 2 . ​ ​ ACM , ​ ​ 2015 . 43 Moritz ​ ​ Hardt , ​ ​ Nimrod ​ ​ Megiddo , ​ ​ Christos ​ ​ Papadimitriou ​ ​ and ​ ​ Mary ​ ​ Wootters , ​ ​ `` Strategic ​ ​ classification . '' ​ ​ ( In ​ ​ Proceedings ​ ​ of ​ ​ the 2016 ​ ​ ACM ​ ​ Conference ​ ​ on ​ ​ Innovations ​ ​ in ​ ​ Theoretical ​ ​ Computer ​ ​ Science , ​ ​ pp . ​ ​ 111-122 , ​ ​ 2016 ) . 44 Matthew ​ ​ Zook , ​ ​ Solon ​ ​ Barocas , ​ ​ Kate ​ ​ Crawford , ​ ​ Emily ​ ​ Keller , ​ ​ Seeta ​ ​ Peña ​ ​ Gangadharan , ​ ​ Alyssa ​ ​ Goodman , ​ ​ Rachelle ​ ​ Hollander , Barbara ​ ​ A . ​ ​ Koenig , ​ ​ Jacob ​ ​ Metcalf , ​ ​ Arvind ​ ​ Narayanan , ​ ​ Alondra ​ ​ Nelson ​ ​ and ​ ​ Frank ​ ​ Pasquale , ​ ​ `` Ten ​ ​ simple ​ ​ rules ​ ​ for ​ ​ responsible big ​ ​ data ​ ​ research , '' ​ ​ PLoS ​ ​ Computational ​ ​ Biology ​ ​ 13 , ​ ​ No . ​ ​ 3 ​ ​ ( 2017 ) : ​ ​ e1005399 . 45 Frank ​ ​ Pasquale , ​ ​​ The ​ ​ black ​ ​ box ​ ​ society : ​ ​ The ​ ​ secret ​ ​ algorithms ​ ​ that ​ ​ control ​ ​ money ​ ​ and ​ ​ information ​ , ​ ​ ( Harvard ​ ​ University ​ ​ Press , 2015 ) . 46 ​ ​​ ​​ ​​ ​ D. ​ ​ Sculley ​ ​ et ​ ​ al. , ​ ​ “ Machine ​ ​ Learning : ​ ​ The ​ ​ High ​ ​ Interest ​ ​ Credit ​ ​ Card ​ ​ of ​ ​ Technical ​ ​ Debt , ” ​ ​ SE4ML : ​ ​ Software ​ ​ Engineering ​ ​ for Machine ​ ​ Learning ​ ​ ( NIPS ​ ​ 2014 ​ ​ Workshop ) , ​ ​ 2014 , ​ ​ https : //research.google.com/pubs/pub43146.html . 47 Amanda ​ ​ Levendowski , ​ ​ “ How ​ ​ copyright ​ ​ law ​ ​ creates ​ ​ biased ​ ​ artificial ​ ​ intelligence , ” http : //www.werobot2017.com/wp-content/uploads/2017/03/Levendowski-How-Copyright-Law-Creates-Biased-Artificial-In telligence-Abstract-and-Introduction-1.pdf ​ . 48 Josh ​ ​ Terrell , ​ ​ Andrew ​ ​ Kofink , ​ ​ Justin ​ ​ Middleton , ​ ​ Clarissa ​ ​ Rainear , ​ ​ Emerson ​ ​ Murphy-Hill , ​ ​ Chris ​ ​ Parnin ​ ​ and ​ ​ Jon ​ ​ Stallings , ​ ​​ Gender differences ​ ​ and ​ ​ bias ​ ​ in ​ ​ open ​ ​ source : ​ ​ Pull ​ ​ request ​ ​ acceptance ​ ​ of ​ ​ women ​ ​ versus ​ ​ men ​ , ​ ​ No . ​ ​ e1733v2 . ​ ​ PeerJ ​ ​ Preprints , ​ ​ 2016 . 49 Ananya ​ ​ Bhattacharya , ​ ​ “ Elon ​ ​ Musk ’ s ​ ​ OpenAI ​ ​ is ​ ​ Using ​ ​ Reddit ​ ​ to ​ ​ Teach ​ ​ AI ​ ​ to ​ ​ Speak ​ ​ Like ​ ​ Humans , ” ​ ​​ Quartz ​ , ​ ​ October ​ ​ 12 , ​ ​ 2016 , https : //qz.com/806321/open-ai-reddit-human-conversation ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 16 nonrepresentative ​ ​ group ​ ​ is ​ ​ used ​ ​ as ​ ​ a ​ ​ stand-in ​ ​ for ​ ​ the ​ ​ whole . Problems ​ ​ may ​ ​ also ​ ​ result ​ ​ from ​ ​ the ​ ​ disconnect ​ ​ between ​ ​ the ​ ​ context ​ ​ in ​ ​ which ​ ​ an ​ ​ AI ​ ​ system ​ ​ is used ​ ​ and ​ ​ the ​ ​ assumptions ​ ​ built ​ ​ into ​ ​ the ​ ​ AI ​ ​ system ​ ​ when ​ ​ it ​ ​ was ​ ​ designed . ​ ​ A ​ ​ group ​ ​ of researchers ​ ​ recently ​ ​ assessed ​ ​ how ​ ​ AI-based ​ ​ mapping ​ ​ apps ​ ​ often ​ ​ provide ​ ​ indirect ​ ​ routes ​ ​ to some ​ ​ users ​ ​ as ​ ​ a ​ ​ way ​ ​ to ​ ​ accomplish ​ ​ traffic ​ ​ load-balancing . ​ ​ The ​ ​ system ​ ​ will ​ ​ not ​ ​ be ​ ​ able ​ ​ to ​ ​ tell when ​ ​ the ​ ​ person ​ ​ asking ​ ​ for ​ ​ directions ​ ​ is ​ ​ driving ​ ​ to ​ ​ the ​ ​ hospital ​ ​ in ​ ​ an ​ ​ emergency . ​ ​ Such decontextualized ​ ​ assumptions ​ ​ can ​ ​ put ​ ​ non-consenting ​ ​ and ​ ​ unaware ​ ​ populations ​ ​ at ​ ​ risk while ​ ​ providing ​ ​ little ​ ​ opportunity ​ ​ for ​ ​ direct ​ ​ input . 50 While ​ ​ widely ​ ​ acknowledged ​ ​ as ​ ​ a ​ ​ problem , ​ ​ bias ​ ​ within ​ ​ and ​ ​ beyond ​ ​ AI ​ ​ is ​ ​ difficult ​ ​ to ​ ​ measure . Unintended ​ ​ consequences ​ ​ and ​ ​ inequalities ​ ​ are ​ ​ by ​ ​ nature ​ ​ collective , ​ ​ relative ​ ​ and 51 contextual , ​ ​ making ​ ​ measurement ​ ​ and ​ ​ baseline ​ ​ comparisons ​ ​ difficult . ​ ​ Information ​ ​ biases 52 in ​ ​ particular ​ ​ are ​ ​ difficult ​ ​ to ​ ​ measure , ​ ​ given ​ ​ the ​ ​ many ​ ​ possible ​ ​ reference ​ ​ points ​ ​ in ​ ​ context : content , ​ ​ users , ​ ​ ranking ​ ​ and ​ ​ access . ​ ​ There ​ ​ is ​ ​ potential ​ ​ for ​ ​ both ​ ​ over- ​ ​ and ​ ​ under-counting 53 biases ​ ​ in ​ ​ measurement ​ ​ of ​ ​ distributions ​ ​ given ​ ​ the ​ ​ limits ​ ​ on ​ ​ observable ​ ​ circumstances ​ ​ for individuals , ​ ​ problems ​ ​ with ​ ​ population ​ ​ gaps ​ ​ and ​ ​ possible ​ ​ measurement ​ ​ errors . 54 Given ​ ​ the ​ ​ difficulty ​ ​ ( and ​ ​ sometimes ​ ​ even ​ ​ technical ​ ​ impossibility ) ​ ​ of ​ ​ understanding ​ ​ exactly how ​ ​ AI ​ ​ systems ​ ​ have ​ ​ reached ​ ​ a ​ ​ given ​ ​ decision , ​ ​ bias ​ ​ is ​ ​ often ​ ​ only ​ ​ revealed ​ ​ by 55 demonstrating ​ ​ an ​ ​ inequality ​ ​ in ​ ​ outcomes , ​ ​ post-hoc . ​ ​ Examples ​ ​ of ​ ​ this ​ ​ are ​ ​ familiar ​ ​ from recent ​ ​ news ​ ​ stories . ​ ​ Julia ​ ​ Angwin ’ s ​ ​ ProPublica ​ ​ piece ​ ​ on ​ ​ Northpointe ’ s ​ ​ racially-biased COMPAS ​ ​ system , ​ ​ used ​ ​ to ​ ​ make ​ ​ sentencing ​ ​ decisions ​ ​ in ​ ​ courts ​ ​ across ​ ​ the ​ ​ United ​ ​ States , ​ ​ is an ​ ​ exemplar ​ ​ of ​ ​ the ​ ​ genre . ​ ​ Similarly , ​ ​ Bloomberg ​ ​ found ​ ​ that ​ ​ Amazon ’ s ​ ​ same-day ​ ​ delivery 56 service ​ ​ was ​ ​ bypassing ​ ​ ZIP ​ ​ codes ​ ​ that ​ ​ are ​ ​ predominantly ​ ​ black . ​ ​ This ​ ​ decision ​ ​ may ​ ​ have ​ ​ been made ​ ​ for ​ ​ many ​ ​ reasons , ​ ​ but ​ ​ its ​ ​ result ​ ​ was ​ ​ racial ​ ​ bias . 57 The ​ ​ AI ​ ​ Field ​ ​ is ​ ​ Not ​ ​ Diverse Bias ​ ​ can ​ ​ also ​ ​ emerge ​ ​ in ​ ​ AI ​ ​ systems ​ ​ because ​ ​ of ​ ​ the ​ ​ very ​ ​ narrow ​ ​ subset ​ ​ of ​ ​ the ​ ​ population that ​ ​ design ​ ​ them . ​ ​ AI ​ ​ developers ​ ​ are ​ ​ mostly ​ ​ male , ​ ​ generally ​ ​ highly ​ ​ paid , ​ ​ and ​ ​ similarly 50 Sarah ​ ​ Bird , ​ ​ Solon ​ ​ Barocas , ​ ​ Kate ​ ​ Crawford , ​ ​ Fernando ​ ​ Diaz ​ ​ and ​ ​ Hanna ​ ​ Wallach , ​ ​ `` Exploring ​ ​ or ​ ​ Exploiting ? ​ ​ Social ​ ​ and ​ ​ Ethical Implications ​ ​ of ​ ​ Autonomous ​ ​ Experimentation ​ ​ in ​ ​ AI , '' ​ ​​ Workshop ​ ​ on ​ ​ Fairness , ​ ​ Accountability , ​ ​ and ​ ​ Transparency ​ ​ in ​ ​ Machine Learning ​ , ​ ​​ https : //papers.ssrn.com/sol3/papers.cfm ? abstract_id=2846909 ​ ​ ​ ( 2016 ) . 51 Marco ​ ​ J ​ ​ Haenssgen ​ ​ and ​ ​ Proochista ​ ​ Ariana , ​ ​ `` The ​ ​ Social ​ ​ Implications ​ ​ of ​ ​ Technology ​ ​ Diffusion : ​ ​ Uncovering ​ ​ the ​ ​ Unintended Consequences ​ ​ of ​ ​ People ’ s ​ ​ Health-Related ​ ​ Mobile ​ ​ Phone ​ ​ Use ​ ​ in ​ ​ Rural ​ ​ India ​ ​ and ​ ​ China , '' ​ ​​ World ​ ​ Development ​ ​ ​ 94 ​ ​ ( 2017 ) : 286-304 . 52 Frank ​ ​ Cowell , ​ ​​ Measuring ​ ​ inequality ​ , ​ ​ ( Oxford ​ ​ University ​ ​ Press , ​ ​ 2011 ) . 53 Evaggelia ​ ​ Pitoura , ​ ​ Panayiotis ​ ​ Tsaparas , ​ ​ Giorgos ​ ​ Flouris , ​ ​ Irini ​ ​ Fundulaki , ​ ​ Panagiotis ​ ​ Papadakos , ​ ​ Serge ​ ​ Abiteboul ​ ​ and ​ ​ Gerhard Weikum , ​ ​ `` On ​ ​ Measuring ​ ​ Bias ​ ​ in ​ ​ Online ​ ​ Information , '' ​ ​​ arXiv ​ ​ preprint ​ ​ arXiv:1704.05730 ​ ​ ​ ( 2017 ) . 54 Ashton ​ ​ Anderson , ​ ​ Jon ​ ​ Kleinberg ​ ​ and ​ ​ Sendhil ​ ​ Mullainathan , ​ ​ `` Assessing ​ ​ Human ​ ​ Error ​ ​ Against ​ ​ a ​ ​ Benchmark ​ ​ of ​ ​ Perfection , '' arXiv ​ ​ preprint ​ ​ arXiv:1606.04956 ​ ​ ( 2016 ) . 55 Jenna ​ ​ Burrell , ​ ​ `` How ​ ​ the ​ ​ machine ​ ​ ‘ thinks ’ : ​ ​ Understanding ​ ​ opacity ​ ​ in ​ ​ machine ​ ​ learning ​ ​ algorithms , '' ​ ​​ Big ​ ​ Data ​ ​ & ​ ​ Society ​ ​ ​ 3 , ​ ​ No . 1 ​ ​ ( 2016 ) : ​ ​ DOI : ​ ​​ https : //doi.org/10.1177/2053951715622512 ​ . 56 Angwin , ​ ​ Larson ​ ​ and ​ ​ Kirchner , ​ ​ “ Machine ​ ​ Bias : ​ ​ There ’ s ​ ​ Software ​ ​ Used ​ ​ Across ​ ​ the ​ ​ Country ​ ​ to ​ ​ Predict ​ ​ Future ​ ​ Criminals . ​ ​ And ​ ​ It ’ s Biased ​ ​ Against ​ ​ Blacks , ” ​ ​​ ProPublica ​ , ​ ​ May ​ ​ 23 , ​ ​ 2016 https : //www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing 57 David ​ ​ Ingold ​ ​ and ​ ​ Spencer ​ ​ Soper , ​ ​ “ Amazon ​ ​ Doesn ’ t ​ ​ Consider ​ ​ the ​ ​ Race ​ ​ of ​ ​ Its ​ ​ Customers . ​ ​ Should ​ ​ It ? , ” ​ ​ ​​ Bloomberg , ​ ​ April ​ ​ 21 , 2016 , ​ ​​ https : //www.bloomberg.com/graphics/2016-amazon-same-day/ ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 17 technically ​ ​ educated . ​ ​ Their ​ ​ interests , ​ ​ needs , ​ ​ and ​ ​ life ​ ​ experiences ​ ​ will ​ ​ necessarily ​ ​ be reflected ​ ​ in ​ ​ the ​ ​ AI ​ ​ they ​ ​ create . ​ ​ Bias , ​ ​ whether ​ ​ conscious ​ ​ or ​ ​ unconscious , ​ ​ reflects ​ ​ problems 58 of ​ ​ inclusion ​ ​ and ​ ​ representation . ​ ​ The ​ ​ lack ​ ​ of ​ ​ women ​ ​ and ​ ​ minorities ​ ​ in ​ ​ tech ​ ​ fields , ​ ​ and artificial ​ ​ intelligence ​ ​ in ​ ​ particular , ​ ​ is ​ ​ well ​ ​ known . ​ ​ But ​ ​ this ​ ​ was ​ ​ not ​ ​ always ​ ​ the ​ ​ case . ​ ​ Early 59 programming ​ ​ and ​ ​ data ​ ​ entry ​ ​ work ​ ​ was ​ ​ characterized ​ ​ as ​ ​ secretarial , ​ ​ and ​ ​ was female-dominated . ​ ​ These ​ ​ women ​ ​ were ​ ​ themselves ​ ​ called ​ ​ “ computers , ” ​ ​ and ​ ​ they ​ ​ were often ​ ​ undercompensated ​ ​ and ​ ​ rarely ​ ​ credited . ​ ​ ​ ​ ​ All ​ ​ the ​ ​ while , ​ ​ they ​ ​ were ​ ​ responsible ​ ​ for 60 things ​ ​ like ​ ​ maintaining ​ ​ sophisticated ​ ​ systems ​ ​ that ​ ​ targeted ​ ​ bomb ​ ​ strikes ​ ​ in ​ ​ World ​ ​ War ​ ​ II 61 and ​ ​ tabulating ​ ​ decades ​ ​ of ​ ​ census ​ ​ data . 62 The ​ ​ history ​ ​ of ​ ​ AI ​ ​ reflects ​ ​ this ​ ​ pattern ​ ​ of ​ ​ gender ​ ​ exclusion . ​ ​ The ​ ​ 1956 ​ ​ Dartmouth ​ ​ Summer Research ​ ​ Project ​ ​ on ​ ​ Artificial ​ ​ Intelligence , ​ ​ which ​ ​ initiated ​ ​ the ​ ​ concept ​ ​ of ​ ​ artificial intelligence , ​ ​ was ​ ​ exclusively ​ ​ attended ​ ​ by ​ ​ men . ​ ​ Pioneering ​ ​ work ​ ​ in ​ ​ natural ​ ​ language 63 processing ​ ​ and ​ ​ computational ​ ​ linguistics , ​ ​ key ​ ​ to ​ ​ contemporary ​ ​ AI ​ ​ systems , ​ ​ has ​ ​ been credited ​ ​ to ​ ​ male ​ ​ colleagues ​ ​ and ​ ​ students ​ ​ rather ​ ​ than ​ ​ to ​ ​ Margaret ​ ​ Masterman , ​ ​ who founded ​ ​ the ​ ​ Cambridge ​ ​ Language ​ ​ Research ​ ​ Unit ​ ​ and ​ ​ was ​ ​ one ​ ​ of ​ ​ the ​ ​ leaders ​ ​ in ​ ​ the ​ ​ field . 64 Intentional ​ ​ exclusion ​ ​ and ​ ​ unintentional ​ ​ “ like-me ” ​ ​ bias ​ ​ is ​ ​ responsible ​ ​ for ​ ​ a ​ ​ continued ​ ​ lack ​ ​ of demographic ​ ​ representation ​ ​ within ​ ​ the ​ ​ AI ​ ​ field ​ ​ and ​ ​ within ​ ​ the ​ ​ tech ​ ​ industry ​ ​ for ​ ​ women , Hispanics , ​ ​ and ​ ​ African ​ ​ Americans . 65 Gender ​ ​ and ​ ​ racial ​ ​ disparities ​ ​ among ​ ​ developer ​ ​ cohorts ​ ​ in ​ ​ tech ​ ​ companies ​ ​ are ​ ​ even ​ ​ more skewed ​ ​ than ​ ​ the ​ ​ demographics ​ ​ of ​ ​ students ​ ​ or ​ ​ academics . ​ ​ In ​ ​ the ​ ​ United ​ ​ States , ​ ​ women make ​ ​ up ​ ​ about ​ ​ 18 ​ ​ percent ​ ​ of ​ ​ computer ​ ​ science ​ ​ ( CS ) ​ ​ graduates , ​ ​ yet ​ ​ only ​ ​ 11 ​ ​ percent ​ ​ of computer ​ ​ engineers ​ ​ are ​ ​ female . ​ ​ African ​ ​ Americans ​ ​ and ​ ​ Hispanics ​ ​ represent ​ ​ only ​ ​ 11 ​ ​ percent of ​ ​ total ​ ​ technology ​ ​ sector ​ ​ employees ​ ​ although ​ ​ they ​ ​ comprise ​ ​ 27 ​ ​ percent ​ ​ of ​ ​ the ​ ​ overall population . ​ ​ Representation ​ ​ in ​ ​ the ​ ​ U.S. ​ ​ context ​ ​ has ​ ​ wide ​ ​ reaching ​ ​ implications , ​ ​ given ​ ​ that 66 33 ​ ​ percent ​ ​ of ​ ​ knowledge ​ ​ and ​ ​ technology ​ ​ intensive ​ ​ ( KTI ) ​ ​ jobs ​ ​ worldwide ​ ​ are ​ ​ U.S. ​ ​ based ​ ​ and those ​ ​ firms ​ ​ contribute ​ ​ 29 ​ ​ percent ​ ​ of ​ ​ global ​ ​ GDP , ​ ​ of ​ ​ which ​ ​ 39 ​ ​ percent ​ ​ are ​ ​ U.S. ​ ​ based . 67 Efforts ​ ​ to ​ ​ address ​ ​ gender ​ ​ biases ​ ​ in ​ ​ Google ​ ​ Ad ​ ​ Settings , ​ ​ revealed ​ ​ in ​ ​ 2015 , ​ ​ have ​ ​ failed ​ ​ to 68 58 Cathy ​ ​ O ’ Neil , ​ ​​ Weapons ​ ​ of ​ ​ math ​ ​ destruction : ​ ​ How ​ ​ big ​ ​ data ​ ​ increases ​ ​ inequality ​ ​ and ​ ​ threatens ​ ​ democracy ​ , ​ ​ ( New ​ ​ York : ​ ​ Crown Publishing ​ ​ Group , ​ ​ 2016 ) . 59 ​ ​​ ​​ ​​ Kate ​ ​ Crawford , ​ ​ “ Artificial ​ ​ Intelligence ’ s ​ ​ White ​ ​ Guy ​ ​ Problem , ” ​ ​​ The ​ ​ New ​ ​ York ​ ​ Times ​ , ​ ​ June ​ ​ 25 , ​ ​ 2016 . 60 Ellen ​ ​ Van ​ ​ Oost , ​ ​ `` Making ​ ​ the ​ ​ Computer ​ ​ Masculine , '' ​ ​ In ​ ​​ Women , ​ ​ Work ​ ​ and ​ ​ Computerization ​ ​​ ( 2000 ) , ​ ​ pp . ​ ​ 9-16 . 61 Nathan ​ ​ Ensmenger , ​ ​ `` Making ​ ​ programming ​ ​ masculine , '' ​ ​ In ​ ​​ Gender ​ ​ codes : ​ ​ Why ​ ​ women ​ ​ are ​ ​ leaving ​ ​ computing ​ ​ ​ ( 2010 ) : ​ ​ 115-42 . 62 Margaret ​ ​ Ann ​ ​ Boden , ​ ​​ Mind ​ ​ as ​ ​ machine : ​ ​ A ​ ​ history ​ ​ of ​ ​ cognitive ​ ​ science ​ , ​ ​ ( Clarendon ​ ​ Press , ​ ​ 2006 ) . 63 Ronald ​ ​ Kline , ​ ​ `` Cybernetics , ​ ​ automata ​ ​ studies , ​ ​ and ​ ​ the ​ ​ Dartmouth ​ ​ conference ​ ​ on ​ ​ artificial ​ ​ intelligence , '' ​ ​​ IEEE ​ ​ Annals ​ ​ of ​ ​ the History ​ ​ of ​ ​ Computing ​ ​ ​ 33 , ​ ​ No . ​ ​ 4 ​ ​ ( 2011 ) : ​ ​ 5-16 . 64 Margaret ​ ​ Masterman , ​ ​ `` 1 ​ ​ Personal ​ ​ background , '' ​ ​​ Early ​ ​ Years ​ ​ in ​ ​ Machine ​ ​ Translation : ​ ​ Memoirs ​ ​ and ​ ​ Biographies ​ ​ of ​ ​ Pioneers 97 ​ ​ ( 2000 ) : ​ ​ 279 ; ​ ​ William ​ ​ Williams ​ ​ and ​ ​ Frank ​ ​ Knowles , ​ ​ `` Margaret ​ ​ Masterman : ​ ​ In ​ ​ memoriam , '' ​ ​​ Computers ​ ​ and ​ ​ translation ​ ​ ​ 2 , No . ​ ​ 4 ​ ​ ( 1987 ) : ​ ​ 197-203 . 65 Google , ​ ​ Inc. ​ ​ and ​ ​ Gallup , ​ ​ Inc. , ​ ​ “ Diversity ​ ​ Gaps ​ ​ in ​ ​ Computer ​ ​ Science : ​ ​ Exploring ​ ​ the ​ ​ Underrepresentation ​ ​ of ​ ​ Girls , ​ ​ Blacks , ​ ​ and Hispanics , ” ​ ​ Retrieved ​ ​ from ​ ​ http : //goo.gl/PG34aH . ​ ​ Additional ​ ​ reports ​ ​ from ​ ​ Google ’ s ​ ​ Computer ​ ​ Science ​ ​ Education ​ ​ Research are ​ ​ available ​ ​ at ​ ​​ https : //edu.google.com/resources/computerscience/research ​ . 66 National ​ ​ Science ​ ​ Foundation , ​ ​ “ Science ​ ​ and ​ ​ Engineering ​ ​ Indicators , ” ​ ​ 2016 , ​ ​ Chapter ​ ​ 2 , https : //nsf.gov/statistics/2016/nsb20161/ # /report/chapter-2 ​ . 67 National ​ ​ Science ​ ​ Foundation , ​ ​ “ Science ​ ​ and ​ ​ Engineering ​ ​ Indicators , ” ​ ​ 2016 , ​ ​ Chapter ​ ​ 6 , https : //nsf.gov/statistics/2016/nsb20161/ # /report/chapter-6 ​ . 68 Amit ​ ​ Datta , ​ ​ Michael ​ ​ Carl ​ ​ Tschantz ​ ​ and ​ ​ Anupam ​ ​ Datta , ​ ​ `` Automated ​ ​ experiments ​ ​ on ​ ​ ad ​ ​ privacy ​ ​ settings , '' ​ ​​ Proceedings ​ ​ on AI ​ ​ Now ​ ​ 2017 ​ ​ Report 18 stop ​ ​ inequality ​ ​ in ​ ​ presentation ​ ​ of ​ ​ STEM ​ ​ job ​ ​ ads , ​ ​ even ​ ​ when ​ ​ language ​ ​ in ​ ​ ads ​ ​ are ​ ​ controlled for ​ ​ gender-neutral ​ ​ language . 69 AI ​ ​ is ​ ​ not ​ ​ impartial ​ ​ or ​ ​ neutral . ​ ​ Technologies ​ ​ are ​ ​ as ​ ​ much ​ ​ products ​ ​ of ​ ​ the ​ ​ context ​ ​ in ​ ​ which they ​ ​ are ​ ​ created ​ ​ as ​ ​ they ​ ​ are ​ ​ potential ​ ​ agents ​ ​ for ​ ​ change . ​ ​ Machine ​ ​ predictions ​ ​ and 70 performance ​ ​ are ​ ​ constrained ​ ​ by ​ ​ human ​ ​ decisions ​ ​ and ​ ​ values , ​ ​ and ​ ​ those ​ ​ who ​ ​ design , 71 develop , ​ ​ and ​ ​ maintain ​ ​ AI ​ ​ systems ​ ​ will ​ ​ shape ​ ​ such ​ ​ systems ​ ​ within ​ ​ their ​ ​ own ​ ​ understanding of ​ ​ the ​ ​ world . ​ ​​ Many ​ ​ of ​ ​ the ​ ​ biases ​ ​ embedded ​ ​ in ​ ​ AI ​ ​ systems ​ ​ are ​ ​ products ​ ​ of ​ ​ a ​ ​ complex 72 history ​ ​ with ​ ​ respect ​ ​ to ​ ​ diversity ​ ​ and ​ ​ equality . Recent ​ ​ Developments ​ ​ in ​ ​ Bias ​ ​ Research In ​ ​ the ​ ​ year ​ ​ since ​ ​ the ​ ​​ AI ​ ​ Now ​ ​ 2016 ​ ​ Symposium , ​ ​​ there ​ ​ ​​ has ​ ​ been ​ ​ a ​ ​ bumper ​ ​ crop ​ ​ of ​ ​ new research ​ ​ on ​ ​ bias ​ ​ in ​ ​ machine ​ ​ learning . ​ ​ One ​ ​ promising ​ ​ development ​ ​ is ​ ​ that ​ ​ many ​ ​ of ​ ​ these studies ​ ​ have ​ ​ reflexively ​ ​ used ​ ​ AI ​ ​ techniques ​ ​ to ​ ​ understand ​ ​ the ​ ​ the ​ ​ ways ​ ​ by ​ ​ which ​ ​ AI ​ ​ systems introduce ​ ​ or ​ ​ perpetuate ​ ​ unequal ​ ​ treatment . New ​ ​ research ​ ​ on ​ ​ word ​ ​ embeddings ​ ​ has ​ ​ shown ​ ​ the ​ ​ ways ​ ​ in ​ ​ which ​ ​ language , ​ ​ as ​ ​ it ​ ​ is ​ ​ used within ​ ​ our ​ ​ complex ​ ​ and ​ ​ often ​ ​ biased ​ ​ social ​ ​ contexts , ​ ​ reflects ​ ​ bias . ​ ​​ ​​ Word ​ ​ embeddings ​ ​ are 73 set ​ ​ of ​ ​ natural ​ ​ language ​ ​ processing ​ ​ techniques ​ ​ that ​ ​ map ​ ​ the ​ ​ semantic ​ ​ relationship ​ ​ between words , ​ ​ creating ​ ​ a ​ ​ model ​ ​ that ​ ​ predicts ​ ​ which ​ ​ words ​ ​ are ​ ​ likely ​ ​ to ​ ​ be ​ ​ associated ​ ​ with ​ ​ which . Researchers ​ ​ looking ​ ​ at ​ ​ word ​ ​ embeddings ​ ​ showed ​ ​ that ​ ​ predictable ​ ​ gendered ​ ​ associations between ​ ​ words , ​ ​ such ​ ​ as ​ ​ “ female ” ​ ​ and ​ ​ “ queen ” ​ ​ are ​ ​ reflected ​ ​ in ​ ​ the ​ ​ models , ​ ​ as ​ ​ are stereotypes , ​ ​ such ​ ​ as ​ ​ “ female ” ​ ​ and ​ ​ “ receptionist , ” ​ ​ while ​ ​ “ man ” ​ ​ and ​ ​ typically ​ ​ masculine names ​ ​ are ​ ​ associated ​ ​ with ​ ​ programming , ​ ​ engineering ​ ​ and ​ ​ other ​ ​ STEM ​ ​ professions . 74 Such ​ ​ biases ​ ​ have ​ ​ daily , ​ ​ real-world ​ ​ impacts . ​ ​ Recent ​ ​ analysis ​ ​ of ​ ​ search ​ ​ results ​ ​ and advertisements ​ ​ similarly ​ ​ reveals ​ ​ persistent ​ ​ gendered , ​ ​ racial ​ ​ and ​ ​ cultural ​ ​ biases . 75 Privacy ​ ​ Enhancing ​ ​ Technologies ​ ​ ​ 2015 , ​ ​ No . ​ ​ 1 ​ ​ ( 2015 ) : ​ ​ 92-112 . 69 Anja ​ ​ Lambrecht ​ ​ and ​ ​ Catherine ​ ​ E. ​ ​ Tucker , ​ ​ “ Algorithmic ​ ​ Bias ? ​ ​ An ​ ​ Empirical ​ ​ Study ​ ​ into ​ ​ Apparent ​ ​ Gender-Based ​ ​ Discrimination in ​ ​ the ​ ​ Display ​ ​ of ​ ​ STEM ​ ​ Career ​ ​ Ads , ” ​ ​ October ​ ​ 13 , ​ ​ 2016 . ​ ​ Available ​ ​ at ​ ​ SSRN : ​ ​​ https : //ssrn.com/abstract=2852260 ​ ​ ​ or http : //dx.doi.org/10.2139/ssrn.2852260 ​ . 70 Zdenek ​ ​ Smutny , ​ ​ `` Social ​ ​ informatics ​ ​ as ​ ​ a ​ ​ concept : ​ ​ Widening ​ ​ the ​ ​ discourse , '' ​ ​​ Journal ​ ​ of ​ ​ Information ​ ​ Science ​ ​ ​ 42 , ​ ​ No . ​ ​ 5 ​ ​ ( 2016 ) : 681-710 . 71 Kenneth ​ ​ A . ​ ​ Bamberger ​ ​ and ​ ​ Deirdre ​ ​ Mulligan , ​ ​ `` Public ​ ​ Values , ​ ​ Private ​ ​ Infrastructure ​ ​ and ​ ​ the ​ ​ Internet ​ ​ of ​ ​ Things : ​ ​ the ​ ​ Case ​ ​ of Automobile , '' ​ ​​ Journal ​ ​ of ​ ​ Law ​ ​ & ​ ​ Economic ​ ​ Regulation ​ ​ ​ 9 ​ ​ ( 2016 ) : ​ ​ 7-44 ; ​ ​ Jon ​ ​ Kleinberg , ​ ​ Himabindu ​ ​ Lakkaraju , ​ ​ Jure ​ ​ Leskovec , ​ ​ Jens Ludwig ​ ​ and ​ ​ Sendhil ​ ​ Mullainathan , ​ ​ “ Human ​ ​ decisions ​ ​ and ​ ​ machine ​ ​ predictions , ” ​ ​ No . ​ ​ w23180 . ​ ​ National ​ ​ Bureau ​ ​ of ​ ​ Economic Research , ​ ​ 2017 . 72 Brent ​ ​ Daniel ​ ​ Mittelstadt , ​ ​ Patrick ​ ​ Allo , ​ ​ Mariarosaria ​ ​ Taddeo , ​ ​ Sandra ​ ​ Wachter ​ ​ and ​ ​ Luciano ​ ​ Floridi , ​ ​ `` The ​ ​ ethics ​ ​ of ​ ​ algorithms : Mapping ​ ​ the ​ ​ debate , '' ​ ​​ Big ​ ​ Data ​ ​ & ​ ​ Society ​ ​ ​ 3 , ​ ​ No . ​ ​ 2 ​ ​ ( 2016 ) : ​ ​ 2053951716679679 . 73 Aylin ​ ​ Caliskan , ​ ​ Joanna ​ ​ J . ​ ​ Bryson ​ ​ and ​ ​ Arvind ​ ​ Narayanan , ​ ​ `` Semantics ​ ​ derived ​ ​ automatically ​ ​ from ​ ​ language ​ ​ corpora ​ ​ contain human-like ​ ​ biases , '' ​ ​​ Science ​ ​ ​ 356 , ​ ​ No . ​ ​ 6334 ​ ​ ( 2017 ) : ​ ​ 183-186 ; ​ ​ Anthony ​ ​ G. ​ ​ Greenwald , ​ ​ `` An ​ ​ AI ​ ​ stereotype ​ ​ catcher , '' ​ ​​ Science ​ ​ ​ 356 , No . ​ ​ 6334 ​ ​ ( 2017 ) : ​ ​ 133-134 . 74 Tolga ​ ​ Bolukbasi , ​ ​ Kai-Wei ​ ​ Chang , ​ ​ James ​ ​ Zou , ​ ​ Venkatesh ​ ​ Saligrama ​ ​ and ​ ​ Adam ​ ​ Kalai , ​ ​ `` Quantifying ​ ​ and ​ ​ reducing ​ ​ stereotypes ​ ​ in word ​ ​ embeddings , '' ​ ​​ arXiv ​ ​ preprint ​ ​ arXiv:1606.06121 ​ ​ ​ ( 2016 ) ; ​ ​ Tolga ​ ​ Bolukbasi , ​ ​ Kai-Wei ​ ​ Chang , ​ ​ James ​ ​ Y . ​ ​ Zou , ​ ​ Venkatesh Saligrama ​ ​ and ​ ​ Adam ​ ​ T. ​ ​ Kalai , ​ ​ `` Man ​ ​ is ​ ​ to ​ ​ computer ​ ​ programmer ​ ​ as ​ ​ woman ​ ​ is ​ ​ to ​ ​ homemaker ? ​ ​ Debiasing ​ ​ word ​ ​ embeddings , '' In ​ ​​ Advances ​ ​ in ​ ​ Neural ​ ​ Information ​ ​ Processing ​ ​ Systems ​ , ​ ​ pp . ​ ​ 4349-4357 , ​ ​ 2016 . 75 Datta , ​ ​ Tschantz , ​ ​ and ​ ​ Datta , ​ ​ 2015 ; ​ ​ Tarleton ​ ​ Gillespie , ​ ​ “ Algorithmically ​ ​ recognizable : ​ ​ Santorum ’ s ​ ​ Google ​ ​ problem , ​ ​ and Google ’ s ​ ​ Santorum ​ ​ problem , ” ​ ​​ Information , ​ ​ Communication ​ ​ & ​ ​ Society ​ ​ ​ 20 , ​ ​ No . ​ ​ 1 ​ ​ ( 2017 ) : ​ ​ 63-80 ; ​ ​ Safiya ​ ​ Umoja ​ ​ Noble , “ Algorithms ​ ​ of ​ ​ Oppression : ​ ​ How ​ ​ Search ​ ​ Engines ​ ​ Enforce ​ ​ Racism , ” ​ ​ ( NYU ​ ​ Press , ​ ​ forthcoming ​ ​ 2018 ) . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 19 New ​ ​ work ​ ​ has ​ ​ also ​ ​ highlighted ​ ​ the ​ ​ way ​ ​ in ​ ​ which ​ ​ AI ​ ​ poses ​ ​ risks ​ ​ of ​ ​ significant ​ ​ bias-driven impacts ​ ​ in ​ ​ the ​ ​ educational ​ ​ context , ​ ​ where ​ ​ K-12 ​ ​ educators ​ ​ subject ​ ​ children ​ ​ to ​ ​ treatment , discipline ​ ​ and ​ ​ tracking ​ ​ decisions ​ ​ based ​ ​ on ​ ​ AI-determined ​ ​ characterizations ​ ​ of ​ ​ their ​ ​ abilities and ​ ​ behaviors . ​ ​ Analysis ​ ​ of ​ ​ large ​ ​ data ​ ​ sets ​ ​ reflecting ​ ​ STEM ​ ​ education ​ ​ in ​ ​ K-12 ​ ​ classrooms 76 reveals ​ ​ racial ​ ​ disparities ​ ​ in ​ ​ disciplinary ​ ​ actions ​ ​ and ​ ​ recommendations ​ ​ for ​ ​ advanced coursework . ​ ​ These ​ ​ data , ​ ​ along ​ ​ with ​ ​ the ​ ​ biases ​ ​ they ​ ​ reflect , ​ ​ are ​ ​ very ​ ​ likely ​ ​ to ​ ​ be ​ ​ used ​ ​ to train ​ ​ these ​ ​ educational ​ ​ AI ​ ​ systems , ​ ​ which ​ ​ would ​ ​ then ​ ​​ reproduce ​ ​ and ​ ​ further ​ ​ normalize these ​ ​ biases . In ​ ​ a ​ ​ study ​ ​ that ​ ​ examined ​ ​ the ​ ​ potential ​ ​ for ​ ​ bias , ​ ​ the ​ ​ Human ​ ​ Rights ​ ​ Data ​ ​ Analysis ​ ​ Group demonstrated ​ ​ how ​ ​ commonly ​ ​ used ​ ​ predictive ​ ​ policing ​ ​ system ​ ​ PredPol , ​ ​ were ​ ​ it ​ ​ used ​ ​ in Oakland , ​ ​ CA , ​ ​ would ​ ​ reinforce ​ ​ racially-biased ​ ​ police ​ ​ practices ​ ​ by ​ ​ recommending ​ ​ increased police ​ ​ deployment ​ ​ in ​ ​ neighborhoods ​ ​ of ​ ​ color . ​ ​ Decades ​ ​ of ​ ​ policing ​ ​ research ​ ​ has ​ ​ shown 77 that ​ ​ foot ​ ​ patrols ​ ​ and ​ ​ community ​ ​ rapport ​ ​ decrease ​ ​ policing ​ ​ biases , ​ ​ while ​ ​ studies ​ ​ of ​ ​ “ driving while ​ ​ black ” ​ ​ and ​ ​ “ hot ​ ​ spots ” ​ ​ illustrate ​ ​ biases ​ ​ in ​ ​ routine ​ ​ strategies . ​ ​ New ​ ​ technologies 78 appear ​ ​ to ​ ​ prevent ​ ​ the ​ ​ former ​ ​ and ​ ​ amplify ​ ​ the ​ ​ latter , ​ ​ reproducing ​ ​ the ​ ​ most ​ ​ extreme ​ ​ racial stereotyping . 79 Legal ​ ​ scholarship ​ ​ has ​ ​ also ​ ​ explored ​ ​ the ​ ​ applications ​ ​ of ​ ​ machine ​ ​ testimony ​ ​ at ​ ​ criminal ​ ​ trials , ​ ​ among ​ ​ many ​ ​ possible ​ ​ instances ​ ​ identified ​ ​ in ​ ​ which ​ ​ these ​ ​ skewed ​ ​ systems ​ ​ and ​ ​ biased 80 data ​ ​ could ​ ​​ ​ negatively ​ ​ impact ​ ​ human ​ ​ lives ​ ​ due ​ ​ to ​ ​ reproducing ​ ​ stereotypes , ​ ​ with ​ ​ the ​ ​ added challenge ​ ​ that ​ ​ the ​ ​ systems ​ ​ are ​ ​ poorly ​ ​ understood ​ ​ and ​ ​ proprietary . When ​ ​ bias ​ ​ is ​ ​ embedded ​ ​ in ​ ​ AI ​ ​ health ​ ​ applications , ​ ​ it ​ ​ can ​ ​ have ​ ​ an ​ ​ incredibly ​ ​ high ​ ​ cost . Worryingly , ​ ​ data ​ ​ sets ​ ​ used ​ ​ to ​ ​ train ​ ​ health-related ​ ​ AI ​ ​ often ​ ​ rely ​ ​ on ​ ​ clinical ​ ​ trial ​ ​ data , ​ ​ which are ​ ​ historically ​ ​ skewed ​ ​ toward ​ ​ white ​ ​ men , ​ ​ even ​ ​ when ​ ​ the ​ ​ health ​ ​ conditions ​ ​ studied primarily ​ ​ affect ​ ​ people ​ ​ of ​ ​ color ​ ​ or ​ ​ women . ​ ​ Even ​ ​ without ​ ​ AI ​ ​ amplifying ​ ​ such ​ ​ biases , ​ ​ African 81 Americans ​ ​ with ​ ​ sickle ​ ​ cell ​ ​ anemia ​ ​ are ​ ​ overdiagnosed ​ ​ and ​ ​ unnecessarily ​ ​ treated ​ ​ for ​ ​ diabetes based ​ ​ on ​ ​ insights ​ ​ from ​ ​ studies ​ ​ that ​ ​ excluded ​ ​ them . ​ ​ The ​ ​ prevalence ​ ​ of ​ ​ biases ​ ​ when 82 combined ​ ​ with ​ ​ opacity ​ ​ and ​ ​ inscrutability ​ ​ leads ​ ​ to ​ ​ a ​ ​ lack ​ ​ of ​ ​ trust ​ ​ in ​ ​ AI ​ ​ currently ​ ​ being 76 Benjamin ​ ​ Herold , ​ ​ “ Algorithmic ​ ​ Bias ​ ​ as ​ ​ a ​ ​ Rising ​ ​ Concern ​ ​ for ​ ​ Ed-Tech ​ ​ Field , ​ ​ RAND ​ ​ Researchers ​ ​ Say , ” ​ ​ Education ​ ​ Week , ​ ​ April ​ ​ 11 , 2017 , ​ ​​ http : //blogs.edweek.org/edweek/DigitalEducation/2017/04/algorithmic_bias_edtech_RAND.html ​ . 77 Kristian ​ ​ Lum ​ ​ and ​ ​ William ​ ​ Isaac , ​ ​ `` To ​ ​ predict ​ ​ and ​ ​ serve ? , '' ​ ​​ Significance ​ ​ ​ 13 , ​ ​ No . ​ ​ 5 ​ ​ ( 2016 ) : ​ ​ 14-19 . 78 Prashan ​ ​ Ranasinghe , ​ ​ `` Rethinking ​ ​ the ​ ​ Place ​ ​ of ​ ​ Crime ​ ​ in ​ ​ Police ​ ​ Patrol : ​ ​ A ​ ​ Re-Reading ​ ​ of ​ ​ Classic ​ ​ Police ​ ​ Ethnographies , '' ​ ​​ British Journal ​ ​ of ​ ​ Criminology ​ ​ ​ ( 2016 ) : ​ ​ azw028 ; ​ ​ Patricia ​ ​ Warren , ​ ​ Donald ​ ​ Tomaskovic-Devey , ​ ​ William ​ ​ Smith , ​ ​ Matthew ​ ​ Zingraff ​ ​ and Marcinda ​ ​ Mason , ​ ​ `` Driving ​ ​ while ​ ​ black : ​ ​ Bias ​ ​ processes ​ ​ and ​ ​ racial ​ ​ disparity ​ ​ in ​ ​ police ​ ​ stops , '' ​ ​​ Criminology ​ ​ ​ 44 , ​ ​ No . ​ ​ 3 ​ ​ ( 2006 ) : 709-738 ; ​ ​ David ​ ​ Weisburd , ​ ​ `` Does ​ ​ Hot ​ ​ Spots ​ ​ Policing ​ ​ Inevitably ​ ​ Lead ​ ​ to ​ ​ Unfair ​ ​ and ​ ​ Abusive ​ ​ Police ​ ​ Practices , ​ ​ or ​ ​ Can ​ ​ We Maximize ​ ​ Both ​ ​ Fairness ​ ​ and ​ ​ Effectiveness ​ ​ in ​ ​ the ​ ​ New ​ ​ Proactive ​ ​ Policing , '' ​ ​​ University ​ ​ of ​ ​ Chicago ​ ​ Legal ​ ​ Forum ​ ​ ​ ( 2016 ) : 661-689 . 79 Andrew ​ ​ Guthrie ​ ​ Ferguson , ​ ​​ The ​ ​ Rise ​ ​ of ​ ​ Big ​ ​ Data ​ ​ Policing : ​ ​ Surveillance , ​ ​ Race , ​ ​ and ​ ​ the ​ ​ Future ​ ​ of ​ ​ Law ​ ​ Enforcement ​ , ​ ​ ( NYU ​ ​ Press , forthcoming ​ ​ 2017 ) . 80 Andrea ​ ​ Roth , ​ ​ “ Machine ​ ​ Testimony , ” ​ ​ Yale ​ ​ Law ​ ​ Journal , ​ ​ forthcoming ​ ​ 2017 . 81 Anita ​ ​ Kurt , ​ ​ Lauren ​ ​ Semler , ​ ​ Jeanne ​ ​ L. ​ ​ Jacoby , ​ ​ Melanie ​ ​ B . ​ ​ Johnson , ​ ​ Beth ​ ​ A . ​ ​ Careyva , ​ ​ Brian ​ ​ Stello , ​ ​ Timothy ​ ​ Friel , ​ ​ Mark ​ ​ C. Knouse , ​ ​ Hope ​ ​ Kincaid ​ ​ and ​ ​ John ​ ​ C. ​ ​ Smulian , ​ ​ `` Racial ​ ​ Differences ​ ​ Among ​ ​ Factors ​ ​ Associated ​ ​ with ​ ​ Participation ​ ​ in ​ ​ Clinical Research ​ ​ Trials , '' ​ ​​ Journal ​ ​ of ​ ​ Racial ​ ​ and ​ ​ Ethnic ​ ​ Health ​ ​ Disparities ​ ​ ​ ( 2016 ) : ​ ​ 1-10 . 82 Mary ​ ​ E ​ ​ Lacy , ​ ​ Gregory ​ ​ A . ​ ​ Wellenius , ​ ​ Anne ​ ​ E. ​ ​ Sumner , ​ ​ Adolfo ​ ​ Correa , ​ ​ Mercedes ​ ​ R. ​ ​ Carnethon , ​ ​ Robert ​ ​ I . ​ ​ Liem , ​ ​ James ​ ​ G. Wilson , ​ ​ David ​ ​ B . ​ ​ Saks , ​ ​ David ​ ​ R. ​ ​ Jacobs ​ ​ Jr. , ​ ​ April ​ ​ Carson , ​ ​ Xi ​ ​ Luo , ​ ​ Annie ​ ​ Gjelsvik , ​ ​ Alexander ​ ​ P. ​ ​ Reiner , ​ ​ Rhaki ​ ​ Naik , ​ ​ Simin ​ ​ Liu , Solomon ​ ​ K. ​ ​ Musani , ​ ​ Charles ​ ​ B . ​ ​ Eaton ​ ​ and ​ ​ Wen-Chih ​ ​ Wu , ​ ​ `` Association ​ ​ of ​ ​ sickle ​ ​ cell ​ ​ trait ​ ​ with ​ ​ hemoglobin ​ ​ A1c ​ ​ in ​ ​ African Americans , '' ​ ​​ Jama ​ ​ ​ 317 , ​ ​ No . ​ ​ 5 ​ ​ ( 2017 ) : ​ ​ 507-515 . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 20 developed ​ ​ for ​ ​ neuroscience ​ ​ and ​ ​ mental ​ ​ health ​ ​ applications . ​ ​ The ​ ​ prospect ​ ​ of ​ ​ misdiagnosis 83 or ​ ​ improper ​ ​ treatment ​ ​ leading ​ ​ to ​ ​ patient ​ ​ death ​ ​ motivates ​ ​ some ​ ​ to ​ ​ avoid ​ ​ AI ​ ​ systems entirely ​ ​ in ​ ​ the ​ ​ health ​ ​ context . 84 Emerging ​ ​ Strategies ​ ​ to ​ ​ Address ​ ​ Bias There ​ ​ is ​ ​ an ​ ​ urgent ​ ​ need ​ ​ to ​ ​ expand ​ ​ cultural , ​ ​ disciplinary ​ ​ and ​ ​ ethnic ​ ​ diversity ​ ​ within ​ ​ the ​ ​ AI field ​ ​ in ​ ​ order ​ ​ to ​ ​ diminish ​ ​ groupthink , ​ ​ mitigate ​ ​ bias ​ ​ and ​ ​ broaden ​ ​ intellectual ​ ​ frames ​ ​ of reference ​ ​ beyond ​ ​ the ​ ​ purely ​ ​ technical . ​ ​ While ​ ​ some ​ ​ have ​ ​ suggested ​ ​ that ​ ​ AI ​ ​ systems ​ ​ can ​ ​ be used ​ ​ to ​ ​ address ​ ​ diversity ​ ​ problems ​ ​ at ​ ​ companies , ​ ​ if ​ ​ AI ​ ​ development ​ ​ is ​ ​ not ​ ​ inclusive , ​ ​ the 85 success ​ ​ of ​ ​ such ​ ​ a ​ ​ bootstrapped ​ ​ approach ​ ​ is ​ ​ doubtful . ​ ​ There ​ ​ have ​ ​ been ​ ​ positive developments ​ ​ prompting ​ ​ inclusion ​ ​ within ​ ​ the ​ ​ AI ​ ​ community , ​ ​ such ​ ​ as ​ ​ Fei-Fei ​ ​ Li ’ s ​ ​ SAILORS summer ​ ​ camp , ​ ​ a ​ ​ program ​ ​ that ​ ​ helps ​ ​ high ​ ​ school ​ ​ girls ​ ​ acquire ​ ​ comfort ​ ​ and ​ ​ experience ​ ​ with AI . ​ ​ Similarly , ​ ​ the ​ ​ Association ​ ​ of ​ ​ Computing ​ ​ Machinery ​ ​ ( ACM ) ​ ​ increasingly ​ ​ recognizes ​ ​ the 86 need ​ ​ to ​ ​ address ​ ​ algorithmic ​ ​ bias ​ ​ and ​ ​ emphasize ​ ​ diversity . ​ ​ Various ​ ​ conferences ​ ​ have ​ ​ also 87 sought ​ ​ to ​ ​ explore ​ ​ accountability ​ ​ and ​ ​ transparency ​ ​ issues ​ ​ surrounding ​ ​ AI ​ ​ and ​ ​ algorithmic systems ​ ​ as ​ ​ a ​ ​ way ​ ​ to ​ ​ better ​ ​ understand ​ ​ and ​ ​ evaluate ​ ​ biases . ​ ​ Among ​ ​ conferences , ​ ​ the 88 Fairness , ​ ​ Accountability , ​ ​ and ​ ​ Transparency ​ ​ in ​ ​ Machine ​ ​ Learning ​ ​ ( FAT/ML ​ ​ and ​ ​ now ​ ​ FAT * ​ ​ ) Conferences ​ ​ are ​ ​ notable ​ ​ for ​ ​ a ​ ​ focus ​ ​ on ​ ​ technical ​ ​ research ​ ​ and ​ ​ experimentation ​ ​ dedicated to ​ ​ making ​ ​ AI ​ ​ more ​ ​ inclusive , ​ ​ legible ​ ​ and ​ ​ representative . 89 While ​ ​ steps ​ ​ are ​ ​ being ​ ​ made ​ ​ to ​ ​ understand ​ ​ and ​ ​ combat ​ ​ bias ​ ​ in ​ ​ some ​ ​ sectors , ​ ​ bias ​ ​ can ​ ​ also be ​ ​ profitable . ​ ​ Insurance ​ ​ and ​ ​ financial ​ ​ lending ​ ​ have ​ ​ long ​ ​ discriminated ​ ​ for ​ ​ their ​ ​ financial advantage , ​ ​ choosing ​ ​ to ​ ​ serve ​ ​ the ​ ​ least ​ ​ risky ​ ​ and , ​ ​ sometimes , ​ ​ leaving ​ ​ the ​ ​ most ​ ​ vulnerable behind . ​ ​ AI ​ ​ systems ​ ​ are ​ ​ now ​ ​ being ​ ​ used ​ ​ to ​ ​ make ​ ​ credit ​ ​ and ​ ​ lending ​ ​ decisions . ​ ​ When 90 underwriting ​ ​ decisions ​ ​ are ​ ​ made ​ ​ by ​ ​ AI ​ ​ systems ​ ​ trained ​ ​ on ​ ​ data ​ ​ that ​ ​ reflects ​ ​ past ​ ​ biased practices ​ ​ and ​ ​ calibrated ​ ​ to ​ ​ detect ​ ​ nuanced ​ ​ signals ​ ​ of ​ ​ “ risk , ” ​ ​ creditors ​ ​ will ​ ​ be ​ ​ able ​ ​ to ​ ​ make more ​ ​ profitable ​ ​ loans ​ ​ while ​ ​ leaving ​ ​ those ​ ​ in ​ ​ precarious ​ ​ situations ​ ​ behind . ​ ​ Due ​ ​ to misaligned ​ ​ interests ​ ​ and ​ ​ the ​ ​ information ​ ​ asymmetry ​ ​ that ​ ​ AI ​ ​ exacerbates ​ ​ in ​ ​ these industries , ​ ​ new ​ ​ incentives ​ ​ for ​ ​ fairness ​ ​ and ​ ​ new ​ ​ methods ​ ​ for ​ ​ validating ​ ​ fair ​ ​ practices ​ ​ need 83 Andreas ​ ​ Holzinger , ​ ​ `` Interactive ​ ​ machine ​ ​ learning ​ ​ for ​ ​ health ​ ​ informatics : ​ ​ when ​ ​ do ​ ​ we ​ ​ need ​ ​ the ​ ​ human-in-the-loop ? , '' ​ ​​ Brain Informatics ​ ​ ​ 3 , ​ ​ No . ​ ​ 2 ​ ​ ( 2016 ) : ​ ​ 119-131 . 84 Rich ​ ​ Caruana , ​ ​ `` Intelligible ​ ​ Machine ​ ​ Learning ​ ​ for ​ ​ Critical ​ ​ Applications ​ ​ Such ​ ​ As ​ ​ Health ​ ​ Care , '' ​ ​​ 2017 ​ ​ AAAS ​ ​ Annual ​ ​ Meeting ( February ​ ​ 16-20 , ​ ​ 2017 ) ​ , ​ ​ AAAS , ​ ​ 2017 . 85 Ji-A ​ ​ Min , ​ ​ “ Ten ​ ​ ways ​ ​ HR ​ ​ tech ​ ​ leaders ​ ​ can ​ ​ make ​ ​ the ​ ​ most ​ ​ of ​ ​ artificial ​ ​ intelligence , ” ​ ​ Personnel ​ ​ Today , ​ ​ April ​ ​ 26 , ​ ​ 2017 , http : //www.personneltoday.com/hr/ten-ways-hr-tech-leaders-can-make-artificial-intelligence/ ​ . 86 Marie ​ ​ E ​ ​ Vachovsky , ​ ​ Grace ​ ​ Wu , ​ ​ Sorathan ​ ​ Chaturapruek , ​ ​ Olga ​ ​ Russakovsky , ​ ​ Richard ​ ​ Sommer ​ ​ and ​ ​ Li ​ ​ Fei-Fei , ​ ​ `` Toward ​ ​ More Gender ​ ​ Diversity ​ ​ in ​ ​ CS ​ ​ through ​ ​ an ​ ​ Artificial ​ ​ Intelligence ​ ​ Summer ​ ​ Program ​ ​ for ​ ​ High ​ ​ School ​ ​ Girls , '' ​ ​ ( In ​ ​​ Proceedings ​ ​ of ​ ​ the ​ ​ 47th ACM ​ ​ Technical ​ ​ Symposium ​ ​ on ​ ​ Computing ​ ​ Science ​ ​ Education ​ , ​ ​ pp . ​ ​ 303-308 ) , ​ ​ ACM , ​ ​ 2016 . 87 Kieth ​ ​ Kirkpatrick , ​ ​ `` Battling ​ ​ algorithmic ​ ​ bias : ​ ​ how ​ ​ do ​ ​ we ​ ​ ensure ​ ​ algorithms ​ ​ treat ​ ​ us ​ ​ fairly ? , '' ​ ​​ Communications ​ ​ of ​ ​ the ​ ​ ACM ​ ​ ​ 59 , No . ​ ​ 10 ​ ​ ( 2016 ) : ​ ​ 16-17 . 88 Algorithms ​ ​ and ​ ​ Explanations , ​ ​ Information ​ ​ Law ​ ​ Insitute , ​ ​ New ​ ​ York ​ ​ University , ​ ​ April ​ ​ 27-28 , ​ ​ 2017 , http : //www.law.nyu.edu/centers/ili/events/algorithms-and-explanations ​ . 89 3rd ​ ​ ​​ Workshop ​ ​ on ​ ​ Fairness , ​ ​ Accountability , ​ ​ and ​ ​ Transparency ​ ​ in ​ ​ Machine ​ ​ Learning , ​ ​ New ​ ​ York , ​ ​ November ​ ​ 18 , ​ ​ 2016 , http : //www.fatml.org/ ​ . 90 JM ​ ​ Schumacher , ​ ​ “ Linear ​ ​ Versus ​ ​ Nonlinear ​ ​ Allocation ​ ​ Rules ​ ​ in ​ ​ Risk ​ ​ Sharing ​ ​ Under ​ ​ Financial ​ ​ Fairness , ” ​ ​ ( March ​ ​ 2 , ​ ​ 2017 ) , http : //dx.doi.org/10.2139/ssrn.2892760 ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 21 to ​ ​ be ​ ​ developed . 91 Part ​ ​ of ​ ​ the ​ ​ fundamental ​ ​ difficulty ​ ​ in ​ ​ defining , ​ ​ understanding ​ ​ and ​ ​ measuring ​ ​ bias ​ ​ stems from ​ ​ the ​ ​ contentious ​ ​ and ​ ​ conceptually ​ ​ difficult ​ ​ task ​ ​ of ​ ​ defining ​ ​ fairness . ​ ​ Tradeoffs ​ ​ are inherent ​ ​ in ​ ​ the ​ ​ adoption ​ ​ of ​ ​ particular ​ ​ fairness ​ ​ definitions , ​ ​ possibly ​ ​ perpetuating ​ ​ particular biases ​ ​ in ​ ​ the ​ ​ service ​ ​ of ​ ​ addressing ​ ​ others . ​ ​ Recent ​ ​ efforts ​ ​ have ​ ​ sought ​ ​ to ​ ​ implement 92 fairness ​ ​ by ​ ​ mathematically ​ ​ specifying ​ ​ social ​ ​ norms ​ ​ and ​ ​ values , ​ ​ then ​ ​ using ​ ​ those specifications ​ ​ as ​ ​ constraints ​ ​ when ​ ​ training ​ ​ AI ​ ​ systems . ​ ​ While ​ ​ these ​ ​ are ​ ​ hopeful 93 developments , ​ ​ none ​ ​ of ​ ​ these ​ ​ methods ​ ​ cleanly ​ ​ solve ​ ​ the ​ ​ problem ​ ​ of ​ ​ bias . ​ ​ Understanding ​ ​ AI not ​ ​ as ​ ​ a ​ ​ purely ​ ​ technical ​ ​ implementation , ​ ​ but ​ ​ as ​ ​ a ​ ​ contextually-specific ​ ​ combination ​ ​ of norms , ​ ​ technical ​ ​ systems ​ ​ and ​ ​ strategic ​ ​ interests ​ ​ is ​ ​ an ​ ​ important ​ ​ step ​ ​ toward ​ ​ addressing bias ​ ​ in ​ ​ AI . ​ ​ There ​ ​ continues ​ ​ to ​ ​ be ​ ​ a ​ ​ deep ​ ​ need ​ ​ for ​ ​ interdisciplinary , ​ ​ socially ​ ​ aware ​ ​ work 94 that ​ ​ integrates ​ ​ the ​ ​ long ​ ​ history ​ ​ of ​ ​ bias ​ ​ research ​ ​ from ​ ​ the ​ ​ social ​ ​ sciences ​ ​ and ​ ​ humanities into ​ ​ the ​ ​ field ​ ​ of ​ ​ AI ​ ​ research . Rights ​ ​ and ​ ​ Liberties In ​ ​ the ​ ​ period ​ ​ since ​ ​ the ​ ​​ AI ​ ​ Now ​ ​ 2016 ​ ​ Symposium ​ , ​ ​ the ​ ​ global ​ ​ political ​ ​ landscape ​ ​ has ​ ​ shifted considerably . ​ ​ The ​ ​ election ​ ​ of ​ ​ Donald ​ ​ Trump ​ ​ is ​ ​ part ​ ​ of ​ ​ a ​ ​ larger ​ ​ wave ​ ​ of ​ ​ populist ​ ​ political movements ​ ​ across ​ ​ the ​ ​ globe , ​ ​ and ​ ​ shares ​ ​ with ​ ​ these ​ ​ a ​ ​ number ​ ​ of ​ ​ hallmark ​ ​ traits . ​ ​ In governing , ​ ​ populists ​ ​ seek ​ ​ to ​ ​ delegitimize ​ ​ political ​ ​ opposition—from ​ ​ opposition ​ ​ parties ​ ​ to institutions ​ ​ like ​ ​ the ​ ​ media ​ ​ and ​ ​ the ​ ​ judiciary—and ​ ​ to ​ ​ crack ​ ​ down ​ ​ on ​ ​ perceived ​ ​ threats ​ ​ to the ​ ​ imagined ​ ​ homogeneous ​ ​ people ​ ​ they ​ ​ claim ​ ​ to ​ ​ represent . ​ ​ While ​ ​ regional ​ ​ instantiations 95 vary , ​ ​ they ​ ​ share ​ ​ an ​ ​ opposition ​ ​ to ​ ​ existing ​ ​ political ​ ​ elites ​ ​ and ​ ​ a ​ ​ nationalist , ​ ​ anti-pluralist approach ​ ​ that ​ ​ claims ​ ​ a ​ ​ moral ​ ​ imperative ​ ​ to ​ ​ represent ​ ​ a ​ ​ silent ​ ​ majority . The ​ ​ election ​ ​ of ​ ​ Emmanuel ​ ​ Macron ​ ​ in ​ ​ France ​ ​ and ​ ​ the ​ ​ gains ​ ​ by ​ ​ Labour ​ ​ in ​ ​ the ​ ​ UK ​ ​ may ​ ​ indicate a ​ ​ coming ​ ​ backlash ​ ​ to ​ ​ the ​ ​ global ​ ​ populist ​ ​ wave , ​ ​ but ​ ​ given ​ ​ the ​ ​ strong ​ ​ showing ​ ​ from Germany ’ s ​ ​ far-right ​ ​ Alternative ​ ​ für ​ ​ Deutschland ​ ​ party ​ ​ in ​ ​ their ​ ​ 2017 ​ ​ elections , ​ ​ this ​ ​ is ​ ​ by ​ ​ no means ​ ​ certain . It ​ ​ remains ​ ​ necessary ​ ​ to ​ ​ ask ​ ​ how ​ ​ AI ​ ​ systems ​ ​ are ​ ​ likely ​ ​ to ​ ​ be ​ ​ deployed ​ ​ in ​ ​ governing , ​ ​ and ​ ​ how 91 Hamid ​ ​ R. ​ ​ Ekbia ​ ​ and ​ ​ Bonnie ​ ​ A . ​ ​ Nardi , ​ ​​ Heteromation , ​ ​ and ​ ​ Other ​ ​ Stories ​ ​ of ​ ​ Computing ​ ​ and ​ ​ Capitalism ​ , ​ ​ ( MIT ​ ​ Press , ​ ​ 2017 ) ; Sampath ​ ​ Kannan , ​ ​ Michael ​ ​ Kearns , ​ ​ Jamie ​ ​ Morgenstern , ​ ​ Mallesh ​ ​ Pai , ​ ​ Aaron ​ ​ Roth , ​ ​ Rakesh ​ ​ Vohra ​ ​ and ​ ​ Z . ​ ​ Steven ​ ​ Wu , ​ ​ `` Fairness Incentives ​ ​ for ​ ​ Myopic ​ ​ Agents , '' ​ ​ arXiv ​ ​ preprint ​ ​ arXiv:1705.02321 ​ ​ ( 2017 ) ; ​ ​ Julia ​ ​ Lane , ​ ​ `` Perspective : ​ ​ Fix ​ ​ the ​ ​ incentives , '' ​ ​​ Nature 537 , ​ ​ No . ​ ​ 7618 ​ ​ ( 2016 ) : ​ ​ S20-S20 . 92 Jon ​ ​ Kleinberg , ​ ​ Sendhil ​ ​ Mullainathan ​ ​ and ​ ​ Manish ​ ​ Raghavan , ​ ​ `` Inherent ​ ​ trade-offs ​ ​ in ​ ​ the ​ ​ fair ​ ​ determination ​ ​ of ​ ​ risk ​ ​ scores , '' arXiv ​ ​ preprint ​ ​ arXiv:1609.05807 ​ ​ ( 2016 ) . 93 Yiling ​ ​ Chen , ​ ​ Arpita ​ ​ Ghosh , ​ ​ Michael ​ ​ Kearns , ​ ​ Tim ​ ​ Roughgarden ​ ​ and ​ ​ Jennifer ​ ​ Wortman ​ ​ Vaughan , ​ ​ `` Mathematical ​ ​ foundations for ​ ​ social ​ ​ computing , '' ​ ​​ Communications ​ ​ of ​ ​ the ​ ​ ACM ​ ​ ​ 59 , ​ ​ No . ​ ​ 12 ​ ​ ( 2016 ) : ​ ​ 102-108 ; ​ ​ Shahin ​ ​ Jabbari , ​ ​ Matthew ​ ​ Joseph , ​ ​ Michael Kearns , ​ ​ Jamie ​ ​ Morgenstern ​ ​ and ​ ​ Aaron ​ ​ Roth , ​ ​ `` Fair ​ ​ Learning ​ ​ in ​ ​ Markovian ​ ​ Environments , '' ​ ​ arXiv ​ ​ preprint ​ ​ arXiv:1611.03071 ( 2016 ) ; ​ ​ Matthew ​ ​ Joseph , ​ ​ Michael ​ ​ Kearns , ​ ​ Jamie ​ ​ Morgenstern , ​ ​ Seth ​ ​ Neel ​ ​ and ​ ​ Aaron ​ ​ Roth , ​ ​ `` Rawlsian ​ ​ fairness ​ ​ for ​ ​ machine learning , '' ​ ​ arXiv ​ ​ preprint ​ ​ arXiv:1610.09559 ​ ​ ( 2016 ) . 94 Mike ​ ​ Ananny , ​ ​ `` Toward ​ ​ an ​ ​ ethics ​ ​ of ​ ​ algorithms : ​ ​ Convening , ​ ​ observation , ​ ​ probability , ​ ​ and ​ ​ timeliness , '' ​ ​​ Science , ​ ​ Technology , ​ ​ & Human ​ ​ Values ​ ​ ​ 41 , ​ ​ No . ​ ​ 1 ​ ​ ( 2016 ) : ​ ​ 93-117 . 95 Jan-Werner ​ ​ Müller , ​ ​​ What ​ ​ Is ​ ​ Populism ? ​ ​ ​ ( Philadelphia : ​ ​ University ​ ​ of ​ ​ Pennsylvania ​ ​ Press , ​ ​ 2016 ) . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 22 they ​ ​ might ​ ​ be ​ ​ used ​ ​ within ​ ​ populist ​ ​ and ​ ​ authoritarian ​ ​ contexts . ​ ​ What ​ ​ effects ​ ​ will ​ ​ these 96 systems ​ ​ have ​ ​ on ​ ​ vulnerable ​ ​ individuals ​ ​ and ​ ​ minorities ? ​ ​ How ​ ​ will ​ ​ AI ​ ​ systems ​ ​ be ​ ​ used ​ ​ by ​ ​ law enforcement ​ ​ or ​ ​ national ​ ​ security ​ ​ agencies ? ​ ​ How ​ ​ will ​ ​ AI ’ s ​ ​ use ​ ​ in ​ ​ the ​ ​ criminal ​ ​ justice ​ ​ system affect ​ ​ our ​ ​ understanding ​ ​ of ​ ​ due ​ ​ process ​ ​ and ​ ​ the ​ ​ principle ​ ​ of ​ ​ equal ​ ​ justice ​ ​ under ​ ​ the ​ ​ law ? How ​ ​ might ​ ​ complex ​ ​ AI ​ ​ systems ​ ​ centralize ​ ​ authority ​ ​ and ​ ​ power ? ​ ​ This ​ ​ section ​ ​ examines these ​ ​ questions , ​ ​ describes ​ ​ applications ​ ​ of ​ ​ AI ​ ​ that ​ ​ pose ​ ​ challenges ​ ​ to ​ ​ rights ​ ​ and ​ ​ liberties , and ​ ​ touches ​ ​ on ​ ​ the ​ ​ technical ​ ​ and ​ ​ normative ​ ​ frameworks ​ ​ we ​ ​ might ​ ​ construct ​ ​ to ​ ​ ensure ​ ​ AI can ​ ​ be ​ ​ a ​ ​ force ​ ​ for ​ ​ good ​ ​ in ​ ​ the ​ ​ face ​ ​ of ​ ​ our ​ ​ contemporary ​ ​ political ​ ​ realities . Population ​ ​ Registries ​ ​ and ​ ​ Computing ​ ​ Power In ​ ​ political ​ ​ contexts ​ ​ where ​ ​ minorities ​ ​ or ​ ​ opposition ​ ​ points ​ ​ of ​ ​ view ​ ​ are ​ ​ seen ​ ​ as ​ ​ threats ​ ​ to ​ ​ an imagined ​ ​ homogeneous ​ ​ “ people , ” ​ ​ information ​ ​ technology ​ ​ has ​ ​ been ​ ​ used ​ ​ to ​ ​ monitor ​ ​ and control ​ ​ these ​ ​ segments ​ ​ of ​ ​ a ​ ​ population . ​ ​ Such ​ ​ techno-political ​ ​ projects ​ ​ often ​ ​ build ​ ​ on ​ ​ older colonial ​ ​ histories ​ ​ of ​ ​ censuses ​ ​ and ​ ​ population ​ ​ registries , ​ ​ as ​ ​ well ​ ​ as ​ ​ racialized ​ ​ modes ​ ​ of 97 surveillance ​ ​ and ​ ​ control ​ ​ rooted ​ ​ in ​ ​ the ​ ​ Atlantic ​ ​ slave ​ ​ trade ​ ​ and ​ ​ the ​ ​ plantation ​ ​ system . ​ ​ In Dark ​ ​ Matters ​ , ​ ​ Simone ​ ​ Browne ​ ​ connects ​ ​ this ​ ​ deep ​ ​ history ​ ​ of ​ ​ surveillance ​ ​ to ​ ​ contemporary biometric ​ ​ techniques ​ ​ of ​ ​ governing ​ ​ black ​ ​ bodies . 98 The ​ ​ Book ​ ​ of ​ ​ Life ​ ​ registry ​ ​ project ​ ​ in ​ ​ apartheid ​ ​ South ​ ​ Africa ​ ​ is ​ ​ a ​ ​ useful ​ ​ modern ​ ​ example . ​ ​ In that ​ ​ project , ​ ​ which ​ ​ ran ​ ​ from ​ ​ 1967 ​ ​ to ​ ​ 1983 , ​ ​ IBM ​ ​ assisted ​ ​ South ​ ​ Africa ​ ​ in ​ ​ classifying ​ ​ its population ​ ​ by ​ ​ racial ​ ​ descent . ​ ​ This ​ ​ system ​ ​ was ​ ​ used ​ ​ to ​ ​ move ​ ​ all ​ ​ so-called ​ ​ ‘ non-white citizens ’ ​ ​ from ​ ​ their ​ ​ homes ​ ​ into ​ ​ segregated ​ ​ neighborhoods . ​ ​ The ​ ​ Book ​ ​ of ​ ​ Life ​ ​ was ​ ​ plagued 99 by ​ ​ technical ​ ​ and ​ ​ operational ​ ​ problems ​ ​ and ​ ​ eventually ​ ​ abandoned . ​ ​ However , ​ ​ as ​ ​ Paul Edwards ​ ​ and ​ ​ Gabrielle ​ ​ Hecht ​ ​ note , ​ ​ “ technopolitical ​ ​ projects ​ ​ do ​ ​ not ​ ​ need ​ ​ to ​ ​ fully ​ ​ achieve their ​ ​ technical ​ ​ goals ​ ​ in ​ ​ order ​ ​ to ​ ​ ‘ work ’ ​ ​ politically … ​ ​ The ​ ​ registries ​ ​ ‘ worked ’ ​ ​ to ​ ​ establish racialized ​ ​ personal ​ ​ identities ​ ​ as ​ ​ elements ​ ​ of ​ ​ governance. ” ​ ​ As ​ ​ Kate ​ ​ Crawford ​ ​ has ​ ​ recently 100 argued , ​ ​ registries ​ ​ like ​ ​ the ​ ​ Book ​ ​ of ​ ​ Life ​ ​ were ​ ​ reinforcing ​ ​ a ​ ​ way ​ ​ of ​ ​ thinking ​ ​ that ​ ​ was ​ ​ itself autocratic . 101 More ​ ​ recent ​ ​ computerized ​ ​ registries ​ ​ like ​ ​ The ​ ​ National ​ ​ Security ​ ​ Entry-Exit ​ ​ Registration System ​ ​ ( NSEERS ) ​ ​ proliferated ​ ​ in ​ ​ the ​ ​ United ​ ​ States ​ ​ and ​ ​ among ​ ​ its ​ ​ allies ​ ​ following ​ ​ the ​ ​ attacks of ​ ​ September ​ ​ 11 , ​ ​ 2001 . ​ ​ NSEERS ​ ​ centralized ​ ​ documentation ​ ​ for ​ ​ non-citizens ​ ​ in ​ ​ the ​ ​ United States ​ ​ who ​ ​ hailed ​ ​ from ​ ​ a ​ ​ list ​ ​ of ​ ​ 25 ​ ​ predominantly ​ ​ Muslim ​ ​ countries ​ ​ that ​ ​ the ​ ​ Bush administration ​ ​ deemed ​ ​ dangerous . ​ ​ As ​ ​ with ​ ​ the ​ ​ Book ​ ​ of ​ ​ Life , ​ ​ NSEERS ’ ​ ​ effectiveness ​ ​ in ​ ​ its 96 See ​ ​ Kate ​ ​ Crawford , ​ ​ “ Dark ​ ​ Days : ​ ​ AI ​ ​ and ​ ​ the ​ ​ Rise ​ ​ of ​ ​ Fascism , ” ​ ​ SXSW ​ ​ featured ​ ​ talk , ​ ​ March ​ ​ 15 ​ ​ 2017 , https : //www.youtube.com/watch ? v=Dlr4O1aEJvI ​ . 97 For ​ ​ just ​ ​ one ​ ​ example ​ ​ in ​ ​ colonial ​ ​ India , ​ ​ v. : ​ ​ Radhika ​ ​ Singha , ​ ​ “ Settle , ​ ​ Mobilize , ​ ​ Verify : ​ ​ Identification ​ ​ Practices ​ ​ in ​ ​ Colonial ​ ​ India , ” Studies ​ ​ in ​ ​ History ​ ​ ​ 16 , ​ ​ No . ​ ​ 2 ​ ​ ( August ​ ​ 1 , ​ ​ 2000 ) : ​ ​ 151–98 , ​ ​ doi:10.1177/025764300001600201 . 98 Simone ​ ​ Browne , ​ ​​ Dark ​ ​ Matters : ​ ​ On ​ ​ the ​ ​ Surveillance ​ ​ of ​ ​ Blackness , ​ ​ ​ ( Durham : ​ ​ Duke ​ ​ University ​ ​ Press , ​ ​ 2015 ) . 99 National ​ ​ Action/Research ​ ​ on ​ ​ the ​ ​ Military-Industrial ​ ​ Complex ​ ​ and ​ ​ American ​ ​ Friends ​ ​ Service ​ ​ Committee , ​ ​​ Automating Apartheid : ​ ​ U.S. ​ ​ Computer ​ ​ Exports ​ ​ to ​ ​ South ​ ​ Africa ​ ​ and ​ ​ the ​ ​ Arms ​ ​ Embargo ​ , ​ ​ ( Philadelphia : ​ ​ NARMIC/America ​ ​ Friends ​ ​ Service Committee , ​ ​ 1982 ) , ​ ​ 19 . 100 Paul ​ ​ N. ​ ​ Edwards ​ ​ and ​ ​ Gabrielle ​ ​ Hecht , ​ ​ `` History ​ ​ and ​ ​ the ​ ​ technopolitics ​ ​ of ​ ​ identity : ​ ​ The ​ ​ case ​ ​ of ​ ​ apartheid ​ ​ South ​ ​ Africa , '' Journal ​ ​ of ​ ​ Southern ​ ​ African ​ ​ Studies ​ ​ ​ 36 , ​ ​ No . ​ ​ 3 ​ ​ ( 2010 ) : ​ ​ 619-639 . 101 ​ ​​ ​​ Kate ​ ​ Crawford , ​ ​ “ Dark ​ ​ Days : ​ ​ AI ​ ​ and ​ ​ the ​ ​ Rise ​ ​ of ​ ​ Fascism , ” ​ ​ SXSW ​ ​ featured ​ ​ talk , ​ ​ March ​ ​ 15 ​ ​ 2017 , https : //www.youtube.com/watch ? v=Dlr4O1aEJvI ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 23 stated ​ ​ goal ​ ​ of ​ ​ stopping ​ ​ domestic ​ ​ terrorism ​ ​ was ​ ​ questionable , ​ ​ and ​ ​ it ​ ​ was ​ ​ dismantled ​ ​ in ​ ​ the final ​ ​ days ​ ​ of ​ ​ the ​ ​ Obama ​ ​ administration ​ ​ ( although ​ ​ the ​ ​ data ​ ​ collected ​ ​ by ​ ​ the ​ ​ program ​ ​ still exists ) . ​ ​ Consistent ​ ​ with ​ ​ Edwards ’ ​ ​ and ​ ​ Hecht ’ s ​ ​ analysis , ​ ​ NSEERS ​ ​ set ​ ​ into ​ ​ motion ​ ​ state 102 projects ​ ​ of ​ ​ Muslim ​ ​ surveillance ​ ​ and ​ ​ deportation . 103 The ​ ​ history ​ ​ and ​ ​ political ​ ​ efficacy ​ ​ of ​ ​ registries ​ ​ exposes ​ ​ the ​ ​ urgent ​ ​ need ​ ​ for ​ ​ lines ​ ​ of ​ ​ research that ​ ​ can ​ ​ examine ​ ​ the ​ ​ way ​ ​ citizen ​ ​ registries ​ ​ work ​ ​ currently , ​ ​ enhanced ​ ​ by ​ ​ data ​ ​ mining ​ ​ and ​ ​ AI techniques , ​ ​ and ​ ​ how ​ ​ they ​ ​ may ​ ​ work ​ ​ in ​ ​ the ​ ​ future . ​ ​ Contemporary ​ ​ AI ​ ​ systems ​ ​ intensify 104 these ​ ​ longer-standing ​ ​ practices ​ ​ of ​ ​ surveillance ​ ​ and ​ ​ control . ​ ​ Such ​ ​ systems ​ ​ require ​ ​ the collection ​ ​ of ​ ​ massive ​ ​ amounts ​ ​ of ​ ​ data , ​ ​ which ​ ​ is ​ ​ now ​ ​ possible ​ ​ at ​ ​ large ​ ​ scale ​ ​ via ​ ​ the ​ ​ Internet and ​ ​ connected ​ ​ devices . ​ ​ When ​ ​ these ​ ​ practices ​ ​ are ​ ​ carried ​ ​ out ​ ​ by ​ ​ private ​ ​ enterprise ​ ​ in addition ​ ​ to ​ ​ states , ​ ​ as ​ ​ we ​ ​ will ​ ​ discuss ​ ​ in ​ ​ the ​ ​ next ​ ​ section , ​ ​ they ​ ​ introduce ​ ​ new ​ ​ forms ​ ​ of ​ ​ value extraction ​ ​ and ​ ​ population ​ ​ control ​ ​ unregulated ​ ​ and ​ ​ often ​ ​ unacknowledged ​ ​ by ​ ​ current ​ ​ legal frameworks . 105 ​ ​ Corporate ​ ​ and ​ ​ Government ​ ​ Entanglements It ​ ​ remains ​ ​ critically ​ ​ important ​ ​ to ​ ​ understand ​ ​ the ​ ​ history ​ ​ of ​ ​ AI ​ ​ and ​ ​ its ​ ​ shifting ​ ​ relationship ​ ​ to the ​ ​ state . ​ ​ In ​ ​ the ​ ​ mid-twentieth ​ ​ century , ​ ​ advanced ​ ​ computing ​ ​ projects ​ ​ tended ​ ​ to ​ ​ be ​ ​ closely associated ​ ​ with ​ ​ the ​ ​ state , ​ ​ and ​ ​ especially ​ ​ the ​ ​ military ​ ​ agencies ​ ​ who ​ ​ funded ​ ​ their fundamental ​ ​ research ​ ​ and ​ ​ development . ​ ​ Although ​ ​ AI ​ ​ emerged ​ ​ from ​ ​ this ​ ​ context , ​ ​ its 106 present ​ ​ is ​ ​ characterized ​ ​ by ​ ​ a ​ ​ more ​ ​ collaborative ​ ​ approach ​ ​ between ​ ​ state ​ ​ agencies ​ ​ and private ​ ​ corporations ​ ​ engaged ​ ​ in ​ ​ AI ​ ​ research ​ ​ and ​ ​ development . ​ ​ As ​ ​ Gary ​ ​ Marchant ​ ​ and Wendell ​ ​ Wallach ​ ​ argue , ​ ​ governance ​ ​ has ​ ​ expanded ​ ​ far ​ ​ beyond ​ ​ both ​ ​ governmental institutions ​ ​ and ​ ​ legal ​ ​ codes ​ ​ to ​ ​ include ​ ​ a ​ ​ wide ​ ​ range ​ ​ of ​ ​ industry ​ ​ standards ​ ​ and ​ ​ practices ​ ​ that will ​ ​ shape ​ ​ how ​ ​ AI ​ ​ systems ​ ​ are ​ ​ implemented . 107 Palantir—co-founded ​ ​ by ​ ​ Trump ​ ​ supporter ​ ​ and ​ ​ advisor ​ ​ Peter ​ ​ Thiel ​ ​ with ​ ​ seed ​ ​ money ​ ​ from the ​ ​ CIA ’ s ​ ​ venture ​ ​ capital ​ ​ fund ​ ​ In-Q-Tel—typifies ​ ​ this ​ ​ dynamic . ​ ​ Gotham , ​ ​ Palantir ’ s 108 national ​ ​ security ​ ​ and ​ ​ government ​ ​ software , ​ ​ allows ​ ​ analysts ​ ​ to ​ ​ easily ​ ​ combine , ​ ​ query ​ ​ and visualize ​ ​ structured ​ ​ and ​ ​ unstructured ​ ​ data ​ ​ at ​ ​ large ​ ​ scales . ​ ​ AI ​ ​ can ​ ​ now ​ ​ be ​ ​ used ​ ​ in ​ ​ Palantir 109 products ​ ​ for ​ ​ activities ​ ​ such ​ ​ as ​ ​ lead ​ ​ generation , ​ ​ including ​ ​ a ​ ​ bank ’ s ​ ​ ability ​ ​ to ​ ​ identify 102 J . ​ ​ David ​ ​ Goodman ​ ​ and ​ ​ Ron ​ ​ Nixon , ​ ​ “ Obama ​ ​ to ​ ​ Dismantle ​ ​ Visitor ​ ​ Registry ​ ​ Before ​ ​ Trump ​ ​ Can ​ ​ Revive ​ ​ It , ” ​ ​​ The ​ ​ New ​ ​ York ​ ​ Times ​ , December ​ ​ 22 , ​ ​ 2016 , https : //www.nytimes.com/2016/12/22/nyregion/obama-to-dismantle-visitor-registry-before-trump-can-revive-it.html ​ . 103 “ Raza ​ ​ v. ​ ​ City ​ ​ of ​ ​ New ​ ​ York ​ ​ - ​ ​ Legal ​ ​ Challenge ​ ​ to ​ ​ NYPD ​ ​ Muslim ​ ​ Surveillance ​ ​ Program , ” ​ ​ American ​ ​ Civil ​ ​ Liberties ​ ​ Union , ​ ​ March ​ ​ 6 , 2017 , ​ ​​ https : //www.aclu.org/cases/raza-v-city-new-york-legal-challenge-nypd-muslim-surveillance-program ​ . 104 Kate ​ ​ Crawford , ​ ​ “ Letter ​ ​ to ​ ​ Silicon ​ ​ Valley , ” ​ ​​ Harper ’ s ​ ​ Magazin ​ e , ​ ​ Feb ​ ​ 9 , ​ ​ 2017 , https : //harpers.org/archive/2017/02/trump-a-resisters-guide/11/ ​ . 105 ​ ​​ ​​ ​ Julie ​ ​ E. ​ ​ Cohen , ​ ​ “ The ​ ​ Biopolitical ​ ​ Public ​ ​ Domain : ​ ​ The ​ ​ Legal ​ ​ Construction ​ ​ of ​ ​ the ​ ​ Surveillance ​ ​ Economy , ” ​ ​ Philosophy ​ ​ & Technology , ​ ​ March ​ ​ 28 , ​ ​ 2017 , ​ ​ 1–21 , ​ ​ doi:10.1007/s13347-017-0258-2 . 106 Paul ​ ​ N. ​ ​ Edwards , ​ ​ The ​ ​ Closed ​ ​ World : ​ ​​ Computers ​ ​ and ​ ​ the ​ ​ Politics ​ ​ of ​ ​ Discourse ​ ​ in ​ ​ Cold ​ ​ War ​ ​ America ​ ​ ​ ( Cambridge , ​ ​ MA : ​ ​ The ​ ​ MIT Press , ​ ​ 1996 ) . 107 ​ ​​ ​​ Gary ​ ​ E. ​ ​ Marchant ​ ​ and ​ ​ Wendell ​ ​ Wallach , ​ ​ “ Coordinating ​ ​ Technology ​ ​ Governance ​ ​ | ​ ​ Issues ​ ​ in ​ ​ Science ​ ​ and ​ ​ Technology , ” ​ ​ Issues in ​ ​ Science ​ ​ and ​ ​ Technology ​ ​ XXXI , ​ ​ no . ​ ​ 4 ​ ​ ( Summer ​ ​ 2015 ) , ​ ​ http : //issues.org/31-4/coordinating-technology-governance/ . 108 Sam ​ ​ Biddle , ​ ​ “ How ​ ​ Peter ​ ​ Thiel ’ s ​ ​ Palantir ​ ​ Helped ​ ​ the ​ ​ NSA ​ ​ Spy ​ ​ on ​ ​ the ​ ​ Whole ​ ​ World , ” ​ ​​ The ​ ​ Intercept ​ , ​ ​ February ​ ​ 22 , ​ ​ 2017 , https : //theintercept.com/2017/02/22/how-peter-thiels-palantir-helped-the-nsa-spy-on-the-whole-world/ ​ . 109 Ibid . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 24 anomalous ​ ​ credit ​ ​ card ​ ​ activity ​ ​ for ​ ​ fraud ​ ​ protection . ​ ​ More ​ ​ advanced ​ ​ capabilities ​ ​ are available ​ ​ to ​ ​ national ​ ​ security ​ ​ clients ​ ​ as ​ ​ well . ​ ​ How ​ ​ rights ​ ​ and ​ ​ liberties ​ ​ need ​ ​ to ​ ​ be ​ ​ understood and ​ ​ reconfigured ​ ​ in ​ ​ the ​ ​ face ​ ​ of ​ ​ opaque ​ ​ public-private ​ ​ AI ​ ​ systems ​ ​ is ​ ​ still ​ ​ an ​ ​ open ​ ​ question . Immigration ​ ​ and ​ ​ law ​ ​ enforcement ​ ​ are ​ ​ critical ​ ​ within ​ ​ this ​ ​ debate . ​ ​ In ​ ​ the ​ ​ United ​ ​ States , Immigration ​ ​ and ​ ​ Customs ​ ​ Enforcement ​ ​ ( ICE ) ​ ​ is ​ ​ expanding ​ ​ its ​ ​ technological ​ ​ reach ​ ​ through tools ​ ​ like ​ ​ Investigative ​ ​ Case ​ ​ Management ​ ​ ( ICM ) , ​ ​ a ​ ​ platform ​ ​ that ​ ​ allows ​ ​ agents ​ ​ to ​ ​ access ​ ​ a wide ​ ​ variety ​ ​ of ​ ​ previously ​ ​ separate ​ ​ databases , ​ ​ including ​ ​ information ​ ​ on ​ ​ a ​ ​ suspect ’ s “ schooling , ​ ​ family ​ ​ relationships , ​ ​ employment ​ ​ information , ​ ​ phone ​ ​ records , ​ ​ immigration history , ​ ​ foreign ​ ​ exchange ​ ​ program ​ ​ status , ​ ​ personal ​ ​ connections , ​ ​ biometric ​ ​ traits , ​ ​ criminal records ​ ​ and ​ ​ home ​ ​ and ​ ​ work ​ ​ addresses. ” ​ ​ This ​ ​ is ​ ​ another ​ ​ Palantir ​ ​ system , ​ ​ first ​ ​ procured ​ ​ by 110 the ​ ​ Obama ​ ​ administration ​ ​ in ​ ​ 2014 ​ ​ and ​ ​ scheduled ​ ​ to ​ ​ become ​ ​ operational ​ ​ late ​ ​ in ​ ​ 2017 . Other ​ ​ law ​ ​ enforcement ​ ​ agencies ​ ​ are ​ ​ ​​ currently ​ ​ integrating ​ ​ AI ​ ​ and ​ ​ related ​ ​ algorithmic decision-support ​ ​ systems ​ ​ from ​ ​ the ​ ​ private ​ ​ sector ​ ​ into ​ ​ their ​ ​ existing ​ ​ arsenals . ​ ​ Axon ( formerly ​ ​ Taser ​ ​ International ) ​ ​ is ​ ​ a ​ ​ publicly ​ ​ traded ​ ​ maker ​ ​ of ​ ​ law ​ ​ enforcement ​ ​ products , including ​ ​ their ​ ​ famous ​ ​ electroshock ​ ​ weapon . ​ ​ The ​ ​ company ​ ​ has ​ ​ now ​ ​ shifted ​ ​ toward ​ ​ body camera ​ ​ technologies , ​ ​ recently ​ ​ offering ​ ​ them ​ ​ for ​ ​ free ​ ​ to ​ ​ any ​ ​ police ​ ​ department ​ ​ in ​ ​ the ​ ​ U.S. 111 In ​ ​ 2017 , ​ ​ Axon ​ ​ started ​ ​ an ​ ​ AI ​ ​ division ​ ​ following ​ ​ their ​ ​ acquisition ​ ​ of ​ ​ two ​ ​ machine ​ ​ vision companies . ​ ​ Among ​ ​ their ​ ​ goals ​ ​ is ​ ​ to ​ ​ more ​ ​ efficiently ​ ​ analyze ​ ​ the ​ ​ over ​ ​ 5.2 ​ ​ petabytes ​ ​ of ​ ​ data that ​ ​ they ​ ​ have ​ ​ already ​ ​ acquired ​ ​ from ​ ​ their ​ ​ existing ​ ​ camera ​ ​ systems . ​ ​ Video ​ ​ expands ​ ​ Axon ’ s existing ​ ​ Digital ​ ​ Evidence ​ ​ Management ​ ​ System , ​ ​ signaling ​ ​ a ​ ​ larger ​ ​ shift ​ ​ beyond ​ ​ machine learning ​ ​ and ​ ​ natural ​ ​ language ​ ​ processing ​ ​ of ​ ​ textual ​ ​ sources . ​ ​ Axon ​ ​ CEO ​ ​ Rick ​ ​ Smith ​ ​ has 112 argued ​ ​ that ​ ​ the ​ ​ vast ​ ​ scale ​ ​ of ​ ​ existing ​ ​ law ​ ​ enforcement ​ ​ data ​ ​ could ​ ​ help ​ ​ drive ​ ​ research ​ ​ in machine ​ ​ vision ​ ​ as ​ ​ a ​ ​ whole : ​ ​ “ We ’ ve ​ ​ got ​ ​ all ​ ​ of ​ ​ this ​ ​ law ​ ​ enforcement ​ ​ information ​ ​ with ​ ​ these videos , ​ ​ which ​ ​ is ​ ​ one ​ ​ of ​ ​ the ​ ​ richest ​ ​ treasure ​ ​ troves ​ ​ you ​ ​ could ​ ​ imagine ​ ​ for ​ ​ machine ​ ​ learning. ” ​ ​ There ​ ​ are ​ ​ real ​ ​ concerns ​ ​ about ​ ​ the ​ ​ forms ​ ​ of ​ ​ bias ​ ​ embedded ​ ​ in ​ ​ these ​ ​ data ​ ​ sets , ​ ​ and ​ ​ how 113 they ​ ​ would ​ ​ subsequently ​ ​ function ​ ​ as ​ ​ training ​ ​ data ​ ​ for ​ ​ an ​ ​ AI ​ ​ system . There ​ ​ are ​ ​ some ​ ​ who ​ ​ argue ​ ​ in ​ ​ favor ​ ​ of ​ ​ body ​ ​ camera ​ ​ and ​ ​ machine ​ ​ vision ​ ​ systems ​ ​ for supporting ​ ​ civil ​ ​ liberties , ​ ​ including ​ ​ enhanced ​ ​ law ​ ​ enforcement ​ ​ transparency ​ ​ and accountability . ​ ​ Axon ​ ​ promises ​ ​ that ​ ​ its ​ ​ AI ​ ​ techniques ​ ​ will ​ ​ reduce ​ ​ the ​ ​ time ​ ​ officers 114 currently ​ ​ spend ​ ​ on ​ ​ report-writing ​ ​ and ​ ​ data ​ ​ entry . ​ ​ However , ​ ​ Axon ’ s ​ ​ new ​ ​ focus ​ ​ on 115 110 Spencer ​ ​ Woodma , ​ ​ “ Palantir ​ ​ Provides ​ ​ the ​ ​ Engine ​ ​ for ​ ​ Donald ​ ​ Trump ’ s ​ ​ Deportation ​ ​ Machine , ” ​ ​​ The ​ ​ Intercept ​ , ​ ​ March ​ ​ 2 , ​ ​ 2017 , https : //theintercept.com/2017/03/02/palantir-provides-the-engine-for-donald-trumps-deportation-machine/ ​ . 111 Laurel ​ ​ Wamsley , ​ ​ “ Taser ​ ​ Changes ​ ​ Its ​ ​ Name ​ ​ To ​ ​ Axon ​ ​ And ​ ​ Offers ​ ​ Free ​ ​ Body ​ ​ Cameras ​ ​ For ​ ​ Police , ” ​ ​​ NPR ​ , ​ ​ Arpil ​ ​ 7 , ​ ​ 2017 , http : //www.npr.org/sections/thetwo-way/2017/04/07/522878573/we-re-more-than-stun-guns-says-taser-as-it-changes-co mpany-name ​ . 112 “ TASER ​ ​ Makes ​ ​ Two ​ ​ Acquisitions ​ ​ to ​ ​ Create ​ ​ ‘ Axon ​ ​ AI , ’ ” ​ ​​ Police ​ ​ Magazine ​ , ​ ​ February ​ ​ 9 , ​ ​ 2017 , http : //www.policemag.com/channel/technology/news/2017/02/09/taser-makes-two-acquisitions-to-create-axon-ai.aspx ​ . 113 Doug ​ ​ Wyllie , ​ ​ “ What ​ ​ TASER ’ s ​ ​ Acquisition ​ ​ of ​ ​ 2 ​ ​ AI ​ ​ Companies ​ ​ Means ​ ​ for ​ ​ the ​ ​ Future ​ ​ of ​ ​ Policing , ” ​ ​​ PoliceOne ​ , ​ ​ February ​ ​ 10 , ​ ​ 2017 , https : //www.policeone.com/police-products/less-lethal/TASER/articles/289203006-What-TASERs-acquisition-of-2-AI-comp anies-means-for-the-future-of-policing/ ​ . 114 Jay ​ ​ Stanley , ​ ​ “ Police ​ ​ Body-Mounted ​ ​ Cameras : ​ ​ With ​ ​ Right ​ ​ Policies ​ ​ in ​ ​ Place , ​ ​ a ​ ​ Win ​ ​ for ​ ​ All , ” ​ ​ ( ACLU , ​ ​ 2013 ) , http : //www.urbanaillinois.us/sites/default/files/attachments/police-body-mounted-cameras-stanley.pdf ​ . 115 Alex ​ ​ Pasternack , ​ ​ “ Police ​ ​ Body ​ ​ Cameras ​ ​ Will ​ ​ Do ​ ​ More ​ ​ Than ​ ​ Just ​ ​ Record ​ ​ You , ” ​ ​​ Fast ​ ​ Company ​ , ​ ​​ March ​ ​ 3 , ​ ​ 2017 , https : //www.fastcompany.com/3061935/police-body-cameras-livestreaming-face-recognition-and-ai ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 25 predictive ​ ​ methods ​ ​ of ​ ​ policing—inspired ​ ​ by ​ ​ Wal-Mart ’ s ​ ​ and ​ ​ Google ’ s ​ ​ embrace ​ ​ of ​ ​ deep learning ​ ​ to ​ ​ increase ​ ​ sales—raises ​ ​ new ​ ​ civil ​ ​ liberties ​ ​ concerns . ​ ​ Instead ​ ​ of ​ ​ purchasing patterns , ​ ​ these ​ ​ systems ​ ​ will ​ ​ be ​ ​ looking ​ ​ for ​ ​ much ​ ​ more ​ ​ vague , ​ ​ context-dependent ​ ​ targets , like ​ ​ “ suspicious ​ ​ activity. ” ​ ​ Behind ​ ​ appearances ​ ​ of ​ ​ technical ​ ​ neutrality , ​ ​ these ​ ​ systems ​ ​ rely ​ ​ on deeply ​ ​ subjective ​ ​ assumptions ​ ​ about ​ ​ what ​ ​ constitutes ​ ​ suspicious ​ ​ behavior ​ ​ or ​ ​ who ​ ​ counts as ​ ​ a ​ ​ suspicious ​ ​ person . 116 Unsurprisingly , ​ ​ machine ​ ​ vision ​ ​ techniques ​ ​ may ​ ​ reproduce ​ ​ and ​ ​ present ​ ​ as ​ ​ objective ​ ​ existing forms ​ ​ of ​ ​ racial ​ ​ bias . ​ ​ Researchers ​ ​ affiliated ​ ​ with ​ ​ Google ’ s ​ ​ Machine ​ ​ Intelligence ​ ​ Group ​ ​ and 117 Columbia ​ ​ University ​ ​ make ​ ​ a ​ ​ compelling ​ ​ comparison ​ ​ between ​ ​ machine ​ ​ learning ​ ​ systems designed ​ ​ to ​ ​ predict ​ ​ criminality ​ ​ from ​ ​ facial ​ ​ photos ​ ​ and ​ ​ discredited ​ ​ theories ​ ​ of physiognomy—both ​ ​ of ​ ​ which ​ ​ problematically ​ ​ claim ​ ​ to ​ ​ be ​ ​ able ​ ​ to ​ ​ predict ​ ​ character ​ ​ or behavioral ​ ​ traits ​ ​ simply ​ ​ by ​ ​ examining ​ ​ physical ​ ​ features . ​ ​ More ​ ​ generally , ​ ​ Cathy ​ ​ O ’ Neil 118 identifies ​ ​ the ​ ​ potential ​ ​ for ​ ​ advanced ​ ​ AI ​ ​ systems ​ ​ in ​ ​ law ​ ​ enforcement ​ ​ to ​ ​ create ​ ​ a ​ ​ “ pernicious feedback ​ ​ loop ” —if ​ ​ these ​ ​ systems ​ ​ are ​ ​ built ​ ​ on ​ ​ top ​ ​ of ​ ​ racially-biased ​ ​ policing ​ ​ practices , ​ ​ then their ​ ​ training ​ ​ data ​ ​ will ​ ​ reflect ​ ​ these ​ ​ existing ​ ​ biases , ​ ​ and ​ ​ integrate ​ ​ such ​ ​ bias ​ ​ into ​ ​ the ​ ​ logic ​ ​ of decision ​ ​ making ​ ​ and ​ ​ prediction . 119 Ethical ​ ​ questions ​ ​ of ​ ​ bias ​ ​ and ​ ​ accountability ​ ​ will ​ ​ become ​ ​ even ​ ​ more ​ ​ urgent ​ ​ in ​ ​ the ​ ​ context ​ ​ of rights ​ ​ and ​ ​ liberties ​ ​ as ​ ​ AI ​ ​ systems ​ ​ capable ​ ​ of ​ ​ violent ​ ​ force ​ ​ against ​ ​ humans ​ ​ are ​ ​ developed and ​ ​ deployed ​ ​ in ​ ​ law ​ ​ enforcement ​ ​ and ​ ​ military ​ ​ contexts . ​ ​ Robotic ​ ​ police ​ ​ officers , ​ ​ for example , ​ ​ recently ​ ​ debuted ​ ​ in ​ ​ Dubai . ​ ​ If ​ ​ these ​ ​ were ​ ​ to ​ ​ carry ​ ​ weapons , ​ ​ new ​ ​ questions 120 would ​ ​ arise ​ ​ about ​ ​ how ​ ​ to ​ ​ determine ​ ​ when ​ ​ the ​ ​ use ​ ​ of ​ ​ force ​ ​ is ​ ​ appropriate . ​ ​ Drawing ​ ​ on analysis ​ ​ of ​ ​ the ​ ​ Black ​ ​ Lives ​ ​ Matter ​ ​ movement , ​ ​ Peter ​ ​ Asaro ​ ​ has ​ ​ pointed ​ ​ to ​ ​ difficult ​ ​ ethical issues ​ ​ involving ​ ​ how ​ ​ lethal ​ ​ autonomous ​ ​ weapons ​ ​ systems ​ ​ ( LAWS ) ​ ​ will ​ ​ detect ​ ​ threats ​ ​ or gestures ​ ​ of ​ ​ cooperation , ​ ​ especially ​ ​ involving ​ ​ vulnerable ​ ​ populations . ​ ​ He ​ ​ concludes ​ ​ that ​ ​ AI and ​ ​ robotics ​ ​ researchers ​ ​ should ​ ​ adopt ​ ​ ethical ​ ​ and ​ ​ legal ​ ​ standards ​ ​ that ​ ​ maintain ​ ​ human control ​ ​ and ​ ​ accountability ​ ​ over ​ ​ these ​ ​ systems . 121 Similar ​ ​ questions ​ ​ apply ​ ​ in ​ ​ the ​ ​ military ​ ​ use ​ ​ of ​ ​ LAWS . ​ ​ Heather ​ ​ Roff ​ ​ argues ​ ​ that ​ ​ fully autonomous ​ ​ systems ​ ​ would ​ ​ violate ​ ​ current ​ ​ legal ​ ​ definitions ​ ​ of ​ ​ war ​ ​ that ​ ​ require ​ ​ human judgment ​ ​ in ​ ​ the ​ ​ proportionate ​ ​ use ​ ​ of ​ ​ force , ​ ​ and ​ ​ guard ​ ​ against ​ ​ targeting ​ ​ of ​ ​ civilians . Furthermore , ​ ​ she ​ ​ argues ​ ​ that ​ ​ AI ​ ​ learning ​ ​ systems ​ ​ may ​ ​ make ​ ​ it ​ ​ difficult ​ ​ for ​ ​ commanders ​ ​ to even ​ ​ know ​ ​ how ​ ​ their ​ ​ weapons ​ ​ will ​ ​ respond ​ ​ in ​ ​ battle ​ ​ situations . ​ ​ Given ​ ​ these ​ ​ legal , ​ ​ ethical 122 116 Ava ​ ​ Kofman , ​ ​ “ Taser ​ ​ Will ​ ​ Use ​ ​ Police ​ ​ Body ​ ​ Camera ​ ​ Videos ​ ​ ‘ to ​ ​ Anticipate ​ ​ Criminal ​ ​ Activity , ’ ” ​ ​​ The ​ ​ Intercept ​ , ​ ​ April ​ ​ 30 , ​ ​ 2017 , https : //theintercept.com/2017/04/30/taser-will-use-police-body-camera-videos-to-anticipate-criminal-activity/ ​ . 117 Clare ​ ​ Garvie ​ ​ and ​ ​ Jonathan ​ ​ Frankle , ​ ​ “ Facial-Recognition ​ ​ Software ​ ​ Might ​ ​ Have ​ ​ a ​ ​ Racial ​ ​ Bias ​ ​ Problem , ” ​ ​​ The ​ ​ Atlantic ​ , ​ ​ April ​ ​ 7 , 2016 , https : //www.theatlantic.com/technology/archive/2016/04/the-underlying-bias-of-facial-recognition-systems/476991/ ​ . 118 Blaise ​ ​ Agüera ​ ​ y ​ ​ Arcas , ​ ​ Margaret ​ ​ Mitchell ​ ​ and ​ ​ Alexander ​ ​ Todorov , ​ ​ “ Physiognomy ’ s ​ ​ New ​ ​ Clothes , ” ​ ​​ Medium ​ , ​ ​ May ​ ​ 7 , ​ ​ 2017 , https : //medium.com/ @ blaisea/physiognomys-new-clothes-f2d4b59fdd6a ​ . 119 Kofman , ​ ​ “ Taser ​ ​ Will ​ ​ Use ​ ​ Police ​ ​ Body ​ ​ Camera ​ ​ Videos ​ ​ ‘ to ​ ​ Anticipate ​ ​ Criminal ​ ​ Activity. ’ ” 120 Rory ​ ​ Cellan-Jones , ​ ​ “ Robot ​ ​ Police ​ ​ Officer ​ ​ Goes ​ ​ on ​ ​ Duty ​ ​ in ​ ​ Dubai , ” ​ ​​ BBC ​ ​ News ​ , ​ ​ May ​ ​ 24 , ​ ​ 2017 , ​ ​ sec . ​ ​ Technology , http : //www.bbc.com/news/technology-40026940 ​ . 121 Peter ​ ​ Asaro , ​ ​ “ ‘ Hands ​ ​ Up , ​ ​ Don ’ t ​ ​ Shoot ! ’ ​ ​ HRI ​ ​ and ​ ​ the ​ ​ Automation ​ ​ of ​ ​ Police ​ ​ Use ​ ​ of ​ ​ Force , ” ​ ​​ Journal ​ ​ of ​ ​ Human-Robot ​ ​ Interaction 5 , ​ ​ No . ​ ​ 3 ​ ​ ( December ​ ​ 14 , ​ ​ 2016 ) : ​ ​ 55–69 . 122 Heather ​ ​ M. ​ ​ Roff , ​ ​ “ Meaningful ​ ​ Human ​ ​ Control ​ ​ or ​ ​ Appropriate ​ ​ Human ​ ​ Judgment ? ​ ​ The ​ ​ Necessary ​ ​ Limits ​ ​ on ​ ​ Autonomous AI ​ ​ Now ​ ​ 2017 ​ ​ Report 26 and ​ ​ design ​ ​ concerns , ​ ​ both ​ ​ researchers ​ ​ call ​ ​ for ​ ​ strict ​ ​ limitations ​ ​ on ​ ​ the ​ ​ use ​ ​ of ​ ​ AI ​ ​ in ​ ​ weapons systems . While ​ ​ predictive ​ ​ policing ​ ​ and ​ ​ the ​ ​ use ​ ​ of ​ ​ force ​ ​ have ​ ​ always ​ ​ been ​ ​ important ​ ​ issues , ​ ​ they ​ ​ take on ​ ​ new ​ ​ salience ​ ​ in ​ ​ populist ​ ​ or ​ ​ authoritarian ​ ​ contexts . ​ ​ As ​ ​ AI ​ ​ systems ​ ​ promise ​ ​ new ​ ​ forms ​ ​ of technical ​ ​ efficiency ​ ​ in ​ ​ the ​ ​ service ​ ​ of ​ ​ safety , ​ ​ we ​ ​ may ​ ​ need ​ ​ to ​ ​ confront ​ ​ a ​ ​ fundamental tension ​ ​ between ​ ​ technological ​ ​ efficiency ​ ​ and ​ ​ a ​ ​ commitment ​ ​ to ​ ​ ideals ​ ​ of ​ ​ justice . AI ​ ​ and ​ ​ the ​ ​ Legal ​ ​ System The ​ ​ legal ​ ​ system ​ ​ is ​ ​ the ​ ​ institution ​ ​ tasked ​ ​ with ​ ​ defending ​ ​ civil ​ ​ rights ​ ​ and ​ ​ liberties . ​ ​ Thus , there ​ ​ are ​ ​ two ​ ​ separate ​ ​ questions ​ ​ to ​ ​ consider ​ ​ regarding ​ ​ AI ​ ​ and ​ ​ the ​ ​ legal ​ ​ system : ​ ​ 1 ) ​ ​ Can ​ ​ the legal ​ ​ system ​ ​ serve ​ ​ the ​ ​ rights-protection ​ ​ functions ​ ​ it ​ ​ is ​ ​ expected ​ ​ to ​ ​ when ​ ​ an ​ ​ AI ​ ​ system produces ​ ​ an ​ ​ unfair ​ ​ result ? ​ ​ And , ​ ​ 2 ) ​ ​ How ​ ​ and ​ ​ where ​ ​ ( if ​ ​ at ​ ​ all ) ​ ​ should ​ ​ the ​ ​ legal ​ ​ system incorporate ​ ​ AI ? Scholars ​ ​ like ​ ​ Kate ​ ​ Crawford ​ ​ and ​ ​ Jason ​ ​ Schultz ​ ​ have ​ ​ identified ​ ​ a ​ ​ series ​ ​ of ​ ​ conflicts ​ ​ between AI ​ ​ techniques ​ ​ and ​ ​ constitutional ​ ​ due ​ ​ process ​ ​ requirements , ​ ​ such ​ ​ as ​ ​ how ​ ​ AI ​ ​ techniques 123 affect ​ ​ procedural ​ ​ considerations ​ ​ and ​ ​ equal ​ ​ justice ​ ​ under ​ ​ the ​ ​ law . ​ ​ The ​ ​ proliferation ​ ​ of predictive ​ ​ systems ​ ​ demands ​ ​ new ​ ​ regulatory ​ ​ techniques ​ ​ to ​ ​ protect ​ ​ legal ​ ​ rights . ​ ​ Danielle Citron ​ ​ and ​ ​ Frank ​ ​ Pasquale ​ ​ argue ​ ​ that ​ ​ safeguards ​ ​ to ​ ​ rights ​ ​ should ​ ​ be ​ ​ introduced ​ ​ at ​ ​ all ​ ​ stages of ​ ​ the ​ ​ implementation ​ ​ of ​ ​ an ​ ​ AI ​ ​ system , ​ ​ from ​ ​ safeguarding ​ ​ privacy ​ ​ rights ​ ​ in ​ ​ data ​ ​ collection to ​ ​ public ​ ​ audits ​ ​ of ​ ​ scoring ​ ​ systems ​ ​ that ​ ​ critically ​ ​ affect ​ ​ the ​ ​ public ​ ​ in ​ ​ areas ​ ​ like ​ ​ employment and ​ ​ healthcare . 124 In ​ ​ a ​ ​ similar ​ ​ vein , ​ ​ Andrew ​ ​ Selbst ​ ​ has ​ ​ argued ​ ​ that ​ ​ an ​ ​ impact ​ ​ assessment ​ ​ requirement ​ ​ can force ​ ​ those ​ ​ building ​ ​ and ​ ​ buying ​ ​ AI ​ ​ systems ​ ​ to ​ ​ make ​ ​ explicit ​ ​ the ​ ​ normative ​ ​ choices ​ ​ they ​ ​ are making ​ ​ before ​ ​ implementing ​ ​ them . ​ ​ And ​ ​ as ​ ​ Lilian ​ ​ Edwards ​ ​ and ​ ​ Michael ​ ​ Veale ​ ​ have 125 126 pointed ​ ​ out , ​ ​ the ​ ​ new ​ ​ EU ​ ​ General ​ ​ Data ​ ​ Protection ​ ​ Regulation ​ ​ ( GDPR ) ​ ​ includes ​ ​ a requirement ​ ​ for ​ ​ data ​ ​ protection ​ ​ impact ​ ​ assessments , ​ ​ the ​ ​ import ​ ​ of ​ ​ which ​ ​ is ​ ​ unclear ​ ​ as ​ ​ yet . There ​ ​ is ​ ​ also ​ ​ a ​ ​ rapidly ​ ​ emerging ​ ​ scholarly ​ ​ debate ​ ​ about ​ ​ the ​ ​ value ​ ​ of ​ ​ requiring ​ ​ an explanation ​ ​ or ​ ​ interpretation ​ ​ of ​ ​ AI ​ ​ and ​ ​ machine ​ ​ learning ​ ​ systems ​ ​ as ​ ​ a ​ ​ regulatory ​ ​ technique to ​ ​ ensure ​ ​ individual ​ ​ rights , ​ ​ how ​ ​ to ​ ​ operationalize ​ ​ such ​ ​ a ​ ​ requirement , ​ ​ whether ​ ​ such ​ ​ a 127 128 Weapons ” ​ ​ ( Geneva : ​ ​ Review ​ ​ Conference ​ ​ of ​ ​ the ​ ​ Convention ​ ​ on ​ ​ Certain ​ ​ Conventional ​ ​ Weapons , ​ ​ December ​ ​ 2016 ) , https : //globalsecurity.asu.edu/sites/default/files/files/Control-or-Judgment-Understanding-the-Scope.pdf ​ . 123 Kate ​ ​ Crawford ​ ​ and ​ ​ Jason ​ ​ Schultz , ​ ​ “ Big ​ ​ Data ​ ​ and ​ ​ Due ​ ​ Process : ​ ​ Toward ​ ​ a ​ ​ Framework ​ ​ to ​ ​ Redress ​ ​ Predictive ​ ​ Privacy ​ ​ Harms , ” Boston ​ ​ College ​ ​ Law ​ ​ Review ​ ​ ​ 55 , ​ ​ No . ​ ​ 1 ​ ​ ( January ​ ​ 29 , ​ ​ 2014 ) : ​ ​ 93 . 124 Danielle ​ ​ Keats ​ ​ Citron ​ ​ and ​ ​ Frank ​ ​ A . ​ ​ Pasquale , ​ ​ “ The ​ ​ Scored ​ ​ Society : ​ ​ Due ​ ​ Process ​ ​ for ​ ​ Automated ​ ​ Predictions , ” ​ ​​ Washington ​ ​ Law Review ​ ​ ​ 89 , ​ ​ No . ​ ​ 1 ​ ​ ( 2014 ) : ​ ​ 1–33 . 125 Andrew ​ ​ D. ​ ​ Selbst , ​ ​ “ Disparate ​ ​ Impact ​ ​ in ​ ​ Big ​ ​ Data ​ ​ Policing ” , ​ ​ Georgia ​ ​ Law ​ ​ Review , ​ ​ forthcoming ​ ​ 2017 . ​ ​ SSRN ​ ​ preprint : https : //papers.ssrn.com/sol3/papers.cfm ? abstract_id=2819182 ​ . 126 Lilian ​ ​ Edwards ​ ​ and ​ ​ Michael ​ ​ Veale , ​ ​ “ Slave ​ ​ to ​ ​ the ​ ​ Algorithm ? ​ ​ Why ​ ​ a ​ ​ ‘ Right ​ ​ to ​ ​ Explanation ’ ​ ​ is ​ ​ Probably ​ ​ Not ​ ​ the ​ ​ Remedy ​ ​ You are ​ ​ Looking ​ ​ for ” , ​ ​ SSRN ​ ​ preprint , ​ ​​ https : //papers.ssrn.com/sol3/papers.cfm ? abstract_id=2972855 ​ . 127 Ibid ; ​ ​ Kiel ​ ​ Brennan-Marquez , ​ ​ “ ’ Plausible ​ ​ Cause ’ : ​ ​ Explanatory ​ ​ Standards ​ ​ in ​ ​ the ​ ​ Age ​ ​ of ​ ​ Powerful ​ ​ Machines , ​ ​​ Vanderbilt ​ ​ Law Review ​ ​ ​ ( 2017 ) ​ ​ vol . ​ ​ 70 , ​ ​ p. ​ ​ 1249 ; ​ ​ Andrew ​ ​ D. ​ ​ Selbst , ​ ​ “ A ​ ​ Mild ​ ​ Defense ​ ​ of ​ ​ Our ​ ​ New ​ ​ Machine ​ ​ Overlords , ” ​ ​​ Vanderbilt ​ ​ Law ​ ​ Review En ​ ​ Banc ​ ​ ​ ( 2017 ) ​ ​ vol . ​ ​ 70 , ​ ​ p. ​ ​ 87 ; ​ ​ Katherine ​ ​ Jo ​ ​ Strandburg , ​ ​ “ Decisionmaking , ​ ​ machine ​ ​ learning ​ ​ and ​ ​ the ​ ​ value ​ ​ of ​ ​ explanation , ” ( The ​ ​ Human ​ ​ Use ​ ​ of ​ ​ Machine ​ ​ Learning : ​ ​ An ​ ​ Interdisciplinary ​ ​ Workshop , ​ ​ 16 ​ ​ December ​ ​ 2016 ) , http : //www.dsi.unive.it/HUML2016/assets/Slides/Talk % 202.pdf ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 27 requirement ​ ​ presently ​ ​ exists ​ ​ under ​ ​ the ​ ​ GDPR ​ ​ and ​ ​ more ​ ​ generally ​ ​ how ​ ​ competing 129 interpretations ​ ​ or ​ ​ explanations ​ ​ might ​ ​ be ​ ​ technically ​ ​ formulated ​ ​ and ​ ​ understood ​ ​ by different ​ ​ stakeholders . 130 The ​ ​ criminal ​ ​ justice ​ ​ system ’ s ​ ​ implementation ​ ​ of ​ ​ risk ​ ​ assessment ​ ​ algorithms ​ ​ provides ​ ​ an example ​ ​ of ​ ​ the ​ ​ legal ​ ​ system ’ s ​ ​ use ​ ​ of ​ ​ AI ​ ​ and ​ ​ its ​ ​ attendant ​ ​ risks . ​ ​ Proponents ​ ​ of ​ ​ risk-based 131 sentencing ​ ​ argue ​ ​ that ​ ​ evidence-based ​ ​ machine ​ ​ learning ​ ​ techniques ​ ​ can ​ ​ be ​ ​ used ​ ​ in ​ ​ concert with ​ ​ the ​ ​ expertise ​ ​ of ​ ​ judges ​ ​ to ​ ​ improve ​ ​ the ​ ​ accuracy ​ ​ of ​ ​ prior ​ ​ statistical ​ ​ and ​ ​ actuarial methods ​ ​ for ​ ​ risk ​ ​ forecasting , ​ ​ such ​ ​ as ​ ​ regression ​ ​ analysis . ​ ​ Along ​ ​ these ​ ​ lines , ​ ​ a ​ ​ recent 132 study ​ ​ by ​ ​ computer ​ ​ scientist ​ ​ Jon ​ ​ Kleinberg , ​ ​ Sendhil ​ ​ Mullainathan , ​ ​ and ​ ​ their ​ ​ co-authors showed ​ ​ that ​ ​ a ​ ​ predictive ​ ​ machine ​ ​ learning ​ ​ algorithm ​ ​ could ​ ​ be ​ ​ used ​ ​ by ​ ​ judges ​ ​ to ​ ​ reduce ​ ​ the number ​ ​ of ​ ​ defendants ​ ​ held ​ ​ in ​ ​ jail ​ ​ as ​ ​ they ​ ​ await ​ ​ trial ​ ​ by ​ ​ making ​ ​ more ​ ​ accurate ​ ​ predictions of ​ ​ future ​ ​ crimes . 133 While ​ ​ algorithmic ​ ​ decision-making ​ ​ tools ​ ​ show ​ ​ promise , ​ ​ many ​ ​ of ​ ​ these ​ ​ researchers ​ ​ caution against ​ ​ misleading ​ ​ performance ​ ​ measures ​ ​ for ​ ​ emerging ​ ​ AI-assisted ​ ​ legal ​ ​ techniques . ​ ​ For 134 example , ​ ​ the ​ ​ value ​ ​ of ​ ​ recidivism ​ ​ as ​ ​ a ​ ​ means ​ ​ to ​ ​ evaluate ​ ​ the ​ ​ correctness ​ ​ of ​ ​ an algorithmically-assigned ​ ​ risk ​ ​ score ​ ​ is ​ ​ questionable ​ ​ because ​ ​ judges ​ ​ make ​ ​ decisions ​ ​ about risk ​ ​ in ​ ​ sentencing , ​ ​ which , ​ ​ in ​ ​ turn , ​ ​ influences ​ ​ recidivism ​ ​ – ​ ​ or , ​ ​ those ​ ​ assessed ​ ​ as ​ ​ “ low ​ ​ risk ” and ​ ​ subsequently ​ ​ released ​ ​ are ​ ​ the ​ ​ only ​ ​ ones ​ ​ who ​ ​ will ​ ​ have ​ ​ an ​ ​ opportunity ​ ​ to ​ ​ re-offend , making ​ ​ it ​ ​ difficult ​ ​ to ​ ​ measure ​ ​ the ​ ​ accuracy ​ ​ of ​ ​ such ​ ​ scoring . ​ ​ Meanwhile , ​ ​ Rebecca ​ ​ Wexler ​ ​ has documented ​ ​ the ​ ​ disturbing ​ ​ trend ​ ​ of ​ ​ trade ​ ​ secret ​ ​ doctrine ​ ​ being ​ ​ expressly ​ ​ adopted ​ ​ in ​ ​ courts to ​ ​ prevent ​ ​ criminal ​ ​ defendants ​ ​ from ​ ​ asserting ​ ​ their ​ ​ rights ​ ​ at ​ ​ trial . 135 Sandra ​ ​ Mayson ​ ​ has ​ ​ recently ​ ​ written ​ ​ on ​ ​ risk ​ ​ assessment ​ ​ in ​ ​ the ​ ​ bail ​ ​ reform ​ ​ movement . Well-intentioned ​ ​ proponents ​ ​ of ​ ​ bail ​ ​ reform ​ ​ argue ​ ​ that ​ ​ risk ​ ​ assessment ​ ​ can ​ ​ be ​ ​ used ​ ​ to spare ​ ​ poor , ​ ​ low-risk ​ ​ defendants ​ ​ from ​ ​ onerous ​ ​ bail ​ ​ requirements ​ ​ or ​ ​ pretrial ​ ​ incarceration . Such ​ ​ arguments ​ ​ tend ​ ​ to ​ ​ miss ​ ​ the ​ ​ potential ​ ​ of ​ ​ risk ​ ​ assessment ​ ​ to ​ ​ “ legitimize ​ ​ and ​ ​ entrench ” problematic ​ ​ reliance ​ ​ on ​ ​ statistical ​ ​ correlation , ​ ​ and ​ ​ to ​ ​ “ [ lend ​ ​ such ​ ​ assessments ] ​ ​ the ​ ​ aura ​ ​ of scientific ​ ​ reliability. ” ​ ​ Mayson ​ ​ argues ​ ​ that ​ ​ we ​ ​ also ​ ​ need ​ ​ to ​ ​ ask ​ ​ deeper ​ ​ questions ​ ​ about 136 128 Andrew ​ ​ D. ​ ​ Selbst ​ ​ and ​ ​ Solon ​ ​ Barocas , ​ ​ “ Regulating ​ ​ Inscrutable ​ ​ Systems , ” ​ ​ in ​ ​ progress . 129 Bryce ​ ​ Goodman ​ ​ and ​ ​ Seth ​ ​ Flaxman , ​ ​ “ European ​ ​ Union ​ ​ regulations ​ ​ on ​ ​ algorithmic ​ ​ decision-making ​ ​ and ​ ​ a ​ ​ ‘ right ​ ​ to explanation , ’ ” ​ ​ ICML ​ ​ Workshop ​ ​ on ​ ​ Human ​ ​ Interpretability ​ ​ in ​ ​ Machine ​ ​ Learning , ​ ​​ arXiv ​ ​ preprint ​ : ​ ​ arXiv:1606.08813 ​ ​ ( v3 ) ​ ​ ( 2016 ) ; forthcoming , ​ ​​ AI ​ ​ Magazine ​ ​​ ( 2017 ) ; ​ ​ Sandra ​ ​ Wachter , ​ ​ Brent ​ ​ Mittelstadt ​ ​ and ​ ​ Luciano ​ ​ Floridi , ​ ​ “ Why ​ ​ a ​ ​ right ​ ​ to ​ ​ explanation ​ ​ of automated ​ ​ decision-making ​ ​ does ​ ​ not ​ ​ exist ​ ​ in ​ ​ the ​ ​ General ​ ​ Data ​ ​ Protection ​ ​ Regulation , ” ​ ​​ International ​ ​ Data ​ ​ Protection ​ ​ Law ( 2017 ) , ​ ​​ https : //doi.org/10.1093/idpl/ipx005 ​ . 130 ​ ​​ ​ ​ Zachary ​ ​ C. ​ ​ Lipton , ​ ​ “ The ​ ​ Mythos ​ ​ of ​ ​ Model ​ ​ Interpretability , ” ​ ​ arXiv ​ ​ preprint ​ ​ [ Cs , ​ ​ Stat ] , ​ ​ June ​ ​ 10 , ​ ​ 2016 , http : //arxiv.org/abs/1606.03490 . 131 Richard ​ ​ Berk ​ ​ and ​ ​ Jordan ​ ​ Hyatt , ​ ​ “ Machine ​ ​ Learning ​ ​ Forecasts ​ ​ of ​ ​ Risk ​ ​ to ​ ​ Inform ​ ​ Sentencing ​ ​ Decisions , ” ​ ​​ Federal ​ ​ Sentencing Reporter ​ ​ ​ 27 , ​ ​ No . ​ ​ 4 ​ ​ ( April ​ ​ 1 , ​ ​ 2015 ) : ​ ​ 222–28 , ​ ​ doi:10.1525/fsr.2015.27.4.222 . 132 Berk ​ ​ and ​ ​ Hyatt , ​ ​ “ Machine ​ ​ Learning ​ ​ Forecasts ​ ​ of ​ ​ Risk ​ ​ to ​ ​ Inform ​ ​ Sentencing ​ ​ Decisions , ” ​ ​ 222 . 133 Jon ​ ​ Kleinberg ​ ​ et ​ ​ al. , ​ ​ “ Human ​ ​ Decisions ​ ​ and ​ ​ Machine ​ ​ Predictions , ” ​ ​ Working ​ ​ Paper ​ ​ ( National ​ ​ Bureau ​ ​ of ​ ​ Economic ​ ​ Research , February ​ ​ 2017 ) , ​ ​ doi:10.3386/w23180 . ​ ​​ http : //nber.org/papers/w23180 ​ . 134 Jon ​ ​ Kleinberg , ​ ​ Jens ​ ​ Ludwig ​ ​ and ​ ​ Sendhil ​ ​ Mullainathan , ​ ​ “ A ​ ​ Guide ​ ​ to ​ ​ Solving ​ ​ Social ​ ​ Problems ​ ​ with ​ ​ Machine ​ ​ Learning , ” ​ ​​ Harvard Business ​ ​ Review ​ , ​ ​ December ​ ​ 8 , ​ ​ 2016 , ​ ​​ https : //hbr.org/2016/12/a-guide-to-solving-social-problems-with-machine-learning ​ . 135 Rebecca ​ ​ Wexler , ​ ​ “ Life , ​ ​ Liberty , ​ ​ and ​ ​ Trade ​ ​ Secrets : ​ ​ Intellectual ​ ​ Property ​ ​ in ​ ​ the ​ ​ Criminal ​ ​ Justice ​ ​ System , ” ​ ​ SSRN ​ ​ preprint : https : //papers.ssrn.com/sol3/papers.cfm ? abstract_id=2920883 ​ . 136 Sandra ​ ​ G. ​ ​ Mayson , ​ ​​ ​ “ Bail ​ ​ Reform ​ ​ and ​ ​ Restraint ​ ​ for ​ ​ Dangerousness : ​ ​ Are ​ ​ Defendants ​ ​ a ​ ​ Special ​ ​ Case ? ” , ​ SSRN ​ ​ Scholarly ​ ​ Paper AI ​ ​ Now ​ ​ 2017 ​ ​ Report 28 how ​ ​ pretrial ​ ​ restraints ​ ​ are ​ ​ justified ​ ​ in ​ ​ the ​ ​ first ​ ​ place . ​ ​ In ​ ​ other ​ ​ words , ​ ​ policymakers ​ ​ who hope ​ ​ to ​ ​ employ ​ ​ risk ​ ​ assessment ​ ​ in ​ ​ bail ​ ​ reform ​ ​ and ​ ​ pretrial ​ ​ forms ​ ​ of ​ ​ detention ​ ​ need ​ ​ to publicly ​ ​ specify ​ ​ what ​ ​ types ​ ​ of ​ ​ risks ​ ​ can ​ ​ justify ​ ​ these ​ ​ such ​ ​ restraints ​ ​ on ​ ​ liberty , ​ ​ as defendants ​ ​ receiving ​ ​ these ​ ​ scores ​ ​ have ​ ​ not ​ ​ been ​ ​ convicted ​ ​ of ​ ​ anything ​ ​ and ​ ​ these ​ ​ restraints are ​ ​ not ​ ​ imposed ​ ​ on ​ ​ dangerous ​ ​ individuals ​ ​ in ​ ​ the ​ ​ rest ​ ​ of ​ ​ society . Separately , ​ ​ criminologist ​ ​ Richard ​ ​ Berk ​ ​ and ​ ​ his ​ ​ colleagues ​ ​ argue ​ ​ that ​ ​ there ​ ​ are ​ ​ intractable tradeoffs ​ ​ between ​ ​ accuracy ​ ​ and ​ ​ fairness—the ​ ​ occurrence ​ ​ of ​ ​ false ​ ​ positives ​ ​ and negatives—in ​ ​ populations ​ ​ where ​ ​ base ​ ​ rates ​ ​ ( the ​ ​ percentage ​ ​ of ​ ​ a ​ ​ given ​ ​ population ​ ​ that ​ ​ fall into ​ ​ a ​ ​ specific ​ ​ category ) ​ ​ vary ​ ​ between ​ ​ different ​ ​ social ​ ​ groups . ​ ​ Difficult ​ ​ decisions ​ ​ need ​ ​ to 137 be ​ ​ made ​ ​ about ​ ​ how ​ ​ we ​ ​ value ​ ​ fairness ​ ​ and ​ ​ accuracy ​ ​ in ​ ​ risk ​ ​ assessment . ​ ​ It ​ ​ is ​ ​ not ​ ​ merely ​ ​ a technical ​ ​ problem , ​ ​ but ​ ​ one ​ ​ that ​ ​ involves ​ ​ important ​ ​ value ​ ​ judgments ​ ​ about ​ ​ how ​ ​ society should ​ ​ work . ​ ​ Left ​ ​ unchecked , ​ ​ the ​ ​ legal ​ ​ system ​ ​ is ​ ​ thus ​ ​ as ​ ​ susceptible ​ ​ to ​ ​ perpetuating AI-driven ​ ​ harm ​ ​ as ​ ​ any ​ ​ other ​ ​ institution . Finally , ​ ​ machine ​ ​ learning ​ ​ and ​ ​ data ​ ​ analysis ​ ​ techniques ​ ​ are ​ ​ also ​ ​ being ​ ​ used ​ ​ to ​ ​ identify ​ ​ and explain ​ ​ the ​ ​ abuses ​ ​ of ​ ​ rights . ​ ​ Working ​ ​ with ​ ​ human ​ ​ rights ​ ​ advocates ​ ​ in ​ ​ Mexico , ​ ​ the ​ ​ Human Rights ​ ​ Data ​ ​ Analysis ​ ​ Group ​ ​ created ​ ​ a ​ ​ machine ​ ​ learning ​ ​ model ​ ​ that ​ ​ can ​ ​ help ​ ​ guide ​ ​ the search ​ ​ for ​ ​ mass ​ ​ graves . 138 AI ​ ​ and ​ ​ Privacy AI ​ ​ challenges ​ ​ current ​ ​ understandings ​ ​ of ​ ​ privacy ​ ​ and ​ ​ strains ​ ​ the ​ ​ laws ​ ​ and ​ ​ regulations ​ ​ we have ​ ​ in ​ ​ place ​ ​ to ​ ​ protect ​ ​ personal ​ ​ information . ​ ​ Established ​ ​ approaches ​ ​ to ​ ​ privacy ​ ​ have become ​ ​ less ​ ​ and ​ ​ less ​ ​ effective ​ ​ because ​ ​ they ​ ​ are ​ ​ focused ​ ​ on ​ ​ previous ​ ​ metaphors ​ ​ of computing , ​ ​ ones ​ ​ where ​ ​ adversaries ​ ​ were ​ ​ primarily ​ ​ human . ​ ​ AI ​ ​ systems ’ ​ ​ intelligence , ​ ​ as such , ​ ​ depends ​ ​ on ​ ​ ingesting ​ ​ as ​ ​ much ​ ​ training ​ ​ data ​ ​ as ​ ​ possible . ​ ​ This ​ ​ primary ​ ​ objective ​ ​ is adverse ​ ​ to ​ ​ the ​ ​ goals ​ ​ of ​ ​ privacy . ​ ​ AI ​ ​ thus ​ ​ poses ​ ​ significant ​ ​ challenges ​ ​ to ​ ​ traditional ​ ​ efforts ​ ​ to minimize ​ ​ data ​ ​ collection ​ ​ and ​ ​ to ​ ​ reform ​ ​ government ​ ​ and ​ ​ industry ​ ​ surveillance ​ ​ practices . Of ​ ​ course , ​ ​ privacy ​ ​ as ​ ​ a ​ ​ “ right ” ​ ​ has ​ ​ always ​ ​ been ​ ​ unevenly ​ ​ distributed . ​ ​ Rights-based discourses ​ ​ are ​ ​ regularly ​ ​ critiqued ​ ​ as ​ ​ being ​ ​ disproportionately ​ ​ beneficial ​ ​ to ​ ​ the ​ ​ privileged while ​ ​ leaving ​ ​ many ​ ​ vulnerable ​ ​ populations ​ ​ partially ​ ​ or ​ ​ entirely ​ ​ exposed . ​ ​ Yet ​ ​ what ​ ​ is different ​ ​ with ​ ​ AI ​ ​ and ​ ​ privacy ​ ​ is ​ ​ that ​ ​ while ​ ​ individualistic ​ ​ and ​ ​ rights-based conceptualizations ​ ​ of ​ ​ privacy ​ ​ remain ​ ​ important ​ ​ to ​ ​ some ​ ​ of ​ ​ the ​ ​ systems ​ ​ at ​ ​ work ​ ​ today , computational ​ ​ systems ​ ​ are ​ ​ now ​ ​ operating ​ ​ outside ​ ​ of ​ ​ the ​ ​ data ​ ​ collection ​ ​ metaphors ​ ​ that privacy ​ ​ law ​ ​ is ​ ​ built ​ ​ on . ​ ​ We ​ ​ are ​ ​ in ​ ​ new ​ ​ terrain , ​ ​ and ​ ​ one ​ ​ that ​ ​ 20th ​ ​ century ​ ​ models ​ ​ of ​ ​ privacy are ​ ​ not ​ ​ designed ​ ​ to ​ ​ contend ​ ​ with . For ​ ​ example , ​ ​ privacy ​ ​ discourse ​ ​ has ​ ​ not ​ ​ sufficiently ​ ​ accounted ​ ​ for ​ ​ the ​ ​ growing ​ ​ power asymmetries ​ ​ between ​ ​ the ​ ​ institutions ​ ​ that ​ ​ accumulate ​ ​ data ​ ​ and ​ ​ the ​ ​ people ​ ​ who ​ ​ generate ( Rochester , ​ ​ NY : ​ ​ Social ​ ​ Science ​ ​ Research ​ ​ Network , ​ ​ August ​ ​ 15 , ​ ​ 2016 ) , ​ ​​ https : //papers.ssrn.com/abstract=2826600 ​ , ​ ​ 2 . 137 Richard ​ ​ Berk , ​ ​ Hoda ​ ​ Heidari , ​ ​ Shahin ​ ​ Jabbari , ​ ​ Michael ​ ​ Kearns ​ ​ and ​ ​ Aaron ​ ​ Roth , ​ ​ “ Fairness ​ ​ in ​ ​ Criminal ​ ​ Justice ​ ​ Risk ​ ​ Assessments : The ​ ​ State ​ ​ of ​ ​ the ​ ​ Art , ” ​ ​ arXiv:1703.09207 , ​ ​ March ​ ​ 27 , ​ ​ 2017 . 138 ​ ​​ ​​ ​ J . ​ ​ M. ​ ​ Porup , ​ ​ “ Hunting ​ ​ for ​ ​ Mexico ’ s ​ ​ Mass ​ ​ Graves ​ ​ with ​ ​ Machine ​ ​ Learning , ” ​ ​ Ars ​ ​ Technica ​ ​ UK , ​ ​ April ​ ​ 17 , ​ ​ 2017 , https : //arstechnica.co.uk/information-technology/2017/04/hunting-for-mexicos-mass-graves-with-machine-learning/ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 29 that ​ ​ data , ​ ​ even ​ ​ as ​ ​ they ​ ​ are ​ ​ approaching ​ ​ threshold ​ ​ levels ​ ​ which ​ ​ may ​ ​ make ​ ​ these asymmetries ​ ​ very ​ ​ hard ​ ​ to ​ ​ reverse . ​ ​ Models ​ ​ of ​ ​ privacy ​ ​ based ​ ​ on ​ ​ data ​ ​ as ​ ​ a ​ ​ tradable ​ ​ good ​ ​ fail to ​ ​ contend ​ ​ with ​ ​ this ​ ​ power ​ ​ difference . ​ ​ People ​ ​ can not ​ ​ trade ​ ​ effectively ​ ​ with ​ ​ systems ​ ​ they do ​ ​ not ​ ​ understand , ​ ​ particularly ​ ​ when ​ ​ the ​ ​ system ​ ​ understands ​ ​ them ​ ​ all ​ ​ too ​ ​ well ​ ​ and ​ ​ knows how ​ ​ to ​ ​ manipulate ​ ​ their ​ ​ preferences . ​ ​ Additionally , ​ ​ adaptive ​ ​ algorithms ​ ​ are ​ ​ changing constantly , ​ ​ such ​ ​ that ​ ​ even ​ ​ the ​ ​ designers ​ ​ who ​ ​ created ​ ​ them ​ ​ can not ​ ​ fully ​ ​ explain ​ ​ the ​ ​ results they ​ ​ generate . ​ ​ In ​ ​ this ​ ​ new ​ ​ model ​ ​ of ​ ​ computational ​ ​ privacy ​ ​ adversaries , ​ ​ both ​ ​ power ​ ​ and knowledge ​ ​ gaps ​ ​ will ​ ​ continue ​ ​ to ​ ​ widen . ​ ​ We ​ ​ must ​ ​ ask ​ ​ how ​ ​ ‘ notice ​ ​ and ​ ​ consent ’ ​ ​ is ​ ​ possible or ​ ​ what ​ ​ it ​ ​ would ​ ​ mean ​ ​ to ​ ​ have ​ ​ ‘ access ​ ​ to ​ ​ your ​ ​ data ’ ​ ​ or ​ ​ to ​ ​ ‘ control ​ ​ your ​ ​ data ’ ​ ​ when ​ ​ so ​ ​ much is ​ ​ unknown ​ ​ or ​ ​ in ​ ​ flux . There ​ ​ has ​ ​ also ​ ​ been ​ ​ a ​ ​ shift ​ ​ in ​ ​ the ​ ​ quality ​ ​ of ​ ​ the ​ ​ data ​ ​ used ​ ​ for ​ ​ AI . ​ ​ In ​ ​ order ​ ​ to ​ ​ help ​ ​ develop sophisticated ​ ​ diagnostic ​ ​ models , ​ ​ designers ​ ​ often ​ ​ seek ​ ​ to ​ ​ use ​ ​ inputs ​ ​ that ​ ​ are ​ ​ extremely sensitive ​ ​ in ​ ​ nature . ​ ​ For ​ ​ example , ​ ​ in ​ ​ the ​ ​ case ​ ​ of ​ ​ DeepMind 's ​ ​ partnership ​ ​ with ​ ​ the ​ ​ UK ’ s National ​ ​ Health ​ ​ Service , ​ ​ the ​ ​ company ​ ​ acquired ​ ​ large ​ ​ amounts ​ ​ of ​ ​ very ​ ​ sensitive ​ ​ public health ​ ​ data . ​ ​ Even ​ ​ though ​ ​ this ​ ​ data ​ ​ may ​ ​ have ​ ​ been ​ ​ required ​ ​ for ​ ​ some ​ ​ of ​ ​ the ​ ​ project ’ s ​ ​ goals , the ​ ​ resulting ​ ​ backlash ​ ​ and ​ ​ government ​ ​ censure ​ ​ illustrate ​ ​ the ​ ​ emerging ​ ​ tensions ​ ​ related 139 to ​ ​ the ​ ​ AI ​ ​ industry ’ s ​ ​ use ​ ​ of ​ ​ such ​ ​ data ​ ​ and ​ ​ the ​ ​ current ​ ​ limits ​ ​ of ​ ​ democratic ​ ​ processes ​ ​ to address ​ ​ questions ​ ​ of ​ ​ agency , ​ ​ accountability ​ ​ and ​ ​ oversight ​ ​ for ​ ​ these ​ ​ endeavors . The ​ ​ expansion ​ ​ of ​ ​ AI ​ ​ into ​ ​ diverse ​ ​ realms ​ ​ like ​ ​ urban ​ ​ planning ​ ​ also ​ ​ raises ​ ​ privacy ​ ​ concerns over ​ ​ the ​ ​ deployment ​ ​ of ​ ​ IoT ​ ​ devices ​ ​ and ​ ​ sensors , ​ ​ arrayed ​ ​ throughout ​ ​ our ​ ​ daily ​ ​ lives , tracking ​ ​ human ​ ​ movements , ​ ​ preferences ​ ​ and ​ ​ environments . ​ ​ These ​ ​ devices ​ ​ and ​ ​ sensors 140 collect ​ ​ the ​ ​ data ​ ​ AI ​ ​ requires ​ ​ to ​ ​ function ​ ​ in ​ ​ these ​ ​ realms . ​ ​ Not ​ ​ only ​ ​ does ​ ​ this ​ ​ expansion significantly ​ ​ increase ​ ​ the ​ ​ amount ​ ​ and ​ ​ type ​ ​ of ​ ​ data ​ ​ being ​ ​ gathered ​ ​ on ​ ​ individuals , ​ ​ it ​ ​ also raises ​ ​ significant ​ ​ questions ​ ​ around ​ ​ security ​ ​ and ​ ​ accuracy ​ ​ as ​ ​ IoT ​ ​ devices ​ ​ are ​ ​ notoriously insecure , ​ ​ and ​ ​ often ​ ​ difficult ​ ​ to ​ ​ update ​ ​ and ​ ​ maintain . 141 AI ’ s ​ ​ capacity ​ ​ for ​ ​ prediction ​ ​ and ​ ​ inference ​ ​ also ​ ​ adds ​ ​ to ​ ​ the ​ ​ set ​ ​ of ​ ​ privacy ​ ​ concerns . ​ ​ Much ​ ​ of the ​ ​ value ​ ​ that ​ ​ AI ​ ​ offers ​ ​ is ​ ​ the ​ ​ ability ​ ​ to ​ ​ predict ​ ​ or ​ ​ “ imagine ” ​ ​ information ​ ​ about ​ ​ individuals and ​ ​ groups ​ ​ that ​ ​ is ​ ​ otherwise ​ ​ difficult ​ ​ to ​ ​ collect , ​ ​ compute ​ ​ or ​ ​ distribute . ​ ​ As ​ ​ more ​ ​ AI ​ ​ systems are ​ ​ deployed ​ ​ and ​ ​ focus ​ ​ on ​ ​ ever-more ​ ​ granular ​ ​ levels ​ ​ of ​ ​ detail , ​ ​ such ​ ​ “ predictive ​ ​ privacy harms ” ​ ​ will ​ ​ become ​ ​ greater ​ ​ concerns , ​ ​ especially ​ ​ if ​ ​ there ​ ​ are ​ ​ few ​ ​ or ​ ​ no ​ ​ due ​ ​ process constraints ​ ​ on ​ ​ how ​ ​ such ​ ​ information ​ ​ impacts ​ ​ vulnerable ​ ​ individuals . ​ ​ Part ​ ​ of ​ ​ the ​ ​ promise 142 of ​ ​ predictive ​ ​ techniques ​ ​ is ​ ​ to ​ ​ make ​ ​ accurate , ​ ​ often ​ ​ intimate ​ ​ deductions ​ ​ based ​ ​ on ​ ​ a seemingly-unrelated ​ ​ pieces ​ ​ of ​ ​ data ​ ​ or ​ ​ information , ​ ​ such ​ ​ as ​ ​ detecting ​ ​ substance ​ ​ abusers from ​ ​ Facebook ​ ​ posts , ​ ​ or ​ ​ identifying ​ ​ gang ​ ​ members ​ ​ based ​ ​ on ​ ​ Twitter ​ ​ data . ​ ​ Significant 143 144 139 ​ ​​ ​​ Information ​ ​ Commissioner ’ s ​ ​ Office , ​ ​ “ Royal ​ ​ Free ​ ​ - ​ ​ Google ​ ​ DeepMind ​ ​ trial ​ ​ failed ​ ​ to ​ ​ comply ​ ​ with ​ ​ data ​ ​ protection ​ ​ law , ” ​ ​ July ​ ​ 3 ( 2017 ) https : //ico.org.uk/about-the-ico/news-and-events/news-and-blogs/2017/07/royal-free-google-deepmind-trial-failed-to-co mply-with-data-protection-law/ 140 ​ ​​ ​​ See , ​ ​ for ​ ​ example , ​ ​ Nvida ’ s ​ ​ Deep ​ ​ Learning ​ ​ for ​ ​ Smart ​ ​ Cities : https : //www.nvidia.com/en-us/deep-learning-ai/industries/ai-cities/ 141 ​ ​​ ​​ There ​ ​ have ​ ​ been ​ ​ several ​ ​ serious ​ ​ attacks ​ ​ on ​ ​ network ​ ​ infrastructure ​ ​ and ​ ​ logistics ​ ​ from ​ ​ hacked ​ ​ IoT ​ ​ networked ​ ​ devices ​ ​ in ​ ​ the last ​ ​ 12 ​ ​ months . ​ ​ For ​ ​ example : ​ ​ Andrew ​ ​ Peterson , ​ ​ “ Internet ​ ​ of ​ ​ Things ​ ​ compounded ​ ​ Friday ’ s ​ ​ hack ​ ​ of ​ ​ major ​ ​ web ​ ​ sites , ” Washington ​ ​ Post , ​ ​ ​ October ​ ​ 21 ​ ​ ( 2016 ) 142 ​ ​​ ​​ Kate ​ ​ Crawford ​ ​ and ​ ​ Jason ​ ​ Schultz , ​ ​ “ Big ​ ​ Data ​ ​ and ​ ​ Due ​ ​ Process : ​ ​ Toward ​ ​ a ​ ​ Framework ​ ​ to ​ ​ Redress ​ ​ Predictive ​ ​ Privacy ​ ​ Harms , ” Boston ​ ​ College ​ ​ Law ​ ​ Review ​ ​ ​ 55 , ​ ​ No . ​ ​ 1 ​ ​ ( January ​ ​ 29 , ​ ​ 2014 ) : ​ ​ 93 . 143 ​ ​​ ​ Tao ​ ​ Ding , ​ ​ Warren ​ ​ Bickel ​ ​ and ​ ​ Shimei ​ ​ Pan , ​ ​ “ Social ​ ​ Media-based ​ ​ Substance ​ ​ Use ​ ​ Prediction , ” ​ ​ May ​ ​ 16 ​ ​ ( 2017 ) AI ​ ​ Now ​ ​ 2017 ​ ​ Report 30 shifts ​ ​ are ​ ​ needed ​ ​ in ​ ​ the ​ ​ legal ​ ​ and ​ ​ regulatory ​ ​ approaches ​ ​ to ​ ​ privacy ​ ​ if ​ ​ they ​ ​ are ​ ​ to ​ ​ keep ​ ​ pace with ​ ​ the ​ ​ emerging ​ ​ capacities ​ ​ of ​ ​ AI ​ ​ systems . Ethics ​ ​ and ​ ​ Governance So ​ ​ far , ​ ​ this ​ ​ report ​ ​ has ​ ​ addressed ​ ​ issues ​ ​ of ​ ​ power , ​ ​ markets , ​ ​ bias , ​ ​ fairness ​ ​ and ​ ​ rights ​ ​ and liberties ​ ​ – ​ ​ all ​ ​ subjects ​ ​ closely ​ ​ tied ​ ​ to ​ ​ ethics . ​ ​ This ​ ​ section ​ ​ presents ​ ​ a ​ ​ distinct ​ ​ discussion ​ ​ of ethics ​ ​ in ​ ​ the ​ ​ uses , ​ ​ deployment ​ ​ and ​ ​ creation ​ ​ of ​ ​ AI . 145 Ethical ​ ​ questions ​ ​ surrounding ​ ​ AI ​ ​ systems ​ ​ are ​ ​ wide-ranging , ​ ​ spanning ​ ​ creation , ​ ​ uses ​ ​ and outcomes . ​ ​ There ​ ​ are ​ ​ important ​ ​ questions ​ ​ about ​ ​ which ​ ​ set ​ ​ of ​ ​ values ​ ​ and ​ ​ interests ​ ​ are reflected ​ ​ in ​ ​ AI , ​ ​ as ​ ​ well ​ ​ as ​ ​ how ​ ​ machines ​ ​ can ​ ​ recognize ​ ​ values ​ ​ and ​ ​ ethical ​ ​ paradigms . ​ ​ An important ​ ​ distinction ​ ​ in ​ ​ this ​ ​ area ​ ​ is ​ ​ between ​ ​ what ​ ​ is ​ ​ called ​ ​ ‘ machine ​ ​ ethics ’ ​ ​ and ​ ​ the ​ ​ wider domain ​ ​ of ​ ​ the ​ ​ ethics ​ ​ of ​ ​ AI . ​ ​ Machine ​ ​ ethics ​ ​ is ​ ​ more ​ ​ narrowly ​ ​ and ​ ​ explicitly ​ ​ concerned ​ ​ with the ​ ​ ethics ​ ​ of ​ ​ artificially ​ ​ intelligent ​ ​ beings ​ ​ and ​ ​ systems ; ​ ​ Isaac ​ ​ Asimov ’ s ​ ​ laws ​ ​ of ​ ​ robotics ​ ​ are one ​ ​ example ​ ​ that ​ ​ captured ​ ​ the ​ ​ popular ​ ​ imagination . ​ ​ AI ​ ​ ethics ​ ​ concerns ​ ​ wider ​ ​ social concerns ​ ​ about ​ ​ the ​ ​ effects ​ ​ of ​ ​ AI ​ ​ systems ​ ​ and ​ ​ the ​ ​ choices ​ ​ made ​ ​ by ​ ​ their ​ ​ designers ​ ​ and users . ​ ​ Here , ​ ​ we ​ ​ are ​ ​ mostly ​ ​ concerned ​ ​ with ​ ​ the ​ ​ latter ​ ​ approach . AI ​ ​ is ​ ​ certainly ​ ​ not ​ ​ unique ​ ​ among ​ ​ emerging ​ ​ technologies ​ ​ in ​ ​ creating ​ ​ ethical ​ ​ quandaries , ​ ​ and , similar ​ ​ to ​ ​ other ​ ​ computational ​ ​ technologies , ​ ​ AI ​ ​ ethics ​ ​ have ​ ​ roots ​ ​ in ​ ​ the ​ ​ complex ​ ​ history ​ ​ of military ​ ​ influence ​ ​ on ​ ​ computing ​ ​ development ​ ​ and ​ ​ the ​ ​ more ​ ​ recent ​ ​ commercialization ​ ​ and corporate ​ ​ dominance ​ ​ of ​ ​ networked ​ ​ technologies . ​ ​​ ​ Yet ​ ​ ethical ​ ​ questions ​ ​ in ​ ​ AI ​ ​ research ​ ​ and development ​ ​ present ​ ​ unique ​ ​ challenges ​ ​ in ​ ​ that ​ ​ they ​ ​ ask ​ ​ us ​ ​ to ​ ​ consider ​ ​ whether , ​ ​ when ​ ​ and how ​ ​ machines ​ ​ should ​ ​ to ​ ​ make ​ ​ decisions ​ ​ about ​ ​ human ​ ​ lives ​ ​ - ​ ​ and ​ ​ whose ​ ​ values ​ ​ should guide ​ ​ those ​ ​ decisions . Ethical ​ ​ Concerns ​ ​ in ​ ​ AI Articulating ​ ​ ethical ​ ​ values ​ ​ for ​ ​ AI ​ ​ systems ​ ​ has ​ ​ never ​ ​ been ​ ​ simple . ​ ​ In ​ ​ the ​ ​ 1960s , ​ ​ AI ​ ​ pioneer Joseph ​ ​ Weizenbaum ​ ​ created ​ ​ the ​ ​ early ​ ​ chatbot ​ ​ system ​ ​ ELIZA ​ ​ as ​ ​ a ​ ​ technical ​ ​ demonstration of ​ ​ a ​ ​ system ​ ​ capable ​ ​ of ​ ​ maintaining ​ ​ an ​ ​ interrogative ​ ​ “ conversation ” ​ ​ with ​ ​ a ​ ​ human counterpart . ​ ​ Rudimentary ​ ​ as ​ ​ it ​ ​ was ​ ​ by ​ ​ today ’ s ​ ​ standards , ​ ​ some ​ ​ psychologists ​ ​ adopted ​ ​ it ​ ​ as a ​ ​ tool ​ ​ for ​ ​ treatment , ​ ​ much ​ ​ to ​ ​ the ​ ​ creator ’ s ​ ​ concern ​ ​ and ​ ​ dismay . ​ ​ In ​ ​ response , ​ ​ Weizenbaum raised ​ ​ ethical ​ ​ concerns ​ ​ around ​ ​ our ​ ​ reflexive ​ ​ reliance ​ ​ and ​ ​ trust ​ ​ in ​ ​ automated ​ ​ systems ​ ​ that may ​ ​ appear ​ ​ to ​ ​ be ​ ​ objective ​ ​ and ​ ​ “ intelligent , ” ​ ​ but ​ ​ are ​ ​ ultimately ​ ​ simplistic ​ ​ and ​ ​ prone ​ ​ to error . 146 https : //arxiv.org/abs/1705.05633 144 ​ ​​ ​​ See ​ ​ Lakshika ​ ​ Balasuriya ​ ​ et ​ ​ al. , ​ ​ “ Finding ​ ​ Street ​ ​ Gang ​ ​ Members ​ ​ on ​ ​ Twitter , ” ​ ​​ ​ 2016 ​ ​ IEEE/ACM ​ ​ International ​ ​ Conference ​ ​ on Advances ​ ​ in ​ ​ Social ​ ​ Networks ​ ​ Analysis ​ ​ and ​ ​ Mining ​ ​ ( ASONAM ​ ​ 2016 ) http : //knoesis.wright.edu/sites/default/files/ASONAM2016_GANG_MEMBER_IDENTIFICATION_LAKSHIKA.pdf 145 Vincent ​ ​ Conitzer , ​ ​ Walter ​ ​ Sinnott-Armstrong , ​ ​ Jana ​ ​ Schaich ​ ​ Borg , ​ ​ Yuan ​ ​ Deng ​ ​ and ​ ​ Max ​ ​ Kramer , ​ ​ `` Moral ​ ​ Decision ​ ​ Making Frameworks ​ ​ for ​ ​ Artificial ​ ​ Intelligence , '' ​ ​ ( Association ​ ​ for ​ ​ the ​ ​ Advancement ​ ​ of ​ ​ Artificial ​ ​ Intelligence , ​ ​ 2017 ) . 146 Hans ​ ​ Pruijt , ​ ​ `` Social ​ ​ Interaction ​ ​ With ​ ​ Computers : ​ ​ An ​ ​ Interpretation ​ ​ of ​ ​ Weizenbaum 's ​ ​ ELIZA ​ ​ and ​ ​ Her ​ ​ Heritage , '' ​ ​​ Social ​ ​ science computer ​ ​ review ​ ​ ​ 24 , ​ ​ No . ​ ​ 4 ​ ​ ( 2006 ) : ​ ​ 516-523 ; ​ ​ Joseph ​ ​ Weizenbaum , ​ ​​ Computer ​ ​ power ​ ​ and ​ ​ human ​ ​ reason : ​ ​ From ​ ​ judgement ​ ​ to calculation , ​ ​ ​ Harmondsworth , ​ ​ UK : ​ ​ Penguin , ​ ​ 1984 . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 31 Currently ​ ​ there ​ ​ are ​ ​ heated ​ ​ debates ​ ​ about ​ ​ whether ​ ​ AI ​ ​ systems ​ ​ should ​ ​ be ​ ​ used ​ ​ in ​ ​ sensitive or ​ ​ high-stakes ​ ​ contexts , ​ ​ who ​ ​ gets ​ ​ to ​ ​ make ​ ​ these ​ ​ important ​ ​ decisions ​ ​ , ​ ​ and ​ ​ what ​ ​ the ​ ​ proper degree ​ ​ of ​ ​ human ​ ​ involvement ​ ​ should ​ ​ be ​ ​ in ​ ​ various ​ ​ types ​ ​ of ​ ​ decision-making . ​ ​ These ​ ​ are 147 ethical ​ ​ questions ​ ​ with ​ ​ a ​ ​ longstanding ​ ​ history . ​ ​ In ​ ​ examining ​ ​ these ​ ​ questions , ​ ​ we ​ ​ must ​ ​ also look ​ ​ at ​ ​ the ​ ​ power ​ ​ dynamics ​ ​ of ​ ​ current ​ ​ AI ​ ​ development ​ ​ and ​ ​ deployment ​ ​ – ​ ​ and ​ ​ the ​ ​ way ​ ​ in which ​ ​ decision-making , ​ ​ both ​ ​ by ​ ​ AI ​ ​ systems ​ ​ and ​ ​ the ​ ​ people ​ ​ who ​ ​ build ​ ​ them , ​ ​ is ​ ​ often obscured ​ ​ from ​ ​ public ​ ​ view ​ ​ and ​ ​ accountability ​ ​ practices . Just ​ ​ in ​ ​ the ​ ​ last ​ ​ year , ​ ​ we ’ ve ​ ​ learned ​ ​ how ​ ​ Facebook ​ ​ mines ​ ​ user ​ ​ data ​ ​ to ​ ​ reveal ​ ​ teenagers ’ emotional ​ ​ state ​ ​ for ​ ​ advertisers , ​ ​ specifically ​ ​ targeting ​ ​ depressed ​ ​ teens . ​ ​ Cambridge 148 Analytica , ​ ​ a ​ ​ controversial ​ ​ data ​ ​ analytics ​ ​ firm ​ ​ that ​ ​ claims ​ ​ to ​ ​ be ​ ​ able ​ ​ to ​ ​ shift ​ ​ election ​ ​ results through ​ ​ micro-targeting , ​ ​ has ​ ​ been ​ ​ reported ​ ​ to ​ ​ have ​ ​ expansive ​ ​​ individual ​ ​ ​ profiles ​ ​ on ​ ​ 220 million ​ ​ adult ​ ​ Americans , ​ ​ and ​ ​ fake ​ ​ news ​ ​ has ​ ​ been ​ ​ instrumented ​ ​ to ​ ​ gain ​ ​ traction ​ ​ within 149 algorithmically ​ ​ filtered ​ ​ news ​ ​ feeds ​ ​ and ​ ​ search ​ ​ rankings ​ ​ in ​ ​ order ​ ​ to ​ ​ influence ​ ​ elections . 150 There ​ ​ are ​ ​ now ​ ​ multiple ​ ​ approaches ​ ​ for ​ ​ using ​ ​ machine ​ ​ learning ​ ​ techniques ​ ​ to ​ ​ synthesize audio- ​ ​ and ​ ​ video-realistic ​ ​ representations ​ ​ of ​ ​ public ​ ​ figures ​ ​ and ​ ​ news ​ ​ events . ​ ​ Each ​ ​ of 151 these ​ ​ examples ​ ​ shows ​ ​ how ​ ​ the ​ ​ interests ​ ​ of ​ ​ those ​ ​ deploying ​ ​ advanced ​ ​ data ​ ​ systems ​ ​ can overshadow ​ ​ the ​ ​ public ​ ​ interest , ​ ​ acting ​ ​ in ​ ​ ways ​ ​ contrary ​ ​ to ​ ​ individual ​ ​ autonomy ​ ​ and collective ​ ​ welfare , ​ ​ often ​ ​ without ​ ​ this ​ ​ being ​ ​ visible ​ ​ at ​ ​ all ​ ​ to ​ ​ those ​ ​ affected . 152 AI ​ ​ Reflects ​ ​ Its ​ ​ Origins The ​ ​ U.S. ​ ​ military ​ ​ has ​ ​ been ​ ​ one ​ ​ of ​ ​ the ​ ​ single ​ ​ most ​ ​ influential ​ ​ institutions ​ ​ in ​ ​ shaping ​ ​ modern AI , ​ ​ with ​ ​ DARPA ’ s ​ ​ funding ​ ​ of ​ ​ AI ​ ​ being ​ ​ among ​ ​ the ​ ​ most ​ ​ visible . ​ ​ Indeed , ​ ​ AI ​ ​ has ​ ​ historically 153 been ​ ​ shaped ​ ​ largely ​ ​ by ​ ​ military ​ ​ goals , ​ ​ with ​ ​ its ​ ​ capabilities ​ ​ and ​ ​ incentives ​ ​ defined ​ ​ by ​ ​ military objectives ​ ​ and ​ ​ desires . ​ ​ AI ​ ​ development ​ ​ continues ​ ​ to ​ ​ be ​ ​ supported ​ ​ by ​ ​ DARPA ​ ​ and ​ ​ other 154 national ​ ​ defense ​ ​ agencies , ​ ​ particularly ​ ​ in ​ ​ the ​ ​ area ​ ​ of ​ ​ lethal ​ ​ autonomous ​ ​ weapons ​ ​ systems , as ​ ​ discussed ​ ​ above . However , ​ ​ current ​ ​ research ​ ​ into ​ ​ AI ​ ​ technology ​ ​ is ​ ​ highly ​ ​ industry-driven , ​ ​ with ​ ​ proprietary systems ​ ​ supplementing ​ ​ military-funded ​ ​ classified ​ ​ systems ​ ​ and ​ ​ AI ​ ​ research ​ ​ increasingly 147 Sue ​ ​ Newell ​ ​ and ​ ​ Marco ​ ​ Marabelli , ​ ​ `` Strategic ​ ​ opportunities ​ ​ ( and ​ ​ challenges ) ​ ​ of ​ ​ algorithmic ​ ​ decision-making : ​ ​ A ​ ​ call ​ ​ for ​ ​ action on ​ ​ the ​ ​ long-term ​ ​ societal ​ ​ effects ​ ​ of ​ ​ ‘ datification ’ , '' ​ ​​ The ​ ​ Journal ​ ​ of ​ ​ Strategic ​ ​ Information ​ ​ Systems ​ ​ ​ 24 , ​ ​ No . ​ ​ 1 ​ ​ ( 2015 ) : ​ ​ 3-14 . 148 Darren ​ ​ Davidson , ​ ​ “ Facebook ​ ​ targets ​ ​ ‘ insecure ’ ​ ​ young ​ ​ people , ” ​ ​ The ​ ​ Australian , ​ ​ May ​ ​ 1 , ​ ​ 2017 . 149 Hannes ​ ​ Grassegger ​ ​ and ​ ​ Mikael ​ ​ Krogerus , ​ ​ `` TheData ​ ​ That ​ ​ Turned ​ ​ the ​ ​ World ​ ​ Upside ​ ​ Down , '' ​ ​​ Motherboard ​ , ​ ​ 2017 , https : //publicpolicy.stanford.edu/news/data-turned-world-upside-down ​ . 150 ​ ​ Chengcheng ​ ​ Shao , ​ ​ Giovanni ​ ​ Luca ​ ​ Ciampaglia , ​ ​ Alessandro ​ ​ Flammini ​ ​ and ​ ​ Filippo ​ ​ Menczer , ​ ​ `` Hoaxy : ​ ​ A ​ ​ platform ​ ​ for ​ ​ tracking online ​ ​ misinformation , '' ​ ​ In ​ ​​ Proceedings ​ ​ of ​ ​ the ​ ​ 25th ​ ​ International ​ ​ Conference ​ ​ Companion ​ ​ on ​ ​ World ​ ​ Wide ​ ​ Web ​ , ​ ​ pp . ​ ​ 745-750. International ​ ​ World ​ ​ Wide ​ ​ Web ​ ​ Conferences ​ ​ Steering ​ ​ Committee , ​ ​ 2016 . 151 ​ ​​ ​​ Simon ​ ​ Adler , ​ ​ “ Breaking ​ ​ News , ” ​ ​ Radiolab , ​ ​ July ​ ​ 27 , ​ ​ 2017 , ​ ​​ http : //www.radiolab.org/story/breaking-news/ ​ . 152 Kate ​ ​ Crawford ​ ​ and ​ ​ Meredith ​ ​ Whittaker , ​ ​ “ Artificial ​ ​ Intelligence ​ ​ is ​ ​ Hard ​ ​ to ​ ​ See , ” ​ ​​ Medium ​ , ​ ​ September ​ ​ 11 , ​ ​ 2016 , ​ ​ : https : //medium.com/ @ katecrawford/artificial-intelligence-is-hard-to-see-a71e74f386db ​ . 153 Sidney ​ ​ G ​ ​ Reed , ​ ​ Richard ​ ​ H. ​ ​ Van ​ ​ Atta ​ ​ and ​ ​ Seymour ​ ​ J . ​ ​ Dietman , ​ ​ “ DARPA ​ ​ Technical ​ ​ Accomplishments : ​ ​ An ​ ​ Historical ​ ​ Review ​ ​ of Selected ​ ​ DARPA ​ ​ Projects , ” ​ ​ Defense ​ ​ Advanced ​ ​ Research ​ ​ Projects ​ ​ Agency , ​ ​ Vol . ​ ​ 1 , ​ ​ 1990 . Sidney ​ ​ G ​ ​ Reed , ​ ​ Richard ​ ​ H. ​ ​ Van ​ ​ Atta ​ ​ and ​ ​ Seymour ​ ​ J . ​ ​ Dietman , ​ ​ “ DARPA ​ ​ Technical ​ ​ Accomplishments : ​ ​ An ​ ​ Historical ​ ​ Review ​ ​ of Selected ​ ​ DARPA ​ ​ Projects , ” ​ ​ Defense ​ ​ Advanced ​ ​ Research ​ ​ Projects ​ ​ Agency , ​ ​ Vol . ​ ​ 2 , ​ ​ 1991 . 154 Alex ​ ​ Roland ​ ​ and ​ ​ Philip ​ ​ Shiman , ​ ​​ Strategic ​ ​ computing : ​ ​ DARPA ​ ​ and ​ ​ the ​ ​ quest ​ ​ for ​ ​ machine ​ ​ intelligence , ​ ​ 1983-1993 ​ , ​ ​ ( MIT ​ ​ Press , 2002 ) . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 32 taking ​ ​ place ​ ​ in ​ ​ closed-door ​ ​ industry ​ ​ settings , ​ ​ often ​ ​ without ​ ​ peer ​ ​ review ​ ​ or ​ ​ oversight . Accordingly , ​ ​ user ​ ​ consent , ​ ​ privacy ​ ​ and ​ ​ transparency ​ ​ are ​ ​ often ​ ​ overlooked ​ ​ in ​ ​ favor ​ ​ of frictionless ​ ​ functionality ​ ​ that ​ ​ supports ​ ​ profit-driven ​ ​ business ​ ​ models ​ ​ based ​ ​ on ​ ​ aggregated data ​ ​ profiles . ​ ​ While ​ ​ there ​ ​ are ​ ​ those ​ ​ advocating ​ ​ for ​ ​ clearer ​ ​ laws ​ ​ and ​ ​ policies , ​ ​ the 155 ambiguous ​ ​ space ​ ​ in ​ ​ which ​ ​ information ​ ​ rights ​ ​ are ​ ​ governed ​ ​ does ​ ​ not ​ ​ clearly ​ ​ regulate ​ ​ in favor ​ ​ of ​ ​ individual ​ ​ control ​ ​ over ​ ​ personal ​ ​ technologies ​ ​ or ​ ​ online ​ ​ services . 156 The ​ ​ make ​ ​ up ​ ​ of ​ ​ AI ​ ​ researchers ​ ​ – ​ ​ what ​ ​ is ​ ​ and ​ ​ is ​ ​ not ​ ​ considered ​ ​ “ AI ​ ​ research ” ​ ​ – ​ ​ also ​ ​ has ​ ​ a history ​ ​ which ​ ​ influences ​ ​ the ​ ​ current ​ ​ state ​ ​ of ​ ​ AI ​ ​ and ​ ​ its ​ ​ ethical ​ ​ parameters . ​ ​ Beginning ​ ​ with the ​ ​ Dartmouth ​ ​ Conference ​ ​ in ​ ​ 1956 , ​ ​ AI ​ ​ researchers ​ ​ established ​ ​ a ​ ​ male-dominated , narrowly-defined ​ ​ community . ​ ​ The ​ ​ boundaries ​ ​ of ​ ​ participation ​ ​ in ​ ​ the ​ ​ AI ​ ​ community ​ ​ were relatively ​ ​ closed , ​ ​ and ​ ​ privileged ​ ​ mathematics , ​ ​ computer ​ ​ science ​ ​ and ​ ​ engineering ​ ​ over perspectives ​ ​ that ​ ​ would ​ ​ provide ​ ​ for ​ ​ a ​ ​ more ​ ​ rigorous ​ ​ discussion ​ ​ of ​ ​ AI ’ s ​ ​ ethical ​ ​ implications . ​ ​ Producing ​ ​ technologies ​ ​ that ​ ​ work ​ ​ within ​ ​ complex ​ ​ social ​ ​ realities ​ ​ and ​ ​ existing ​ ​ systems 157 requires ​ ​ understanding ​ ​ social , ​ ​ legal ​ ​ and ​ ​ ethical ​ ​ contexts , ​ ​ which ​ ​ can ​ ​ only ​ ​ be ​ ​ done ​ ​ by incorporating ​ ​ diverse ​ ​ perspectives ​ ​ and ​ ​ disciplinary ​ ​ expertise . Ethical ​ ​ Codes While ​ ​ decades ​ ​ of ​ ​ AI ​ ​ research ​ ​ have ​ ​ cited ​ ​ Asimov ’ s ​ ​ three ​ ​ laws ​ ​ of ​ ​ robotics , ​ ​ and ​ ​ some 158 applied ​ ​ AI ​ ​ systems ​ ​ have ​ ​ been ​ ​ designed ​ ​ to ​ ​ comply ​ ​ with ​ ​ biomedical ​ ​ ethics , ​ ​ the ​ ​ tools ​ ​ that 159 have ​ ​ been ​ ​ available ​ ​ to ​ ​ developers ​ ​ to ​ ​ contend ​ ​ with ​ ​ social ​ ​ and ​ ​ ethical ​ ​ questions ​ ​ have ​ ​ been relatively ​ ​ limited . ​ ​ Ethical ​ ​ codes ​ ​ are ​ ​ gradually ​ ​ being ​ ​ developed ​ ​ in ​ ​ the ​ ​ AI ​ ​ research ​ ​ space , ​ ​ as we ​ ​ discuss ​ ​ below , ​ ​ but ​ ​ they ​ ​ are ​ ​ necessarily ​ ​ incomplete : ​ ​ they ​ ​ will ​ ​ always ​ ​ need ​ ​ to ​ ​ evolve ​ ​ in ways ​ ​ that ​ ​ are ​ ​ sensitive ​ ​ to ​ ​ the ​ ​ rapidly ​ ​ changing ​ ​ contexts ​ ​ and ​ ​ conditions ​ ​ in ​ ​ which ​ ​ AI ​ ​ systems are ​ ​ deployed . ​ ​ These ​ ​ codes ​ ​ constitute ​ ​ one ​ ​ form ​ ​ of ​ ​ soft ​ ​ governance , ​ ​ where ​ ​ industry standards ​ ​ and ​ ​ technical ​ ​ practices ​ ​ serve ​ ​ as ​ ​ alternatives ​ ​ to ​ ​ more ​ ​ traditional ​ ​ “ hard ” ​ ​ forms ​ ​ of government ​ ​ regulation ​ ​ and ​ ​ legal ​ ​ oversight ​ ​ of ​ ​ AI . ​ ​​ ​ As ​ ​ AI ​ ​ systems ​ ​ are ​ ​ woven ​ ​ through ​ ​ a growing ​ ​ number ​ ​ of ​ ​ domains , ​ ​ the ​ ​ needs ​ ​ for ​ ​ such ​ ​ a ​ ​ contextually-anchored ​ ​ approach ​ ​ to ethics ​ ​ and ​ ​ governance ​ ​ only ​ ​ grows . 160 Two ​ ​ related ​ ​ problems ​ ​ have ​ ​ emerged : ​ ​ there ​ ​ is ​ ​ no ​ ​ tracking ​ ​ of ​ ​ adherence ​ ​ to ​ ​ ethical guidelines ​ ​ or ​ ​ soft ​ ​ governance ​ ​ standards ​ ​ in ​ ​ the ​ ​ AI ​ ​ industry , ​ ​ and ​ ​ we ​ ​ have ​ ​ not ​ ​ developed ways ​ ​ to ​ ​ link ​ ​ the ​ ​ adherence ​ ​ to ​ ​ ethical ​ ​ guidelines ​ ​ to ​ ​ the ​ ​ ultimate ​ ​ impact ​ ​ of ​ ​ an ​ ​ AI ​ ​ systems ​ ​ in 155 Elaine ​ ​ Sedenberg ​ ​ and ​ ​ Ann ​ ​ Lauren ​ ​ Hoffmann , ​ ​ “ Recovering ​ ​ the ​ ​ History ​ ​ of ​ ​ Informed ​ ​ Consent ​ ​ for ​ ​ Data ​ ​ Science ​ ​ and ​ ​ Internet Industry ​ ​ Research ​ ​ Ethics ” ​ ​ ( September ​ ​ 12 , ​ ​ 2016 ) . ​ ​ Available ​ ​ at ​ ​ SSRN : ​ ​​ https : //ssrn.com/abstract=2837585 ​ . 156 United ​ ​ States ​ ​ ( 2016 ) ​ ​ Executive ​ ​ Office ​ ​ of ​ ​ the ​ ​ President ​ ​ and ​ ​ Jason ​ ​ Furman , ​ ​ John ​ ​ P. ​ ​ Holdren , ​ ​ Cecilia ​ ​ Muñoz , ​ ​ Megan ​ ​ Smith ​ ​ and Jeffery ​ ​ Zients , ​ ​ “ Artificial ​ ​ Intelligence , ​ ​ Automation , ​ ​ and ​ ​ the ​ ​ Economy , ” ​ ​ Technical ​ ​ report , ​ ​ National ​ ​ Science ​ ​ and ​ ​ Technology Council , ​ ​ Washington ​ ​ D.C. ​ ​ 20502 , ​ ​ October ​ ​ 2016 , https : //obamawhitehouse.archives.gov/sites/whitehouse.gov/files/documents/Artificial-Intelligence-Automation-Economy . PDF ​ . 157 Nils ​ ​ J ​ ​ Nilsson , ​ ​​ The ​ ​ quest ​ ​ for ​ ​ artificial ​ ​ intelligence ​ , ​ ​ ( Cambridge ​ ​ University ​ ​ Press , ​ ​ 2009 ) . 158 Susan ​ ​ Leigh ​ ​ Anderson , ​ ​ `` Asimov ’ s ​ ​ “ three ​ ​ laws ​ ​ of ​ ​ robotics ” ​ ​ and ​ ​ machine ​ ​ metaethics , '' ​ ​​ Ai ​ ​ & ​ ​ Society ​ ​ ​ 22 , ​ ​ No . ​ ​ 4 ​ ​ ( 2008 ) : ​ ​ 477-493 . 159 Raymond ​ ​ Heatherly , ​ ​ `` Privacy ​ ​ and ​ ​ security ​ ​ within ​ ​ biobanking : ​ ​ The ​ ​ role ​ ​ of ​ ​ information ​ ​ technology , '' ​ ​​ The ​ ​ Journal ​ ​ of ​ ​ Law , Medicine ​ ​ & ​ ​ Ethics ​ ​ ​ 44 , ​ ​ No . ​ ​ 1 ​ ​ ( 2016 ) : ​ ​ 156-160 . 160 Robert ​ ​ Rosenberger , ​ ​ `` Phenomenological ​ ​ Approaches ​ ​ to ​ ​ Technological ​ ​ Ethics , '' ​ ​​ The ​ ​ Ethics ​ ​ of ​ ​ Technology : ​ ​ Methods ​ ​ and Approaches ​ ​ ​ ( 2017 ) : ​ ​ 67 . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 33 the ​ ​ world . Examples ​ ​ of ​ ​ intertwined ​ ​ practice ​ ​ and ​ ​ ethics ​ ​ can ​ ​ be ​ ​ found ​ ​ in ​ ​ the ​ ​ biomedical ​ ​ uses ​ ​ of ​ ​ AI . Bioethics ​ ​ already ​ ​ offers ​ ​ a ​ ​ series ​ ​ of ​ ​ standards , ​ ​ values ​ ​ and ​ ​ procedures , ​ ​ along ​ ​ with 161 enforcement ​ ​ and ​ ​ accountability ​ ​ mechanisms . ​ ​ But ​ ​ how ​ ​ these ​ ​ should ​ ​ apply ​ ​ to ​ ​ medical ​ ​ AI systems ​ ​ is ​ ​ often ​ ​ unclear , ​ ​ and ​ ​ researchers ​ ​ have ​ ​ been ​ ​ tracking ​ ​ the ​ ​ disparities . ​ ​ This ​ ​ is ​ ​ also 162 true ​ ​ of ​ ​ privacy ​ ​ requirements , ​ ​ which , ​ ​ given ​ ​ modern ​ ​ AI ’ s ​ ​ capability ​ ​ to ​ ​ make ​ ​ very ​ ​ personal inferences ​ ​ given ​ ​ only ​ ​ limited ​ ​ data , ​ ​ are ​ ​ increasingly ​ ​ insufficient . ​ ​ Where ​ ​ ethical ​ ​ standards 163 aimed ​ ​ at ​ ​ protecting ​ ​ patient ​ ​ privacy ​ ​ have ​ ​ been ​ ​ proposed , ​ ​ some ​ ​ biomedical ​ ​ researchers have ​ ​ rejected ​ ​ them , ​ ​ seeing ​ ​ them ​ ​ as ​ ​ an ​ ​ impediment ​ ​ to ​ ​ innovation . 164 A ​ ​ more ​ ​ intentional ​ ​ approach ​ ​ to ​ ​ ethics ​ ​ is ​ ​ needed , ​ ​ and ​ ​ some ​ ​ are ​ ​ working ​ ​ toward ​ ​ this . Teaching ​ ​ ethics ​ ​ to ​ ​ practitioners ​ ​ is ​ ​ one ​ ​ such ​ ​ example . ​ ​ The ​ ​ Blue ​ ​ Sky ​ ​ Agenda ​ ​ for ​ ​ AI 165 Education , ​ ​ a ​ ​ collection ​ ​ of ​ ​ ideas ​ ​ for ​ ​ ethics ​ ​ education ​ ​ in ​ ​ AI , ​ ​ seeks ​ ​ democratization ​ ​ of ​ ​ AI education ​ ​ and ​ ​ emphasizes ​ ​ inclusiveness ​ ​ in ​ ​ development ​ ​ toward ​ ​ the ​ ​ goal ​ ​ of ​ ​ respecting ​ ​ the values ​ ​ and ​ ​ rights ​ ​ of ​ ​ diverse ​ ​ populations . ​ ​ But ​ ​ education ​ ​ is ​ ​ not ​ ​ enough . ​ ​ Opportunities 166 must ​ ​ open ​ ​ up ​ ​ for ​ ​ ethics ​ ​ to ​ ​ be ​ ​ integrated ​ ​ in ​ ​ early ​ ​ stage ​ ​ design , ​ ​ and ​ ​ incentives ​ ​ for ​ ​ designing and ​ ​ implementing ​ ​ AI ​ ​ ethically ​ ​ must ​ ​ be ​ ​ built ​ ​ into ​ ​ the ​ ​ companies ​ ​ and ​ ​ institutions ​ ​ currently driving ​ ​ development . Ethical ​ ​ values ​ ​ and ​ ​ norms ​ ​ around ​ ​ accountability , ​ ​ social ​ ​ and ​ ​ political ​ ​ responsibility , 167 inclusion ​ ​ and ​ ​ connectivity , ​ ​ legibility ​ ​ and ​ ​ security ​ ​ and ​ ​ privacy ​ ​ are ​ ​ embedded ​ ​ in ​ ​ every 168 169 system ​ ​ via ​ ​ their ​ ​ default ​ ​ settings , ​ ​ whether ​ ​ intentionally ​ ​ or ​ ​ not . ​ ​ Often , ​ ​ these 170 invisibly-embedded ​ ​ values ​ ​ reflect ​ ​ the ​ ​ status ​ ​ quo , ​ ​ the ​ ​ context ​ ​ and ​ ​ interests ​ ​ of ​ ​ their developers , ​ ​ and ​ ​ matters ​ ​ of ​ ​ convenience ​ ​ and ​ ​ profit . ​ ​ Once ​ ​ set , ​ ​ these ​ ​ implicit ​ ​ values ​ ​ are ​ ​ hard to ​ ​ change ​ ​ for ​ ​ a ​ ​ variety ​ ​ of ​ ​ reasons , ​ ​ even ​ ​ as ​ ​ they ​ ​ tend ​ ​ to ​ ​ shape ​ ​ the ​ ​ capabilities ​ ​ and ​ ​ roles 171 of ​ ​ systems ​ ​ within ​ ​ various ​ ​ lived ​ ​ contexts . ​ ​ Ethical ​ ​ codes ​ ​ should ​ ​ work ​ ​ to ​ ​ ensure ​ ​ that ​ ​ these 172 161 Stephen ​ ​ Brotherton , ​ ​ Audiey ​ ​ Kao , ​ ​ and ​ ​ B . ​ ​ J . ​ ​ Crigger , ​ ​ `` Professing ​ ​ the ​ ​ values ​ ​ of ​ ​ medicine : ​ ​ the ​ ​ modernized ​ ​ AMA ​ ​ Code ​ ​ of Medical ​ ​ Ethics , '' ​ ​​ JAMA ​ ​ ​ 316 , ​ ​ No . ​ ​ 10 ​ ​ ( 2016 ) : ​ ​ 1041-1042 . 162 ​ ​​ ​​ Jake ​ ​ Metcalf ​ ​ and ​ ​ Kate ​ ​ Crawford , ​ ​ “ ​ Where ​ ​ are ​ ​ human ​ ​ subjects ​ ​ in ​ ​ big ​ ​ data ​ ​ research ? ​ ​ The ​ ​ emerging ​ ​ ethics ​ ​ divide , '' ​ ​​ Big ​ ​ Data ​ ​ & Society ​ ​ ​ 3.1 ​ ​ ( 2016 ) : ​ ​​ http : //journals.sagepub.com/doi/pdf/10.1177/2053951716650211 163 Stephen ​ ​ J ​ ​ Tipton , ​ ​ Sara ​ ​ Forkey ​ ​ and ​ ​ Young ​ ​ B ​ ​ Choi , ​ ​ `` Toward ​ ​ Proper ​ ​ Authentication ​ ​ Methods ​ ​ in ​ ​ Electronic ​ ​ Medical ​ ​ Record Access ​ ​ Compliant ​ ​ to ​ ​ HIPAA ​ ​ and ​ ​ CIA ​ ​ Triangle , '' ​ ​​ Journal ​ ​ of ​ ​ medical ​ ​ systems ​ ​ ​ 40 , ​ ​ No . ​ ​ 4 ​ ​ ( 2016 ) : ​ ​ 1-8 . 164 Wendy ​ ​ Lipworth ​ ​ and ​ ​ Renata ​ ​ Axler , ​ ​ `` Towards ​ ​ a ​ ​ bioethics ​ ​ of ​ ​ innovation , '' ​ ​​ Journal ​ ​ of ​ ​ medical ​ ​ ethics ​ ​ ​ ( 2016 ) : ​ ​ medethics-2015 . 165 Judy ​ ​ Goldsmith ​ ​ and ​ ​ Emanuelle ​ ​ Burton , ​ ​ `` Why ​ ​ Teaching ​ ​ Ethics ​ ​ to ​ ​ AI ​ ​ Practitioners ​ ​ Is ​ ​ Important , '' ​ ​ ( 2017 ) . 166 Eric ​ ​ Eaton , ​ ​ Sven ​ ​ Koenig , ​ ​ Claudia ​ ​ Schulz , ​ ​ Francesco ​ ​ Maurelli , ​ ​ John ​ ​ Lee , ​ ​ Joshua ​ ​ Eckroth , ​ ​ Mark ​ ​ Crowley , ​ ​ Richard ​ ​ G. ​ ​ Freedman , Rogelio ​ ​ E. ​ ​ Cardona-Rivera , ​ ​ Tiago ​ ​ Machado ​ ​ and ​ ​ Tom ​ ​ Williams , ​ ​ `` Blue ​ ​ Sky ​ ​ Ideas ​ ​ in ​ ​ Artificial ​ ​ Intelligence ​ ​ Education ​ ​ from ​ ​ the EAAI ​ ​ 2017 ​ ​ New ​ ​ and ​ ​ Future ​ ​ AI ​ ​ Educator ​ ​ Program , '' ​ ​​ arXiv ​ ​ preprint ​ ​ arXiv:1702.00137 ​ ​ ​ ( 2017 ) . 167 Wendell ​ ​ Wallach , ​ ​​ A ​ ​ Dangerous ​ ​ Master : ​ ​ How ​ ​ to ​ ​ keep ​ ​ technology ​ ​ from ​ ​ slipping ​ ​ beyond ​ ​ our ​ ​ control ​ , ​ ​ ( Basic ​ ​ Books , ​ ​ 2014 ) . 168 Anna ​ ​ Lauren ​ ​ Hoffmann , ​ ​ Nicholas ​ ​ Proferes ​ ​ and ​ ​ Michael ​ ​ Zimmer , ​ ​ `` “ Making ​ ​ the ​ ​ world ​ ​ more ​ ​ open ​ ​ and ​ ​ connected ” : ​ ​ Mark Zuckerberg ​ ​ and ​ ​ the ​ ​ discursive ​ ​ construction ​ ​ of ​ ​ Facebook ​ ​ and ​ ​ its ​ ​ users . '' ​ ​​ new ​ ​ media ​ ​ & ​ ​ society ​ ​ ​ ( 2016 ) : ​ ​ 1461444816660784 . 169 John ​ ​ Wilbanks , ​ ​ `` Public ​ ​ domain , ​ ​ copyright ​ ​ licenses ​ ​ and ​ ​ the ​ ​ freedom ​ ​ to ​ ​ integrate ​ ​ science , '' ​ ​​ Public ​ ​ Communication ​ ​ of ​ ​ Science and ​ ​ Technology ​ ​​ 25 ​ ​ ( 2016 ) : ​ ​ 11 . 170 Ian ​ ​ Kerr , ​ ​ `` The ​ ​ Devil ​ ​ is ​ ​ in ​ ​ the ​ ​ Defaults , '' ​ ​​ Critical ​ ​ Analysis ​ ​ of ​ ​ Law ​ ​ ​ 4 , ​ ​ No . ​ ​ 1 ​ ​ ( 2017 ) . 171 Dylan ​ ​ Wesley ​ ​ Mulvin , ​ ​ `` Embedded ​ ​ dangers : ​ ​ The ​ ​ history ​ ​ of ​ ​ the ​ ​ year ​ ​ 2000 ​ ​ problem ​ ​ and ​ ​ the ​ ​ politics ​ ​ of ​ ​ technological ​ ​ repair , '' A ​ oIR ​ ​ Selected ​ ​ Papers ​ ​ of ​ ​ Internet ​ ​ Research ​ ​ ​ 6 ​ ​ ( 2017 ) . 172 Narendra ​ ​ Kumar , ​ ​ Nidhi ​ ​ Kharkwal , ​ ​ Rashi ​ ​ Kohli ​ ​ and ​ ​ Shakeeluddin ​ ​ Choudhary , ​ ​ `` Ethical ​ ​ aspects ​ ​ and ​ ​ future ​ ​ of ​ ​ artificial intelligence , '' ​ ​ ( In ​ ​​ Innovation ​ ​ and ​ ​ Challenges ​ ​ in ​ ​ Cyber ​ ​ Security ​ ​ ( ICICCS-INBUSH ) , ​ ​ 2016 ​ ​ International ​ ​ Conference ​ ​ on ​ , ​ ​ pp . 111-114 ) , ​ ​ IEEE , ​ ​ 2016 . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 34 values ​ ​ are ​ ​ expressly ​ ​ designed ​ ​ into ​ ​ AI ​ ​ systems ​ ​ through ​ ​ processes ​ ​ of ​ ​ open ​ ​ and well-documented ​ ​ decision-making ​ ​ that ​ ​ center ​ ​ the ​ ​ populations ​ ​ who ​ ​ will ​ ​ be ​ ​ most ​ ​ affected . While ​ ​ nascent , ​ ​ efforts ​ ​ to ​ ​ address ​ ​ these ​ ​ concerns ​ ​ have ​ ​ emerged ​ ​ in ​ ​ recent ​ ​ years . ​ ​ A ​ ​ series ​ ​ of White ​ ​ House ​ ​ reports ​ ​ under ​ ​ President ​ ​ Obama ​ ​ examined ​ ​ tensions ​ ​ between ​ ​ social ​ ​ interests and ​ ​ ethical ​ ​ values ​ ​ on ​ ​ one ​ ​ hand , ​ ​ and ​ ​ business ​ ​ and ​ ​ industry ​ ​ objectives ​ ​ on ​ ​ the ​ ​ other . 173 Recent ​ ​ soft ​ ​ governance ​ ​ efforts ​ ​ from ​ ​ IEEE , ​ ​ The ​ ​ Future ​ ​ of ​ ​ LIfe ​ ​ Institute , ​ ​ ​ the ​ ​ ACM ​ ​ and 174 175 176 the ​ ​ Oxford ​ ​ Internet ​ ​ Institute ​ ​ have ​ ​ produced ​ ​ principles ​ ​ and ​ ​ codes ​ ​ of ​ ​ ethics ​ ​ for ​ ​ AI . 177 Perspectives ​ ​ from ​ ​ diverse ​ ​ industry ​ ​ and ​ ​ intellectual ​ ​ leaders ​ ​ are ​ ​ often ​ ​ reflected ​ ​ in ​ ​ these documents . ​ ​ While ​ ​ these ​ ​ are ​ ​ positive ​ ​ steps , ​ ​ they ​ ​ have ​ ​ real ​ ​ limitations . ​ ​ Key ​ ​ among ​ ​ these ​ ​ is that ​ ​ they ​ ​ share ​ ​ an ​ ​ assumption ​ ​ that ​ ​ industry ​ ​ will ​ ​ voluntarily ​ ​ begin ​ ​ to ​ ​ adopt ​ ​ their approaches . ​ ​ They ​ ​ rarely ​ ​ mention ​ ​ the ​ ​ power ​ ​ asymmetries ​ ​ that ​ ​ complicate ​ ​ and ​ ​ underlie terms ​ ​ like ​ ​ “ social ​ ​ good , ” ​ ​ and ​ ​ the ​ ​ means ​ ​ by ​ ​ which ​ ​ such ​ ​ a ​ ​ term ​ ​ would ​ ​ be ​ ​ defined ​ ​ and measured . ​ ​ The ​ ​ codes ​ ​ are ​ ​ necessarily ​ ​ limited ​ ​ in ​ ​ what ​ ​ they ​ ​ address , ​ ​ how ​ ​ much ​ ​ insider information ​ ​ they ​ ​ have ​ ​ access ​ ​ to ​ ​ and ​ ​ what ​ ​ mechanisms ​ ​ would ​ ​ be ​ ​ used ​ ​ for ​ ​ monitoring ​ ​ and enforcement . ​ ​ ​ While ​ ​ these ​ ​ efforts ​ ​ set ​ ​ moral ​ ​ precedents ​ ​ and ​ ​ start ​ ​ conversations , ​ ​ ​ they 178 179 provide ​ ​ little ​ ​ to ​ ​ help ​ ​​ ​ practitioners ​ ​ in ​ ​ navigating ​ ​ daily ​ ​ ethical ​ ​ problems ​ ​ in ​ ​ practice ​ ​ or ​ ​ in 180 diagnosing ​ ​ ethical ​ ​ harms , ​ ​ and ​ ​ do ​ ​ little ​ ​ to ​ ​ directly ​ ​ change ​ ​ ethics ​ ​ in ​ ​ the ​ ​ design ​ ​ and ​ ​ use ​ ​ of 181 AI . 182 Challenges ​ ​ and ​ ​ Concerns ​ ​ Going ​ ​ Forward Current ​ ​ framings ​ ​ of ​ ​ AI ​ ​ ethics ​ ​ are ​ ​ failing ​ ​ partly ​ ​ because ​ ​ they ​ ​ rely ​ ​ on ​ ​ individual ​ ​ responsibility , placing ​ ​ the ​ ​ onus ​ ​ of ​ ​ appropriate ​ ​ information ​ ​ flow ​ ​ with ​ ​ users ​ ​ and ​ ​ concentrating decision-making ​ ​ power ​ ​ in ​ ​ individual ​ ​ AI ​ ​ developers ​ ​ and ​ ​ designers . ​ ​ In ​ ​ order ​ ​ to ​ ​ achieve 183 ethical ​ ​ AI ​ ​ systems ​ ​ in ​ ​ which ​ ​ their ​ ​ wider ​ ​ implications ​ ​ are ​ ​ addressed , ​ ​ there ​ ​ must ​ ​ be 173 United ​ ​ States ​ ​ ( 2016 ) ​ ​ Executive ​ ​ Office ​ ​ of ​ ​ the ​ ​ President ​ ​ and ​ ​ M. ​ ​ Holden , ​ ​ J.P. ​ ​ and ​ ​ Smith . ​ ​ “ Preparing ​ ​ for ​ ​ the ​ ​ future ​ ​ of ​ ​ artificial intelligence , ” ​ ​ Technical ​ ​ report , ​ ​ National ​ ​ Science ​ ​ and ​ ​ Technology ​ ​ Council , ​ ​ Washington ​ ​ D.C. ​ ​ 20502 , ​ ​ October ​ ​ 2016 , https : //obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_futu re_of_ai.pdf ​ . 174 IEEE , ​ ​ “ Ethically ​ ​ Aligned ​ ​ Design : ​ ​ A ​ ​ Vision ​ ​ for ​ ​ Prioritizing ​ ​ Human ​ ​ Wellbeing ​ ​ with ​ ​ Artificial ​ ​ Intelligence ​ ​ and ​ ​ Autonomous Systems , ” ​ ​ The ​ ​ IEEE ​ ​ Global ​ ​ Initiative ​ ​ for ​ ​ Ethical ​ ​ Considerations ​ ​ in ​ ​ Artificial ​ ​ Intelligence ​ ​ and ​ ​ Autonomous ​ ​ Systems , ​ ​ December 13 , ​ ​ 2016 , ​ ​​ http : //standards.ieee.org/develop/indconn/ec/ead_v1.pdf ​ . 175 “ Asilomar ​ ​ AI ​ ​ Principles , ” ​ ​ 2017 , ​ ​ https : //futureoflife.org/ai-principles/ . 176 ACM , ​ ​ “ Code ​ ​ 2018 , ” ​ ​ version ​ ​ 2 , ​ ​ 2017 , ​ ​ https : //ethics.acm.org/2018-code-draft-2/ . 177 Mike ​ ​ Wooldridge , ​ ​ Peter ​ ​ Millican , ​ ​ and ​ ​ Paula ​ ​ Boddington , ​ ​ “ Towards ​ ​ a ​ ​ Code ​ ​ of ​ ​ Ethics ​ ​ for ​ ​ Artificial ​ ​ Intelligence ​ ​ Research , ” Oxford , ​ ​ 2017 ​ ​​ https : //www.cs.ox.ac.uk/efai/towards-a-code-of-ethics-for-artificial-intelligence/ ​ . 178 Bo ​ ​ Brinkman , ​ ​ Catherine ​ ​ Flick , ​ ​ Don ​ ​ Gotterbarn , ​ ​ Keith ​ ​ Miller , ​ ​ Kate ​ ​ Vazansky ​ ​ and ​ ​ Marty ​ ​ J . ​ ​ Wolf , ​ ​ `` Listening ​ ​ to ​ ​ professional voices : ​ ​ draft ​ ​ 2 ​ ​ of ​ ​ the ​ ​ ACM ​ ​ code ​ ​ of ​ ​ ethics ​ ​ and ​ ​ professional ​ ​ conduct , '' ​ ​​ Communications ​ ​ of ​ ​ the ​ ​ ACM ​ ​ ​ 60 , ​ ​ No . ​ ​ 5 ​ ​ ( 2017 ) : ​ ​ 105-111 . 179 Eugene ​ ​ Schlossberger , ​ ​ `` Engineering ​ ​ Codes ​ ​ of ​ ​ Ethics ​ ​ and ​ ​ the ​ ​ Duty ​ ​ to ​ ​ Set ​ ​ a ​ ​ Moral ​ ​ Precedent , '' ​ ​​ Science ​ ​ and ​ ​ engineering ​ ​ ethics 22 , ​ ​ No . ​ ​ 5 ​ ​ ( 2016 ) : ​ ​ 1333-1344 . 180 Stuart ​ ​ Ferguson , ​ ​ Clare ​ ​ Thornley ​ ​ and ​ ​ Forbes ​ ​ Gibb , ​ ​ `` Beyond ​ ​ codes ​ ​ of ​ ​ ethics : ​ ​ how ​ ​ library ​ ​ and ​ ​ information ​ ​ professionals navigate ​ ​ ethical ​ ​ dilemmas ​ ​ in ​ ​ a ​ ​ complex ​ ​ and ​ ​ dynamic ​ ​ information ​ ​ environment , '' ​ ​​ International ​ ​ Journal ​ ​ of ​ ​ Information Management ​ ​ ​ 36 , ​ ​ No . ​ ​ 4 ​ ​ ( 2016 ) : ​ ​ 543-556 . 181 Christian ​ ​ Sandvig , ​ ​ Kevin ​ ​ Hamilton , ​ ​ Karrie ​ ​ Karahalios ​ ​ and ​ ​ Cedric ​ ​ Langbort , ​ ​ `` Automation , ​ ​ Algorithms , ​ ​ and ​ ​ Politics| ​ ​ When ​ ​ the Algorithm ​ ​ Itself ​ ​ is ​ ​ a ​ ​ Racist : ​ ​ Diagnosing ​ ​ Ethical ​ ​ Harm ​ ​ in ​ ​ the ​ ​ Basic ​ ​ Components ​ ​ of ​ ​ Software , '' ​ ​​ International ​ ​ Journal ​ ​ of Communication ​ ​ ​ 10 ​ ​ ( 2016 ) : ​ ​ 19 . 182 Mike ​ ​ Ananny , ​ ​ `` Toward ​ ​ an ​ ​ ethics ​ ​ of ​ ​ algorithms : ​ ​ Convening , ​ ​ observation , ​ ​ probability , ​ ​ and ​ ​ timeliness , '' ​ ​​ Science , ​ ​ Technology , ​ ​ & Human ​ ​ Values ​ ​ ​ 41 , ​ ​ No . ​ ​ 1 ​ ​ ( 2016 ) : ​ ​ 93-117 . 183 Florencia ​ ​ Marotta-Wurgler , ​ ​ `` Self-Regulation ​ ​ and ​ ​ Competition ​ ​ in ​ ​ Privacy ​ ​ Policies . '' ​ ​​ The ​ ​ Journal ​ ​ of ​ ​ Legal ​ ​ Studies ​ ​ ​ 45 , ​ ​ No . ​ ​ S2 ( 2016 ) : ​ ​ S13-S39 . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 35 institutional ​ ​ changes ​ ​ to ​ ​ hold ​ ​ power ​ ​ accountable . ​ ​ Yet , ​ ​ there ​ ​ are ​ ​ obvious ​ ​ challenges ​ ​ to ​ ​ this 184 approach , ​ ​ such ​ ​ as ​ ​ disagreement ​ ​ about ​ ​ the ​ ​ risks ​ ​ of ​ ​ AI , ​ ​ the ​ ​ potential ​ ​ for ​ ​ greenwashing 185 ethical ​ ​ AI ​ ​ as ​ ​ a ​ ​ superficial ​ ​ marketing ​ ​ strategy ​ ​ rather ​ ​ than ​ ​ a ​ ​ substantive ​ ​ commitment , ​ ​ the 186 practical ​ ​ challenges ​ ​ of ​ ​ stopping ​ ​ unethical ​ ​ AI ​ ​ research ​ ​ designed ​ ​ to ​ ​ privilege ​ ​ the ​ ​ interests ​ ​ of a ​ ​ few ​ ​ over ​ ​ the ​ ​ many ​ ​ and ​ ​ the ​ ​ current ​ ​ economic ​ ​ system ​ ​ within ​ ​ which ​ ​ the ​ ​ incentives 187 driving ​ ​ AI ​ ​ development ​ ​ are ​ ​ embedded . ​ ​ In ​ ​ addition , ​ ​ the ​ ​ effective ​ ​ invisibility ​ ​ of ​ ​ many ​ ​ of these ​ ​ systems ​ ​ to ​ ​ the ​ ​ people ​ ​ on ​ ​ whom ​ ​ they ​ ​ act , ​ ​ the ​ ​ obscurity ​ ​ of ​ ​ their ​ ​ algorithmic mechanisms , ​ ​ the ​ ​ ambiguity ​ ​ of ​ ​ their ​ ​ origins ​ ​ and ​ ​ their ​ ​ inescapable ​ ​ pervasiveness ​ ​ make public ​ ​ discourse ​ ​ difficult ​ ​ and ​ ​ opting-out ​ ​ impossible . ​ ​ The ​ ​ responsibility ​ ​ to ​ ​ strive ​ ​ for ​ ​ better 188 outcomes ​ ​ thus ​ ​ falls ​ ​ squarely ​ ​ on ​ ​ creators ​ ​ and ​ ​ regulators , ​ ​ who ​ ​ are ​ ​ only ​ ​ beginning ​ ​ to establish ​ ​ dialogue ​ ​ even ​ ​ as ​ ​ there ​ ​ are ​ ​ few ​ ​ incentives ​ ​ for ​ ​ change ​ ​ and ​ ​ significant ​ ​ tension 189 between ​ ​ ethics ​ ​ and ​ ​ “ compliance. ” 190 This ​ ​ brings ​ ​ us ​ ​ to ​ ​ the ​ ​ wider ​ ​ political ​ ​ landscape ​ ​ in ​ ​ which ​ ​ AI ​ ​ is ​ ​ being ​ ​ created ​ ​ in ​ ​ the ​ ​ U.S. : ​ ​ how will ​ ​ the ​ ​ Trump ​ ​ administration ​ ​ affect ​ ​ the ​ ​ use ​ ​ of ​ ​ these ​ ​ technologies ? ​ ​ Prior ​ ​ to ​ ​ the ​ ​ election , over ​ ​ 100 ​ ​ technology ​ ​ sector ​ ​ leaders ​ ​ articulated ​ ​ their ​ ​ priorities : ​ ​ “ freedom ​ ​ of ​ ​ expression , openness ​ ​ to ​ ​ newcomers , ​ ​ equality ​ ​ of ​ ​ opportunity , ​ ​ public ​ ​ investments ​ ​ in ​ ​ research ​ ​ and infrastructure ​ ​ and ​ ​ respect ​ ​ for ​ ​ the ​ ​ rule ​ ​ of ​ ​ law . ​ ​ We ​ ​ embrace ​ ​ an ​ ​ optimistic ​ ​ vision ​ ​ for ​ ​ a ​ ​ more inclusive ​ ​ country ​ ​ where ​ ​ American ​ ​ innovation ​ ​ continues ​ ​ to ​ ​ fuel ​ ​ opportunity , ​ ​ prosperity ​ ​ and leadership. ” ​ ​ ​ President ​ ​ Trump ’ s ​ ​ policies ​ ​ do ​ ​ not ​ ​ reflect ​ ​ these ​ ​ priorities . ​ ​ Rather , ​ ​ there ​ ​ has 191 been ​ ​ significant ​ ​ defunding ​ ​ of ​ ​ research , ​ ​ an ​ ​ increase ​ ​ in ​ ​ deportations , ​ ​ and ​ ​ heightened screening ​ ​ of ​ ​ personal ​ ​ communications ​ ​ and ​ ​ social ​ ​ media ​ ​ at ​ ​ national ​ ​ borders , ​ ​ among ​ ​ many other ​ ​ concerning ​ ​ policy ​ ​ shifts . ​ ​ Simply ​ ​ put , ​ ​ it ​ ​ does ​ ​ not ​ ​ appear ​ ​ that ​ ​ the ​ ​ current administration ​ ​ can ​ ​ be ​ ​ counted ​ ​ on ​ ​ to ​ ​ support ​ ​ the ​ ​ creation ​ ​ and ​ ​ adoption ​ ​ of ​ ​ ethical frameworks ​ ​ for ​ ​ AI . 184 Ira ​ ​ S. ​ ​ Rubinstein , ​ ​ “ The ​ ​ Future ​ ​ of ​ ​ Self-Regulation ​ ​ is ​ ​ Co-Regulation ” ​ ​ ( October ​ ​ 5 , ​ ​ 2016 ) . ​ ​ The ​ ​ Cambridge ​ ​ Handbook ​ ​ of ​ ​ Consumer Privacy , ​ ​ From ​ ​ Cambridge ​ ​ University ​ ​ Press ​ ​ ( Forthcoming ) . ​ ​ Available ​ ​ at ​ ​ SSRN : ​ ​​ https : //ssrn.com/abstract=2848513 ​ . 185 Vincent ​ ​ C ​ ​ Müller , ​ ​ ed . ​ ​​ Risks ​ ​ of ​ ​ artificial ​ ​ intelligence ​ . ​ ​ CRC ​ ​ Press , ​ ​ 2016 . 186 Michael ​ ​ Stocker , ​ ​ `` Decision-making : ​ ​ Be ​ ​ wary ​ ​ of ​ ​ 'ethical ' ​ ​ artificial ​ ​ intelligence , '' ​ ​​ Nature ​ ​ ​ 540 , ​ ​ No . ​ ​ 7634 ​ ​ ( 2016 ) : ​ ​ 525-525 . 187 Federico ​ ​ Pistono ​ ​ and ​ ​ Roman ​ ​ V. ​ ​ Yampolskiy , ​ ​ `` Unethical ​ ​ Research : ​ ​ How ​ ​ to ​ ​ Create ​ ​ a ​ ​ Malevolent ​ ​ Artificial ​ ​ Intelligence , '' ​ ​​ arXiv preprint ​ ​ arXiv:1605.02817 ​ ​ ​ ( 2016 ) . 188 Jatin ​ ​ Borana , ​ ​ `` Applications ​ ​ of ​ ​ Artificial ​ ​ Intelligence ​ ​ & ​ ​ Associated ​ ​ Technologies . '' ​ ​​ Proceeding ​ ​ of ​ ​ International ​ ​ Conference ​ ​ on Emerging ​ ​ Technologies ​ ​ in ​ ​ Engineering , ​ ​ Biomedical , ​ ​ Management ​ ​ and ​ ​ Science ​ ​ [ ETEBMS-2016 ] ​ ​ ​ 5 ​ ​ ( 2016 ) : ​ ​ 64-67 ; ​ ​ Ira ​ ​ S. Rubinstein , ​ ​ `` Big ​ ​ Data : ​ ​ The ​ ​ End ​ ​ of ​ ​ Privacy ​ ​ or ​ ​ a ​ ​ New ​ ​ Beginning ? . '' ​ ​​ International ​ ​ Data ​ ​ Privacy ​ ​ Law ​ ​ ​ ( 2013 ) : ​ ​ ips036 ; ​ ​ Fred ​ ​ Turner . `` Can ​ ​ we ​ ​ write ​ ​ a ​ ​ cultural ​ ​ history ​ ​ of ​ ​ the ​ ​ Internet ? ​ ​ If ​ ​ so , ​ ​ how ? . '' ​ ​ Internet ​ ​ Histories ​ ​ 1 , ​ ​ No . ​ ​ 1-2 ​ ​ ( 2017 ) : ​ ​ 39-46 . 189 Kate ​ ​ Darling ​ ​ and ​ ​ Ryan ​ ​ Calo , ​ ​ `` Introduction ​ ​ to ​ ​ Journal ​ ​ of ​ ​ Human-Robot ​ ​ Interaction ​ ​ Special ​ ​ Issue ​ ​ on ​ ​ Law ​ ​ and ​ ​ Policy , '' ​ ​ Journal of ​ ​ Human-Robot ​ ​ Interaction ​ ​ 5 , ​ ​ No . ​ ​ 3 ​ ​ ( 2016 ) : ​ ​ 1-2 . 190 Kate ​ ​ Crawford ​ ​ and ​ ​ Ryan ​ ​ Calo , ​ ​ `` There ​ ​ is ​ ​ a ​ ​ blind ​ ​ spot ​ ​ in ​ ​ AI ​ ​ research , '' ​ ​​ Nature ​ ​ ​ 538 ​ ​ ( 2016 ) : ​ ​ 311-313 ; ​ ​ Gary ​ ​ E ​ ​ Merchant ​ ​ and Wendell ​ ​ Wallach , ​ ​​ Emerging ​ ​ Technologies : ​ ​ Ethics , ​ ​ Law ​ ​ and ​ ​ Governance , ​ ​ ​ ( Ashgate ​ ​ Publishing , ​ ​ 2017 ) . 191 Marvin ​ ​ Ammori , ​ ​ Adrian ​ ​ Aoun , ​ ​ Greg ​ ​ Badros , ​ ​ Clayton ​ ​ Banks , ​ ​ Phin ​ ​ Barnes , ​ ​ Niti ​ ​ Bashambu , ​ ​ et ​ ​ al. , ​ ​ “ An ​ ​ open ​ ​ letter ​ ​ from technology ​ ​ sector ​ ​ leaders ​ ​ on ​ ​ Donald ​ ​ Trump ’ s ​ ​ candidacy ​ ​ for ​ ​ President , ” ​ ​ 2016 , https : //shift.newco.co/an-open-letter-from-technology-sector-leaders-on-donald-trumps-candidacy-for-president-5bf734c1 59e4 ​ . AI ​ ​ Now ​ ​ 2017 ​ ​ Report 36 Conclusion AI ​ ​ systems ​ ​ are ​ ​ now ​ ​ being ​ ​ adopted ​ ​ across ​ ​ multiple ​ ​ sectors , ​ ​ and ​ ​ the ​ ​ social ​ ​ effects ​ ​ are already ​ ​ being ​ ​ felt : ​ ​ so ​ ​ far , ​ ​ the ​ ​ benefits ​ ​ and ​ ​ risks ​ ​ are ​ ​ unevenly ​ ​ distributed . ​ ​ Too ​ ​ often , ​ ​ those effects ​ ​ simply ​ ​ happen , ​ ​ without ​ ​ public ​ ​ understanding ​ ​ or ​ ​ deliberation , ​ ​ led ​ ​ by ​ ​ technology companies ​ ​ and ​ ​ governments ​ ​ that ​ ​ are ​ ​ yet ​ ​ to ​ ​ understand ​ ​ the ​ ​ broader ​ ​ implications ​ ​ of ​ ​ their technologies ​ ​ once ​ ​ they ​ ​ are ​ ​ released ​ ​ into ​ ​ complex ​ ​ social ​ ​ systems . ​ ​ We ​ ​ urgently ​ ​ need rigorous ​ ​ research ​ ​ that ​ ​ incorporates ​ ​ diverse ​ ​ disciplines ​ ​ and ​ ​ perspectives ​ ​ to ​ ​ help ​ ​ us ​ ​ measure and ​ ​ understand ​ ​ the ​ ​ short ​ ​ and ​ ​ long-term ​ ​ effects ​ ​ of ​ ​ AI ​ ​ across ​ ​ our ​ ​ core ​ ​ social ​ ​ and ​ ​ economic institutions . Fortunately , ​ ​ more ​ ​ researchers ​ ​ are ​ ​ turning ​ ​ to ​ ​ these ​ ​ tasks ​ ​ all ​ ​ the ​ ​ time . ​ ​ But ​ ​ research ​ ​ is ​ ​ just the ​ ​ beginning . ​ ​ Advocates , ​ ​ members ​ ​ of ​ ​ affected ​ ​ communities ​ ​ and ​ ​ those ​ ​ with ​ ​ practical domain ​ ​ expertise ​ ​ should ​ ​​ ​ be ​ ​ included ​ ​ at ​ ​ the ​ ​ center ​ ​ of ​ ​ decision ​ ​ making ​ ​ around ​ ​ how ​ ​ AI ​ ​ is deployed , ​ ​ assessed ​ ​ and ​ ​ governed . ​ ​ Processes ​ ​ must ​ ​ be ​ ​ developed ​ ​ to ​ ​ accommodate ​ ​ and ​ ​ act on ​ ​ these ​ ​ perspectives , ​ ​ which ​ ​ are ​ ​ traditionally ​ ​ far ​ ​ removed ​ ​ from ​ ​ engineering ​ ​ and ​ ​ product development ​ ​ practices . ​ ​ There ​ ​ is ​ ​ a ​ ​ pressing ​ ​ need ​ ​ now ​ ​ to ​ ​ understand ​ ​ these ​ ​ technologies ​ ​ in the ​ ​ context ​ ​ of ​ ​ existing ​ ​ social ​ ​ systems , ​ ​ to ​ ​ connect ​ ​ technological ​ ​ development ​ ​ to ​ ​ social ​ ​ and political ​ ​ concerns , ​ ​ to ​ ​ develop ​ ​ ethical ​ ​ codes ​ ​ with ​ ​ force ​ ​ and ​ ​ accountability , ​ ​ to ​ ​ diversify ​ ​ the field ​ ​ of ​ ​ AI ​ ​ and ​ ​ to ​ ​ integrate ​ ​ diverse ​ ​ social ​ ​ scientific ​ ​ and ​ ​ humanistic ​ ​ research ​ ​ practices ​ ​ into the ​ ​ core ​ ​ of ​ ​ AI ​ ​ development . ​ ​ Only ​ ​ then ​ ​ can ​ ​ the ​ ​ AI ​ ​ industry ​ ​ ensure ​ ​ that ​ ​ its ​ ​ decisions ​ ​ and practices ​ ​ are ​ ​ sensitive ​ ​ to ​ ​ the ​ ​ complex ​ ​ social ​ ​ domains ​ ​ into ​ ​ which ​ ​ these ​ ​ technologies ​ ​ are rapidly ​ ​ moving .
