HUMAN RIGHTS IMPACT ASSESSMENTS FOR AI : ANALYSIS AND RECOMMENDATIONS Access Now ( accessnow.org ) defends and extends the digital rights of people and communities at risk . As a grassroots-to-global organization , we partner with local actors to bring a human rights agenda to the use , development , and governance of digital technologies , and to intervene where technologies adversely impact our human rights . By combining direct technical support , strategic advocacy , grassroots grantmaking , and convenings such as RightsCon , we ﬁght for human rights in the digital age . Human rights impact assessments for AI : analysis an d recommendations Brandie Nonnecke , Director , CITRIS Policy Lab , UC Berkeley Philip Dawson , 2020-2021 Technology & Human Rights Fellow , Harvar d Kennedy School Carr Center for Human Rights Policy ABSTRACT 2 INTRODUCTION 3 AI GOVERNANCE AND HUMAN RIGHTS 5 AlGORITHMIC IMPACT ASSESSMENTS AND HUMAN RIGHTS IMP ACT ASSESSMENTS 7 Canadaʼs Directive on Automated Decision-Making 10 EU AI Governance Strategy 10 Towards a Systemic Approach 14 THE ROLE OF STANDARDS AND CERTIFICATIONS 17 CONCLUSIONS AND RECOMMENDATIONS 19 ACKNOWLEDGMENT 21 Human rights impact assessments for AI : analysis an d recommendations ABSTRACT The public and private sectors are increasingly tur ning to the use of algorithmic or artificial intelligence impact assessments ( AIAs ) as a means t o identify and mitigate harms from Artificial Intelligence ( AI ) Systems . While promising , lack of clarity on the proper scope , methodology , and best practices for AIAs could inadvertently perpetu ate the harms they seek to mitigate , especially to human rights . We explore the emerging integratio n of the human rights legal framework into AI governance strategies , including within Canadaʼs Di rective on Automated Decision-Making and the European Commissionʼs proposed Digital Services Act and Artificial Intelligence Act , as well as the implementation of human rights impact assessments ( HRIAs ) for AI Systems . The benefits and drawbacks from recent implementations of AIAs and H RIAs to assess AI systems adopted by the public and private sectors are explored and conside red in the context of an emerging trend toward the development of standards and certifications for responsible AI governance practices . We conclude with priority recommendations for how the human rights framework can help better ensure AIAs and their corresponding responsible AI go vernance strategies live up to their promise . October 2022 This report is a publication of Access Now , which c ommissioned this research as part of our work on the intersection of human rights law and Artific ial Intelligence ( AI ) Systems . It is written by Brandie Nonnecke , Director , CITRIS Policy Lab , UC B erkeley and Philip Dawson , 2020-2021 Technology & Human Rights Fellow , Harvard Kennedy S chool Carr Center for Human Rights Policy . A previous version of the report was published in O ctober 2021 as part of the Harvard Kennedy School Carr Center for Human Rights Policy Discussi on Paper Series . It has been updated and expanded in this version . Access Now would like to thank Brandie and Phil for their excellent and insightful work . Access Now ( https : //www.accessnow.org ) defends and extends the digital rights of users at risk around the world . By combining direct technical sup port , comprehensive policy engagement , global advocacy , grassroots grantmaking , legal inte rventions , and convenings such as RightsCon , we fight for human rights in the digital age . 2 Human rights impact assessments for AI : analysis an d recommendations INTRODUCTION In response to growing recognition of the societal risks of artificial intelligence ( AI ) broadly and automated decision-making systems ( ADS ) in particul ar , algorithmic or AI impact assessments ( AIAs ) are increasingly being considered by the pub lic and private sectors to identify , prevent , and mitigate harms , or as a means to improve the qualit y of AI products and services.1The term “ algorithmic impact assessment ” currently lacks def initional clarity . In general , AIAs aim to identify potential risks and impacts—including to health , sa fety , ethics and , in some implementations , to human rights —arising from the development and depl oyment of an algorithmic system as well as appropriate risk mitigation strategies , such as use of “ algorithmic audits ” , “ datasheets for datasets ” , and “ model cards . ” 2 Implementations of AIAs are gaining momentum as a v iable AI governance strategy , finding their way into binding regulation and legislation.3Corporate policies are also requiring implementation of AIAs as a mechanism to reduce legal risks stemmi ng from liability and negligence.4The European Commissionʼs Artificial Intelligence Act s uggests a risk-based approach to AI governance , prohibiting certain harmful applications of AI and calling for developers to go through a form of impact assessment ( called a “ conformity assessment ” ) for high-risk applications to identify necessary oversight mechanisms.5The Algorithmic Accountability Act proposed in the U nited States Congress in 2019 would have required compani es with large user bases to conduct impact assessments of highly sensitive ADS ( the Act is exp ected to be reintroduced ) .6In 2021 , the National Institute of Standards and Technology ( NIST ) was ta sked by Congress to develop an “ AI risk management framework ” to guide the “ reliability , ro bustness , and trustworthiness of AI systems ” used in the federal government.7In 2021 , the National Security Commission on Artific ial Intelligence issued a report recommending that gove rnment agencies deploying AI systems conduct ex ante risk assessments and ex post impact assessments to “ increase public transparency 7 '' Commerce , Justice , Science And Related Agencies Ap propriations Bill , 2021 - Report Together With Mino rity Views , '' House Committee on Appropriations , July 2020 , https : //appropriations.house.gov/sites/democrats.ap propriations.house.gov/files/July % 209th % 20report % 20 for % 20circulation_0.pdf , 23.6Algorithmic Accountability Act , 116th Cong . ( 2019 ) . https : //www.wyden.senate.gov/imo/media/doc/Algorith mic % 20Accountability % 20Act % 20of % 202019 % 20Bill % 20Tex t.pdf ; Grace Dill , “ Sen . Wyden to Reintroduce AI Bias Bill in Coming M onths , ” MeriTalk , Feb. 19 , 2021 , https : //www.meritalk.com/articles/sen-wyden-to-rein troduce-ai-bias-bill-in-coming-months/ .5 “ Proposal for a Regulation laying down harmonized r ules on artificial intelligence ( Artificial Intelli gence Act ) . ” European Commission , last modified April 21 , 2021 , https : //digital-strategy.ec.europa.eu/en/library/pr oposal-regulation-laying-down-harmonised-rules-arti ficial-intelligence-artificial-intell igence .4Andrew D. Selbst , Negligence and AIʼs Human Users , 100 B.U.L.REV . 1315 ( 2020 ) ; 3Kent Walker and Jeﬀ Dean , “ An Update on Our work on AI and Responsible Innovation , ” Google , July 9 , 2021 , https : //blog.google/technology/ai/update-work-ai-re sponsible-innovation .2Casey , B. , Farhangi , A. , & Vogl , R. ( 2019 ) . Rethink ing Explainable Machines : The GDPR 's ' Right to Expla nation ' Debate and the Rise of Algorithmic Audits in Enterprise . Berkeley Tech . LJ ,34 , 143 ; Gebru , Timnit , Jamie Morgenstern , Briana Vecc hione , Jennifer Wortman Vaughan , Hanna Wallach , Hal Daumé III , and Kate Cra wford . `` Datasheets for datasets . '' arXiv preprint arXiv:1803.09010 ( 2018 ) ; Mitchell , Margaret , Simone Wu , Andrew Zaldivar , Parker Barnes , Lucy Vasserman , Ben Hutchinson , Elena Spitzer , In ioluwa Deborah Raji , and Timnit Gebru . `` Model cards for model reporting . '' In Proceedings of the conference on fairness , accounta bility , and transparency , pp . 220-229 . 2019 ; Emanuel Moss et al . `` Governing with algorithmic impact assessments : six observations . '' Available at SSRN ( 2020 ) .1The term artificial intelligence ( AI ) is typically used to refer to a computer system capable of perfo rming tasks that would ordinarily require some form of intelligence to accomplish , su ch as decision-making , visual perception , speech re cognition , and more . Methods for doing so are wide ranging and vary significantly in complexity , such as algorithms , predictive models , computer vision , deep learning , machine learning , natural language processing , neur al nets , and more . For more on the problematic nat ure of the term itself , see Daniel Leufer , “ The term “ AI ” has a clear meaning , ” https : //www.aimyths.org/the-term-ai-has-a-clear-mea ning . 3 Human rights impact assessments for AI : analysis an d recommendations about AI use through improved reporting . ” 8In this instance , risk assessments and impact assessments are diﬀerentiated , with risks being ide ntified at the outset and impacts being evaluated a er deployment to quantify and mitigate the identified risks . Canadaʼs “ Directive on Automated Decision-Making ” , which came into eﬀect i n 2020 , led to the development of one of the first AIAs to identify and mitigate a range of risk s—to individual rights , economic interests , health and well-being , and sustainability—arising from ADS developed and deployed in the public sector.9 While AIAs hold promise to promote the development of regulatory , policy , and governance mechanisms by government and corporate actors to id entify potential harms , human rights organizations have warned that using risk-based AIA s may be inadequate.10Most guidance for implementation of AIAs indicates their use should b e reserved for “ high-risk ” AI applications ( e.g. , use of AI in biometric identification or judicial s entencing ) . However , applications wrongly categorized as “ low-risk ” can thereby evade proper oversight . This would be especially problematic in a case where the onus of determining risk level is placed on the entity developing the AI . This could lead to a troubling scenario whe re developers artificially reduce the perception of risk in order to evade oversight . Further , a lac k of common or internationally standardized approaches to the development of AIAs could lead to confusion and complicate their eﬀectiveness . As a risk-based approach increasingly dominates AI governance strategies , important questions emerge regarding the proper scope , methodology , and best practices that might protect AIAs from inadvertently becoming smokescreens for human right s and other abuses . In short , the ill-conceived development and deployment of AIAs po se substantial risk themselves . This is not to say that the implementation of AIAs can not provide benefits now , but rather that significant work remains to determine how to appropriately develop a nd apply AIAs to ensure long-term eﬀectiveness . If done inappropriately , their use ma y ultimately enable and perpetuate the harms they seek to mitigate . We explore the emerging integration of the human ri ghts framework into AI governance strategies , including the human rights implications of AIAs . We rely on the international human rights law framework , including the UN Declaration of Human Ri ghts ( UDHR ) as well as the UN Guiding Principles on Business and Human Rights ( UNGPs ) , to provide an analysis of emerging proposals for the use of AIAs , including in recommendations m ade by international and intergovernmental organizations , regulatory and legislative proposals from government bodies , and usage to date in the private sector . We conclude by analyzing the in tegration of human rights into emerging legislative proposals , such as in Canadaʼs Directiv e on Automated Decision-Making and the European Unionʼs Artificial Intelligence Act and Di gital Services Act , and oﬀer recommendations to help guide the eﬀective development and use of huma n rights-based AIAs more broadly . 10Fanny Hidvegi , Daniel Leufer , and Estelle Massé , “ T he EU Should Regulate AI on the Basis of Rights , No t Risks , ” Access Now , Feb. 17 , 2021 , https : //www.accessnow.org/eu-regulation-ai-risk-bas ed-approach/ .9 “ Directive on Automated Decision-Making ” Canadian G overnment , last modified April 1 , 2021 , https : //www.tbs-sct.gc.ca/pol/doc-eng.aspx ? id=32592 & section=html .8Eric Schmidt et al . “ National Security Commission o n Artificial Intelligence ( AI ) Final Report ” . National Security Commission on Artificial Intelligence , ( 2021 ) , 395 , https : //www.nscai.gov/wp-content/uploads/2021/03/Fu ll-Report-Digital-1.pdf . 4 Human rights impact assessments for AI : analysis an d recommendations AI GOVERNANCE AND HUMAN RIGHTS Over the last few years , at least 170 sets of ethic al or human-rights based AI principles , frameworks , and guidelines have been developed to s upport responsible AI development and deployment within the public and private sectors.11Research has shown that a growing consensus is forming around core principles , including the ne ed for accountability , privacy and security , transparency and explainability , fairness and non-d iscrimination , professional responsibility , human control , and the promotion of human values.12As these AI principles gain acceptance within the public and private sectors , the focus is now shi ing to the development of appropriate strategies to operationalize the principles into re sponsible practices . Yet this process is not straightforward . Despite the relative convergence of AI principles p roposals there is seldom consensus in the interpretation of the principles in practice , espec ially when it comes to the details.13AI principles have been developed by diverse institutions ( e.g. , academia , civil society , governments ) with varying multistakeholder representation . Because th ese institutions have diﬀering priorities and needs and have o en applied diﬀerent ethical framew orks ( e.g. , deontological , consequentialist , utilitarian approaches ) to evaluate the benefits an d risks of AI , there is great heterogeneity in how AI principles are defined and in recommendations fo r their appropriate operationalization . Certain scholars have argued that “ AI principle proliferati on ” has perpetrated a crisis of legitimacy , complicating the already complex task of identifyin g and mitigating risks and harms of AI-enabled technologies.14In response , the international human rights framewor k and its normative and legal guidance has been proposed as a mechanism to suppor t more consistent framing and operationalization of AI principles , and many promi nent professional associations , consortia , intergovernmental organizations , governments , and c ompanies seem to agree.15 The Institute of Electrical and Electronics Enginee rs ( IEEE ) , the worldʼs largest technical professional organization , issued a report in 2017 stating as its first principle that AI should be “ created and operated to respect , promote , and prot ect internationally recognized human rights ” and emphasized that human rights should be part of AI risk assessments.16The Asilomar Principles , with over 5,000 signatories from the pu blic and private sectors , include the need to protect human rights in the design and deployment o f AI systems.17The OECD AI Principles , which 17 “ Asilomar AI Principles , ” Future of Life Institute ( 2017 ) , https : //futureoflife.org/ai-principles/16 “ Ethically Aligned Design : A Vision for Prioritizin g Human Well-being with Autonomous and Intelligence Systems , ” The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems , https : //standards.ieee.org/content/dam/ieee-standar ds/standards/web/documents/other/ead1e.pdf ? utm_medi um=undefined & utm_s ource=undefined & utm_campaign=undefined & utm_content= undefined & utm_term=undefined15Mark Latonero , `` Governing Artificial Intelligence : U pholding Human Rights & Dignity . '' Data & Society ( 2018 ) : 1-37 ; Alessandro Mantelero and Samantha Esposito . `` An evidence-based methodology for human rights impact assessment ( HR IA ) in the development of AI data-intensive systems . '' Computer Law & Security Review ( 2021 ) ; Eileen Donahoe and Megan MacDuﬀee Metzger. `` Artificial Intelligence and Human Rights . '' Journal of Democracy 30 , no . 2 ( 2019 ) : 115-126.14Mark Latonero , “ AI Principle Proliferation as a Cri sis of Legitimacy . ” Carr Center Discussion Paper Series , ( Sept. 30 , 2020 ) .13 '' AI Ethics Guidelines Global Inventory , '' Algorithm Watch , accessed June 20 , 2021 , https : //inventory.alg orithmwatch.org/.12Jessica Fjeld et al. , `` Principled Artificial Intell igence : Mapping Consensus in Ethical and Rights-bas ed Approaches to Principles for AI , '' Berkman Klein Center for Internet & Society , Harvar d University , 2020 , https : //dash.harvard.edu/bitstream/handle/1/4216042 0/HLS % 20White % 20Paper % 20Final_v3.pdf .11 '' AI Ethics Guidelines Global Inventory , '' Algorithm W atch , accessed June 20 , 2021 , https : //inventory.algorithmwatch.org/ . 5 Human rights impact assessments for AI : analysis an d recommendations 42 countries have pledged to uphold , specifically c all for the protection of human rights.18The European Commissionʼs High-Level Expert Group on AI has also established a set of principles , one of which calls for ensuring AI respects fundamental rights.19The White House Oﬀice of Science and Technology Policy ( OSTP ) in its National AI Initiat ive identified the need to ensure AI systems do not infringe upon human rights , especially rights t o privacy , civil rights , and civil liberties.20 Canada , through its Directive on Automated Decision -Making , is one of the first countries to develop an Algorithmic Impact Assessment tool that seeks to measure and mitigate the human rights harms of ADS used in public services.21In the private sector , technology companies like Salesforce have explicitly identified protecting hu man rights in their AI ethics strategy.22And Microso  and Intel are among the first global tech companies to conduct HRIAs on their development and use of AI.23 Centering human rights within AI governance strateg ies can help operationalize AI principles across sectors , international contexts , and domain application areas.24Through codification in charters , case law , regulation and industry standar ds , human rights norms and values have gained broad global consensus.25The UDHR and corresponding international human right s instruments and guiding principles , UN treaties and commentarie s , national laws , and related policies and guidelines have helped to clarify core definitions and interpretations of human rights over decades.26As such , international human rights norms and values may be “ clearer , better defined , and [ more ] stable ” than AI principles alone . Applyi ng a human rights framework “ facilitates better harmonization and reduces the risk of uncertainty ” in defining and applying AI principles in practice.27 27Alessandro Mantelero and Samantha Esposito . `` An evi dence-based methodology for human rights impact ass essment ( HRIA ) in the development of AI data-intensive systems . '' Computer Law & Security Review ( 2021 ) 26 “ Universal Declaration of Human Rights , ” United Nat ions , ( 1948 ) , https : //www.un.org/en/about-us/universal-declaratio n-of-human-rights ; “ The Core International Human Rights Instruments a nd Their Monitoring Bodies , ” United Nations Human Rights Oﬀice of the High Commis sioner , ( 2021 ) , https : //www.ohchr.org/en/professionalinterest/pages /coreinstruments.aspx ; “ Guiding Principles on Business and Human Rights : Implementing the United Nations ʻProtect , Respect a nd Remedyʼ Framework , ” United Nations Human Rights Oﬀice of the High Commissioner , ( 2011 ) , https : //www.ohchr.org/documents/publications/guidin gprinciplesbusinesshr_en.pdf ; Mark Latonero . `` Governing Artificial Intelligence : Upholding Human Rights & D ignity . '' Data & Society ( 2018 ) : 1-37.25Eileen Donahoe and Megan MacDuﬀee Metzger . `` Artific ial intelligence and human rights . '' Journal of Democracy 30 , no . 2 ( 2019 ) : 115-126.24Alessandro Mantelero and Samantha Esposito . `` An Evi dence-Based Methodology for Human Rights Impact Asse ssment ( HRIA ) in the Development of AI Data-Intensive Systems . '' Computer Law & Security Review ( 2021 ) ; Eileen Donahoe and Megan MacDuﬀee Metzger . `` Artificial intelligence and human rights . '' Journal of Democracy 30 , no . 2 ( 2019 ) : 115-126 ; Charles Bradley , Richard Wingfield , and Megan Metzger . “ National Artificial Intelligence Strategi es and Human Rights : A Review . Second Edition . ” Global Partners Digital and Stanford Global Digital Policy Incubator ( April 2021 ) : 1-70.23 “ Human Rights Annual Report , ” Microso  , ( 2018 ) , https : //query.prod.cms.rt.microso .com/cms/api/am/b inary/RE2FMZY ; “ Intel Human Rights Impact Assessment , ” Article One Advisors , ( 2018 ) , https : //www.articleoneadvisors.com/intel-hria ; 22 “ AI Ethics , ” Salesforce , accessed Aug. 22 , 2021 , https : //einstein.ai/ethics .21 “ Directive on Automated Decision-Making ” Canadian G overnment , last modified April 1 , 2021 , https : //www.tbs-sct.gc.ca/pol/doc-eng.aspx ? id=32592 & section=html20 “ Advancing Trustworthy AI , ” National AI Initiative Oﬀice , ( 2021 ) , https : //www.ai.gov/strategic-pillars/advancing-trus tworthy-ai/ # Metrics-Assessment-Tools-and-Technical- Standards-for-AI19 “ Ethics Guidelines for Trustworthy Artificial Intel ligence , ” European Commission High-Level Expert Gro up on AI , Last modified April 8 , 2019 , https : //digital-strategy.ec.europa.eu/en/library/pr oposal-regulation-laying-down-harmonised-rules-arti ficial-intelligence-artificial-intell igence .18 '' OECD/LEGAL/0449 : Recommendation of the Council on A rtificial Intelligence , '' OECD Legal Instruments , OE CD , last modified May 21 , 2019 , https : //legalinstruments.oecd.org/en/instruments/OE CD % 20-LEGAL-0449 . 6 Human rights impact assessments for AI : analysis an d recommendations Take , for example , the principle of “ non-discrimina tion , ” which exists in Article 2 of the UDHR and has also been widely adopted as an AI principle in the public and private sectors . The operationalization of “ non-discrimination ” is compl icated due to the absence of a shared understanding of what it means in the development a nd deployment of AI systems . By applying a human rights framework and relevant charters , case law , and regulation to identify how “ non-discrimination ” has been interpreted in a part icular domain , appropriate strategies to move the concept of “ non-discrimination ” from the abstra ct to the concrete can become clearer . Human rights principles also highlight that the res ponsible design of AI systems , including transparency , explainability , and accountability , a re not only desirable from a commercial or ethical standpoint , but prerequisites to upholding existing legal obligations . For instance , a lack of transparency regarding the use of AI systems can ma ke it diﬀicult to determine whether a violation of human rights or any other legal obligation has o ccurred , undermining the ability to seek redress . Similarly , and especially in the public sector , the reliance on a recommendation , decision , or insight provided by an AI system that is not explai nable or accountable is at odds with human rights principles incorporated into national admini strative law , which generally requires that an individual be provided with reasons for a decision made against them , as well as an opportunity to contest that decision and receive remedy ( ies ) .28 The human rights framework can provide the substant ive foundation and governance architecture needed to produce greater specificity in defining a nd operationalizing AI principles . As the public and private sectors increase their eﬀorts to implem ent AIAs , calls to require Human Rights Impact Assessments ( HRIAs ) for AI are also on the rise.29Legislative approaches , such as the EU AI Act , are beginning to codify human rights-based principles i n governance and oversight practices.30While promising , it is important to consider how AIAs and HRIAS should be implemented to better identify and mitigate risks . We next evaluate the d esign and scope of AIAs and HRIAs for AI and then turn to a discussion of the challenges associa ted with their implementation . AlGORITHMIC IMPACT ASSESSMENTS AND HUMAN RIGHTS IMPACT ASSESSMENTS There is a long history of using impact assessments in a variety of domains , including to assess and mitigate harms to the environment , data security , p rivacy , and human rights . For each , the appropriate scoping and implementation methods must be carefully negotiated and constructed 30 “ Proposal for a Regulation laying down harmonized ru les on artificial intelligence ( Artificial Intellig ence Act ) . ” European Commission , last modified April 21 , 2021 , https : //digital-strategy.ec.europa.eu/en/library/pr oposal-regulation-laying-down-harmonised-rules-arti ficial-intelligence-artificial-intell igence .29 “ Human Rights and Technology Final Report 2021 , ” Au stralian Human Rights Commission , last modified May 27 , 2021 , https : //tech.humanrights.gov.au/downloads ? _ga=2.118 186762.280577679.1628185379-267672886.1624901321 ; “ Human Rights , Democracy and Rule of Law Impact Assessment of AI s ystems ” , Council of Europe Ad Hoc Committee on Arti ficial Intelligence Policy Develop Group ( CAHAI-PDG ) , last modified May 21 , 20 21 , https : //rm.coe.int/cahai-pdg-2021-05-2768-0229-3507 -v-1/1680a291a328 “ Human Rights and Technology Final Report ( 2021 ) , ” Australia Human RIghts Commission , 2021 , https : //humanrights.gov.au/our-work/rights-and-free doms/publications/human-rights-and-technology-final -report-2021 7 Human rights impact assessments for AI : analysis an d recommendations to support accountability.31In a recent study of impact assessments in diﬀerent sectors , researchers noted that the methodology is largely d riven by 10 constitutive components , including criteria such as source ( s ) of legitimacy ( e.g. , leg islative or regulatory mandates that define who must implement an impact assessment and when ) , iden tifying potential “ impacts ” to be assessed and mitigated ( e.g. , risks to non-discrimination ) , and the appropriate methods for doing so ( e.g. , consultation with diverse subject matter experts an d those directly aﬀected ) .32 The design and implementation of impact assessments in the field of AI is nascent . As such , there is a lack of consensus or common standards regarding t he appropriate configuration or application of such constitutive components , including which en tities should administer , enforce and oversee AIAs or HRIAs to support legitimacy ; how to adopt m eaningful governance and engagement processes to support accountability ; and what are t he appropriate methods for implementation , including how to eﬀectively define , identify , and m itigate risks.33 Metcalf et al . ( 2021 ) define AIAs as “ emerging gove rnance practices for delineating accountability , rendering visible the harms caused by algorithmic s ystems , and ensuring practical steps are taken to ameliorate those harms . ” 34Typical sources of risk to be identified include the presence of bias in datasets used to train an AI system , as well as the fairness and explainability of the model ; identification of potential impacts can include con textual considerations related to equity and justice , as well as the economic interests , health , and well-being of users or populations potentially aﬀected by the proposed system . Companies may integ rate AIAs in whole or in part into traditional product design , reviews , risk management , and due d iligence processes . Further , implementation of AIAs early , perhaps during the initial design pro cess , is likely more eﬀective at identifying and mitigating risks before widespread investment and d eployment . AIAs should also consider leading to the termination of an AI application if appropri ate safeguards can not be put in place . Like “ privacy by design ” concepts , AIAs could support “ A I responsibility by design . ” The goal of an AIA , as with other impact assessment s , is ultimately to identify technical adjustments that can be made to the AI system in or der to eliminate the risks identified or to reduce them to an acceptable level . Because of thei r deep expertise and knowledge of the AI system being assessed , technology firms will likely be the primary administrators of AIAs , creating a potential situation where these firms have an out sized eﬀect on what is included in AIAs and how they are implemented in practice.35Thus , transparency in how AIAs are developed and implemented by technology firms is critical . 35Andrew Selbst . “ An Institutional View of Algorithmi c Impact Assessments . ” Harvard Journal of Law and Technology , 35 ( 10 ) , 78 , ( 2021 ) , https : //papers.ssrn.com/sol3/papers.cfm ? abstract_id =3867634 .34Ibid . , 26.33Ibid . , 28.32Emanuel Moss et al. , “ Assembling Accountability : Alg orithmic Impact Assessment for the Public Interest , ” Data & Society , June 29 , 2021 , https : //datasociety.net/library/assembling-accounta bility-algorithmic-impact-assessment-for-the-public -interest/31Jacob Metcalf et al .. `` Algorithmic impact assessmen ts and accountability : The co-construction of impact s. '' In Proceedings of the 2021 ACM Conference on Fairness , Accountability , and Tra nsparency , pp . 735-746 . 2021 . 8 Human rights impact assessments for AI : analysis an d recommendations An HRIA is “ a tool to evaluate the potential or act ual impact of an organizationʼs strategy , practice , or product on peopleʼs human rights . ” 36Endorsed by the UN Human Rights Council in 2011 , the UNGPs underpin much of the criteria and guidance ap plicable to best practices of HRIAs . The UNGPs recommend that assessments of human rights im pacts should be undertaken regularly and at appropriate stages of a businessʼs operations as part of its human rights due diligence processes , for instance , prior to a new activity or relationship ; major decisions or changes in its operations ( e.g. , market entry , product launch , pol icy change , or wider changes to the business ) ; and periodically throughout the life of an activity or relationship . In general , the assessment should include identifying who may be aﬀected ; cata loging the relevant human rights standards and issues ; projecting how the proposed activity an d associated business relationships could have adverse human rights impacts on those identified ; a nd identifying mitigations that might eliminate or reduce the level of risk to an acceptable level . Large technology companies like Microso  and Facebo ok have begun conducting HRIAs to identify and address technology-related human rights risks , including those emanating from AI.37Microso  publishes a “ Human Rights Annual Report ” within whi ch the human rights eﬀects of its technologies are explored and certain risk mitigati on strategies are discussed . However , the company has no obligation to publish reports outlin ing the details of any mitigation actions it undertakes , or how feedback from civil society orga nizations has been addressed . Facebook commissioned an HRIA to evaluate its role in the ge nocide of the Rohingya in Myanmar . Yet scholars have criticized the HRIA for failing to un cover the most salient human rights harms of Facebookʼs AI-enabled tools and appropriate mechani sms to mitigate those harms moving forward.38 In the remainder of this section , we explore propos ed and existing AIAs and related impact assessment strategies , such as conformity assessmen ts and data protection impact assessments , in the public and private sectors to better underst and emerging trends in their scope and structure and the corresponding benefits and risks associated with their implementation , especially to human rights . We first review Canadaʼs “ Directive o n Automated Decision-Making ” and its development and use of AIAs to evaluate and mitigat e harms of ADS in government public service delivery . We next consider the EUʼs AI governance s trategy through an evaluation of the EU AI Act and its risk-based approach to AI governance , inclu ding proposed implementation of “ conformity assessments ” to identify and mitigate AI risks emer ging from the private sector . We then explore the relationship between the EU AI Act and the EUʼs General Data Protection Regulation ( GDPR ) and the feasibility of GDPR-mandated data protectio n impact assessments ( DPIAs ) to evaluate and mitigate human rights-based risks of AI . We then ev aluate the EUʼs Digital Services Act ( DSA ) and its oversight mechanisms to mitigate risks of very larg e , AI-driven online platforms for human rights . 38Mark Latonero and Aaina Agarwal . “ Human Rights Impa ct Assessments for AI : Learning from Facebookʼs Fai lure in Myanmar . ” Carr Center Discussion Paper Series , March 19 , 2021 , https : //carrcenter.hks.harvard.edu/publications/hum an-rights-impact-assessments-ai-learning-facebook % E 2 % 80 % 99s-failure-myanm ar.37 “ Microso  Global Human Rights Statement . ” Microso  Corporation , last modified December 11 , 2020 , https : //query.prod.cms.rt.microso .com/cms/api/am/b inary/RE4JIiU ; “ An Independent Assessment of the Human Rights Imp act of Facebook in Myanmar . ” Facebook , last modified Novem ber 5 , 2018 , https : //about.fb.com/news/2018/11/myanmar-hria/36Mark Latonero and Aaina Agarwal . “ Human rights impa ct assessments for AI : Learning from Facebookʼs fai lure in Myanmar . ” Carr Center Discussion Paper Series , March 19 ( 2021 ) 9 Human rights impact assessments for AI : analysis an d recommendations We conclude with a discussion of the implementation of HRIAs for AI and how these may diﬀer from , complement , or should be integrated into AIAs and other impact assessment strategies to better ensure the protection of fundamental human r ights . Canadaʼs Directive on Automated Decision-Making In 2019 , the Canadian government released its Direc tive on Automated Decision-Making ( the Directive ) .39The Directiveʼs principal objectives were to ensure the incorporation of ADS into external public service delivery respects “ core adm inistrative law principles such as transparency , accountability , legality , and procedural fairness ” and to ensure harmful eﬀects of algorithms on administrative decisions are assessed and reduced.40To this end , the Directive includes an accompanying impact assessment tool in the form of a questionnaire that must be completed prior to the development of any ADS . Completion of the qu estionnaire helps internal teams compute a raw impact score that measures the risk of the auto mation , for instance , to the rights of individuals or communities , their health , well-being or economi c interests , as well as eﬀects on the overall “ sustainability of the ecosystem . ” 41Depending on the level of impact , the Directive prov ides for increasingly rigorous mitigation requirements , such as extensive peer review , notice , human intervention in the decision-making process , the pr ovision of a “ meaningful explanation ” , or personnel training . While the Directive received attention both within Canada and globally , the government has been criticized for failing to enforce its requirements . Since the Directive came into force in May 2020 , few AIAs have been completed and published per its requirements.42In a sense , Canadaʼs experience with the Directive highlights a challeng e that is well-known to global technology companies—obtaining institutional support and deplo ying the resources and expertise necessary to support the implementation of organization-wide compliance tools is not a straightforward process , particularly for emerging and poorly under stood technologies such as ADS and AI . EU AI Governance Strategy In April 2021 , the European Commission released its dra  “ Artificial Intelligence Act ” ( AI Act ) .43 While the AI Act is the most comprehensive approach to AI governance proposed in the EU , it is part of a larger EU AI governance strategy , includi ng the GDPRʼs mandated evaluation and oversight of automated decision systems and the DSA ʼs proposed oversight of algorithmic-driven very large online platforms . We first provide a hig h-level overview of the AI Act , then discuss its 43 “ Proposal for a Regulation laying down harmonized r ules on artificial intelligence ( Artificial Intelli gence Act ) . ” European Commission , last modified April 21 , 2021 , https : //digital-strategy.ec.europa.eu/en/library/pr oposal-regulation-laying-down-harmonised-rules-arti ficial-intelligence-artificial-intell igence .42Tom Cardoso and Bill Curry , “ National defense skirt ed federal rules in using artificial intelligence , privacy commissioner says ” , The Globe and Mail , last modified February 8 , 2021 , https : //www.theglobeandmail.com/canada/article-nati onal-defence-skirted-federal-rules-in-using-artific ial/ ; “ Open Government Portal , ” Government of Canada , accessed Sept. 7 , 2021 , https : //search.open.canada.ca/en/od/ ? search_text=AI A41Ibid . , 3540Ibid . , 3539 “ Directive on Automated Decision-Making ” Canadian G overnment , April 1 , 2021 , https : //www.tbs-sct.gc.ca/pol/doc-eng.aspx ? id=32592 & section=html 10 Human rights impact assessments for AI : analysis an d recommendations connection with oversight requirements of automated decision systems instituted through the GDPR and proposed in the DSA . The dra  AI Act takes a risk-based approach to AI r egulation , establishing three primary levels of risk : low/minimal , high , and unacceptable .. The Act proposes diﬀerent levels of oversight for low/minimal and high-risk AI applications . Addition ally , applications posing a risk of manipulation , such as AI applications that manipulate images , aud io , video content ( Title IV ) , would have transparency obligations . Applications that fall wi thin the category of unacceptable risk are prohibited ( e.g. , uses of AI that are capable of ma nipulating individuals through subliminal techniques ) . High-risk applications , such as use of AI in critical infrastructure , medical devices , an d education or which pose a risk to health , safety , a nd/or fundamental rights , such as credit scoring or hiring decisions , must undergo pre-market confor mity assessments attesting to their compliance with the Act . Providers of high-risk AI systems must have a post-market monitoring system in place , in which they actively collect , do cument , and analyze relevant data throughout the AI systemʼs lifetime . The development and use o f harmonized technical standards , such as those in relation to bias mitigation , risk or quali ty management , is encouraged to facilitate the implementation of conformity assessments . Other than certain remote biometric identification systems , however , which must be assessed by independent third parties ( “ notified bodies ” ) , prov iders of most high-risk AI systems can attest to their conformity with requirements of the Act throu gh a self-assessment . Moreover , Article 43 of the Act indicates that high-risk systems that are i n conformity with “ harmonized standards ” shall be presumed to be in compliance with the Act , poten tially enabling a self-assessment regime for all high-risk systems . Commentators have expressed conc ern that barriers to participation by human rights experts and civil society organizations in t he development of technical standards could lead to the recognition of “ harmonized standards ” that d o not address the Actʼs human rights objectives.44 To strengthen accountability for the protection of human rights , Article 9 of the EU AI Act could be revised to make the assessment of AI systemsʼ human rights risks an explicit feature of high-risk providersʼ risk management systems . This would help incentivize the harmonized development of human rights-based approaches to risk management st andards and conformity assessments both within and outside the EU . Furthermore , lawmakers m ay consider requiring providers of high-risk systems to submit to a conformity assessment conduc ted by an independent third party in certain cases , for instance , in the event of non-compliance with post-market monitoring requirements , or where serious incidents are reported ( in addition t o a potential requirement to remove the system from the market until such a conformity assessment or re-evaluation has been completed ) . Finally , while the Commission has chosen to designate a defi ned list of AI systems as high-risk a priori , it should consider developing and publishing a clear , objective methodology for assigning a level of risk to new AI systems , or for re-evaluating the in itial risk designation for existing systems in ligh t of new evidence . As per recommendations by civil socie ty organizations,45policy makers should also 45Access Now , European Digital Rights ( EDRi ) , Panopty kon Foundation , epicenter.works , AlgorithmWatch , Eu ropean Disability Forum ( EDF ) , Bits of Freedom , Fair Trials , ANEC ( European consumer voice in standardisation ) Platform for Un documented Migrants , “ An EU44Michael Veale and Frederik Zuiderveen Borgesius . “ D emystifying the Dra  EU Artificial Intelligence Act . ” SocArXiv , 6 July 2021 11 Human rights impact assessments for AI : analysis an d recommendations consider allowing for the list of prohibited practi ces ( Title II ) and the list of practices in Title I V to be updated , and provide clear criteria to guide such d ecisions about risk designation . Such criteria could be based , for instance , on the OECDʼs ongoing work to develop a framework for classification and risk identification.46The methodology should take into account the criter ia listed by Mantelero and Esposito for HRIAs for AI,47and include guidance to clarify situations in which the deployment of an AI system poses unacceptable risks to society and should not be allowed to proceed . In light of the need for harmonized techniques , pro posals to use the GDPRʼs Data Protection Impact Assessments ( DPIAs ) have been oﬀered as an e ﬀective mechanism to fulfill the risk and impact assessment obligations put forth in the EU A I Act , especially for high-risk applications involving personal data.48Article 35 of the GDPR states that DPIAs are require d where a type of data processing “ is likely to result in a high risk to t he rights and freedoms of natural persons . ” 49DPIAs are especially required if an entity is implementin g a new technology or if the data processing is used to make automated decisions.50This supports Article 22 of the GDPR , which aﬀords d ata subjects the right to not be subject to a decision based solely on automated p rocessing.51Providing data subjects with insights from a DPIA can help to inform their decision for whether they approve of the implementation of automated decision-making . While DPIAs may be eﬀective at helping to identify and mitigate risks of AI , we believe there are three primary limitations that must be considered . First , DPIAs are primarily flagged for applications that use personal data . While these ap plications are o en high-risk , this scoping may inadvertently overlook applications that pose signi ficant human rights risks without the use of personal data . Additional high-risk applications , s uch as use of AI in control systems for critical infrastructure , may not be flagged for more strenuo us oversight and evaluation even though their failure poses catastrophic human rights implication s. Second , DPIAs are framed to the individual rather than groups , including broader societal or e nvironmental risks . DPIAs are primarily focused on evaluating individual rights ( e.g. , to privacy , human agency ) and as such may not adequately cover the identification and mitigation of risks of AI for collective rights ( e.g. , cultural identity ) . Third , while there are overarching guidelines for w hat should be included and assessed in DPIAs , 51European Union General Data Protection Regulation . “ Article 22 . Automed individual decision-making , in cluding profiling . ” https : //gdpr.eu/article-22-automated-individual-dec ision-making/50European Data Protection Board ( EDPB ) and European D ata Protection Supervisor ( EDPS ) . “ Joint Opinion on the proposal for a regulation of the European Parliament and of the Co uncil laying down harmonized rules on artificial in telligence ( Artificial Intelligence Act ) , May 2021 , https : //edps.europa.eu/system/files/2021-06/2021-06 -18-edpb-edps_joint_opinion_ai_regulation_en.pdf .49European Union General Data Protection Regulation . “ Article 35 . Data protection impact assessment . ” https : //gdpr.eu/article-35-impact-assessment/ .48Access Now . “ Hereʼs how to fix the EUʼs Artificial I ntelligence Act . ” Sept. 7 , 2021 , https : //www.accessnow.org/how-to-fix-eu-artificial- intelligence-act/ ; European Data Protection Board ( EDPB ) and European Data Protection Supervisor ( EDPS ) . “ Joint Opinion on the proposal for a regulation of the European Parliame nt and of the Council laying down harmonized rules on artificial intelligence ( A rtificial Intelligence Act ) , May 2021 , https : //edps.europa.eu/system/files/2021-06/2021-06 -18-edpb-edps_joint_opinion_ai_regulation_en.pdf . United Kingdom Information Commissionerʼs Oﬀice ( ICO ) “ The Information Commiss ionerʼs response to the European Commission 's white paper on artificial intelligence - a European approach to excellence an d trust . June 10 , 2020 , https : //ico.org.uk/media/about-the-ico/consultation -responses/2617826/ico-response-to-eu-commission-wh ite-paper-on-ai.pdf .47Alessandro Mantelero and Samantha Esposito . `` An evi dence-based methodology for human rights impact asse ssment ( HRIA ) in the development of AI data-intensive systems . '' Computer Law & Security Review ( 2021 ) 46OECD , Network of AI Experts , Classifying and Risk , ( 2021 ) https : //oecd.ai/en/network-of-experts/working-group /1137Artificial Intelligence Act for Fundamental Rights - A Civil Society Statement , ” https : //www.accessnow.org/cms/assets/uploads/2021/1 1/joint-statement-EU-AIA.pdf . For specifc details on the proposals to update the risk categories , see this issue paper on future-pro ofing the risk-based approach ” https : //accessnow.org/AIAct-risk-approach 12 Human rights impact assessments for AI : analysis an d recommendations there is great variability in how these guidelines are implemented in practice across the EU.52 Further , DPIAs are not strictly mandated to be made publicly available . As such , this poses significant risk to the realization of transparency and accountability in AI governance . The EU AI Act could complement the GDPR by providing measures add ressing these points . Another eﬀort underway to provide greater transpare ncy and oversight over AI-driven systems is the EU Digital Services Act ( DSA ) . The DSA represen ts a significant development in ongoing eﬀorts to identify and address the human rights impacts of very large online platforms ( VLOPs ) , as well as the algorithmic systems they employ . In particular , the DSA could impose an obligation on VLOPs ( currently defined as having 45 million active mont hly users in the EU , but still under negotiation ) to conduct annual risk assessments specific to thei r services , which appears to combine aspects of AIAs and HRIAs . Specifically , Article 26 of the DSA provides that VLOPs must analyze and assess “ systemic risks ” to fundamental rights enshrined in the EU Charter , taking into account the impact of content moderation and recommender systems on su ch rights.53In this way , platforms are required to evaluate the impacts of these algorithm ic systems on the rights enumerated . VLOPs must also submit to independent audits assessing an d reporting on their compliance with obligations under the DSA , including the requiremen t to conduct annual risk assessments . The platforms must report on the outcome of these risk assessments , mitigations , as well as the implementation of mitigations and audit recommendat ions every six months . As compared to DPIAs , the public reporting requirements of risk as sessments proposed in the DSA are a promising step forward . Several improvements to certain provisions of the D SA could help close potential gaps in oversight and mitigation of human rights harms . First , Articl e 26 of the DSA could be amended to clarify that the requirement to conduct annual risk assessments flows from platformsʼ more general obligation to institute organization-wide human rights due dil igence processes , including conducting risk assessments regularly and at critical stages of the AI lifecycle , ( as informed by the UNGPs and Latonero and Agarwalʼs emerging scholarship on HRIA s for AI ) .54This could help avoid some of the pitfalls and misconceptions associated with HRIAs , as noted above , including their potential to be manipulated through decisions with respect to timin g and scope , and promote the implementation of ongoing human rights-based risk m anagement processes by platforms . Second , given their complexity , scale , and scope of impact , risks assessments performed by VLOPs should follow Mantelero and Espositoʼs recommendati on for HRIAs conducted in complex multi-factor scenarios ( e.g. , as in the case of Sid ewalk Labs ) and include consultations with independent human rights experts , civil society , an d stakeholder groups.55This would add a higher level of accountability and legitimacy to the risk assessments as well as to the mitigations 55Alessandro Mantelero and Samantha Esposito . `` An evi dence-based methodology for human rights impact ass essment ( HRIA ) in the development of AI data-intensive systems . '' Computer Law & Security Review ( 2021 ) 54Mark Latonero and Aaina Agarwal . “ Human rights impa ct assessments for AI : Learning from Facebookʼs fai lure in Myanmar . ” Carr Center Discussion Paper Series , March 19 ( 2021 ) 53European Commission . “ EU Charter of Fundamental Rig hts ” ( 2012 ) . https : //ec.europa.eu/info/aid-development-cooperati on-fundamental-rights/your-rights-eu/eu-charter-fun damental-rights_en52United Kingdom Information Commissionerʼs Oﬀice . “ D ata protection impact assessments . ” Last accessed O ct. 21 , 2021 , https : //ico.org.uk/for-organisations/guide-to-data- protection/guide-to-the-general-data-protection-reg ulation-gdpr/accountability-and -governance/data-protection-impact-assessments/ 13 Human rights impact assessments for AI : analysis an d recommendations proposed . Similarly , Article 28 could be amended to include a requirement for organizations performing the independent audits of platformsʼ obl igations to have specific expertise in human rights , given its identification as a “ systemic ris k ” ( currently , this provision requires “ proven expertise in the area of risk management , technical competence and capabilities ” as well as “ professional ethics ” ) . Finally , given the global scale of platforms and th e likely impact of the DSA outside of the EU , the supervisory authorities that will be tasked with th e enforcement of the DSA should consider using the UDHR and the UNGPs as a basis for developing gu idelines for content moderation-specific HRIAs and human rights due diligence . This would he lp safeguard against the problem of scoping risk assessments too narrowly , and help promote gre ater consideration of collective and societal issues , such as public health / mental health , the environment , and electoral integrity . Towards a Systemic Approach If AIAs are to be relied upon to protect society fr om potential AI harms , inclusion of risks to fundamental human rights will be critical to their success . Generally , the object of AIAs consists of the algorithmic or AI system ( s ) , including the data sets used to train these systems . One of the current trends associated with AIAs is to focus on assessing the sociotechnical aspects ( e.g. , the potential for bias , fairness , or explainability of the system ) and their immediately foreseeable and measurable risks or consequences . In doing so , Metc alf et al . ( 2021 ) caution that AIAs may lead to an “ ontological flattening ” of the risks of AI-driv en systems.56Approaching AIAs in this manner may inadvertently lead to overlooking human rights risk s altogether , or a failure to identify the connection between technical weaknesses and downstr eam , context-dependent impacts , including to human rights—especially those that occ ur secondarily ( e.g. , the chilling eﬀect of misidentification by facial recognition systems on an individualʼs freedom of assembly and expression or the tendency of misinformation to amp lify online misogyny and radicalization ) . In this sense , the range of issues to consider in the context of AIAs can be far more extensive than for traditional product reviews . As such , scoping AIAs too narrowly can lead to a false sense of due diligence in risk identification and mitigation , al lowing tools with non-trivial risks to human rights to operate freely . Defining the scope of an HRIA also presents specifi c challenges . Because the focus of the exercise shi s from an assessment of the quantifiable techni cal risks of an AI system to the potential for real-life impacts on the rights and freedoms of ind ividuals and communities , the scope of HRIAs tend to be broader and more forward-looking than th at of AIAs by default . Accordingly , while the subject of an HRIA could be the AI system itself , t he assessment is more likely to require consideration of risk and impacts at a higher level , for example , resulting from the deployment of the product in diﬀerent contexts ; the nature of the overarching business or public activity ; the presence of adequate legal protections or governanc e structures , including whether there is a history of human rights abuses where the AI is to b e deployed ; the track record of supply chain 56Jacob Metcalf et al . `` Algorithmic Impact Assessment s and Accountability : The Co-Construction of Impact s. '' In Proceedings of the 2021 ACM Conference on Fairness , Accountability , and Tra nsparency , pp . 735-746 . 2021 . 14 Human rights impact assessments for AI : analysis an d recommendations partners ; or all of the above . Furthermore , HRIA gu idance cautions against preemptively narrowing the scope of human rights and freedoms to be invest igated at the outset of an assessment , for instance , to consider only risks or impacts related to the right to privacy or equality and non-discrimination . In addition to the need to design appropriate metho dologies for conducting AIAs and HRIAs for AI in diﬀerent contexts , their operationalization also raises important considerations , for instance , in light of the administrative burden and costs involv ed . One approach taken by companies is to set up a central unit that develops internal policies a nd procedures for AI governance , which may incorporate components of AIAs and/or HRIAs . This r equires hiring additional personnel with appropriate socio-technical expertise , consequently increasing operating costs . Even with a central “ responsible AI ” unit in place , additional hurdles arise with respect to training diﬀerent teams to identify and mitigate potential AI risks , in partic ular on account of the distinct skill sets , roles , and responsibilities of personnel at various stages of the AI lifecycle ( e.g. , design , development , or deployment ) . Companies may opt to conduct training one multidisciplinary workshop at a time and struggle to administer AI governance at the ent erprise level . Scalability challenges may be further compounded by the potential for AI systems to exhibit diﬀerent risks depending on the context of deployment , and the global scale at whic h systems may operate . Alternatively , another approach taken by companies , especially small- to m edium-size enterprises that may not have the financial backing to develop a standalone “ responsi ble AI ” unit , may be to hire external consultants to help adapt existing policies and pro cedures to the AI context ; upskill employees ; or to prioritize conducting in-depth , standalone AIAs and/or HRIAs for applications believed to be higher risk . In the absence of proper guidance , the timing of im pact assessments can also have significant eﬀects on their outcomes and credibility . For examp le , a recent study of the HRIA commissioned by Facebook regarding its potential implication in the genocide in Myanmar cautioned against the use of HRIAs as one-time , ex post exercises , which could become a form of AI “ ethics w ashing . ” 57Rather , and as instructed by the UNGPs , HRIAs should be con ducted at appropriate intervals , aligned with critical stages of the AI lifecycle and as part of ongoing risk management processes such as human rights due diligence.58In addition , the study concluded that HRIAs should b e conducted at the earliest stages of the design or conception of AI s ystems . The ex ante HRIA conducted on Alphabet-aﬀiliate Sidewalk Labsʼ “ smart-city ” project in the City of Toronto represents one potential example of this ap proach . More than 50 proposed digital solutions , including some anticipated to leverage t he use of AI , were assessed prior to the confirmation of the project . The project was ultima tely abandoned and some experts involved in the consultation pointed to human rights flaws of t he proposed plans.59While the final report of this HRIA was never publicly released , the exercise , which included extensive consultation with subject matter experts and local stakeholders , cont ributed to the acceleration and enhancement of 59Gabrielle Canon . `` 'City of surveillance ' : privacy e xpert quits Toronto 's smart-city project . '' The Guardian , October 2018. https : //www.theguardian.com/world/2018/oct/23/toron to-smart-city-surveillance-ann-cavoukian-resigns-pr ivacy58Mark Latonero and Aaina Agarwal . “ Human rights impa ct assessments for AI : Learning from Facebookʼs fai lure in Myanmar . ” Carr Center Discussion Paper Series , March 19 ( 2021 ) 57Ibid . , 41 . 15 Human rights impact assessments for AI : analysis an d recommendations existing human rights-based governance eﬀorts relat ed to the project.60However , as Mantelero and Esposito ( 2021 ) point out , while labor-intensive HR IAs that involve extensive research and field work , including consultations with local stakeholde rs and subject matter experts , may be desirable in complex multi-factor scenarios ( e.g. , large smar t-city projects ) , they are likely too burdensome and costly to serve as appropriate models for proje cts of a smaller scale.61Consideration should be given to developing light touch HRIAs , with methodo logies calibrated to the nature of the context , risk profile , and/or stage of the AI lifecycle . In light of the dynamic nature of AI systems , which can evolve , dri  , or adapt in unpredictable ways , reliance on static governance tools , such as AIAs , may capture only a snapshot of an AI systemʼs operations upfront and be ineﬀective at id entifying potential downstream risks and necessary mitigations . Rather , continuous monitorin g and auditing of deployed systems by regulatory authorities may require the development of technologies that can help automate verification of compliance and complement human ove rsight.62Given that AIʼs technical capabilities are progressing at a pace that greatly outstrips the ability to govern their harms through primarily manual risk management processes , the adaptation of policy frameworks and increased investment by both the public and private sectors could help incentivize the development of technologies that can help implement AI governance at scale more eﬀectively.63 Ultimately , design specifications and implementatio n tactics for AIAs and HRIAs will have to be tailored to the complexity , scale , context , and sco pe of the projects they are intended to assess , including their phase of development . Without secto r-specific guidance , standards , or training of qualified personnel , the operationalization of AIAs and HRIAs is likely to face significant hurdles and be inadequate to address specific impacts on ri ghts . In this context , poor outcomes associated with conducting AIAs or HRIAs for AI , whether due t o their administrative burden or failure to identify and mitigate risks , should be expected to have negative feedback eﬀects on their legitimacy . At least part of the solution to this p roblem could reside with standards bodies , such as the IEEE , International Organization for Standardiz ation ( ISO ) , NIST and national counterparts , which are beginning to develop standards and confor mity assessments to guide the responsible development and deployment of AIAs and related risk management processes . These so  law tools may have significant eﬀects on human rights d ue diligence in the context of AI , providing enterprise-level guidance regarding best practices and clarifying expectations for accountability . 63Daniel Zhang , Saurabh Mishra , Erik Brynjolfsson , Joh n Etchemendy , Deep Ganguli , Barbara Grosz , Terah Ly ons , James Manyika , Juan Carlos Niebles , Michael Sellitto , Yoav Shoham , Jack Clark , and Raymond Perrault , “ The AI Index 2021 An nual Report , ” AI Index Steering Committee , Human-Centered AI Institute , Stanford Un iversity , Stanford , CA , March 2021.62Gillian Hadfield , Rules for a Flat World , Oxford Un iversity Press , May 14 ( 2020 ) ; Jack Clark & Gillian K. Hadfield , 2019 . `` Regulatory Markets for AI Safety , '' Papers 2001.00078 , arXiv.or g61Alessandro Mantelero and Samantha Esposito . `` An evi dence-based methodology for human rights impact ass essment ( HRIA ) in the development of AI data-intensive systems . '' Computer Law & Security Review ( 2021 ) .60Corporation of the Canadian Civil Liberties Associa tion and Lester Brow n v. Toronto Waterfront Revitalization Corporation , et a l. , ( Ontario Superior Court of Justice File No . 211/19 ) , Aﬀidavit of Kristina Verner , January 17 , 2020 , https : //ccla.org/wp-content/uploads/2021/06/Aﬀidavi t-of-Kristina-Verner_TSC.pdf ; 16 Human rights impact assessments for AI : analysis an d recommendations THE ROLE OF STANDARDS AND CERTIFICATIONS In parallel with the development of AI principles a nd the exploration of regulations , standard development organizations ( SDOs ) , at both the natio nal and international levels , have been actively working on developing AI standards and con formity assessments . The standards may provide helpful guidance on creating and implementi ng eﬀective AIAs by oﬀering definitional clarity on how to operationalize responsible AI pri nciples in practice . Conformity assessments will be used to verify that a companyʼs product , service , management/governance process meets the normative and/or technical requirements contained i n those standards . As an additional step , certification schemes are being developed to enable accredited third-party assessors to certify conformity with AI standards by issuing a certifica tion “ mark ” or “ label . ” However , caution must be taken to ensure certifications are not confusing or deceptive , leading to a sense of “ false trust ” in AI products and services , as has been witnessed in oth er industries.64 As these processes mature , it is likely that certai n AI-related industry standards and conformity assessments will be incorporated into legislation o r regulation as a condition of compliance . With diverging approaches to AI regulation being propose d , in Europe and elsewhere , the international harmonization and mutual recognition of AI standard s and conformity assessments will emerge as significant geopolitical issues , which is critical to the protection against AI harms but also to the international trade of AI goods and services . In recognition of the global importance of AI stand ards , the IEEE has demonstrated a commitment to the development of a human rights-driven approac h. Its report outlines a conceptual framework for addressing universal human values , data agency , and technical dependability through a set of principles to guide developers and users engaged in the design , development , and deployment of AI systems . Human rights are identified as the firs t General Principle , with explicit reference to the international human rights framework and the releva nce of the UNGPs . Additionally , the IEEE is developing an Ethics Certification Program for Auto nomous and Intelligent Systems ( ECPAIS ) . The ECPAIS is currently developing a set of standards f ocused on bias , transparency , and accountability . If a developer implements the ECPAI S standards , it can add a quality assurance mark to its products and services with the intent t o raise consumer trust and market power.65 The ISO and the International Electrotechnical Comm ission ( IEC ) are advancing a conformity assessment standard for AI risk management through the work of a joint committee on artificial intelligence ( ISO/IEC JTC1/SC 42 ) .66The proposed ISO/EC 42001 - Artificial Intelligence Management System ( AIMS ) standard will enable organ izations to show they have implemented and continually work on improving processes to addr ess bias , fairness , inclusiveness , safety , security , privacy , accountability , applicability , a nd transparency in AI . 66Standards by ISO/IEC JTC 1/SC 42 Artificial intelli gence , International Organizations for Standardizat ion , accessed August 2021 , https : //www.iso.org/committee/6794475/x/catalogue/p /0/u/1/w/0/d/065 “ The Ethics Certification Program for Autonomous an d Intelligent Systems ( ECPAIS ) , ” IEEE , 2021 , https : //standards.ieee.org/industry-connections/ecp ais.html64Lesley Fair , “ Deceptive ʻCertified Organicʼ claims leave consumers verklempt , ” Federal Trade Commission , https : //www. c.gov/news-events/blogs/business-blog/ 2019/09/deceptive-certified-organic-claims-leave-co nsumers-verklempt 17 Human rights impact assessments for AI : analysis an d recommendations In January 2021 , the US Congress mandated that NIST identify and provide “ standards , guidelines , best practices , methodologies , procedures , and proc esses for developing trustworthy AI systems . ” 67Within two years , NIST is required to develop an AI risk management framework that enables the assessment of “ trustworthy ” AI and iden tification of appropriate risk mitigation strategies on a voluntary basis in the public and p rivate sectors.68NIST is to establish common definitions and characterizations for AI principles , such as explainability , transparency , and fairness . In June 2021 , NIST issued a dra  report d efining diﬀerent types of bias and mitigation strategies—an important first step in establishing standards for appropriate oversight and risk mitigation.69Given the important role that standards and conformi ty assessments are expected to play in supporting compliance with the proposed EU AI Act , more explicit linkages should be made between the technical assessments of AI systems and their potential downstream human rights impacts as these eﬀorts evolve . In March 2021 , the European Committee for Standardi zation ( CEN ) and the European Committee for Electrotechnical Standardization ( CENELEC ) esta blished Joint Technical Committee 21 on Artificial Intelligence ( CEN-CLC/JTC 21 ) to proceed with the development and adoption of standards for AI and related data , including intern ational standards already available or under development from organizations like ISO/IEC JTC 1 a nd its subcommittees , such as SC 42 Artificial Intelligence . CEN-CLC/JTC 21 will focus on producin g standardization deliverables that address “ European market and societal needs , as well as und erpinning EU legislation , policies , principles , and values . ” 70 The European Commission issued a report in 2021 out lining relevant standards that support compliance with its AI Act , including standards fro m the IEEE and ISO to guide appropriate data governance ; risk management ; technical data and rec ord keeping ; transparency and accountability ; human oversight ; accuracy , robustne ss , and cybersecurity ; and implementation of a quality management system to ensure compliance wi th regulation.71 As AI standards and conformity assessments mature , implementation of certification schemes designed to operationalize them are gaining promine nce . Certification can be defined as the “ attestation that a product , process , person , or or ganization meets specified criteria . ” 72In AI , certifications are emerging for both the technology itself ( e.g. , training data and model attributes ) and the development process ( e.g. , organizational e thics and risk management processes ) , or a combination of both . Certifications can be voluntar y or mandatory , self-assessed or third-party assessed . At this stage , self-certifications are th e most common with third-party certifications 72Peter Cihon et al. , `` AI Certification : Advancing Eth ical Practice by Reducing Information Asymmetries . '' IEEE Transactions on Technology and Society ( 2021 ) .71S . Nativi , and S. De Nigris . `` AI Watch : AI Standard ization Landscape . '' ( 2021 ) .70Other national standards organizations are undertak ing similar eﬀorts . In Canada , the national counter part to NIST and CEN-CENELEC recently received additional funding from the Canad ian government to advance the development and adopt ion of AI standards , including risk management standards and conformity assessment schemes for AI.69Reva Schwartz et al . “ A Proposal for Identifying an d Managing Bias in Artificial Intelligence , ” National Institute of Standards and Technology ( NIST ) , June 2021 , https : //nvlpubs.nist.gov/nistpubs/SpecialPublicatio ns/NIST .SP .1270-dra .pdf68Ibid . , 49.67NIST was assigned the task of creating an AI risk m anagement framework in the National Artificial Inte lligence Initiative Act of 2020 ( the AI Act ) , which was included in the 2021 Nation al Defense Authorization Act ; `` H.R.6395 - National Defense Authorization Act for Fiscal Year 2021 : Actions , '' Congress.gov , Library of Congr ess , accessed April 2 , 2021 , https : //www.congress.gov/bill/116th-congress/house- bill/6395/text . 18 Human rights impact assessments for AI : analysis an d recommendations being proposed for high-risk applications of AI . In the EUʼs AI Act , for example , developers of “ low-risk ” applications can perform voluntary self- assessments and certain “ high-risk ” applications are required to complete mandatory thi rd-party “ conformity assessments . ” Self-assessments or self-certifications are widely used in many industries but may lack legitimacy due to the inherent potential for conflicts of inte rest and low accountability for lack of enforcement . Third-party assessments are more rigor ous , but can be extraordinarily costly and require qualified assessors , which can be diﬀicult to find for complex AI systems.73The development of so ware-based assessment and certifi cation methods that automate and streamline regulatory compliance is one way that re searchers and industry are investigating new ways of implementing AI governance at scale.74 While AI certification processes are still at an ea rly stage , initiatives like the Responsible Artific ial Intelligence ( RAI ) Certification developed by the R esponsible AI Institute in collaboration with the World Economic Forum hold promise.75One of the first independent , accredited certificati on programs to emerge , the RAI Certification seeks to support the implementation of responsibly built AI systems through an objective third-party review process . Certification can incentivize implementation of appropriate risk identification a nd mitigation strategies ; however , there are significant challenges to successful implementation . For example , false positives where certification is provided even though certain crite ria have not been met or false negatives where certification is not provided even though all crite ria have been satisfied . Development of appropriate standards and certificat ions will depend on access to high quality data about AI operations in specific contexts.76Data collection from AI monitoring and measurement , therefore , will be critical to the eﬀe ctiveness of standards and certifications and essential to protecting human rights in high-risk c ontexts and applications . For human rights , appropriately defining evaluation criteria , assessm ent , and verification processes contained in standards and certifications will be critical . In a field where concepts of “ fair , ” “ accountable , ” and “ trustworthy ” AI are still under development , defin ing and enforcing appropriate procedures to uphold human rights in AI is equally muddled . While promising to uncover human rights risks of AI and whether strategies are in place to mitigate the se risks , use of standards and certifications to indicate human rights due diligence should be cauti ously implemented . CONCLUSIONS AND RECOMMENDATIONS Given the important human rights considerations rai sed by the use of AI systems , closer linkages should be made between the study and practice of AI As and lessons learned from the implementation of HRIAs . In particular , AIAs can pl ay an important role in identifying the technical foundations needed to promote respect for human rig hts . In turn , international human rights law 76Jess Whittlestone and Jack Clark . `` Why and How Gove rnments Should Monitor AI Development . '' arXiv preprint arXiv:2108.12427 ( 2021 ) .75 “ RAI Certification Beta , ” Responsible Artificial In telligence Institute , accessed Sept. 1 , 2021 , https : //www.responsible.ai/certification .74Gillian Hadfield : Regulatory technologies can solve the problem of AI , University of Toronto Schwartz Reisman Institute for Technology and Society , last modified April 19 , 2021 ; see also Gillian Hadfield , Rules for a Flat World , Oxford Uni versity Press , May 14 ( 2020 ) ; Jack Clark & Gillian K. Hadfield , 2019 . `` Regulatory Mark ets for AI Safety , '' Papers 2001.00078 , arXiv.org73Ibid . , 54 . 19 Human rights impact assessments for AI : analysis an d recommendations can serve as a helpful guide for identifying connec tions between AI systemsʼ technical features and human rights implications , especially for vulnerabl e individuals and communities . Significant work remains to develop best practices for successful implementation of AIAs and HRIAs for AI , including considerations related to h ow AIAs should integrate features of HRIAs and their appropriate scope , structure , scalability , ti ming , and administrative burden . In this respect , however , the emergence of common approaches and met hodologies for AIAs and HRIAs for AI will be aided by the development of human-rights based t echnical standards , conformity assessments , and certification schemes , as well as customized gu idance for their implementation in a variety of contexts . As policymakers advance discussions on dra  legisla tion covering algorithmic and AI systems , the following recommendations could help ensure respect for human rights—within the EU and beyond . 1 . Legislation requiring large platforms to conduct risk assessments of their operations should require considerations of risks to internati onal human rights and freedoms and disclosure of actions taken to mitigate these risks . 2 . AI quality management and risk management standar ds developed should explicitly address risks to international human rights and fre edoms . 3 . Organizations and consortia empowered by legislat ion to perform independent risk assessments , conformity assessments , and audits sho uld include personnel and civil society organizations with proven human rights expe rtise . 4 . Self-assessment regimes for AI systems should be complemented by post-market monitoring that triggers independent conformity ass essments or audits in certain cases ; for instance , in situations where a companyʼs viola tion of its obligations raises human rights concerns . Clear avenues should also be estab lished for people aﬀected by AI systems , or groups representing them , to flag harms and thereby trigger investigations by enforcement bodies . 5 . A model risk assessment methodology that explicit ly addresses human rights concerns triggered by AI systems should be developed with th e involvement of all relevant stakeholders . a . This should begin by defining approaches to human rights risk assessments of specific applications of algorithmic and AI systems , notably in the context of large digital platforms and high-risk AI systems by build ing oﬀ of best practices identified in the UNGPs , the technical assessment defined by t he EU High-level Expert Group on AI , and emerging scholarship . b . The methodology should include procedures for lin king the technical performance of AI systems with potential downstream human right s impacts on individuals and communities , in particular , those at a higher risk of vulnerability . 6 . Given the critical role of human rights considera tions in the development of AI standards , conformity assessments , and certification schemes , governments should establish meaningful opportunities and support mechanisms ( e. g. , subsidies for travel ) for human 20 Human rights impact assessments for AI : analysis an d recommendations rights experts and civil society organizations to p articipate in these processes at the national and international levels . 7 . To facilitate rights-respecting AI governance at scale , policymakers should ensure appropriate coordination of research & development spending with AI standardization pilot programs , such as regulatory sandboxes , to ac celerate : a. the adoption of privacy-enhancing technologies , t echnical tools that enable bias and fairness detection and mitigation , or the conti nuous monitoring of AI system performance ; and , b . The customization of certification schemes to ens ure their robust implementation in a variety of contexts , beginning with high-risk applications of AI systems . ACKNOWLEDGMENT An earlier version of this paper was published as p art of the Harvard Kennedy School Carr Center for Human Rights Policy Discussion Paper Series . 21
