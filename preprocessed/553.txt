United Nations A/73/348 General Assembly Distr . : General 29 August 2018 Original : English 18-14238 ( E ) 1 90918 * 1814238 * Seventy -third session Item 74 ( b ) of the provisional agenda * Promotion and protection of human rights : human rights questions , including alternative approaches for improving the effective enjoyment of human rights and fundamental freedoms Promotion and protection of the right to freedom of opinion and expression * * Note by the Secretary -General The Secretary -General has the honour to transmit to the General Assembly the report prepared by the Special Rapporteur on the promotion and protection of the right to freedom of opinion and expression , David Kaye , submitted in accordance with Human Rights Council resolution 34/18 . In the present report , the Special Rapporteur explores the implications of artificial intelligence technologies for human rights in the information environment , focusing in particular on rights to freedom of opinion and expression , privacy and non -discrimination . * A/73/150 . * * The present report was submitted after the deadline in order to reflect the most recent developments . A/73/348 18-14238 2/23 Report of the Special Rapporteur on the promotion and protection of the right to freedom of opinion and expression Contents Page I . Introduction ................................ ................................ ... 3 II . Understanding artificial intelligence ................................ ............... 3 A . What is artificial intelligence ? ................................ ................ 3 B . Artificial intelligence and the information environment ........................... 6 III . A human rights legal framework for artificial intelligence ............................. 10 A . Scope of human rights obligations in the context of artificial intelligence ............ 10 B . Right to freedom of opinion ................................ .................. 11 C. Right to freedom of expression ................................ ............... 12 D. Right to privacy ................................ ............................ 13 E. Obligation of non -discrimination ................................ .............. 14 F. Right to an effective remedy ................................ ................. 15 G. Legislative , regulatory and policy responses to artificial intelligence ................ 16 IV . A human rights -based approach to artificial intelligence ............................... 17 A . Substantive standards for artificial intelligence systems ........................... 18 B . Processes for artificial intelligence systems ................................ ..... 19 V. Conclusions and recommendations ................................ ................ 21 A/73/348 3/23 18-14238 I . Introduction 1 . Artificial intelligence is increasingly influencing the information environment worldwide . It enables companies to curate search results and newsfeeds as well as advertising placement , organizing what users see and when they see it . Artificial intelligence technologies are used by social media companies to help moderate content on their platforms , often acting as the first line of defence against content that may violate their rules . Artificial intelligence recommends people to friend or follow , news articles to read and places to visit or eat , shop or sleep . It offers speed , efficiency and scale , operating to help the largest companies in the information and communications technology sector manage the huge amounts of content uploaded to their platforms every day . Artificial intelligence technologies may enable b roader and quicker sharing of information and ideas globally , a tremendous opportunity for freedom of expression and access to information . At the same time , the opacity of artificial intelligence also risks interfering with individual self -determination , or what is referred to in the present report as “ individual autonomy and agency ” .1 A great global challenge confronts all those who promote human rights and the rule of law : how can States , companies and civil society ensure that artificial intelligence technologies reinforce and respect , rather than undermine and imperil , human rights ? 2 . The present report does not pretend to be the last word in artificial intelligence and human rights . Rather , it tries to do three things : define key terms essential to a human rights discussion about artificial intelligence ; identify the human rights legal framework relevant to artificial intelligence ; and present some preliminary recommendations to ensure that , as the technologies comprising artificial intelligence evolv e , human rights considerations are baked into that process . The report should be read as a companion to my most recent report to the Human Rights Council ( A/HRC/38/35 ) , in which a human rights approach to onli ne content moderation was presented.2 II . Understanding artificial intelligence A . What is artificial intelligence ? 3 . Artificial intelligence is often used as shorthand for the increasing independence , speed and scale connected to automated , comp utational decision making . Artificial intelligence is not one thing only , but rather refers to a “ constellation ” of processes and technologies enabling computers to complement or replace specific tasks otherwise performed by humans , such as making decision s and solving problems.3 “ Artificial intelligence ” can be a problematic term , suggesting as it does that machines can operate according to the same concepts and rules of human intelligence . They can not . Artificial intelligence generally optimizes the work of __________________ 1 See Mariarosaria Taddeo and Luciano Floridi , “ How AI can be a force for good ” , Science , vol . 361 , No . 6404 ( 24 August 2018 ) . Available at http : //science.sciencemag.org/content/ 361/6404/751.full . 2 The present report benefited from an expert consultation conducted in Geneva in June 2018 , supported with a grant from the Eu ropean Union , and the input from experts as part of the development of document A/HRC/35/38 in 2017 and 2018 . I especially wish to thank Carly Nyst and Amos Toh , who contributed essential research and drafting to this project . 3 See AI Now , “ The AI now report : the social and economic implications of artificial intelligence technologies in the near term ” , 2016 . Available at https : //ainowinstitute .org/AI_Now_ 2016 _ Report.pdf ; United Kingdom of Great Britain and Northern Ireland House of Lords Select Committee on Artificial Intelligence , “ AI in the United Kingdom : ready , willing and able ? ” , 2018 . A/73/348 18-14238 4/23 computerized tasks assigned by humans through iterative repetition and attempt . That said , it is the language of the culture , of companies and of governments , and I use it here . 4 . Popular culture often suggests that society is headed towards artificial general intelligence , a still -distant capability ( the “ singularity ” ) for a computer system to approximate or surpass human intelligence across multiple domains.4 For the foreseeable future , there will continue to be advancements with respect to narrow artificial intelligence , according to which computer systems perform programmed tasks ( human -developed algorithms ) in specific domains . Narrow artificial intelligence underpins , for example , voice assistance on mobile devices and customer service chatbots , on line translation tools and self -driving cars , search engine results and mapping services . Machine -learning is a category of narrow artificial intelligence techniques used to train algorithms to use datasets to recognize and help solve problems . For example , artificial intelligence -powered smart home devices are continuously “ learning ” from data collected about everyday language and speech patterns in order to process and respond to questions from their users more accurately . In all circumstances , humans pla y a critical role in designing and disseminating artificial intelligence systems , defining the objectives of an artificial intelligence application and , depending on the type of application , selecting and labelling datasets and classifying outputs . Humans always determine the application and use of artificial intelligence outputs , including the extent to which they complement or replace human decision -making . 5 . At the foundation of artificial intelligence are algorithms , computer code designed and written by humans , carrying instructions to translate data into conclusions , information or outputs . Algorithms have long been essential to the oper ation of everyday systems of communication and infrastructure . The enormous volume of data in modern life and the capacity to analyse it fuel artificial intelligence . The private sector certainly sees data that way : the more data available to feed algorith ms and the better the quality of that data , the more powerful and precise the algorithms can become . Algorithmic systems can analyse huge volumes of data rapidly , enabling artificial intelligence programmes to perform decision -making functions that were pr eviously the domain of humans acting without computational tools . __________________ 4 Article 19 and Privacy International , “ Privacy and freedom of expression in an age of artificial intelligence ” , London , 2018 . A/73/348 5/23 18-14238 Human agency is integral to artificial intelligence , but the distinctive characteristics of artificial intelligence deserve human rights scrutiny with respect to at least three of its aspects : automation , data analysis and adaptability.5 6 . Automation . Automation removes human intervention from parts of a decision making process , completing specific tasks with computational tools . This can have positive implications from a human rights perspective if a design limits human bias . For example , an automated border entry system may flag individuals for scrutiny based on objective features such as criminal history or visa status , limiting reliance on subjective ( and bias -prone ) assessments of physical presentation , ethnicity , age or religion . Automation also enables the processing of vast amounts of data at a speed and scale not achievable by humans , potentially serving public safety , health and national security . However , automated systems rely on datasets that , in their design or implementation , may allow for bias and thus produce discriminatory effects . For instance , the underlying criminal history or visa data suggested above itself may incorporate biases . Excessive reliance on and confidence in automated decisions and a failur e to recognize this foundational point may in turn undermine scrutiny of artificial intelligence outcomes and disable individuals from accessing remedies to adverse artificial intelligence -driven decisions . Automation may impede the __________________ 5 Council of Europe , Algorithms and Human Rights : Study on the Human Rights Dimensions of Automated Data Processing Techniques and Possible Regulatory Implications , Council of Europe study , No . DGI ( 2017 ) 12 , 2018 . Available at https : //www.coe.int/en/web/freedom expression/ -/algorithms -and-human -rights -a-new-study-has-been -published . A/73/348 18-14238 6/23 transparency and scrutability of a process , preventing even well -meaning authorities from providing an explanation of outcomes.6 7 . Data analysis . Vast datasets support most artificial intelligence applications . Any dataset could form the basis of an artificial intelligen ce system , from Internet browsing habits to data on traffic flows on highways . Some datasets contain personal data , while many others involve anonymized data . The use by artificial intelligence of such datasets raises serious concerns , including regarding their origins , accuracy and individuals ’ rights over them ; the ability of artificial intelligence systems to de-anonymize anonymized data ; and biases that may be ingrained within the datasets or instilled through human training or labelling of the data . Ar tificial intelligence evaluation of data may identify correlations but not necessarily causation , which may lead to biased and faulty outcomes that are difficult to scrutinize . 8 . Adaptability . Machine -learning artificial intelligence systems are adaptabl e , as the algorithms that power them are able to progressively identify new problems and develop new answers . Depending on the level of supervision , systems may identify patterns and develop conclusions unforeseen by the humans who programmed or tasked the m. This lack of predictability holds the true promise of artificial intelligence as a transformational technology , but it also illuminates its risks : as humans are progressively excluded from defining the objectives and outputs of an artificial intelligenc e system , ensuring transparency , accountability and access to effective remedy becomes more challenging , as does foreseeing and mitigating adverse human rights impacts . B . Artificial intelligence and the information environment 9 . Artificial intellig ence has particularly important , and sometimes problematic , consequences for the information environment , the complex ecosystem of technologies , platforms and private and public actors that facilitate access to and dissemination of information through digi tal means . Algorithms and artificial intelligence applications are found in every corner of the Internet , on digital devices and in technical systems , and in search engines , social media platforms , messaging applications and public information mechanisms . In keeping with the focus of my mandate , I note the following three applications of artificial intelligence in the information environment that raise concerns : 10 . Content display and personalization . Social media and search platforms increasingly dominat e how individuals access and share information and ideas and how news is disseminated . Algorithms and artificial intelligence applications determine how widely , when and with which audiences and individuals content is shared . Massive datasets that combine browsing histories , user demographics , semantic and sentiment analyses and numerous other factors feed into increasingly personalized algorithmic models to rank and curate information , that is , to show information to individuals or implicitly exclude it . P aid , sponsored or hashtagged content may be promoted to the exclusion or demotion of other content . Social media newsfeeds display content according to subjective assessments of how interesting or engaging content might be to a user ; as a result , individua ls may be offered little or no exposure to certain types of critical social or political stories and content posted __________________ 6 Council of Europe , Algorithms and Human Rights . A/73/348 7/23 18-14238 to their platforms.7 Artificial intelligence shapes the world of information in a way that is opaque to the user and often even to the platf orm doing the curation . 11 . Online search is one of the most pervasive forms of artificial intelligence powered content display and personalization . Search engines deliver results for queries ( and complete or predict queries ) using artificial intelligence systems that process extensive data about individual and aggregate users . Because poorly ranked content or content entirely excluded from search results is unlikely to be seen , the artificial intelligence applications for search have enormous influence ove r the dissemination of knowledge.8 Content aggregators and news sites9 similarly choose which information to display to an individual based not on recent or important developments , but on artificial intelligence applications that predict users ’ interests and news patterns based on extensive datasets . Consequently , artificial intelligence plays a large but usually hidden role in shaping what information individuals consume or even know to consume . 12 . Artificial intelligence in the field of content display is driving towards greater personalization of each individual ’ s online experience ; in an era of information abundance,10 personalization promises to order the chaos of the Internet , allowing individuals to find requested information . Benefits may include th e ability to access information and services in a greater range of languages11 or information that is more timely and relevant to one ’ s personal experience or preferences . Artificial intelligence -driven personalization may also minimize exposure to diverse views , interfering with individual agency to seek and share ideas and opinions across ideological , political or societal divisions . Such personalization may reinforce biases and incentivize the promotion and recommendation of inflammatory content or disinf ormation in order to sustain users ’ online engagement.12 To be sure , all sorts of social and cultural settings may limit an individual ’ s exposure to information . But by optimizing for engagement and virality at scale , artificial intelligence -assisted person alization may undermine an individual ’ s choice to find certain kinds of content . This is especially so because algorithms typically will deprioritize content with lower levels of engagement , banishing independent and user -generated content into __________________ 7 World Wide Web Foundation , “ The invisible curation of content : Facebook ’ s News Feed and our information diets ” , 22 April 2018 . Available at https : //webfoundation.org/research/the -invisible curation -of-content -facebooks -news -feed -and-our-information -diets/ . 8 Counc il of Europe , Algorithms and Human Rights . 9 For example , see “ How Reuters ’ s revolutionary AI system gathers global news , ” MIT Technology Review , 27 November 2017 . Available at https : //www.technologyreview.com/s/ 609558/how -reuterss -revolutionary -ai-system -gathers -global -news/ ; Paul Armstrong and Yue Wang , “ China ’ s $ 11 billion news aggregator Jinri Toutiao is no fake , ” Forbes , 26 May 2017 . Available at https : //www.forbes.com/sites/ywang/2017/05/26/jinri -toutiao -how-chinas -11-billion news -aggregator -is-no-fake/ # 1d8b97804d8a . 10 Carly Nyst and Nick Monaco , State -Sponsored Trolling : How Governments are Deploying Disinformation as Part of Broader Digital Harassment Campaigns ( Palo Alto , Institute for the Future , 2018 ) . 11 World Wide Web Foundation , “ Artificial intelligence : the road ahead in low - and middle -income countries ” , Washington , D.C. , 2017 . 12 Zeynep Tufekci , “ YouTube , the great radicaliser ” , New York Times , 10 March 2018 . Available at https : //www.nytimes.com/2018/03/10/opinion/sunday/youtube -politics -radical.html ; James Williams , Stand Out of our Light : Freedom and Resistance in the Attention Economy ( Cambridge , Cambridge University Press , 2018 ) . A/73/348 18-14238 8/23 obscurity.13 Savvy actors can exploit rule -based artificial intelligence systems optimized for engagement to gain higher levels of exposure , and by appropriating popular hashtags or using bots , they can achieve outsized online reach to the detriment of information div ersity . 13 . Content moderation and removal . Artificial intelligence helps social media companies to moderate content in accordance with platform standards and rules , including spam detection , hash -matching technology ( using digital fingerprints to identif y , for instance , terrorist or child exploitation content ) , keyword filters , natural language processing ( by which the nature of the content is assessed for prohibited words or imagery ) and other detection algorithms . Artificial intelligence may be used to subject user accounts to warnings , suspension or deactivation on the basis of violations of terms of service or may be employed to block or filter websites on the basis of prohibited domain data or content . Social media companies use artificial intelligenc e to filter content across the range of their rules ( from nudity to harassment to hate speech and so on ) , although the extent to which such companies rely on automation without human input on specific cases is not known.14 14 . Support and pressure for incr easing the role of artificial intelligence come from both the private and public sectors . Companies claim that the volume of illegal , inappropriate and harmful content online far exceeds the capabilities of human moderation and argue that artificial intell igence is one tool that can assist in better tackling this challenge . According to some platforms , artificial intelligence is not only more efficient in identifying inappropriate ( according to their rules ) and illegal content for removal ( usually by a huma n moderator ) but also has a higher accuracy rate than human decision -making . States , meanwhile , are pressing for efficient , speedy automated moderation across a range of separate challenges , from child sexual abuse and terrorist content , where artificial i ntelligence is already extensively deployed , to copyright and the removal of ‘ “ extremist ” and “ hateful ” content.15 The European Commission Recommendation on measures to further improve the effectiveness of the fight against illegal content online of March 2018 calls upon Internet platforms to use automatic filters to detect and remove terrorist content , with __________________ 13 Recently , some tech platforms have indicated their intention to move away from “ engagement ” — driven personalization to personalization which prioritizes the quality of a user ’ s experience online ; see Julia Carrie Wong , “ Facebook overhauls News Feed in f avour of ‘ meaningful social interactions ’ ” , The Guardian , 11 January 2018 . Available at https : //www.theguardian.com/ technology/2018/ja n/11/facebook -news -feed-algorithm -overhaul -mark -zuckerberg . However , without thorough transparency , reporting and metrics around how AI systems make and implement such assessments , it is difficult to assess whether this change is having a demonstrable effe ct on internet users ’ experience . 14 An Instagram tool , Deep Text , attempts to judge the “ toxicity ” of the context , as well as permitting users to customize their own word and emoji filers , and also assesses user relationship in a further attempt to establ ish context ( such as whether a comment is just a joke between friends ) . Andrew Hutchison , “ Instagram ’ s rolling out new tools to remove ‘ toxic comments ’ ” , Social Media Today , 30 June 2017 . Available at https : //www.socialmediatoday.com/social networks/instagrams -rolling -out-new-tools -remove -toxic -comments . 15 The United Kingdom reportedly developed a tool to automatically detect and remove terrorist content at the point of upload . See , for example , Home Office , “ New technology revealed to help fight terrorist content online ” , press release , 13 February 2018 . Se e European Commission , proposal for a directive of the European Parliament and of the Council on Copyright in the Digital Single Market , COM ( 2016 ) 593 final , art . 13 ; Letter from the Special Rapporteur to the President of the European Commission , reference No . OL OTH 41/2018 , 13 June 2018 . Available at https : //www.ohchr.org/Documents/Issues/Opinion/Legislation/OL -OTH -41-2018.pdf . A/73/348 9/23 18-14238 human review in some cases suggested as a necessary counterweight to the inevitable errors caused by the automated systems.16 15 . Efforts to autom ate content moderation may come at a cost to human rights ( see A/HRC/38/35 , para . 56 ) . Artificial intelligence -driven content moderation has several limitations , including the challenge of assessing context an d taking into account widespread variation of language cues , meaning and linguistic and cultural particularities . Because artificial intelligence applications are often grounded in datasets that incorporate discriminatory assumptions,17 and under circumstan ces in which the cost of over -moderation is low , there is a high risk that such systems will default to the removal of online content or suspension of accounts that are not problematic and that content will be removed in accordance with biased or discrimin atory concepts . As a result , vulnerable groups are the most likely to be disadvantaged by artificial intelligence content moderation systems . For example , Instagram ’ s DeepText identified “ Mexican ” as a slur because its datasets were populated with data in which “ Mexican ” was associated with “ illegal ” , a negatively coded term baked into the algorithm.18 16 . Artificial intelligence makes it difficult to scrutinize the logic behind content actions . Even when algorithmic content moderation is complemented by hu man review — an arrangement that large social media platforms argue is increasingly infeasible on the scale at which they operate — a tendency to defer to machine -made decisions ( on the assumptions of objectivity noted above ) impedes interrogation of conte nt moderation outcomes , especially when the system ’ s technical design occludes that kind of transparency . 17 . Profiling , advertising and targeting . Advances in artificial intelligence have both benefited from and further incentivized the data -driven busin ess model of the Internet , namely , that individuals pay for free content and services with their personal data . With the vast data resources amassed from years of online monitoring and profiling , companies are able to equip artificial intelligence systems with rich datasets to develop ever more precise prediction and targeting models . Today , advertising by private and public actors can be achieved at an individual level ; consumers and voters are the subject of “ microtargeting ” designed to respond to and exp loit individual idiosyncrasies . 18 . Artificial intelligence -driven targeting incentivizes the widespread collection and exploitation of personal data and increases the risk of manipulation of individual users through the spread of disinformation . Targeting can perpetuate discrimination , as well as users ’ exclusion from information or opportunities by , for example , permitting targeted job and housing advertisements that exclude older workers , __________________ 16 Commission recommendation of 1 M arch 2018 on measures to effectively tackle illegal content online ( C ( 2018 ) 1177 final ) . Available at https : //ec .europa.eu/digital -single -market/en/news/ commission -recommendation -measures -effectively -tackle -illegal -content -online ; see also Daphne Keller , “ Comment in response to European Commission ’ s March 2018 recommendation on measures to further improve the effec tiveness of the fight against illegal content online ” , Stanford Law School , Center for Internet and Society , 29 March 2018 . Available at http : //cyberlaw.stanford.edu/publications/comment -response -european -commissions -march -2018 recommendation -measures -further . 17 See Aylin Caliskan , Joanna Bryson and Arvind Narayanan , “ Semantics derived automatically from language corpo ra contain human -like biases ” , Science , vol . 356 , No . 6334 ( 14 April 2017 ) ; Solon Barocas and Andrew Selbst , “ Big data ’ s disparate impac t ” , California Law Review , vol . 104 , No . 671 ( 2016 ) . 18 Nicholas Thompson , “ Instagram ’ s Kevin Systrom wants to clean up the & # % @ ! Internet ” , Wired , 14 August 2017 . Available at https : //www.wired.com/2017/08/instagram -kevin -systrom wants -to-clean -up-the-internet/ . A/73/348 18-14238 10/23 women or ethnic minorities.19 Rather than individuals bein g exposed to parity and diversity in political messaging , for example , the deployment of microtargeting through social media platforms is creating a curated worldview inhospitable to pluralistic political discourse . III . A human rights legal framework f or artificial intelligence A . Scope of human rights obligations in the context of artificial intelligence 19 . Artificial intelligence tools , like all technologies , must be designed , developed and deployed so as to be consistent with the obligations o f States and the responsibilities of private actors under international human rights law . Human rights law imposes on States both negative obligations to refrain from implementing measures that interfere with the exercise of freedom of opinion and expressi on and positive obligations to promote rights to freedom of opinion and expression and to protect their exercise . 20 . With respect to the private sector , States are bound to guarantee respect for individual rights,20 especially the rights to freedom of opi nion and expression , including by protecting individuals from infringing acts committed by private parties ( article 2 ( 1 ) of the International Covenant on Civil and Political Rights ) . States can meet this obligation through legal measures to restrict or in fluence the development and implementation of artificial intelligence applications , through policies regarding the procurement of artificial intelligence applications from private companies by public sector actors , through self - and co -regulatory schemes a nd by building the capacity of private sector companies to recognize and prioritize the rights to freedom of opinion and expression in their corporate endeavours . 21 . Companies also have responsibilities under human rights law that should guide their const ruction , adoption and mobilization of artificial intelligence technologies ( A/HRC/38/35 , para . 10 ) . The Guiding Principles on Business and Human Rights : Implementing the United Nations “ Protect , Respect and Remedy ” Framework provide a “ global standard of expected conduct for all businesses wherever they operate ” ( principle 11 ) , including social media and search companies . To adapt the conclusions from the Guiding Principles to the domain of artificial intelli gence ( ibid. , para . 11 ) , the Guiding Principles require that companies , at a minimum , make high -level policy commitments to respect the human rights of their users in all artificial intelligence applications ( principle 16 ) ; avoid causing or contributing to adverse human rights impacts through their use of artificial intelligence technology and prevent and mitigate any adverse effects linked to their operations ( principle 13 ) ; conduct due diligence on artificial intelligence systems to identify and address a ctual and potential huma n rights impacts ( principles 17–19 ) ; engage in prevention and mitigation strategies ( principle 24 ) ; conduct ongoing review of artificial intelligence -related activities , including through stakeholder and public consultation ( princip les 20–21 ) , __________________ 19 Julia Angwin , Noam Scheiber and Ariana Tobin , “ Dozens of companies are using Facebook to exclude older workers from job ads ” , ProPublica , 20 December 2017 . Available at https : //www.propublica.org/article/facebook -ads-age-discrimination -targeting ; Julia Angwin , Ariana Tobin and Madeleine Varner , “ Facebook ( still ) letting housing advertisers exclude users by race ” , ProPublica , 21 November 2017 . Available at https : //www.propublica.org/article/ facebook -advertising -discrimination -housing -race-sex-national -origin . 20 See principle 3 of the Guiding Princi ples on Business and Human Rights : Implementing the United Nations “ Protect , Respect and Remedy ” Framework ( A/HRC/17/31 ) and A/HRC/38/35 , paras . 6–8 . A/73/348 11/23 18-14238 and provide accessible remedies to remediate adverse human rights impacts from artificial intelligence systems ( principles 22 , 29 and 31 ) . B . Right to freedom of opinion 22 . The freedom to hold opinions without interference is an absolute right , enshrined in article 19 ( 1 ) of the International Covenant on Civil and Political Rights and article 19 of the Universal Declaration of Human Rights . It “ permits no exception or restriction , ” whether “ by law or other power ” .21 In my 2015 report to th e Human Rights Council on encryption and anonymity in digital communications ( A/HRC/29/32 ) , I observed that the ways in which information is stored , transmitted and secured in the digital age uniquely affect t he exercise of the right to hold opinions . Search queries , browsing activities , email and text communications , and documents and mementos held in the cloud — together , these digital activities and records form the fabric of the opinions users hold ( ibid. , para . 12 ) . Both State and non -State actors may interfere with these mechanics and processes of forming and holding opinions . 23 . An essential element of the right to hold an opinion is the “ right to form an opinion and to develop this by way of reasoning ” .22 The Human Rights Committee has concluded that this right requires freedom from undue coercion in the development of an individual ’ s beliefs , ideologies , reactions and positions.23 Accordingly , forced neurological interventions , indoctrination programmes ( such as “ re-education camps ” ) or threats of violence designed to compel individuals to form particular opinions or change their opinion violate article 19 ( 1 ) of the Covenant . The Committee has also found that coercive “ inducements of preferential treatme nt ” may rise to a level of persuasion that interferes with the right to form and hold opinions ( see CCPR/C/78/D/878/1999 ) . 24 . The intersection of technology and content curation raises novel questions about the types of coercion or inducement that may be considered an interference with the right to form an opinion . Content curation has long informed the capacity of the individual to form opinions : for example , media outlets elevate particular stories to the front page with the intention of shaping and influencing individual knowledge about significant news of the day . Commercial advertising has also sought to induce favourable opinions of and cultivate desire for particular products and service s. 25 . The use of artificial intelligence extends and enhances the tradition of content curation on the Internet , providing more sophisticated and efficient means of personalizing and curating content for the user at a scale beyond the reach of traditiona l media . The dominance of particular modes of artificial intelligence assisted curation raises concern about its impact on the capacity of the individual to form and develop opinions . For example , a handful of technology companies lay claim to the vast maj ority of search queries conducted online . Corporate monopoly of the search market makes it extremely difficult for users to opt out of the algorithmic ranking and curation of search results and may also induce users to believe ( as companies intend it ) that the results generated are the most relevant or objective information available on a particular subject . The lack of transparency about how search criteria are developed and implemented through the use of artificial __________________ 21 Human Rights Committee , general comment No . 34 ( 2011 ) on the freedoms of opinion and expression , para . 9 , available at www2.ohchr.org/english/bodies/hrc/docs/GC34.pdf ; Manfred Nowak , U.N. Covenant on Civil and Political Rights : CCPR Commentary ( 1993 ) . 22 Nowak , U.N. Covenant on Civil and Political Rights . 23 Yong Joo -Kang v. Republic of Korea , Human Rights Committee communication No . 878/1999 , 16 July 2003 ( CCPR/C/78/D/878/1999 ) . A/73/348 18-14238 12/23 intelligence may also reinforce the assu mption that search results generated on a particular platform are an objective presentation of factual information . 26 . The issues that market dominance raises in the field of artificial intelligence assisted curation therefore test historical understandin gs of how content curation affects or does not affect the capacity to form an opinion . The novelty of the issues raised , coupled with the general lack of jurisprudence concerning interferences with the right of opinion , provide more questions than answers about the human rights impact of artificial intelligence -assisted curation in the contemporary digital environment . Nevertheless , these questions should drive rights -oriented research into the social , economic and political effects of artificial intelligen ce-assisted curation . Companies should , at the very least , provide meaningful information about how they develop and implement criteria for curating and personalizing content on their platforms , including policies and processes for detecting social , cultur al or political biases in the design and development of relevant artificial intelligence systems . C. Right to freedom of expression 27 . Article 19 ( 2 ) of the Covenant guarantees an expansive right to “ seek , receive and impart information and ideas of all kinds ” , one which must be protected and respected regardless of frontiers or type of media . Enjoyment of the right to freedom of expression is intimately related to the exercise of other rights and foundational to the effective functioning of democrati c institutions and , accordingly , the protection , respect and promotion of the right to freedom of expression entails the duty to include the promotion of media diversity and independence and the protection of access to information.24 28 . Unlike the right t o form and hold opinions , the rights to express and access information and ideas may be subject to restrictions under limited circumstances ( article 19 ( 3 ) of the Covenant ) . Restrictions must meet the standards of legality , meaning that they are publicly p rovided by a law that meets standards of clarity and precision and are interpreted by independent judicial authorities ; necessity and proportionality , meaning that they are the least intrusive measure necessary to achieve the legitimate interest at hand an d do not imperil the essence of the right ; and legitimacy , meaning that they must be in pursuit of an enumerated legitimate interest , namely , the protection of rights or reputations of others , national security or public order , or public health or morals ( A/HRC/38/35 , para . 7 ) . Within this framework , expression rights can also be restricted pursuant to article 20 ( 2 ) of the Covenant , which requires States to prohibit “ advocacy of national , racial or religious h atred that constitutes incitement to discrimination , hostility or violence ” , but restrictions must still satisfy the cumulative conditions of legality , necessity and legitimacy.25 29 . The complexity of decision -making inherent in content moderation may be exacerbated by the introduction of automated processes . Unlike humans , algorithms are today not capable of evaluating cultural context , detecting irony or conducting the critical analysis necessary to accurately identify , for example , “ extremist ” content o r __________________ 24 Special Rapporteur on the promotion and protection of the right to freedom of opinion and expression , Organization for Security and Co -operation in Europe Representative on Freedom of the Media , Organization of American States Special Rapporteur on freedom of expression and African Commission on Human and Peoples ’ Rights Special Rapporteur on freedom of expression and access to information , “ Joint Declaration on freedom of expression and ‘ fake news ’ ” , disinformation and propaganda ” , 3 March 2017 . Available at https : //www.osce.org/fom/ 302796 ; see also Human Rights Committee , general comment No . 34 ( 2011 ) on the freedoms of opinion and expression ; A/HRC/29/32 , para . 61 and A/HRC/32/38 , para . 86 . 25 Human Rights Committee , general comment No . 34 ( 2011 ) on the freedoms of opinion and expression , para . 50 . A/73/348 13/23 18-14238 hate speech26 and are thus more likely to default to content blocking and restriction , undermining the rights of individual users to be heard as well as their right to access information without restriction or censorship . 30 . In an artificial intelligenc e-governed system , the dissemination of information and ideas is governed by opaque forces with priorities that may be at odds with an enabling environment for media diversity and independent voices . Relevantly , the Human Rights Committee has found that St ates should “ take appropriate action … to prevent undue media dominance or concentration by privately controlled media groups in monopolistic situations that may be harmful to a diversity of sources and views ” .27 31 . Users also lack access to the rules of the game when it comes to artificial intelligence -driven platforms and websites . A lack of clarity about the extent and scope of artificial intelligence and algorithmic applications online prevent individuals from understanding when and according to what m etric information is disseminated , restricted or targeted . Small concessions to addressing this problem such as selective identification of sponsored search results,28 or social media platforms highlighting when advertising is paid for by political actors , may contribute slightly to helping users to understand the rules of the information environment , but these neither capture nor resolve the concerns around the scale at which algorithmic processes are shaping that environment . 32 . Even when individuals are informed about the existence , scope and operation of artificial intelligence systems , those systems may frustrate efforts at transparency and suitability . To date , no sophisticated and scalable means for scrutinizing and making transparent the technical un derpinnings of automated decisions in the online sphere have been developed.29 This means that individuals will often have their expression rights adversely affected without being able to investigate or understand why , how or on what basis . D. Right to p rivacy 33 . The right to privacy often acts as a gateway to the enjoyment of freedom of opinion and expression.30 Article 17 of the Covenant protects the individual against “ arbitrary or unlawful interference with his or her privacy , family , home or corres pondence ” and “ unlawful attacks on his or her honour and reputation ” and provides that “ everyone has the right to the protection of the law against such interference or attacks ” . The Office of the High Commissioner for Human Rights and the Human Rights Council have emphasized that any interference with privacy must meet standards of legality , necessity and proportionality ( A/HRC/27/37 , para . 23 and Human Rights Council resolution 34/7 , para . 2 ) . 34 . Artifici al intelligence -driven decision -making systems depend on the collection and exploitation of data , ranging from ambient , non -personal data to personally __________________ 26 Council of Europe , Algorithms and Human Rights . 27 Human Rights Committee , general comment No . 34 ( 2011 ) on the freedoms of opinion and expression , para . 40 . 28 Safiya Umoja Noble , Algorithms of Oppression : How Search Engines Reinforce Racism ( New York , New York University Press , 2018 ) . 29 Mike Ananny and Kate Crawford , “ Seeing without knowing : limitations of the transparency ideal and its application to algorithmic accountability ” , New Media and Society , vol . 20 , No . 3 ( 13 December 2016 ) . Available at http : //journals.sagepub.com/doi/abs/10.1177/ 1461444816676645 ? journalCode=nmsa . 30 See A/HRC/29/32 , para . 16 , General Assembly resolution 68/167 and Human Rights Council resolution 20/8 . A/73/348 18-14238 14/23 identifiable information , with the vast majority of data used to feed artificial intelligence systems be ing somewhere in the middle — data that are inferred or extracted from personal data , or personal data that have been anonymized ( often imperfectly ) . Companies use data derived from online profiling and digital fingerprinting , procure datasets from third p arties such as data brokers and derive new data from vast aggregated datasets to feed artificial intelligence systems . Artificial intelligence -driven consumer products and autonomous systems are frequently equipped with sensors that generate and collect va st amounts of data on individuals within their proximity31 and artificial intelligence methods on social media platforms are used to infer and generate sensitive information about people that they have not provided or confirmed , such as sexual orientation , family relationships , religious views , health conditions or political affiliation . 35 . Artificial intelligence challenges traditional notions of consent , purpose and use limitation , transparency and accountability — the pillars upon which international data protection standards rest.32 Because artificial intelligence systems work by exploiting existing datasets and creating new ones , the ability of individuals to know , understand and exercise control over how their data are used is deprived of practical mean ing in the context of artificial intelligence . Once data are repurposed in an artificial intelligence system , they lose their original context , increasing the risk that data about individuals will become inaccurate or out of date and depriving individuals of the ability to rectify or delete the data . Artificial intelligence -based systems are being used to make consequential decisions using those data , some of which profoundly affect people ’ s lives,33 and yet , individuals have few avenues to exercise control over data that have been derived from their personal data , even as anonymization techniques continue to suffer from inadequacies . E. Obligation of non -discrimination 36 . Non-discrimination is an intrinsic principle of human rights law , existing not only as a qualifier on the obligations of States to ensure enjoyment of all other human rights without discrimination , but also , as enshrined in article 26 of the Covenant , as a stand -alone guarantee of equality before the law and equal protection of the law . States are under a clear obligation to “ prohibit any discrimination and guarantee to all persons equal and effective protection against discrimination on any ground such as race , colour , sex , language , religion , political or other opinion , national or so cial origin , property , birth or other status ” . Thus , articles 17 and 19 incorporate individual rights to freedom from discrimination in the holding and forming of opinions , the expression of and access to ideas and information , and the exercise of privacy and the protection of personal data . 37 . The potential for artificial intelligence to embed and perpetuate bias and discrimination extends to discrimination in the exercise of freedom of opinion and expression . Moderation algorithms may fail to take into a ccount cultural , language or gender -based contexts and sensitivities , or public interest in the content.34 Artificial intelligence -driven newsfeeds may perpetuate and reinforce discriminatory attitudes , __________________ 31 Article 19 and Privacy International , “ Privacy and freedom of expression ” . 32 Human Rights Committee , general comment No . 16 : Article 17 ( 1988 ) on the right to privacy , para . 10 . 33 Article 19 and Privacy International , “ Privacy and freedom of expression ” . 34 This has led to , for example , the removal of historical photographs with particular cultural significance . See Julia Carrie Wong , “ Mark Zuckerberg accused of abusing power after Facebook deletes ‘ napalm girl ’ post ” , The Guardian , 9 September 2016 . Available at https : //www.theguardian.com/technology/2016/sep/08/facebook -mark -zuckerberg -napalm -girlphoto -vietnam -war ; see also A/HRC/38/35 , para . 29 . A/73/348 15/23 18-14238 while artificial intelligence profiling and advertisin g systems have demonstrably facilitated discrimination along racial , religious and gender lines.35 “ Autocomplete ” artificial intelligence functions have also produced racially discriminatory results.36 38 . A number of factors ingrain bias into artificial intelligence systems , increasing their discriminatory potential . These include the way in which artificial intelligence systems are designed , decisions as to the origin and scope of the datasets on which these systems are trained , societal and cultural biases that developers may build into those datasets , the artificial intelligence models themselves and the way in which the outputs of the artificial intelligence model are implemented in practice . For example , facial recognition applications suffer from being grounded in predominantly white , male datasets , with errors occurring in up to 20 per cent of the time for women and people with darker skin colours.37 When such systems are used to , for example , categorize images available through a search engine , th eir discriminatory potential can translate into concrete interferences with individuals ’ exercise of their rights to seek , receive and impart information and freely assemble or associate . F. Right to an effective remedy 39 . Human rights law guarantee s individuals a remedy determined by competent judicial , administrative or legislative authorities ( article 2 ( 3 ) of the Covenant ) . Remedies must be known by and accessible to anyone who has had their rights violated ; must involve prompt , thorough and impa rtial investigation of alleged violations ; 38 and must be capable of ending ongoing violations ( A/HRC/27/37 , paras . 39–41 ) . 40 . Artificial intelligence systems often interfere with the right to a remedy . First , individual notice is almost inherently unavailable . In almost all applications of artificial intelligence technology in the information environment , individuals are not aware of the scope , extent or even existence of the algorithmic decision -making proces ses that may have an impact on their enjoyment of rights to opinion and expression . The second and more challenging aspect is the scrutability of the artificial intelligence system itself . The logic behind an algorithmic decision may not be evident even to an expert trained in the underlying mechanics of the system . Although it is logical to assume that more transparency around artificial intelligence systems would enable greater scrutiny , algorithmic transparency does not necessarily equate with intelligib le explanations of decision -making processes . Algorithms may obscure that a consequential decision has been taken or be so complex and context -dependent as to frustrate explanation . The situation is further complicated because companies __________________ 35 Julia Angwin , Madeleine Varner and Ariana Tobin , “ Facebook enabled advertisers to reach ‘ Jew haters ’ ” , ProPublica , 14 September 2017 . Available at https : //www.propublica.org/article/ facebook -enabled -advertisers -to-reach -jew-haters ; Ariana Tobin , “ Why we had to buy racist , sexist , xenophobic , ableist and otherwise awful Fa cebook ads , ” ProPublica , 27 November 2017 . Available at https : //www.propublica.org/article/why -we-had-to-buy-racist -sexis t-xenophobic ableist -and-otherwise -awful -facebook -ads . 36 Paris Martineau , “ YouTube ’ s search suggests racist autocompletes ” , The Outline , 13 May 2018 . Available at https : //theoutline.com/post/4536/youtube -s-search -autofill -suggests -racist results ? zd=1 & zi=3ygzt6hw . 37 Joy Buolamwini , “ The dangers of suprem ely white data and the coded gaze ” , presented at Wikimania 2018 , Cape Town . Available at https : //www.youtube.com/watch ? v= ZSJXKoD6mA8 & feature=youtu.be . 38 Human Rights Committee , general comment No . 31 ( 2004 ) on the nature of the general legal obligation imposed on States par ties to the Covenant , para . 15 . A/73/348 18-14238 16/23 operating in the in formation environment frequently update their algorithms ; 39 equally , machine -learning applications may change their own rules and algorithms over time . 41 . Compounding these concerns is the shift towards the automation of remedy systems themselves , accordin g to which complaints of individual users , either about content moderation decisions or about the adverse human rights impacts of artificial intelligence technologies , are considered and determined by artificial intelligence technologies.40 Automatic respon se processes raise concerns about whether complaint redress mechanisms constitute an effective remedy , given the lack of discretion , contextual analysis and independent determination built into such processes.41 G. Legislative , regulatory and policy res ponses to artificial intelligence 42 . Many States are now devising national artificial intelligence strategies in order to explore and develop policies and initiatives designed to maximize the potential benefits of artificial intelligence for their citiz ens.42 Although no State has yet to propose a comprehensive law or regula tion of artificial intelligence , there are reasons to be cautious about such an approach , which may be ill -suited to such an innovative field and may compensate for lack of detail with overly restrictive or overly permissive provisions . Sectoral regulation may be preferable although , arguably , existing law and regulation , for example in the field of data protection , could be flexible and available without the need to legislate further . 43 . At the same time , States should ensure that artificial intelligence is developed in keeping with human rights standards . Any efforts to develop State policy or regulation in the field of artificial intelligence should ensure consideration of human rights concerns.43 The rights to freedom of opinion and expression , in particular , are often excluded from public and political debates on artificial intelligence , which , to the extent that they tackle human rights issues , tend to focus on bias and discrimination in service delivery . 44 . Because the development of effective artificial intelligence systems depends on the acquisition of large datasets as well as long -term investment in technological capabilities , private sector entities are likely to d ominate in terms of development , production and capacity , leading to increased public sector reliance on companies for access to artificial intelligence systems . This increases the likelihood and public perception that corporate and government interests wi ll become increasingly intertwined . This is especially the case in the information environment , in which governments are often users — of social media platforms , search engines and other technologies — rather than providers . The alignment of public and pri vate interests does not in itself create human rights interferences but raises concerns about transparency and accountability . As private development of artificial intelligence __________________ 39 Barry Schwartz , “ Google : we make thousands of updates to search algorithms each year ” , Search Engine Roundtable , 5 June 2015 . Available at https : //www.seroundtable.com/google -updates thousands -20403.html . 40 Council of Europe , Algorithms and Human Rights . 41 Pei Zhang , Sophie Stalla -Bourdillon and Lester Gilbert , “ A content -linking -context model for ‘ notice -and-take-down ’ procedures ” , WebSci ‘ 16 , May 2016 . Available at http : //takedownproject.org/ wp-content/uploads/2016/04/ContentLinkingModelZhangStallaGilbert.pdf . 42 Tim Dutton , “ An overview of national AI strategies ” , Medium , 28 June 2018 . Available at https : //medium.com/politics -ai/an -overview -of-national -ai-strategies -2a70ec6edfd . 43 It is concerning , for example , that a parliamentary committee in the United Kingdom of Great Britain and Northern Ireland issued a 200 -page r eport that does not mention human rights even once . See United Kingdom of Great Britain and Northern Ireland House of Lords Select Committee on Artificial Intelligence , “ AI in the United Kingdom ” . A/73/348 17/23 18-14238 proceeds , there is a very real risk that States will delegate increasingly comp lex and onerous censorship and surveillance mandates to companies . 45 . Any State attempts to articulate law or policy in the field of artificial intelligence should speak both to public and private sector artificial intelligence applications , rather than simply focusing on public sector artificial intelligence regulation . As the Council of Europe concluded , “ Issues related to algorithmic governance and/or regulation are public policy prerogatives and should not be left to private actors alone . ” 44 State appr oaches may involve enhanced transparency and disclosure obligations on companies and robust data protection legislation that addresses artificial intelligence -related concerns . 46 . Public and private sector initiatives designed to explore and integrate et hics into the procurement , design , deployment and implementation of artificial intelligence systems are proliferating . I strongly encourage the integration of human rights concerns into these efforts . The private sector ’ s focus on and the public sector ’ s push for ethics often imply resistance to human rights -based regulation.45 While ethics provide a critical framework for working through particular challenges in the field of artificial intelligence , it is not a replacement for human rights , to which every S tate is bound by law . Companies and governments should ensure that human rights considerations and responsibilities are firmly integrated into all aspects of their artificial intelligence operations even as they are developing ethical codes and guidance.46 IV . A human rights -based approach to artificial intelligence 47 . In recent reports , the mandate holder has set out legal and practical considerations for companies , to put human rights principles at the heart of their content regulation policies , and has detailed both substantive standards and processes that ensure that companies can comply with their human rights responsibilities under the Guiding Principles on Business and Human Rights in all aspects of their operations . That same framework frame s the approach offered here with respect to artificial intelligence technologies . The substantive standards and processes proposed below apply to companies , in their capacity as actors that design , deploy and implement artificial intelligence systems , and to States , which have an obligation to refrain from interfering with human rights in their own adoption and use of artificial intelligence systems . These standards and processes are designed to ensure that human rights law is placed at the heart of advance ments in the field of artificial intelligence . Two fundamental principles are woven throughout the standards and processes offered : the need to protect and respect individual agency and autonomy , a key precondition to the exercise of the right to freedom o f opinion and expression ; and the importance of meaningful disclosure on the part of public and sector actors , defined by open and innovative efforts to explain artificial intelligence technologies to the public and facilitate their scrutiny . __________________ 44 Council of Europe , Algorithms and Human Rights . 45 Ben W agner , “ Ethics as an escape from regulation : from ethics -washing to ethics -shopping ? ” , in Being Profiling : Cogitas Ergo Sum , Mireille Hildebrandt , ed . ( Amsterdam University Press ( forthcoming ) ) . 46 Article 19 and Privacy International , “ Privacy and freedom of expression ” . A/73/348 18-14238 18/23 A. Substan tive standards for artificial intelligence systems 48 . Companies should orient their standards , rules and system design around universal human rights principles ( A/HRC/38/35 , paras . 41 –43 ) . Public -facing terms and guidelines should be complemented by internal policy commitments to mainstreaming human rights considerations throughout a company ’ s operations , especially in relation to the development and deployment of artificial intelligence and algorithmic s ystems . Companies should consider how to elaborate professional standards for artificial intelligence engineers , translating human rights responsibilities into guidance for technical design and operation choices . The development of codes of ethics and acco mpanying institutional structures may be an important complement to , but not a substitute for , commitments to human rights . Codes and guidelines issued by both public and private sector bodies should emphasize that human rights law provides the fundamental rules for the protection of individuals in the context of artificial intelligence , while ethics frameworks may assist in further developing the content and application of human rights in specific circumstances . 49 . Companies and governments must be explic it with individuals about which decisions in the information environment are made by automated systems and which are accompanied by human review , as well as the broad elements of the logic used by those systems . Individuals should also be informed when the personal data they provide to a private sector actor ( either explicitly or through their use of a service or site ) will become part of a dataset used by an artificial intelligence system , to enable them to factor that knowledge into their decision about w hether to consent to data collection and which types of data they wish to disclose.47 Similarly to the public notices required for the use of closed -circuit television cameras , artificial intelligence systems should actively disclose to individuals ( through innovative means such as pop-up boxes ) in a clear and understandable manner that they are subject or contributing data to an artificial intelligence -driven decision -making process , as well as meaningful information about the logic involved in the process and the significance of the consequences to the individual . 50 . Transparency does not stop with the disclosure to individual users about the existence of artificial intelligence technologies in the platforms and online services they use . Companies and gov ernments need to embrace transparency throughout each aspect of the artificial intelligence value chain . Transparency need not be complex to be effective ; even simplified explanations of the purpose , policies , inputs and outputs of an artificial intelligen ce system can contribute to public education and debate.48 Rather than grapple with the predicament of making convoluted technical processes legible to lay audiences , companies should strive to achieve transparency through the provision of non -technical ins ights into a system . To that end , the focus should be on educating individual users about an artificial intelligence system ’ s existence , purpose , constitution and impact , rather than about the source code , training data and inputs and outputs .49 51 . Radica l transparency about the impact of an artificial intelligence system in the information environment requires disclosure of , for example , data on how much content is removed by artificial intelligence systems , how often artificial intelligence suggested con tent removals are approved by a human moderator , how often content removals are contested and how often challenges to content removals are upheld . __________________ 47 Human Rights Committee , general comment No . 16 ( 1988 ) on the right to privacy . 48 Aaron Rieke , Miranda Bogen and David Robinson , “ Public scrutiny of automated decisions : early lessons and emerging methods ” ( Omidyar and Upturn , 2018 ) . 49 Rieke , Bogen and Robinson , “ Public scrutiny of automated decisions ” . A/73/348 19/23 18-14238 Aggregate data illustrating trends in content display should be available for users to inspect , alongside cas e studies that illustrate why certain content will be prioritized over other content . Disclosure about the sources and beneficiaries of political and commercial advertising is a critical element of radical transparency . Public and private sector actors imp lementing artificial intelligence -driven systems should also be transparent about the limits of the artificial intelligence system , including for example , confidence measures , known failure scenarios and appropriate limitations on use.50 52 . Tackling the p revalence of discrimination in artificial intelligence systems is an existential challenge for companies and governments ; failure to address and resolve the discriminatory elements and impacts will render the technology not only ineffective but dangerous . There is ample thought leadership and resources for companies and governments to draw on in considering how to address bias and discrimination in artificial intelligence systems ; broadly speaking , it necessitates isolating and accounting for discrimination at both the input and output levels . This involves , at a minimum , addressing sampling errors ( where datasets are non-representative of society ) , scrubbing datasets to remove discriminatory data and putting in place measures to compensate for data that “ contain the imprint of historical and structural patterns of discrimination ” 51 and from which artificial intelligence systems are likely to develop discriminatory proxies . Active monitoring of discriminatory outcomes of artificial intelligence systems is also integral to avoiding and mitigating adverse effects on the human rights of individuals . B . Processes for artificial intelligence systems 53 . Human rights impact assessments . Embracing radical transparency throughout the artificial intelligence life c ycle requires companies and governments to take steps to permit systems to be scrutinized and challenged from conception to implementation . Human rights impact assessments are one tool that can demonstrate a commitment to addressing the human rights implic ations of artificial intelligence systems and should be performed prior to procurement , development or use and involve both self -assessment and external review . The think tank AI Now has proposed a public agency algorithmic impact assessment that stipulate s that governments should undertake an internal review of artificial intelligence systems as well as facilitate external research review processes to test and verify assumptions and conclusions.52 Companies should also conduct assessments along similar lines . 54 . Public sector procurement of artificial intelligence technologies from private vendors must be accompanied by a public consultation to elicit societal views and input on the design and impl ementation of the artificial intelligence system before it is acquired . Both companies and governments must conduct meaningful and sustained consultations with civil society , human rights groups , relevant local communities and representatives of historical ly marginalized or underrepresented populations before developing , procuring or using artificial intelligence systems and technologies . __________________ 50 Amnesty International and Access Now , “ Toronto declaration : protecting the right to equality and non-discrimination in machine learning systems ” , art . 27 ( d ) , 2018 . Available at https : //www.accessnow.org/the -toronto -declaration -protecting -the-rights -to-equality -and-nondiscrimination -in-machine -learning -systems/ . 51 Iason Gabriel , “ The case for fairer algorithms ” , Medium , 14 March 2018 . Available at https : //medium.com/ @ Ethics_Society/the -case-for-fairer -algorithms -c008a12126f8 . 52 Dillon Reisman and others , “ Algorithmic impact assessments : a practical framework for public agency accountability ” ( AI Now , 2018 ) . Available at https : //a inowinstitute.org/aiareport2018.pdf . A/73/348 18-14238 20/23 55 . Audits . Facilitating external review of artificial intelligence systems provides a critical guarantee of rigour and independence in transparency . For this reason , ongoing independent audits should supplement pre -procurement human rights impact assessments as an important mechanism of transparency and accountability in artificial intelligence systems . Private sector acto rs have raised objections to the feasibility of audits in the artificial intelligence space , given the imperative to protect proprietary technology . While these concerns may be well founded , I agree with AI Now that , especially when an artificial intellige nce application is being used by a public sector agency , refusal on the part of the vendor to be transparent about the operation of the system would be incompatible with the public body ’ s own accountability obligations . 56 . In any event , innovative suggest ions for audits of artificial intelligence technology that permit proprietary secrecy abound : zero -knowledge proofs could conceivably be generated by algorithms to demonstrate that they conform to certain properties , obviating the need to scrutinize the un derlying algorithm,53 or algorithms could be disclosed to expert third parties who would hold them in escrow on the condition of confidentiality , permitting public interest scrutiny but not allowing the algorithm to become public.54 Government regulators fro m the domains of telecommunications or competition could be permitted access to artificial intelligence systems on a confidential basis , as already occurs , for example , in the regulation of gambling machines in Australia and New Zealand , in which companies must submit their algorithmic systems to regulatory audit review.55 Academic literature contains other suggestions for innovative forms of artificial intelligence audits.56 57 . Each of these mechanisms may face challenges in implementation , especially in the information environment , but companies should work towards making audits of artificial intelligence systems feasible . Governments should contribute to the effectiveness of audits by considering policy or legislative interventions that require companies to make artificial intelligence code auditable , guaranteeing the existence of audit trails and thus greater opportunities for transparency to individuals affected . 58 . Individual autonomy . Artificial intelligence must not invisibly supplant , manipulate or interfere with the ability of individuals to form and hold their opinions or access and express ideas in the information environment . Respecting individual autonomy means , at the very least , ensuring that users have knowledge , choice and control . Pervasiv e and hidden artificial intelligence applications that obscure the processes of content display , personalization , moderation and profiling and targeting undermine the ability of individuals to exercise the rights to freedom of opinion , expression and priva cy . Companies should be mindful of the adverse human rights impacts that flow from artificial intelligence applications that prioritize commercial or political interests over transparency and individual choice . 59 . Notice and consent . Companies must ensur e that users are fully informed about how algorithmic decision -making shapes their use of a platform , site or service . This can be achieved through education campaigns , pop -up boxes , interstitials and other means of signalling when an artificial intelligen ce system is determining a user ’ s experience of a search engine , news site or social media platform . State -imposed disclosure requirements may be an appropriate means of protecting notice and __________________ 53 Council of Europe , Algorithms and Human Rights . 54 Frank Pasquale , The Black Box Society : The Secret Algorithms That Control Money and Information ( Cambridge , Harvard University Press , 2015 ) . 55 Council of Europe , Algorithms and Human Rights . 56 Christian Sandvig and others , “ Auditing algorithms : research methods for detecting discrimination on Internet platforms ” , paper presented at Data and Discrimination : Converting Critical Concerns into Productive Inquiry , a pr e-conference at the 64th annual meeting of the International Communication Association , Seattle , 22 May 2014 . A/73/348 21/23 18-14238 consent . Individuals also have a right to know when their data a re being collected by an artificial intelligence application and whether the data will become part of a dataset that will subsequently inform an artificial intelligence application , as well as the conditions on which that data will be used , stored and dele ted . 60 . Remedy . Adverse impacts of artificial intelligence systems on human rights must be remediable and remedied by the companies responsible . The precondition to the establishment of effective remedy processes is ensuring that individuals know that they have been subject to an algorithmic decision ( including one that is suggested by an artificial intelligence system and approved by a human interlocutor ) and are equipped with information about the logic behind that decision . Beyond that , companies sho uld ensure human review of requests for remedy , in order to provide an appropriate check on the systems and guarantee accountability . Data should be published on the frequency at which remedial mechanisms are triggered for decisions made by artificial inte lligence technologies . V. Conclusions and recommendations 61 . In the present report , I have explored existing and potential impacts of artificial intelligence on the rights to freedom of opinion and expression , positing that artificial intelligence is now a critical part of the information environment , posing benefits and risks to individuals ’ enjoyment of their rights . I have proposed a conceptual framing for thinking about the obligations of States and responsibilities of companies to uphold these rights in the face of expanding technological capabilities and have suggested concrete measures that could be implemented by both governments and companies to ensure that human rights are respected as the power , reach and scope of artificial intelligence t echnology grows . Recommendations for States 62 . When procuring or deploying artificial intelligence systems or applications , States should ensure that public sector bodies act consistently with human rights principles . This includes , inter alia , con ducting public consultations and undertaking human rights impact assessments or public agency algorithmic impact assessments prior to the procurement or deployment of artificial intelligence systems . Particular attention should be given to the disparate im pact of such technologies on racial and religious minorities , political opposition and activists . Government deployment of artificial intelligence systems should be subject to regular audits by external , independent experts . 63 . States should ensure that h uman rights are central to private sector design , deployment and implementation of artificial intelligence systems . This includes updating and applying existing regulation , particularly data protection regulation , to the artificial intelligence do main , pur suing regulatory or co-regulatory schemes designed to require businesses to undertake impact assessments and audits of artificial intelligence technologies and ensuring effective external accountability mechanisms.57 Where applicable , sectoral regulation of particular artificial intelligence applications may be necessary and effective for the protection of human rights . To the extent that such restrictions introduce or facilitate interferences with freedom of expression , States should ensure that they are ne cessary and proportionate to accomplish a legitimate __________________ 57 Wagner , “ Ethics as an escape from regulation ” . A/73/348 18-14238 22/23 objective in accordance with article 19 ( 3 ) of the Covenant . Artificial intelligence related regulation should also be developed through extensive public consultation involving engagement with civil soci ety , human rights groups and representatives of marginalized or underrepresented end users . 64 . States should create a policy and legislative environment conducive to a diverse , pluralistic information environment . This includes taking measures to ensure a competitive field in the artificial intelligence domain . Such measures may include the regulation of technology monopolies to prevent the concentration of artificial intelligence expertise and power in the hands of a few dominant companies , regulation des igned to increase interoperability of services and technologies , and the adoption of policies supporting network neutrality and device neutrality.58 Recommendations for companies 65 . All efforts to formulate guidelines or codes on ethical implication s of artificial intelligence technologies should be grounded in human rights principles . All private and public development and deployment of artificial intelligence should provide opportunities for civil society to comment . Companies should reiterate in c orporate policies and technical guidance to engineers , developers , data technicians , data scrubbers , programmers and others involved in the artificial intelligence life cycle that human rights responsibilities guide all of their business operations and tha t ethical principles can assist by facilitating the application of human rights principles to specific situations of artificial intelligence design , deployment and implementation . In particular , the terms of service of platforms should be based on universal h uman rights principles . 66 . Companies should make explicit where and how artificial intelligence technologies and automated techniques are used on their platforms , services and applications . The use of innovative means to signal to individuals when they ar e subject to an artificial intelligence -driven decision -making process , when artificial intelligence plays a role in displaying or moderating content or when individuals ’ personal data may be integrated into a dataset that will be used to inform artificial intelligence systems is critical to giving users the notice necessary to understand and address the impact of artificial intelligence systems on their enjoyment of human rights . Companies should also publish data on content removals , including how often removals are contested and challenges to removals are upheld , as well as data on trends in content display , along side case studies and education on commercial and political profiling . 67 . Companies must prevent and account for discrimination at both the input and output levels of artificial intelligence systems . This involves ensuring that teams designing and deploy ing artificial intelligence systems reflect diverse and non-discriminat ory attitudes and prioritizing the avoidance of bias and discrimination in the choice of datasets and design of the system , including by addressing sampling errors , scrubbing datasets to remove discriminat ory data and putting in place measures to compensate for such data . Active monitoring of discriminatory outcomes of artificial intelligence systems is also essential . __________________ 58 Autorité de régulation des communications électroniques et des postes , Devices , the Weak Link in Achieving an Open Internet ( 2018 ) . Available at https : //www.arcep.fr/uploads/tx_gspublication/ rapport -terminaux -fev2018 -ENG.pdf . A/73/348 23/23 18-14238 68 . Human rights impact assessments and public consultations s hould be carried out during the design and deployment of new artificial intelligence systems , including the deployment of existing systems in new global markets . Public consultations and engagement should occur prior to the finalization or roll-out of a pr oduct or service , in order to ensure that they are meaningful , and should encompass engagement with civil society , human rights defenders and representatives of marginalized or underrepresented end users . The results of human rights impact assessments and public consultations should themselves be made public . 69 . Companies should make all artificial intelligence code fully auditable and should pursue innovative means for enabling external and independent auditing of artificial intelligence systems , separately from regulatory requirements . The results of artificial intelligence audits should themselves be made public . 70 . Individual users must have access to remedies for the adverse human rights impacts of artificial intelligence systems . Companies sh ould put in place systems of human review and remedy to respond to the complaints of all users and appeals levied at artificial intelligence -driven systems in a timely manner . Data on the frequency at which artificial intelligence systems are subject to co mplaints and requests for remedies , as well as the types and effectiveness of remedies available , should be published regularly .
