OPINION ON THE IMPACT OF ARTIFICIAL INTELLIGENCE ON FUNDAMENTAL RIGHTS 7 APRIL 2022OPINION A - 2022 - 6 A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights2 Opinion on the impact of artificial intelligence on fundamental rights was adopted unanimously at the plenary session of 7 April 2022 . While research on artificial intelligence ( AI ) and the implementation of its practical applications are developing , current regulations remain incomplete when it comes to preventing possible major infringements of fundamental rights . In the context of the forthcoming adoption of the proposal for an EU Regulation on the subject , and the work being done in the Council of Europe , the National Consultative Commission on Human Rights ( CNCDH ) is calling on the public authorities to promote an ambitious legal framework in this area . On the one hand , it recommends prohibiting certain uses of AI considered to be too harmful to fundamental rights , such as social scoring or remote biometric identification of people in publicly accessible spaces . On the other hand , it recommends placing on users of an AI system requirements that can guarantee respect for fundamental rights : an impact assessment , stakeholder consultation , and supervision of the system throughout its life cycle . The CNCDH finally calls for the recognition of rights for persons who have been the subject of a decision involving an algorithm , in particular the right to human intervention in the decisionmaking process , or a right to configure the operating criteria of the AI system . SUMMARY . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights3 TABLE OF CONTENTS Summary . 2 Introduction . 4 1 . Red lines . 8 1.1 . The contributions and limitations of the prohibitions laid down by the proposal for an EU regulation . 9 1.2 . The need to extend the ban to other areas . 12 2 . A framework that guarantees respect for fundamental rights . 14 2.1 . Control at all stages of AI system development . 14 2.2 . Guaranteed respect for fundamental rights with regard to individual decisions . 22 Summary of recommendations . 29 List of people heard . 32 A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights4 INTRODUCTION . 1 . For some time now , what is now commonly called “ artificial intelligence ” or “ AI ” is at the heart of each individual ’ s daily life : suggested content on social networks or online platforms , access to applications or locations using biometric authentication , automated medical diagnoses , etc . It has however received a great deal of attention in recent years , by taking advantage of massive public and private investment . While some place it at the heart of a “ new industrial revolution ” , others are concerned about a new wave of automation of activities previously reserved for human beings , and more generally the downward spiral of new governance by data , as well as , more broadly , about the possible major infringements of fundamental rights , not to mention its growing impact on the environment . 2 . As a starting point , the National Advisory Commission on Human Rights ( CNCDH ) wishes to express its reservations with regard to the terminology used in this area . In fact , it observes an excess of anthropomorphisation in the terms used , starting with “ artificial intelligence ” , but also “ neural networks ” , “ deep learning ” , etc . This creates confusion about the real possibilities offered by data processing systems , which are based on procedures coded in computer systems , above all a question of mathematics . All those involved , in both the public and private sectors , should therefore do away with this expression because of its psychological impact , a source of reluctance or , on the contrary , excessive confidence and acceptance . For this reason , the CNCDH recommends that public institutions and the media adopt more neutral expressions , such as “ Algorithmic Decision Support System ” ( ADSS ) . Nevertheless , for reasons of editorial convenience , and because this is the current practice , the CNCDH will refer to “ AI ” in this opinion . Recommendation 1 : The CNCDH recommends favouring , in institutional communication , a more neutral and objective terminology than the term “ artificial intelligence ” , such as the term “ Algorithmic Decision Support System ” ( ADSS ) . 3 . This term covers , more specifically , IT technologies that are based on different operating logics : a distinction is mainly made between symbolic ( or cognitive ) AI and connectionist AI . The first involves programming a series of explicit and unambiguous instructions – in other words an algorithm – to give a result that is predictable because it presents itself as the logical processing of the data entered into the system . The second , more recently created , is based on another type of algorithm , no longer focused on a logical approach to information processing but on a probabilistic approach : programmers design a learning algorithm and submit to the computer a data set from which it will “ learn ” or , more precisely , infer rules . This learning can be supervised or unsupervised : in the former case , the data used for learning is labelled , while in the latter it is “ raw ” . In the latter case , machine learning establishes correlations between the information fed into the system . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights5 4 . This type of learning , machine learning , raises unprecedented challenges in regard to symbolic AI . While the instructions coded in conventional software can easily be communicated ( although the system can struggle to understand when there are a lot of instructions and a lot of data to process , as is the case with the online higher education admissions platform , Parcoursup ) , the model that the machine reaches at the end of its learning can more or less easily be the subject of information , as the system designers can not in some extreme cases ( in particular in the case of deep learning ) tell the operating model that the machine has reached to achieve its results . 5 . While AI could , according to some , enable us to “ activate our fundamental rights ” 1 , it nevertheless poses undeniable risks to them . At national level , the National Commission on Computer Technology and Freedom ( CNIL ) published a report on algorithms in 2017 , the result of extensive consultation with stakeholders in the sector and citizens , faced with the need to “ enable man to keep the upper hand ” 2 , and also focused its reflection on specific applications of AI3 . In 2017 , again , the Defender of Rights warned about the risks of discrimination caused by the use of algorithms in the fight against social security fraud , targeting categories of people to be checked as a priority4 . Since then , the Defender of Rights has been pursuing a broader reflection on AI and discrimination5 . At international level , many bodies have also warned about the impact of AI on fundamental rights6 . In particular , the European Union Agency for Fundamental Rights ( FRA ) and the Ad Hoc Committee on Artificial Intelligence ( CAHAI ) , a Council of Europe body responsible for examining the possibilities of establishing a legal framework on AI , have drawn up an inventory of fundamental rights likely to be threatened by AI : in particular respect for human dignity , respect for privacy and data protection , equality and non-discrimination , access to justice , access to social rights , etc . 6 . The deployment of AI is all the more concerning as there is currently no comprehensive legal framework , both nationally and internationally , to stem its flow . The regulations in force provide only partial references , whether it concerns the protection of personal data – with , in particular within the European Union , the General 1 According to the formula in the Villani report : “ Donner un sens à l ’ intelligence artificielle : pour une stratégie nationale et européenne ” , 28 March 2018 . 2 CNIL , “ Comment permettre à l ’ homme de garder la main ? Les enjeux éthiques des algorithmes et de l ’ intelligence artificielle ” , December 2017 . 3 CNIL , “ Chatbots : des humains comme les autres ” , LINC , 3 February 2017 ; “ Reconnaissance faciale : pour un débat à la hauteur des enjeux ” , 2019 . 4 Defender of Rights , “ Lutte contre la fraude aux prestations sociales : à quel prix pour les droits des usagers ” , September 2017 . 5 Defender of Rights , in partnership with the CNIL , “ Algorithmes : prévenir l ’ automatisation des discriminations ” , 2020 . More recently , the Defender of Rights published a report on biometric technologies : “ Technologies biométriques : l ’ impératif respect des droits fondamentaux ” , 2021 . 6 See in particular : EU Agency for Fundamental Rights , “ Getting the future right : Artificial Intelligence and Fundamental Rights ” , 14 December 2020 ; UNESCO , Recommendation on the Ethics of Artificial Intelligence , November 2021 ; CAHAI , “ Feasibility Study ” , 17 December 2020 ; OECD , Recommendation of the Council on Artificial Intelligence , 22 May 2019 . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights6 Data Protection Regulation ( GDPR ) – or non-discrimination . However , this is still not enough since a large number of AI systems operate using non-identifying data and can have consequences on fundamental rights exceeding the protection of personal data and non-discrimination , as well as forms of discrimination likely to target groups not covered by the criteria of discrimination prohibited by law7 . 7 . For several years now , initiatives have come from the private sector . Recognising the need to offer trustworthy AI solutions to ensure commercial success , professionals offer ethics guides for designers and developers8 . In addition , international institutions make recommendations to States along the same lines , such as those adopted by UNESCO on 24 November 2021 in order to “ make AI systems work for the good of humanity , individuals , societies and the environment and ecosystems , and to prevent harm ” 9 . 8 . The concerns expressed through these texts , often formulated on the basis of a reference to “ ethical principles ” , largely coincide with human rights , particularly when it comes to autonomy or freedom , respect for human dignity , or non-discrimination . They are , however , of limited scope , relying on the self-regulation of stakeholders , the good will of manufacturers and companies , and do not impose any obligations on the States . 9 . Given the significant impact of AI on fundamental rights , this approach does not seem sufficient . That is why the CNCDH has taken the initiative to review the issue . The magnitude of the issues raised with regard to fundamental rights through the design , deployment and use of AI systems calls for a binding legal framework to be put in place to ensure that these rights are respected . The CNCDH has carefully followed the discussions and work currently undertaken in this regard within the Council of Europe by CAHAI10 , which could lead to the adoption of a legal framework for “ the development , design and application of artificial intelligence , based on Council of Europe ’ s standards on human rights , democracy and the rule of law ” 11 . The CNCDH calls for the adoption of 7 CNIL , “ Comment permettre à l ’ homme de garder la main ? Les enjeux éthiques des algorithmes et de l ’ intelligence artificielle ” , December 2017 , p. 49 . Some algorithms are designed to establish correlations between different individual characteristics , from which they constitute groups and make predictions about behaviour at group level , for example “ dog owners living in the Paris region , aged 35 to 40 , who do a sporting activity at least twice a week ” . Being identified as a member of this group can therefore lead to automated decisions that have adverse or beneficial effects for individual members , such as a differentiated health insurance tariff . 8 See in particular the practical guide “ Ethical AI ” , launched in September 2021 by Numeum , the leading professional association of digital companies in France . 9 Most recent text : UNESCO , Recommendation on the Ethics of Artificial Intelligence , November 2021 . 10 Since the CAHAI fulfilled its mandate ( 2019-2021 ) , it has been replaced by the Committee on Artificial Intelligence ( CAI ) for the development of an appropriate legal framework on the development , design and application of artificial intelligence , on the basis of the Council of Europe ’ s standards . 11 See in particular its latest publication : CAHAI , “ Possible elements of a legal framework on artificial intelligence , based on the Council of Europe ’ s standards on human rights , democracy and the rule of law ” , 2 December 2021 . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights7 an “ AI Convention 108+ ” 12 . With regard to the proposal for a European Union Regulation on AI ( hereinafter referred to as the ‘ proposal for a Regulation ’ ) , which establishes a first regional legal framework in order to “ foster the development , use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests , such as health and safety and the protection of fundamental rights , as recognised and protected by Union law ” 13 , it is a prerequisite for the CNCDH to take fundamental rights into account . Nevertheless , the Commission notes that there are insufficient guarantees to ensure effective compliance with the latter . Insofar as the proposal for a Regulation must meet the challenges of protecting these rights in the use of AI systems , the Commission recommends that this text ensure , to this end , the creation of a binding legal framework . Recommendation 2 : The Commission recommends strengthening , in the proposal for a European Union regulation on AI , the provisions to ensure the establishment of a binding legal framework guaranteeing effective respect for fundamental rights . In addition , the CNCDH recommends the adoption , within the framework of the Council of Europe , of a “ Convention 108+ on AI ” . 10 . Thus , by drawing inspiration from the various steps already outlined by national bodies such as the Defender of Rights or the CNIL and European and international bodies , the CNCDH wishes to define the outline of a general framework , respectful of fundamental rights , for AI systems . In doing so , its opinion will help feed the necessary amendments to the EU AI Regulation . Further opinions will follow in the future in order to identify the risks to human rights , specific to the use of AI in certain sectors , as well as the guarantees that may be addressed . Moreover , the CNCDH has already expressed itself , in its opinion on the fight against online hate14 , on the use of algorithms for moderation of content on social networks . 11 . The CNCDH specifies that the use of a binding legal framework does not , of course , exclude flexible legal systems , with certifications and labels , under the supervision of the regulatory authority , in order to promote the development of good practices likely to accompany the implementation of this regulation . 12 “ Convention 108+ ” : The Convention for the Protection of Individuals with Regard to Automatic Processing of Personal Data , available online : https : //www.europarl.europa.eu/meetdocs/2014_2019/plmrep/COMMITTEES/LIBE/DV/2018/09-10/ Convention_108_EN.pdf . Opened for signature in the Council of Europe on 28 January 1981 , Convention 108 was the first binding international legal instrument in the field of data protection . In order to respond to the challenges raised by digital media , on 10 October 2018 , an amendment protocol was opened for signature by the States parties to the Convention . This new version is now referred to as “ Convention 108+ ” . 13 Proposal for a Regulation on AI , cons . 5 . Available online : https : //eur-lex.europa.eu/legal-content/EN/ ALL/ ? uri=CELEX % 3A52021PC0206 . 14 CNCDH , Opinion on the fight against online hate , Plenary session of 8 July 2021 , Official Journal of the French Republic No . 0170 of 24 July 2021 , text No . 79 . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights8 12 . Concerned with promoting an approach based on human rights15 , the CNCDH will aim here both to highlight the need to include in the reflection and supervision of AI systems the most marginalised segments of the population and , above all , to stress the importance of establishing a legal framework to ensure respect for fundamental rights . The human rights approach involves putting humans in a position to define their needs and therefore support the development of an AI at the service of humans and their autonomy . This approach should further irrigate the ongoing reforms , as they aim to ensure respect for fundamental rights . Recommendation 3 : The CNCDH recommends that a human rights-based approach be taken into account in the ongoing reforms , as they aim to ensure respect for fundamental rights . 13 . The observations and recommendations of the CNCDH will address the two components of an AI framework respectful of fundamental rights : a definition of “ red lines ” , in other words the uses of AI to be prohibited ( 1 ) ; guarantees to be promoted in order to ensure a framework of AI systems respectful of fundamental rights ( 2 ) . 14 . Within the framework of this opinion , and based on the terminology enshrined by the bodies of the European Union16 , the CNCDH will designate as “ designer ” or “ supplier ” the natural or legal person who develops an AI system , as “ user ” any natural or legal person , public authority , agency or other body , including under private law , using an AI system under its own authority , and as “ affected person ” or “ person targeted by an AI system ” any natural person exposed to or impacted by an AI system . 1 . RED LINES . 15 . Some uses of AI cause too serious an infringement of fundamental rights to be permitted . It is the responsibility of the public authorities to prohibit their implementation . The proposal for an EU Regulation prohibits some use cases of AI , rightly considered as “ particularly harmful ” 17 . The CNCDH however points to certain limits in the definition of prohibited uses . In addition , other uses seem equally dangerous for fundamental rights and human dignity and , as such , would deserve to be banned as well . 15 CNCDH , Opinion “ Pour une approche fondée sur les droits de l ’ homme ” , Plenary session of 3 July 2018 , Official Journal of the French Republic No . 0161 of 14 July 2018 , text No . 104 . 16 See the GDPR and the proposal for a Regulation on AI . 17 Proposal for a Regulation on AI , “ Explanatory Memorandum ” . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights9 1.1 . The contributions and limitations of the prohibitions laid down by the proposal for an EU regulation . 16 . The proposal for an EU regulation on AI lists uses “ prohibited as contravening Union values ” 18 . This technology can be “ misused and provide novel and powerful tools for manipulative , exploitative and social control practices ” 19 . The CNCDH agrees with the idea that certain uses of AI should be purely and simply banned , given the extent of their impact on fundamental rights and freedoms . 17 . The systems particularly problematic from this point of view , identified by the proposal for a Regulation , pose serious threats to the protection of fundamental rights and freedoms : • systems based on subliminal components individuals can not perceive , or exploiting vulnerabilities of children and people due to their age , physical or mental incapacities , and which , by distorting their behaviour , are likely to cause them psychological or physical harm ; • AI systems allowing the social scoring of natural persons , depending on their behaviour or personal characteristics , by or on behalf of the public authorities , for the purposes of the harmful or unfavourable treatment of certain natural persons or groups of persons ; • ‘ real-time ’ remote biometric identification in publicly accessible spaces for the purpose of law enforcement . 18 . The CNCDH questions the definition and scope of the prohibitions thus imposed on the use of AI systems . 19 . First of all , with regard to the first case , the terms used to define “ malware ” applications could cover a large number of situations , interfaces and online services , currently very much in vogue : “ nudge ” or “ sludge ” devices20 , set up by social media or online shopping sites , exploit cognitive biases to guide user behaviour , in order to capture their attention , encourage them to buy a product , etc . Terms such as “ subliminal ” or “ substantial ” are particularly complex and make the reach of this first limitation to the use of AI uncertain . This prohibition , however , has the merit of raising questions about the permissible risks of AI manipulation , in particular with regard to the processes of automating the processing of information resulting from 18 Proposal for a Regulation on AI , “ Explanatory Memorandum ” . 19 Ibid . , cons . 15 . 20 “ Nudges ” correspond to techniques to guide the user of an online service to make a choice . For example , indicating on a site that an item or product is “ popular ” will encourage an internet user to view or buy it . See in particular : “ La forme des choix ” , CNIL , 2019 . On the other hand , “ sludges ” have a deterrent purpose . For example , with regard to the management of cookies on a site , some sites simplify the “ Accept All ” option and make the “ Configure Cookies ” option more complex , so that users tend to favour the former . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights10 neuromarketing research21 . It is all the more essential , moreover , in the development of “ augmented dark patterns ” allowing , through algorithmic processing , dynamic action to be taken on users ’ stimuli to exploit their vulnerabilities . In this respect , the CNCDH supports the will of European parliamentarians , expressed elsewhere22 , to prohibit the use of manipulation techniques , which aim to encourage users to make certain choices , on very large online platforms . Recommendation 4 : The CNCDH recommends the prohibition of the use of choice interfaces insofar as they have the purpose or effect of manipulating users , to their detriment , by exploiting their vulnerabilities . 20 . Secondly , and more worryingly , the proposal for the EU text prohibits social scoring , which aims to evaluate people based on their social behaviour or known or predicted personal or personality characteristics , only when it is practised by or on behalf of the authorities . However , private companies , for example social networks , can also process large amounts of personal data and perform scoring . Therefore , the CNCDH endorses the position of the European data protection authorities , in favour of a ban on any type of social scoring , regardless of the nature , public or private , of the entity that implements it23 . Recommendation 5 : The CNCDH recommends banning any type of social scoring set up by government authorities or by any company , public or private . 21 . Finally , the prohibition on the real-time biometric identification of natural persons – i.e . the use of AI for automated recognition of human characteristics such as face , voice , gait , etc . – in publicly accessible spaces and for the purpose of law enforcement raises , in its current form , questions about its scope of application , but also concerns about the scope of derogations accepted by the proposal for a Regulation . 22 . The CNCDH joins the European Commission in highlighting not only the risk to respect for privacy generated by such a system , but also the “ feeling of constant surveillance ” likely to be generated by this technology and the risk of “ indirectly dissuading the exercise of the freedom of assembly and other fundamental rights ” 24 , 21 Neuromarketing looks at the functioning of the brain , particularly using brain imaging ( MRI ) , to better understand how consumers react to advertising and sales devices . 22 See the debates on the Proposal for a Regulation of the European Parliament and of the Council on a Single Market for Digital Services ( Digital Services Act ) . 23 EDPS-EDPB Joint Opinion 05/2021 on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence ( Artificial Intelligence Act ) . Available online : https : //edpb.europa.eu/system/files/2021-06/edpb-edps_joint_opinion_ai_regulation_en.pdf . 24 Proposal for a Regulation on AI , Cons . 18 . The CNCDH also pointed out this “ chilling effect ” risk associated with the use of drones equipped with facial recognition software : CNCDH , Opinion on the proposal for a law on global security , Plenary session of 26 November 2020 , Official Journal of the French Republic No . 0290 of 1 December 2020 , text No . 83 . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights11 starting with freedom of movement . The CNCDH points out , however , that this dissuasive effect is equally valid for a post biometric identification system , i.e . one that is not in real time compared to when the images were collected25 . Furthermore , in the absence of an express prohibition , the use of this technology for preventive purposes is permitted ; but the reservations expressed above with regard to its use for the purpose of law enforcement are also valid , if not more so , when it is used to prevent violations of public order . 23 . Furthermore , the proposal for a Regulation provides for three exceptions to the prohibition of real-time biometric identification of people : the targeted search for specific potential victims of crime , including missing children ; the prevention of a specific , substantial and imminent threat to the life or physical safety of natural persons or of a terrorist attack ; the detection , localisation , identification or prosecution of a perpetrator or suspect of a criminal offence referred to in Article 2 ( 2 ) of Council Framework Decision 2002/584/JHA26 . This third exception is particularly worrying since by accepting this technology for more than thirty offences , it in principle waters down the effectiveness of the prohibition . 24 . In short , the CNCDH therefore recommends the prohibition of remote biometric identification of people in publicly accessible spaces , due to the risks of serious infringement of fundamental rights and freedoms linked to a real or alleged challenge of anonymity in the public space27 . However , it recognises the legitimacy of the first two types of exceptions envisaged by the proposal for a Regulation , while stressing the need to ensure , where appropriate , strict supervision of them . The exception should therefore be limited in particular to the targeted search for specific potential victims of crime or to the prevention of a specific , substantial and imminent threat to the life or physical safety of persons and that of structures , facilities and establishments of vital importance . Recommendation 6 : The CNCDH recommends prohibiting the remote biometric identification of persons in publicly accessible spaces , by way of exception permitting its use , insofar as it is strictly necessary , adapted and proportionate , for the prevention of a serious and imminent threat to the life or physical safety of persons and that of 25 The proposal for a Regulation accepts the possibility of using this technology in this case , nevertheless categorising it as a high-risk AI system . 26 In particular : participation in a criminal organisation , illicit trafficking in arms , ammunition and explosives , corruption , environmental crimes , including illicit trafficking in endangered animal species and illicit trafficking in endangered species and plant species , aid for irregular entry and residence , racism and xenophobia , organised or armed robbery , illicit trafficking in cultural assets , including antiques and works of art , fraud , racketeering and extortion , falsification of administrative documents and trafficking in forgeries , illicit trafficking in hormone substances and other growth drivers , illicit trafficking in nuclear and radioactive materials , trafficking in stolen vehicles . 27 In the same vein : EDPB-EDPS Joint Opinion 05/2021 on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence ( Artificial Intelligence Act ) , 18 June 2021 . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights12 structures , facilities and establishments of vital importance . 25 . After reviewing the uses of AI deemed unacceptable by the proposal for a Regulation , the CNCDH questions the method used by the European Commission to achieve this identification , in the absence of sufficient indications in this regard . Based on the “ triple test ” derived from the case law of the European Court of Human Rights , endorsed by the case law of the Constitutional Council and the Council of State28 , the CNCDH considers that , in order to be considered legitimate , the infringement by an AI system of freedom must be “ appropriate , necessary and proportionate ” : appropriate , i.e . relevant for the legitimate objective pursued ; necessary , provided that it does not exceed what is required to achieve this objective and other means less detrimental to freedom were not possible ; proportionate , in that it should not , by the workload it creates , be disproportionate to the desired result29 . 1.2 . The need to extend the ban to other areas . 26 . The CNCDH has opted in this opinion for a comprehensive and cross-sectional approach to AI systems . It has not therefore carried out a detailed examination of the various applications likely to cause an excessive infringement of fundamental rights justifying their prohibition . This type of analysis may be carried out in future opinions devoted to new technologies or specific sectors . However , in addition to the observations on use cases prohibited by the proposal for an EU regulation on AI , the CNCDH would like to highlight two types of use of particular concern for the respect of human rights : predictive justice and the recognition of emotions in support of a selection process . 1.2.1 . AI in court . 27 . Applications such as those used in the United States to assess the risk of convicts reoffending30 pose a serious threat to their fundamental rights . The lack of transparency of the software used ( designed by private companies ) calls into question its compatibility with the fundamental rights of individuals and the guarantee of the rights of defence . In fact , under the pretext of the right to business secrecy and intellectual property , software designers are not required to share the source code of their algorithms , from which their instructions derive . It is therefore impossible for both the judge and the parties to the trial to understand precisely the methodology used by the algorithm to produce its results31 . 28 CC , Decision No . 2008-562 DC of 21 February 2008 , Act pertaining to post-sentence preventive detention and diminished criminal responsibility due to mental deficiency . 29 See in particular : M. Guyomar , “ Le passeport biométrique au contrôle : empreintes et cliché ” , Actualité Juridique Droit Administratif , 2012 , p. 35 ; J.-M. Sauvé , “ Le principe de proportionnalité , protecteur des libertés ” , Institut Portalis , Aix-en-Provence , 17 March 2017 . 30 See in particular COMPAS , the re-offending risk analysis and assessment algorithm used in several US States . 31 See : Supreme Court of Wisconsin , July 13 , 2016 , State v. Loomis . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights13 28 . This type of recourse to AI is already prohibited in France since “ no court decision involving an assessment of a person ’ s behaviour can have as a basis automated processing of personal data intended to assess certain aspects of the person ’ s personality ” 32 . The formula adopted does not , however , preclude any possibility of providing magistrates with an AI application for other purposes , for example to automate the calculation of compensation for damage . 29 . In view of the workload currently weighing on magistrates , the guarantees that could be provided in order to preserve the impartiality of the judge ( explicability/ intelligibility of how the algorithm works , reinforcement of the means of remedying automation bias ) do not seem sufficient to prevent the risk of an almost systematic take-up of the machine ’ s results . In addition , the intervention of an AI system to provide assistance to the judge may raise doubts among litigants as to its impartiality . However , the judge must not only be independent and impartial , he/she must also appear to be it33 . It is therefore the right to a fair trial that is thus threatened by this type of software . 30 . Furthermore , if the judge can be allowed a computer tool to facilitate his/her assessment of compensation for victims34 , it can not be based on machine learning , which is too opaque to meet the requirement of explicability that litigants are entitled to expect35 . The CNCDH also notes that the Ministry of Justice prematurely terminated , in January , its experimentation with such software , by establishing the multiplicity of criteria to be taken into account to characterise the extent of bodily injury and the excessive importance of the means to be mobilised to study and prevent algorithmic biases in order to achieve a satisfactory level of performance36 . Recommendation 7 : The CNCDH recommends continuing and deepening reflection in order to identify the contributions and limits of the use of AI in the context of judicial proceedings . 1.2.2 . AI and recognition of emotions . 31 . Emotion recognition technologies are based on a premise that is not very scientific , namely that emotions can be detected by facial expressions or , more 32 French Data Protection Act No . 78-17 of 6 January 1978 , Art . 47 amended by Order No . 2018-1125 of 12 December 2018 . On this point , French law goes further than the GDPR , since the GDPR only prohibits decisions , including court decisions , based “ exclusively ” on automated processing . 33 According to appearance theory , enshrined by the European Court of Human Rights , “ justice must not only be done , it must also be seen to be done ” . 34 Automated analysis of court decisions is technically based on natural language processing and machine learning : they can be a useful information tool for legal professionals . 35 On this point , Law No . 2016-1321 of 7 October 2016 ( known as the “ Digital Republic ” calls for the transparency of public algorithms , thus offering a guarantee against a possible “ black box ” phenomenon in terms of judicial use of artificial intelligence . See the response of the Minister of Justice published in the OJ Senate of 01/10/2020 , page 4462 , available online : https : //www.senat.fr/questions/base/2020/qSEQ200616942.html . 36 E. Marzolf , “ Le ministère de la Justice renonce à son algorithme DataJust ” , Acteurs publics , 14 January 2022 . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights14 generally , by body language . However , as the CAHAI recalls , “ no solid scientific evidence corroborates the idea that it would be possible to ‘ read ’ the emotions or the state of mind of a person on his/her face or using other biometric data ” 37 . In addition to the performance of the AI system , which progress in the design could remedy , its character may be inappropriate . Several national38 and international39 authorities have already expressed their concern in this regard , as European data protection bodies have even recently recommended banning the deduction of emotions through the use of AI , except in certain specific cases “ particularly for health and research purposes ” 40 . 32 . Sharing the same fears , the CNCDH therefore recommends applying a prohibition principle in this area , unless it can demonstrate that this biometric technology is able to reinforce the independence of people , or more broadly the effectiveness of their fundamental rights . To this end and despite its approximations , this AI technology can , for example , promote learning activities for people with disabilities or be useful for other human-machine interactions ( such as robot companions for the elderly ) . Recommendation 8 : The CNCDH recommends that emotion recognition technologies should be banned , by way of exception permitting their use as long as they aim to reinforce the independence of people , or more broadly the effectiveness of their fundamental rights . 2 . A FRAMEWORK THAT GUARANTEES RESPECT FOR FUNDAMENTAL RIGHTS . 33 . Although many AI applications do not cause a disproportionate infringement of fundamental rights and freedoms that would justify their prohibition , the CNCDH calls on public authorities to enforce certain guarantees by public and private stakeholders when AI is being designed , developed and used . This must mainly involve control and supervision of the AI system , at all stages of its life cycle , in view of its impact on fundamental rights . In addition to the vigilance to be exercised with regard to the system envisaged as a whole ( 2.1 ) , the decisions resulting from its implementation must be accompanied by safeguards that are able to protect individuals ( 2.2 ) . 2.1 . Control at all stages of AI system development . 34 . Depending on the areas concerned ( organisation of work , calculation of 37 CAHAI , Feasibility Study , p. 8 . 38 Defender of Rights , “ Technologies biométriques : l ’ impératif respect des droits fondamentaux ” , 2021 ; CNIL , “ Reconnaissance faciale : pour un débat à la hauteur des enjeux ” , 2019 . 39 EDPS , EDPB , EDPB-EDPS Joint Opinion 05/2021 on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence ( Artificial Intelligence Act ) , 18 June 2021 . 40 EDPS , EDPB , ibid . , p.14 A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights15 social security benefits , aid to companies , etc . ) , opting for an AI solution can have repercussions on employees or on the persons targeted by the software decisions . A human rights-based AI approach means involving those affected by its deployment , in particular the most vulnerable41 : 1. upstream , carrying out an impact assessment on fundamental rights is an essential step , and the result will affect the level of stakeholder involvement at this stage ; 2. secondly , the monitoring phase of AI system implementation must include them in order to assess whether its functioning leads to infringements of fundamental rights ; 3. more generally , the extent of this human control requires an improved offering of professional training and awareness-raising for individuals . 2.1.1 . An impact assessment on fundamental rights : a necessary prerequisite . 35 . Apart from certain uses of AI to be prohibited due to the seriousness of the risks they pose to fundamental rights and freedoms42 , other uses may have a more or less adverse impact on fundamental rights . These adverse effects are most often mentioned on the basis of a reference to the “ sensitive ” sectors in which the system is deployed , such as police , justice or health , or an inventory of fundamental rights and freedoms that may be challenged by AI technology43 . The approach of the EU AI Regulation : an a priori and centralised definition of risks for fundamental rights . 36 . For its part , the proposal for a Regulation reserves special treatment for AI systems classified as “ high-risk ” , identified as such by the European Commission because of the “ extent ” of their adverse impact on health , safety or fundamental rights . These are AI systems that operate in “ sensitive ” areas : biometric identification , management and operation of critical infrastructure , education and training , employment , access to essential private services , public services and social benefits , police , justice , migration management . The assessment of the impact of AI systems on human rights is therefore centralised and carried out a priori : high-risk systems are included in a list44 that the Commission may expand , under certain conditions and in accordance with a procedure that may take time45 . Of course , data controllers still have an obligation to carry out , in accordance with GDPR requirements , an impact assessment relating to data protection 41 See the opinion of the CNCDH , Opinion “ Pour une approche fondée sur les droits de l ’ Homme ” , Plenary session of 3 July 2018 , Official Journal of the French Republic No . 0161 of 14 July 2018 , text No . 104 . 42 See above . 43 See in particular the CAHAI feasibility study . 44 Appendix 3 to the proposal for a Regulation . 45 The Commission has the power to amend the list in Appendix 3 by “ delegated acts ” , the adoption procedure of which is set out in Article 73 of the proposal for a Regulation : it can take 6 months from notification of the proposed amendment to its entry into force , given that the European Parliament and the Council may also oppose it . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights16 and , more generally , the rights and freedoms of natural persons46 . 37 . The CNCDH has reservations about this system for several reasons . First , technological innovation is moving faster than regulations , and the list of high-risk AI systems used by the European institutions may not take into account current and future uses of particular concern for fundamental rights . Then , AI can , particularly with regard to machine learning in its most automated version ( deep learning ) , evolve independently of the intentions of its designers , which is why risks ignored at the system ’ s design stage may occur when the algorithm is developed or used47 . Above all , inclusion in this list gives rise to a series of obligations , mainly incumbent on suppliers , relating to risk management , data integrity , control and monitoring of the system , etc. , while users of a high-risk AI system have few obligations48 . An impact study on fundamental rights incumbent on users . 38 . The CNCDH considers that it is necessary to go further by also requiring the user of the AI system to carry out a study of its impact on fundamental rights . For several reasons : firstly , because this study places the responsibility on the public or private body wishing to use this IT option ; secondly , because it could possibly lead to dialogue between all stakeholders ( employees , customers , users of a public service , etc . ) on the appropriateness of its use ; finally , because it will be a source of information for oversight of the AI system throughout its life cycle , or even for the person who is ultimately affected by an automated decision . 39 . If there is no question , within the framework of this general opinion , of detailing the elements to be taken into account when carrying out this analysis , the CNCDH would like to formulate a certain number of recommendations relating to the broad lines that could guide its implementation . 40 . It therefore recommends that users should assess the impact of the use of the AI system on fundamental rights and , if risks are identified , carry out an assessment taking into account the probability and severity of these risks . This would include , for example , tax fraud detection algorithms , social security benefits , personnel management support systems , voice control software in distribution platforms , etc . 41 . This analysis may include the communication of elements provided by the designer , when they have an impact on fundamental rights : data sets used for machine 46 In accordance with Article 35 of the GDPR . The proposal for a Regulation also takes care to reiterate it in Article 29 . 47 For example , Microsoft ’ s chatbot , intended in 2016 to participate in social media exchanges , quickly made abusive and racist comments , reversing the intention of its programmers . This drift is the result of the interaction of the AI system with ill-intentioned people : M. Tual , “ A peine lancée , une intelligence artificielle de Microsoft dérape sur Twitter ” , Le Monde , 24 March 2016 . 48 See Article 29 of the Proposal for a Regulation on AI and Article 52 for certain more specific AI systems . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights17 learning , for example for suspicious behaviour detection software intended to equip surveillance cameras ; the different types of settings possible , for example , for automated content moderation tools , etc . The user must therefore indicate and justify his/her configuration choices by putting in perspective the intended purpose and the risks of infringement of fundamental rights . The content of the impact assessment . 42 . As regards its content , any impact assessment should , as a minimum , consist initially of mentioning the purposes of using the envisaged AI system , and identifying the fundamental rights that may be affected . 43 . Secondly , this study should reveal the answers provided by the user to the questions that usually feed into the system controlling fundamental rights and freedoms : • Is an AI system necessary for the task at hand ? To what extent does it bring added value to prior operation ? • Is the AI system appropriate for the intended purpose ? To what extent will it be possible to accomplish the task ? • Is the AI system proportionate ? To what extent is the potential infringement of human rights , including the environmental impact , justified in relation to the benefits expected of achieving the legitimate objective of the system ? 44 . Finally , on a more technical level , the impact study should include the procedures put in place to monitor the application , as well as the measures taken to mitigate the risks involved49 . Stakeholder consultation procedures subject to the conclusions of the impact assessment . 45 . Depending on the level of risk for fundamental rights identified at the end of the impact assessment , stakeholder consultation should potentially be planned in order to discuss whether or not to use the AI solution envisaged by management . Three risk levels could be selected to determine the terms and conditions of this consultation : • a high level ( for all processing affecting the rights of individuals ) : consultation of all stakeholders , including staff representatives and associations of public service users or consumers , ensuring the inclusion of associations of disadvantaged persons ; • a moderate level ( for processing that does not directly affect people ’ s rights , such as leave management software ) : the consultation referred to in the case of high level 49 The High-Level Expert Group on Artificial Intelligence ( EU ) refers to this type of analysis , upstream of the selfassessment it recommends and for which it relies on a multi-criteria checklist for “ Trustworthy AI ” : High-Level Expert Group on Artificial Intelligence , The Assessment List for Trustworthy Artificial Intelligence ( ALTAI ) , June 2020 . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights18 would be optional , and could be requested for example by the affected persons or their representatives ; • a low level : the impact assessment would be communicated at the request of the persons affected by the AI system or their representatives . 46 . An impact assessment by the user is all the more necessary as it will facilitate supervision of the AI system once it is set up . Recommendation 9 : The CNCDH recommends that the user of an AI system assess the impact of using this system on fundamental rights and , if risks are identified , carry out an assessment taking into account the probability and severity of these risks . The impact assessment should include at least : • a statement of the purpose ( s ) attached to the use of the envisaged AI system ; • identification of the fundamental rights likely to be affected by the system ; • a review of the envisaged AI system , based on an assessment of its necessity , its suitability and the proportionality to the infringements of fundamental rights in relation to the intended purpose ; • the procedures put in place to monitor the application , and the mitigation measures with regard to the risks incurred . Recommendation 10 : Depending on the risks posed by an AI system to fundamental rights in a particular context of use , the CNCDH recommends ensuring , prior to the decision to use it , a stakeholder consultation , according to appropriate procedures , including for example staff representatives and , more broadly , the persons affected by the AI system . 2.1.2 Supervision of the system throughout the life cycle . 47 . The quality and relevance of the data selected to design algorithms and the proportionality of possible infringements of fundamental rights by the AI system may be subject to control prior to the use of the AI system . However , infringements of fundamental rights may occur after the system is taken over by the user . This is why continuous vigilance over the functioning of the AI system must be ensured . This supervision must be organised , as has been explained above , with regard to stakeholder consultation at the end of the impact study , according to procedures that are more or less demanding on the user depending on the risks for the fundamental rights identified by this study . Continuous vigilance over the effects of the AI system on fundamental rights . 48 . Periodic control must be carried out at the various stages of use of an AI system . It must be based on the human rights impact assessment by ensuring that the risks identified upstream have not materialised or , where required , identify the measures to A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights19 be taken to neutralise these risks . Infringements of fundamental rights not identified by the impact assessment may also be identified on this occasion . 49 . In this regard , particular attention should be paid to the risks of discrimination caused by AI systems . These risks have already been widely documented50 . While the automation of decision-making processes may give the impression at first glance of being free from the prejudices inherent to human subjectivity , algorithms may reproduce , strengthen or generate biases , particularly systemic ones , likely to aggravate discriminatory situations . As the Commissioner for Human Rights of the Council of Europe points out , there is a major risk of “ essentialisation ” and strengthening “ stereotypes ” because the predictive nature of the algorithm is based on the homogenised behaviour or characteristics of groups . 50 . These biases can be caused by the data used to feed into the machine . The data sets used to train the algorithmic model may indeed include discrimination , for example if an AI is expected to select the best applications for a given position only from the files of previously recruited people , the model may reproduce discriminatory biases as long as they characterise these recruitments ( racist or sexist for example ) . These data sets may also not be sufficiently representative of the diversity of the population , for example if facial recognition software is populated primarily by photographs of Caucasian people . The software will tend to make identification errors on black people , leading to unjustified questioning . 51 . Algorithm classifications are therefore likely to generate discrimination against individuals , because of their membership of a group , which can indirectly correspond to a group protected by non-discrimination law ( by proxy , i.e . a variable linked to a prohibited discrimination criterion , for example a dietary habit that would be evidence of religious beliefs ) . 52 . In addition , the discriminatory effects of algorithms are not noticeable on an individual scale , given the opacity of the functioning of algorithms , but also because these discriminations are much easier to observe at group level than at individual level51 . For this reason , continuous vigilance must be exercised over AI systems and algorithm classification when these systems produce results that have , even indirectly , effects on the rights and freedoms of individuals52 . 50 See in particular the reports of the Defender of Rights : see above , footnote no . 4 . 51 Defender of Rights , in partnership with the CNIL , “ Algorithmes : prévenir l ’ automatisation des discriminations ” , 2020 , p. 6 . 52 In the same vein , the Defender of Rights and the CNIL recommend regular monitoring of the effects of algorithms after their deployment , along the same lines as control of adverse drug reactions : Defender of Rights and the CNIL , “ Algorithmes : prévenir l ’ automatisation des discriminations ” , p. 10 . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights20 Conditions for supervision by the user . 53 . The proposal for a Regulation on AI requires suppliers of high-risk AI systems to establish a quality management system , which should gather information about all the procedures and instructions they put in place to comply with the requirements of the Regulation53 . These instructions must include , among other things , the terms of the monitoring established by the supplier after marketing of an AI system , in order for it to comply with the regulation in the long term54 . Control over how the supplier performs this monitoring is entrusted by the proposal for a Regulation to approved conformity assessment bodies55 , or even to a supplier internal assessment56 . 54 . The CNCDH considers this approach necessary but insufficient where AI systems are concerned for which the impact assessment reveals a significant risk of infringement of fundamental rights . In this case , supervision should be based in addition on tests and surveys carried out by the stakeholders , the user and the affected persons , at intervals to be determined depending on the context of use . From this point of view , the “ human oversight ” provided for in Article 14 of the proposal for a Regulation , in particular to “ fully understand the capacities and limitations of the AI system ” , should be clarified and supplemented by a reference to the collegial nature of the process and the opening up of this college to the various stakeholders . 55 . Oversight relating to the impact of a recruitment or management AI system within a company or administration must , for example , include the involvement of staff representatives . These representatives are in fact key players , particularly within the company ’ s social and economic committee ( CSE ) , in the assessment of the psychosocial risks caused by an AI system within their organisation but also its impact on the organisation of work . 56 . If the impact assessment does not reveal any significant risks to fundamental rights , supervision of the AI system may be the sole responsibility of the user . The CNCDH indeed insists on the need to highlight its role in the vigilance to be maintained throughout deployment of the system , beyond the mere communication of “ relevant data ” to the supplier in order to comply with the monitoring obligation incumbent upon the supplier , in accordance with the proposal for a Regulation57 . Recommendation 11 : The CNCDH recommends setting up oversight of the AI system , according to a procedure likely to vary according to the risks of infringements of fundamental rights as identified by the impact assessment , in order to maintain 53 Proposal for a Regulation on AI , Art . 17 . 54 Proposal for a Regulation on AI , Art . 61 . 55 “ Notified bodies ” . 56 Proposal for a Regulation on AI , Art . 43 . 57 According to Article 61 of the proposal for a Regulation on AI . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights21 continuous vigilance on the part of the user with regard to the effects of the system , including its discriminatory effects . 2.1.3 Training and awareness of AI issues . 57 . In view of the place to be assigned to stakeholders in the control and supervision of AI systems , particularly employees and the individuals affected by the system ’ s decisions , the CNCDH recommends the implementation of training and awareness campaigns on AI technologies . Training modules could be widely disseminated to employees and everyone , for example in the form of MOOCs . The CNCDH therefore recommends public investment in the design of training and information tools accessible to as many people as possible . Recommendation 12 : The CNCDH recommends encouraging public investment in the design of training and information tools accessible to as many people as possible . 58 . In addition , public authorities should organise public debates on this issue . Based on the model of the États Généraux de la Bioéthique ( Bioethics Forum ) organised by the National Advisory Committee on Ethics ( CCNE ) , these consultations would have a dual purpose : on the one hand , to inform citizens about how these systems operate and their purpose , and , on the other hand , to enable them to position themselves on national guidelines on this subject . In doing so , it is a question of promoting the expression of a diversity of views on a number of uses of AI . Special attention should be paid to the poorest people , in order to ensure that they are able to participate . Recommendation 13 : The CNCDH recommends organising national consultations along the same lines as the États généraux de la bioéthique ( Bioethics Forum ) organised by the National Advisory Committee on Ethics . 59 . The proportion of algorithms in daily life and the functioning of society calls for the acquisition of a true computer culture from a very young age . The CNCDH thus emphasises the need for National Education to strengthen the training of students in the technical , political and societal challenges of artificial intelligence and to propose , to this end , educational materials for teachers . Recommendation 14 : The CNCDH recommends that the Ministry of Education strengthens the training of students in the technical , political and societal issues surrounding artificial intelligence and proposes , to this end , educational materials for teachers . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights22 2.2 . Guaranteed respect for fundamental rights with regard to indivi dual decisions 60 . The weight given to the algorithm in decision-making varies from one use to another : human intervention may be anticipated , or even required , but in some cases decision-making can also be fully automated . The need for and the conditions of this intervention depend on the level of risk of infringement of fundamental rights . Moreover , the persons targeted by an AI system must be informed , and also have intelligible information on how the algorithm works and on the weight given to the algorithm in the individual decision . 2.2.1 Human intervention guaranteeing consideration of individual specificities . 61 . To “ enable humans to keep the upper hand ” 58 over AI , the CNCDH calls for the reintroduction of humans at the end of the automated decision-making process : either at user level , responsible for checking the result produced by the algorithm , or at the level of the person affected . Human intervention at user level : checking the algorithmic result . 62 . In some cases , human oversight of the general functioning of the AI system , as mentioned above , requires in addition human intervention in relation to individual decisions based on the results of this system . The GDPR has enshrined a right “ not to be subject to a decision based solely on automated processing , including profiling , which produces legal effects concerning him or her or similarly significantly affects him or her ” 59 . The legislator has taken care to include it in the 1978 French Data Protection Act , specifying , in light of the scope of the GDPR , that the prohibition concerns any “ automated processing of personal data ” 60 . 63 . However , the current regulation has two limitations : on the one hand , it does not cover algorithmic processing using anonymised data and , on the other hand , it envisages many derogations from its prohibition in principle , starting with the ability of EU Member States to authorise such processing , on condition , however , that it provides for “ suitable measures to safeguard the data subject ’ s rights and freedoms and legitimate interests ” . 64 . The need for human intervention has already been highlighted by the Constitutional Council for individual decisions adopted on the basis of an algorithm whose operating procedures can not be communicated ( because their communication 58 According to the formula enshrined by the CNIL in its report of 2017 . See above . 59 Proposal for a Regulation on AI , Art . 22 . 60 French Data Protection Act 78-17 of 6 January 1978 , Art . 47 ( 2 ) A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights23 would undermine defence secrecy , State security , etc. ) 61 . More recently , with regard to the French system for detecting connections likely to reveal a terrorist threat , the CJEU argued that “ any positive result obtained following automated processing must be subject to an individual re-examination by non-automated means before an individual measure adversely affecting the persons concerned is adopted ” 62 . 65 . The CNCDH considers that the need for human intervention should be imposed more generally , to varying degrees , depending on the field in question , for all algorithmic processing having effects on the rights of individuals . 66 . In order to ensure effective human control in the context of use of the AI system , the conditions of human intervention may vary depending on the impact of the AI system on fundamental rights , in terms of : • the composition of the supervisory body ( individual or college ) ; • the extent of the information made available to stakeholders , knowing that , in some cases , it will be necessary to provide additional data to those processed by the system ; • the type of training to be provided to persons in charge of intervention ; • the appropriate moment to intervene ( at the end of the result obtained by the machine , therefore upstream of the decision , or ex post at the request of the person ) . 67 . Ensuring effective human intervention means informing the stakeholder about the characteristics of the algorithm used : the technology at the origin of its design , the type of data used for its modelling , the operating parameters and the weighting of the criteria used by the algorithm designer , reliability , etc . This information on the “ inner workings of the machine ” is required to encourage standing back from the AI system used , thereby reducing the cognitive automation bias of placing excessive reliance on automated decision-making processes . This is why the CNCDH reiterates the need for professionals assisted by an AI application in the performance of their duties ( doctors , magistrates , administrative agents , recruiters , etc . ) to have clear , complete and comprehensible information on these aspects . 68 . In order to neutralise the automation bias , the CNCDH also recommends training any stakeholder called upon to oversee the individual results produced by the AI system , concentrating in particular on its limits ( biases derived from the data , the probabilistic nature of the results obtained , etc. ) . In addition , the Commission recommends that no particular constraints should be imposed , for example by additional formalities , on these persons when they depart from the algorithmic indication . 69 . Finally , and particularly with regard to citizens , automation of decisions can 61 Constitutional Council , Decision No . 2018-765 DC , Law related to the protection of personal data , § 70 . 62 CJEU , 6 October 2020 , Case C 511/18 , La Quadrature du Net , § 182 . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights24 only aggravate the alienation felt , faced with the growing dematerialisation of public services and the difficulty of asserting their rights63 . The CNCDH therefore endorses the recommendation of the Defender of Rights when it advocates “ the systematic maintenance of alternative access and the possibility of sufficiently close , competent and available support ” 64 . Recommendation 15 : The CNCDH recommends : • ensuring human intervention to oversee individual decisions resulting from an AI system in accordance with the latter ’ s risk level ; • ensuring effectiveness thereof through appropriate training and information for personnel on the characteristics of the system , without imposing any particular constraint on them when they deviate from the AI system ’ s recommendation ; • ensuring the systematic maintenance of alternative access to a human agent for public service users . Human intervention at the level of the person affected : the right t o configuration . 70 . In some cases , the person affected by the functioning of AI is directly exposed to the results of an algorithm designed , according to the accompanying marketing message , to “ meet its needs ” . From this perspective , the algorithm presents itself as a tool shaped by an operator and used by the people who resort to its services . In this respect , the CNCDH is particularly concerned about the lack of control by the user of the system ’ s operating parameters . 71 . As the CNCDH has already noted in its opinion on the fight against online hate , such control seems particularly necessary with regard to content selection algorithms on social networks . Freedom of conscience in fact requires the autonomy of users to be strengthened and their control over the content offered to them to be increased . The CNCDH therefore renews its recommendation to recognise a right to configure the criteria for determining the content received , as regards both their selection and their presentation65 . 72 . More generally , this right to adapt the parameters of the algorithm would result in a new manifestation of the role that should be recognised for the user the moment a genuine “ human-centred AI ” is advocated for , an intention shared by the French , European and international authorities . The user could thus be given the right to adapt the parameters of AI systems when they feed into interpersonal or human-machine interactions , which in particular will be developed with the growth of chatbots . 63 Defender of Rights , “ Dématérialisation des services publics : trois ans après , où en est-on ? ” , Report , 2022 . 64 Ibid . , p. 5 . 65 For a more detailed presentation of this right and its implications , see : CNCDH , Opinion on the fight against online hate , Plenary session of 8 July 2021 , Official Journal of the French Republic No . 0170 of 24 July 2021 , text No . 79 . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights25 Recommendation 16 : The CNCDH recommends that users of AI systems be given the right to configure their criteria , in particular in order to determine the selection and presentation of the content received , and more generally in the event of humanmachine interactions . 2.2.2 . Information guaranteeing human dignity . 73 . Information on the characteristics of the AI system used is necessary for monitoring individual decisions . It must also allow the affected person to understand the reasons and , possibly , to challenge it . To do so , the person must be informed that the decision to which he or she is subject is based , in part or in full , on an automated process . He/she must then have the information enabling him/her to understand how the algorithm used works . Information on the intervention of an AI system in the decision . 74 . The person exposed to an AI system must be informed about it . The CNCDH considers that this is a prerequisite for which there should be no exceptions . 75 . The current regulations already recognise a right for users of the administration concerned by the use of an AI system to be informed that an individual administrative decision has been made on the basis of an algorithm66 . However , there are a large number of exceptions to this right . Indeed , the law provides for the exclusion of this information in the event that it has an impact on : the secrecy of the deliberations of the Government and of the responsible authorities under the executive power ; national defence ; the conduct of France ’ s foreign policy ; State security , public security , the security of persons or the security of the administrations ’ information systems ; currency and public credit ; the conduct of proceedings before the courts or preliminary operations in such proceedings , unless authorised by the competent authority ; the investigation and prevention , by the competent authorities , of offences of any kind ; other secrets protected under Article L.124-4 of the French Environmental Code67 . 76 . The CNCDH regrets the extent of these grounds , especially since they were initially intended to justify the exclusion of a communication or a consultation of an administrative document . There are two different types of information to distinguish : on the one hand , information on the nature of the process at the origin of the decision , in this case the use of an automated decision-making process and , on the other hand , information on how the algorithm used works . If it is acceptable that the logical rules making up the algorithm can escape communication due to a certain number of public imperatives , this can not justify there being no mention that the individual decision is based on the use of an algorithm , even though this piece of information will be useful 66 Art . L.311-3-1 of the French Code of Relations between the Public and the Administration . 67 Art . L.311-5 of the French Code of Relations between the Public and the Administration . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights26 to the interested party in the event of a judicial appeal . 77 . The CNCDH believes that this obligation of information should be extended to private individuals , while being designed more broadly with regard to public entities . 78 . While the proposal for an EU Regulation mentions a “ transparency obligation ” , it does so restrictively by reserving it for certain AI systems . First , it imposes an obligation on providers of AI systems intended to interact with humans , forcing them to design them in such a way that the persons affected are warned about them . Secondly , it imposes an obligation on users of emotion recognition systems , or a biometric categorisation system , to inform the persons exposed to it . Finally , the manipulation of images or audio or video content , such as deepfakes , must be accompanied by a warning message . Recommendation 17 : The CNCDH recommends systematically informing people when they are exposed to or required to interact with an AI system and , when they are the subject of a decision , that this decision is based , where applicable , in part or in full on algorithmic processing . 79 . Those affected by a decision resulting from an AI system must not only have a right to be informed of it , but also have a right to contest this decision with a human being . This human being must be able to review the individual ’ s file . In order to make this right to review effective , easily accessible channels must be made available to those concerned . Recommendation 18 : The CNCDH recommends guaranteeing the affected person the right to review , by a human being , of any individual decision based totally or partly on algorithmic processing , provided that it has significant consequences for him/her . Information on the conditions of automated decision-making . 80 . The requirement for AI system explicability is found in most of the reference texts on governance of AI68 . Sometimes understood to mean “ transparency ” , in any case likely to have diverse acceptations , explicability essentially refers to the need for the designer and/or the user of an AI system to be able to provide affected persons with comprehensible information on the functioning of the algorithm . The right to have the means to understand the reasons why AI has achieved its outcome should be guaranteed . This right to information should extend to communicating the conditions of possible human intervention in the decision-making process . 68 See in particular : OECD , Council Recommendation on AI , 1.3. ; UNESCO , Recommendation on the Ethics of AI , §§ 37 et seq . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights27 81 . Currently , the regulations impose this requirement only with respect to the administration . In fact , it must communicate “ in an intelligible form ” to persons who have been the subject of an individual decision taken on the basis of algorithmic processing , if they so request , the following information69 : • The degree and method of contribution of the algorithmic processing to decisionmaking ; • The data processed and its sources ; • The processing parameters and , where applicable , their weighting , applied to the situation of the affected person ; • The operations carried out by the processing . 82 . The CNCDH recommends considering the extension of this obligation to private organisations whose activity may affect the rights of individuals ( social networks , banks , insurance , etc. ) . In this regard , the protection of intellectual property can not constitute an insurmountable obstacle : it would not mean making the source code of a software public , but rather communicating to the person requesting it information on the elements taken into account by the machine ( including the main criteria relating to his or her individual situation ) , in a language that is easy to understand , in order to explain the process that led to the decision . 83 . In subsequent opinions , the CNCDH will examine the proportionality of the restrictions on the requirement of disclosure of such information by the administration and private bodies , when they are likely to infringe a secret protected by law70 , depending on the fields in question . 84 . The explicability requirement applies more broadly to ensure the effectiveness of the right to appeal against individual decisions based on an algorithm . 85 . Keeping the person who is the subject of a decision fully informed , by the administration or by a private body , a banking institution for example , means providing him/her with explanations about how the algorithm applied to his/her personal situation works and , more importantly , returning , where applicable , the conditions for human intervention . Recommendation 19 : The CNCDH recommends that administrations communicate in an intelligible form information on the functioning of the algorithm , as well as on the possible part played by human intervention in the decision-making process . It also recommends thinking about extending this obligation to private organisations . 86 . In view of the growing importance of the deployment of artificial intelligence systems , the resulting major challenges for the respect of fundamental rights , as well 69 R.311-3-1-2 of the Code of Relations between the Public and the Administration . 70 In particular “ defence secrecy ” and business secrecy . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights28 as the preservation of the rule of law and democracy , not forgetting the environment , the National Consultative Commission on Human Rights intends to continue its work on artificial intelligence in the future , especially to examine its impacts , particularly in the areas of health , education , employment and justice . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights29 SUMMARY OF RECOMMENDATIONS . Recommendation 1 : The CNCDH recommends favouring , in institutional communication , a more neutral and objective terminology than the term “ artificial intelligence ” , such as “ Algorithmic Decision Support System ” ( ADSS ) . Recommendation 2 : The Commission recommends strengthening , in the proposal for a European Union regulation on AI , the provisions to ensure the establishment of a binding legal framework guaranteeing effective respect for fundamental rights . In addition , the CNCDH recommends the adoption , within the framework of the Council of Europe , of a “ Convention 108+ on AI ” . Recommendation 3 : The CNCDH recommends that a human rights-based approach be taken into account in the ongoing reforms , as they aim to ensure respect for fundamental rights . Recommendation 4 : The CNCDH recommends the prohibition of the use of choice interfaces insofar as they have the purpose or effect of manipulating users , to their detriment , by exploiting their vulnerabilities . Recommendation 5 : The CNCDH recommends banning any type of social scoring set up by government authorities or by any company , public or private . Recommendation 6 : The CNCDH recommends prohibiting the remote biometric identification of persons in publicly accessible spaces , by way of exception permitting its use , insofar as it is strictly necessary , adapted and proportionate , for the prevention of a serious and imminent threat to the life or physical safety of persons and that of structures , facilities and establishments of vital importance . Recommendation 7 : The CNCDH recommends continuing and deepening reflection in order to identify the contributions and limits of the use of AI in the context of judicial proceedings . Recommendation 8 : The CNCDH recommends that emotion recognition technologies should be banned , by way of exception permitting their use as long as they aim to reinforce the independence of people , or more broadly the effectiveness of their fundamental rights . Recommendation 9 : The CNCDH recommends that the user of an AI system assess the impact of using this system on fundamental rights and , if risks are identified , carry out an assessment taking into account the probability and severity of these risks . The A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights30 impact assessment should include at least : • a statement of the purpose ( s ) attached to the use of the envisaged AI system ; • identification of the fundamental rights likely to be affected by the system ; • a review of the envisaged AI system , based on an assessment of its necessity , its suitability and the proportionality to the infringements of fundamental rights in relation to the intended purpose ; • the procedures put in place to monitor the application , and the mitigation measures with regard to the risks incurred . Recommendation 10 : Depending on the risks posed by an AI system to fundamental rights in a particular context of use , the CNCDH recommends ensuring , prior to the decision to use it , a stakeholder consultation , according to appropriate procedures , including for example staff representatives and , more broadly , the persons affected by the AI system . Recommendation 11 : The CNCDH recommends setting up oversight of the AI system , according to a procedure likely to vary according to the risks of infringements of fundamental rights as identified by the impact assessment , in order to maintain continuous vigilance on the part of the user with regard to the effects of the system , including its discriminatory effects . Recommendation 12 : The CNCDH recommends encouraging public investment in the design of training and information tools accessible to as many people as possible . Recommendation 13 : The CNCDH recommends organising national consultations along the same lines as the États généraux de la bioéthique ( Bioethics Forum ) organised by the National Advisory Committee on Ethics . Recommendation 14 : The CNCDH recommends that the Ministry of Education strengthens the training of students in the technical , political and societal issues surrounding artificial intelligence and proposes , to this end , educational materials for teachers . Recommendation 15 : The CNCDH recommends : • ensuring human intervention to oversee individual decisions resulting from an AI system in accordance with the latter ’ s risk level ; • ensuring effectiveness thereof through appropriate training and information for personnel on the characteristics of the system , without imposing any particular constraint on them when they deviate from the AI system ’ s recommendation ; • ensuring the systematic maintenance of alternative access to a human agent for public service users . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights31 Recommendation 16 : The CNCDH recommends that users of AI systems be given the right to configure their criteria , in particular in order to determine the selection and presentation of the content received , and more generally in the event of humanmachine interactions . Recommendation 17 : The CNCDH recommends systematically informing people when they are exposed to or required to interact with an AI system and , when they are the subject of a decision , that this decision is based , where applicable , in part or in full on algorithmic processing . Recommendation 18 : The CNCDH recommends guaranteeing the affected person the right to review , by a human being , of any individual decision based totally or partly on algorithmic processing , provided that it has significant consequences for him/her . Recommendation 19 : The CNCDH recommends that administrations communicate in an intelligible form information on the functioning of the algorithm , as well as on the possible part played by human intervention in the decision-making process . It also recommends thinking about extending this obligation to private organisations . A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights32 LIST OF PEOPLE HEARD Celine Castets-Renard , Professor of Private Law , University of Ottawa Research Chair on Accountable Artificial Intelligence in a Global Context For the CGT-UGICT : Sylvain Delaitre ( CGT Thalès ) , Jean-Luc Molins ( National Secretary ) , Matthieu Trubert ( joint organiser of the UGICT-CGT Digital Collective ) Mona Caroline Chammas , Attorney & Integrity Director GOVERN & LAW Régis Chatellier , Innovation , Research and Foresight Project Manager at the French Data Protection Authority ( CNIL ) Raja Chatilla , INRIA Research Director , Institute of Intelligent Systems and Robotics , Sorbonne University Defender of Rights , represented by Sarah Bénichou , Assistant to the Director of “ Promotion of Equality and Access to Rights ” and Gaëtan Goldberg , Head of Digital , Rights and Freedoms Sonia Desmoulin-Canselier , CNRS Research Officer at the University of Nantes Laboratory of Law and Social Change Laurence Devillers , Professor of Computer Science Applied to Social Sciences at Paris IV University and IT Researcher at CNRS ’ s Interdisciplinary Laboratory of Digital Sciences in Saclay Tim Engelhardt , Head of Human Rights in the Rule of Law and Democracy Section ( United Nations High Commissioner for Human Rights ) Jean-Gabriel Ganascia , Professor of Computer Science at the Faculty of Science at Sorbonne University and Researcher in Artificial Intelligence at LIP6 Alexei Grinbaum , Director of Research at the Laboratory for Philosophy of Science ( LARSIM ) at CEA-Saclay David Gruson , Member of the Management Board for the Po Paris Sciences Health Programme Joanne Kirkham , Researcher at the University of Paris II Panthéon-Assas Claude Kirchner , Director of the National Digital Ethics Pilot Committee Michel Lansard , ATD Fourth World Karine Lefeuvre , Vice-Chair of the National Ethics Advisory Committee Daniel Le Métayer , Director of Research at the National Institute for Research in Digital Science and Technology ( INRIA ) Grégoire Loiseau , Professor of Private Law , University of Paris 1 Panthéon-Sorbonne A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights33 Frédéric Marty , CNRS Research Officer at GREDEG , Université Côte d ’ Azur Winston Maxwell , Director of Law and Digital Studies at Telecom Paris , Institut Polytechnique de Paris Yannick Meneceur , Magistrate seconded to the Council of Europe , adviser in digital transformation and artificial intelligence and research associated with IHEJ Arthur Messaud , La Quadrature du Net For Numeum : Valentin Hueber , ESN/ICT Delegate ; Anissa Kemiche , Head of European Affairs ; Katya Lainé , Chairman of the Numeum AI Committee ; François Lhemery , Deputy Director of Public Affairs and Communication Louis Perez , Researcher at the University of Paris II Panthéon-Assas Patrick Perrot , Gendarmerie Officer , AI Coordinator at the Gendarmerie Fabien Tarissan , Researcher in Computer Science at CNRS ( Section 06 ) , Professor attached to ENS Paris-Saclay Romain Tinière , Professor of Public Law , Grenoble-Alpes University Renaud Vedel , Prefect , Coordinator of the National Strategy for Artificial Intelligence Cédric Villani , Deputy Serena Villata , Research Officer at CNRS , Laboratory of Computer Science , Signals and Systems ( I3S ) , Sophia Antipolis University A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights34 A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights35 20 Avenue Ségur - TSA 40 720 - 75334 PARIS Cedex 07 Tel : 01.42.75.77.09 Mail : cncdh @ cncdh.fr www.cncdh.fr @ CNCDH @ cncdh.france Created in 1947 at the instigation of René Cassin , the National Consultative Commission on Human Rights ( CNCDH ) is the French national institution responsible for promoting and protecting human rights with level ‘ A ’ accreditation from the United Nations . The CNCDH performs a threepronged role that involves the following : • enlightening the public decision-making process with regards to human rights ; • monitoring the effectiveness in France of rights protected by international human rights conventions ; • overseeing France ’ s implementation of recommendations made by inter- national committees . The CNCDH is independent and operates based on the principle of the pluralism of ideas . This being the case , as the only institution that maintains continuous dialogue between civil society and French experts in the field of human rights , the Committee comprises 64 qualified individuals and reprsentatives of nongovernmental organisations with their roots in civil society . The CNCDH has been an independent National Rapporteur on the fight against all forms of racism since 1990 , on the fight against the trafficking and exploitation of human beings since 2014 , on `` Business and Human rights '' since 2017 , on the fight against homophobia since 2018 and on the right of persons with disabilities sinces 2020 .
