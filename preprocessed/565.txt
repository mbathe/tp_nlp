stop explaining black box machine learning models high stakes decisions use interpretable models instead cynthia rudin duke university cynthia abstract black box machine learning models currently used high stakes throughout society causing problems throughout healthcare criminal justice domains people hoped creating methods explaining black box models alleviate problems trying toexplain black box models rather creating models interpretable ﬁrst place likely perpetuate bad practices potentially cause catastrophic harm society way forward design models inherently interpretable manuscript clariﬁes chasm explaining black boxes using inherently interpretable models outlines several key reasons explainable black boxes avoided decisions identiﬁes challenges interpretable machine learning provides several example applications interpretable models could potentially replace black box models criminal justice healthcare computer vision introduction increasing trend healthcare criminal justice leverage machine learning prediction applications deeply impact human lives many models black boxes explain predictions way humans understand lack transparency accountability predictive models already severe consequences cases people incorrectly denied parole poor bail decisions leading release dangerous criminals pollution models stating highly polluted air safe breathe generally poor use limited valuable resources criminal justice medicine energy reliability ﬁnance domains rather trying create models inherently interpretable recent explosion work explainable second posthoc model created explain ﬁrst black box model problematic explanations often reliable misleading discuss instead use models inherently interpretable provide explanations faithful model actually computes follows discuss problems explainable followed challenges interpretable document mainly relevant decision making troubleshooting models main two reasons one might require interpretable explainable model interpretability notion deﬁnition usually however interpretable machine learning model constrained model form either useful someone obeys structural knowledge domain monotonicity causality structural generative constraints additivity physical constraints come domain knowledge interpretable models could use reasoning complex domains often structured data sparsity useful measure interpretability since humans handle cognitive entities sparse models allow view variables interact jointly rather individually discuss several forms interpretable machine learning models different applications never single deﬁnition domains sparsity useful others spectrum fully transparent models understand variables jointly related models lightly constrained model form models forced increase one variables increases models else equal prefer variables domain experts identiﬁed important see preliminary version manuscript appeared workshop entitled please stop explaining black box machine learning models high stakes decisions sep key issues explainable black box model could either function complicated human comprehend function proprietary see appendix deep learning models instance tend black boxes ﬁrst kind highly recursive term presently used common form explanation separate model supposed replicate behavior black box black box says people delinquent current credit likely default new loan note term explanation refers understanding model works opposed explanation world works terminology explanation discussed later misleading concerned ﬁeld machine learning strayed away needs real problems ﬁeld dates back early least see huge number papers interpretable various ﬁelds often word interpretable explainable title recent papers recent work explainability black boxes rather interpretability models contains perpetuates critical misconceptions generally gone unnoticed lasting negative impact widespread use machine learning models society let spend time discussing discussing possible solutions myth necessarily accuracy interpretability widespread belief complex models accurate meaning complicated black box necessary top predictive performance however often true particularly data structured good representation terms naturally meaningful features considering problems structured data meaningful features often signiﬁcant difference performance complex classiﬁers deep neural networks boosted decision trees random forests much simpler classiﬁers logistic regression decision lists preprocessing appendix discusses data science problems structured data meaningful features constructed part data science process tends little difference algorithms assuming data scientist follows standard process knowledge discovery kdd bigdata see even applications computer vision deep learning major performance gains interpretability much difﬁcult deﬁne forms interpretability imbued directly models without losing accuracy discussed later challenges section uninterpretable algorithms still useful decisions part knowledge discovery process instance obtain baseline levels performance generally ﬁnal goal knowledge discovery figure taken darpa explainable artiﬁcial intelligence program broad agency announcement exempliﬁes blind belief myth real ﬁgure generated data axes quantiﬁcation speciﬁc meaning horizontal vertical axes image appears illustrate experiment static dataset several machine learning algorithms applied dataset however kind smooth atypical data science applications meaningful features even one quantify axis aim show exist clear algorithms would applied produce ﬁgure would one actually claim fair compare decision tree algorithm cart deep learning model conclude interpretable models accurate one always create artiﬁcial accuracy removing parts complex model reduce accuracy representative analysis one would perform real problem also clear comparison performed static dataset formal process deﬁning knowledge data would require iterative process one reﬁnes data processing interpreting results generally practice data science small difference performance machine learning algorithms overwhelmed ability interpret results process data better next iteration cases tradeoff reversed interpretability leads better overall accuracy figure ﬁctional depiction taken darpa xai explainable artiﬁcial intelligence broad agency announcement worse efforts working within knowledge discovery process led work interpretable machine learning speciﬁcally participated effort predict electrical grid failures across new york city data messy including free text documents trouble tickets accounting data electrical cables far back inspections data brand new manhole inspections program even structured data easily integrated database confounding issues problems algorithms static dataset different performance ability interpret reprocess data led signiﬁcant improvements performance including correcting problems dataset revealing false assumptions data generation process accurate predictors found sparse models meaningful features constructed iterative process belief always accuracy interpretability led many researchers forgo attempt produce interpretable model problem compounded fact researchers trained deep learning interpretable machine learning worse toolkits machine learning algorithms offer little way useful interfaces interpretable machine learning methods knowledge recent review commentary articles topic imply implicitly explicitly interpretability accuracy generally occurs could possible application domains complete black box required high stakes decision yet encountered application despite worked numerous applications healthcare criminal justice energy reliability ﬁnancial risk assessment explainable methods provide explanations faithful original model computes explanations must wrong perfect ﬁdelity respect original model explanation completely faithful original model computes explanation would equal original model one would need original model ﬁrst place explanation words case original model would interpretable leads danger explanation method black box model inaccurate representation original model parts feature space see also instance among others inaccurate explanation model limits trust explanation extension trust black box trying explain explainable model agreement original model indeed explains original model time however explanation model correct time wrong time tenth explanations incorrect one trust explanations thus one trust original black box know certain whether explanation correct know whether trust either explanation original model important misconception explanations stems terminology explanation often used misleading way explanation models always attempt mimic calculations made original model even explanation model performs almost identically black box model might use completely different features thus faithful computation black box consider black box model criminal recidivism prediction goal predict whether someone arrested within certain time released recidivism prediction models depend explicitly age criminal history explicitly depend race since criminal history age correlated race datasets fairly accurate explanation model could construct rule person predicted arrested might accurate explanation model since correctly mimics predictions original model would faithful original model computes possibly main ﬂaw identiﬁed criminologists propublica analysis accused proprietary compas recidivism model racially biased compas correctional offender management proﬁling alternative sanctions proprietary model used widely justice system parole bail decisions propublica created linear explanation model compas depended race accused black box compas model depending race conditioned age criminal history fact compas seems nonlinear entirely possible compas depend race beyond correlations age criminal history propublica linear model truly explanation compas concluded explanation model uses important features black box approximating lot discussion compas later document easy problem change terminology let stop calling approximations black box model predictions explanations model use race explicitly automated explanation model predicts arrested black explanation model actually would confusing judge lawyer defendant recidivism prediction discussed later key application interpretable machine learning necessary case much easier detect debate possible bias unfairness interpretable model black box similarly could easier detect avoid data privacy issues interpretable models black boxes recidivism example many methods claim produce explanations instead compute useful summary statistics predictions made original model rather producing explanations faithful original model show trends predictions related features calling summaries predictions summary statistics trends rather explanations would less misleading iii explanations often make sense provide enough detail understand black box even models correct original black box correct prediction explanation model correct approximation black box prediction possible explanation leaves much information makes sense give example image processing decision decision explanations needed explanation methods often demonstrated saliency maps often considered explanatory saliency maps useful determine part image omitted classiﬁer leaves information relevant information isbeing used knowing network looking within image tell user part image illustrated figure fact saliency maps multiple classes could essentially case explanation image might contain siberian husky would explanation image might contain transverse ﬂute unfortunate trend recent work show explanations observation correct label demonstrating method figure would appear demonstrating method using explanations correct class misleading practice instill false sense conﬁdence explanation method black box consider instance case explanations multiple classes identical situation would happen often saliency maps explanations tend evidence animal siberian huskyevidence animal ransverse flute explanations using attention mapst est image figure saliency explain anything except network looking idea image labeled either dog musical instrument considering saliency explanations look essentially classes figure credit chaofan chen highlight edges thus provide similar explanations class explanations could identical even model always wrong showing explanations image correct class misleads user thinking explanation useful andthat black box useful even neither one saliency maps one example explanations incomplete might convey black box predicted similar arguments made kinds explanation methods poor explanations make hard troubleshoot black box black box models often compatible situations information outside database needs combined risk assessment high stakes decisions often considerations outside database need combined risk calculation instance circumstances crime much worse generic assigned charge often circumstances whose knowledge could either increase decrease someone risk model black box difﬁcult manually calibrate much additional information raise lower estimated risk issue arises constantly instance proprietary compas model used justice system recidivism risk prediction depend seriousness current crime instead judge instructed somehow manually combine current crime compas actually possible many judges know fact model transparent judge could see directly seriousness current crime considered risk assessment black box models explanations lead overly complicated decision pathway ripe human error typographical errors seem common computing compas typographical errors sometimes determine bail decision outcomes exempliﬁes important drawback using overly complicated black box models recidivism prediction may incorrectly calculated practice computation compas requires factors typographical errors humans entering data survey occur rate every surveys average least one typographical error multitude typographical errors argued type procedural unfairness whereby two individuals identical might randomly given different parole bail decisions types errors potential reduce accuracy complicated models separate topic model troubleshooting overly complicated black box model may ﬂawed know difﬁcult troubleshoot incomplete explanation may help must troubleshoot two models rather one black box model explanation model next section completely switch gears discuss reasons many people appear advocate black box models separate explanation models rather inherently interpretable models even decisions key issues interpretable many cases black boxes explanations preferred interpretable models even decisions however applications hopeful ways around problems whether computational problems problems training researchers availability code ﬁrst problem however currently major obstacle see way avoiding policy discussed next section corporations make proﬁts intellectual property afforded black box companies charge individual predictions could ﬁnd proﬁts obliterated interpretable model used instead consider compas proprietary recidivism risk prediction tool discussed widespread use justice system predicting probability someone arrested release compas model equally accurate recidivism prediction simple three rule interpretable machine learning model involving age number past crimes shown figure however clear business model would suggest proﬁting simple transparent model simple model figure created algorithm called certiﬁably optimal rule lists corels looks patterns data even though model figure looks like rule thumb human may designed without data instead machine learning model qualitative comparison compas corels models table standard machine learning tools interpretable machine learning tools seem approximately equally accurate predicting recidivism even deﬁne recidivism many different ways many different crime types evidence however changed momentum justice system towards proprietary models writing california recently eliminated cash bail system instead enforcing decisions made algorithms unclear whether compas algorithm used despite fact known accurate models simple corels model figure age sex male predict arrest within years else age prior offenses predict arrest else three priors predict arrest else predict arrest figure machine learning model certiﬁably optimal rule lists corels algorithm model minimizer special case equation discussed later challenges section corels code open source publicly available along data florida needed produce model compas corels black box full model figure factors age priors optional gender might include info information expensive software license free transparent within software used justice system table comparison compas corels models models similar true false positive rates true false negative rates data broward county florida compas machine learning model created standard machine learning algorithm designed experts based carefully designed surveys expertise seem depend heavily past criminal history interestingly compas model proprietary documentation indicates would actually interpretable predictive model black box second type proprietary ﬁrst type complicated discussed revealing model however would revealing trade secret let switch examples consider proprietary machine learning model breezometer used google california wildﬁres predicted air quality good ideal air quality outdoor activities air quality dangerously bad according multiple models people reported cars covered ash environmental protection agency free air quality index would provided reliable result could breezometer machine learning method badly wrong put many danger never ﬁnd breezometer probably made proﬁt making predictions may developed new technology models forced transparent medicine trend towards blind acceptance black box models open door companies sell models hospitals instance radiology patient monitoring areas medicine stand gain tremendously automation humans process data fast enough rapidly enough compete machines however trusting automated systems must also trust full database trained processing data along completeness database database represent full set possible situations arise model could making predictions cases different anything trained example wrong given zech noticed neural network picking word portable within image representing type equipment rather medical content image used interpretable model even explainable model issue would never gone unnoticed zech pointed issue confounding generally fact plague confounding haunts vast number datasets particularly medical datasets means proprietary models medicine serious errors models also fragile model used practice slightly different setting trained new equipment accuracy substantially drop examples compas breezometer black box medical diagnosis illustrate problem business model machine learning particular conﬂict responsibility use black box models decisions companies proﬁt models necessarily responsible quality individual predictions prisoner serving excessively long sentence due mistake entered risk score could suffer years whereas company constructed complicated model unaffected contrary fact model complicated proprietary allowed company proﬁt sense model designers incentivized careful design performance ease use types problems affecting credit rating agencies priced mortgages problems contributed ﬁnancial crisis united states time one argument favoring black boxes keeping models hidden prevents gamed clear argument generally makes sense fact reason system may gamed likely designed properly ﬁrst place leading form goodhart law revealed quoting chang product rating systems ratings accurate measures quality making ratings transparent could uniformly positive impact would help companies make better rated products would help consumers higher quality products would encourage rating companies receive feedback whether rating systems fairly represent thus transparency could help improve quality system whereby attempting game would genuinely align overall goal improvement instance improving one credit score actually correspond improvement creditworthiness another argument favoring black boxes belief counterfactual explanations black boxes sufﬁcient counterfactual explanation describes minimal change input would result opposite prediction instance possible counterfactual explanation might loan application denied less debt would qualiﬁed type explanation suffer key issue discussed combining information outside database black box particular minimal change input might different different individuals appendix discusses depth counterfactual explanations generally sufﬁce high stakes decisions black boxes interpretable models entail signiﬁcant effort construct terms computation domain expertise discussed interpretability usually translates practice set constraints model solving constrained problems generally harder solving unconstrained problems domain expertise needed construct deﬁnition interpretability domain features machine learning data unconfounded complete clean much easier use black box machine learning method troubleshoot solve computationally hard problems however decisions analyst time computational time less expensive cost ﬂawed overly complicated model worthwhile devote extra effort cost constructing model even many organizations analysts training expertise construct interpretable models companies started provide interpretable solutions using proprietary software step right direction clear proprietary software better publicly available software instance claims made companies performance proprietary algorithms impressive interpretable whose decision tree performance using mixed integer programming software reported often beaten comparable classiﬁcation regression tree algorithm cart discussed earlier interpretability constraints like sparsity lead optimization problems proven computationally hard worst case theoretical hardness problems mean solve though real cases optimization problems often difﬁcult solve major improvements made last decade discussed later challenges section explanation methods hand usually based derivatives lead easier optimization iii black box models seem uncover hidden fact many scientists difﬁculty constructing interpretable models may fueling belief black boxes ability uncover subtle hidden patterns data user previously aware transparent model may able uncover patterns pattern data important enough black box model could leverage obtain better predictions interpretable model might also locate pattern use depends machine learning researcher ability create models researcher needs create model capability uncovering types patterns user would ﬁnd interpretable also model needs ﬂexible enough data accurately optimization challenges discussed difﬁculty lies constructing interpretable models encouraging responsible governance currently european union revolutionary general data protection regulation regulation plans govern right explanation explanation required interpretable model particular data subject shall right subject decision based solely automated processing including proﬁling produces legal effects concerning similarly signiﬁcantly affects article gdpr regulations one provide explanation automated decision clear whether explanation required accurate complete faithful underlying model see explanations easily undermine new policies let consider possible mandate certain decisions black box deployed exists interpretable model level performance mandate deployed organizations produce sell black box models could held accountable equally accurate transparent model exists could considered form false advertising sell black box model interpretable model onus would fall organizations produce black box models transparent model exists task possible mandate could produce change business model machine learning opacity viewed essential protecting intellectual property odds requirements many domains involve public health welfare however combination opacity explainability way incentivize machine learning experts invest creating systems compensation developing interpretable model could provided lump sum model could released public creator model would able proﬁt licensing model period time fact models useful public good applications would make problems appeal academics charitable foundations proposal solve problems could least rule companies selling recidivism prediction models possibly credit scoring models kinds models construct alternatives applied broadly could reduce industrial participation cases machine learning might beneﬁt society consider second proposal weaker one provided might similar effect let consider possibility organizations introduce black box models would mandated report accuracy interpretable modeling methods case one could easily determine whether claimed organization worthwhile also forces organization try using interpretable modeling methods also encourages organization use methods carefully otherwise risking possibility criticism mentioned earlier yet found application fully black box model necessary despite worked many applications long continue allow broad deﬁnition interpretability adapted domain able improve decision making serious tasks societal importance however order people design interpretable models technology must exist discussed earlier formidable computational hurdle designing interpretable models even standard structured data features algorithmic challenges interpretable every black box machine learning model could replaced one equally accurate also interpretable could would identify ﬂaws models data could see perhaps could prevent poor decisions criminal justice medicine caused problems using black box models could also eliminate need explanations misleading often wrong since interpretability large toolbox possible techniques come handy expand three challenges interpretable machine learning appear often three cases something common people providing interpretable predictive models problems decades models look like type model want create machine learning also discuss current work problems challenges representative major class models modeling uses logical conditions challenge linear modeling challenge reasoning challenge means set challenges close encompassing large number challenges exist creating interpretable models challenge constructing optimal logical models logical model consists statements involving etc corels model figure logical model called rule list decision trees logical models well conjunctions disjunctions instance condition true conditions true predict yes otherwise predict logical models crafted hand expert systems far back since many heuristics creating logical models instance one might add logical conditions one one greedily prune conditions away helpful greedily heuristic methods tend inaccurate uninterpretable choose globally best choice approximately best choice logical conditions designed optimally sparse might use logical conditions accuracy could obtained logical conditions cart decision trees suffer problems well vast number models associative classiﬁcation literature issue algorithms aim optimal solutions optimization problems becomes difﬁcult tell whether poor performance due choice algorithm combination choice model class constraints algorithm perform poorly optimize objective chose constraints allow enough ﬂexibility model data well question computing optimal logical models existed since least mid would like models look like created hand need accurate machine learning models end let consider following optimization problem asks ﬁnd model minimizes combination fraction misclassiﬁed training points size model training observations indexed andfis family logical models decision trees optimization problem min nnx training observation iis misclassiﬁed size model measured number logical conditions model number leaves decision tree parameter classiﬁcation error one would sacriﬁce order one fewer term model means would sacriﬁce training accuracy order reduce size model one another way say model would contain additional term additional term reduced error least optimization problem generally known computationally hard versions optimization problem fundamental problems artiﬁcial intelligence challenge whether solve approximately solve problems like practical ways leveraging new theoretical techniques advances hardware model figure machine learning model comes corels algorithm corels solves special case special choice fas set rule lists size model measured number rules list figure three rules size order minimize corels needs avoid enumerating possible models would take extremely long time perhaps end universe modern laptop fairly small dataset technology underlying corels algorithm able solve optimization problem optimality minute broward county dataset discussed corels backbone set theorems allowing massive reductions search space rule lists custom fast library allows fast exploration search space corels need enumerate rule lists iii specialized data structures keep track intermediate computations symmetries set ingredients proved powerful cocktail handling tough computational problems example corels enforces two points discussed ﬁrst interpretable models sometimes entail hard computational problems second computational problems solved leveraging combination theoretical techniques corels creates one type logical model however many formally ﬁrst challenge create algorithms solve logical modeling problems reasonable amount time practical datasets extending corels complex problems falling rule lists optimal decision trees much work done types logical models various kinds constraints note possible construct interpretable logical models global model large yet explanation small discussed appendix challenge construct optimal sparse scoring systems scoring systems designed hand since least burgess criminological model burgess model designed predict whether criminal would violate bail individuals received points well recent immigrant increased predicted probability parole violation course model created using machine learning invented yet scoring system sparse linear model integer coefﬁcients coefﬁcients point scores example scoring system criminal recidivism shown figure predicts whether someone arrested within years release scoring systems used pervasively throughout medicine hundreds scoring systems developed physicians challenge whether scoring systems look like could produced human absence data produced machine learning algorithm accurate model machine learning algorithm figure scoring system risk recidivism grew model created human selection numbers features come riskslim machine learning algorithm several ways formulate problem producing scoring system see instance could use special case model size number terms model figure machine learning model terms sometimes one round coefﬁcients logistic regression model produce scoring system method tend give accurate models tend produce models particularly nice coefﬁcients used figure however solving variants computationally hard domain solve optimization problem integer lattice see consider axis bjcan take integer values lattice deﬁnes feasible region optimization problem model figure arose solution hard optimization problem let discuss optimization problem brieﬂy goal ﬁnd coefﬁcients pfor linear predictive model jbjzjwherezjis thejth covariate test observation figure point scores turned result optimization nonzero coefﬁcients displayed ﬁgure particular want solve min nnx point scores bjare constrained integers training observations indexed andpis total number covariates data model size number coefﬁcients parameter ﬁrst term logistic loss used logistic regression problem hard speciﬁcally program minlp whose domain integer lattice despite hardness problem new cutting plane algorithms able solve problem optimality arbitrarily large sample sizes moderate number variables within minutes latest attempt solving problem riskslim algorithm specialized cutting plane method adds cutting planes whenever solution linear program otherwise performs branching optimization problem similar physicians attempt solve manually without writing optimization problem like physicians use optimization tools accurate scoring systems tend difﬁcult physicians create data one collaborators spent months trying construct scoring system adding removing variables rounding using heuristics decide variables add remove round riskslim useful helping task formally second challenge create algorithms scoring systems computationally efﬁcient ideally would increase size optimal scoring system problems current methods practically handle order magnitude challenge deﬁne interpretability speciﬁc domains create methods accordingly including computer vision since interpretability needs deﬁned way important technical challenges future tied speciﬁc important domains let start computer vision classiﬁcation images vast growing body research posthoc explainability deep neural networks much work designing interpretable neural networks goal section demonstrate even classic domains machine learning latent representations data need constructed could exist interpretable models accurate black box models computer vision particular clear deﬁnition interpretability models discussed apply sparsity pixel space make sense many different ideas constitutes interpretability even different computer vision applications however deﬁne interpretability somehow particular application embed deﬁnition algorithm let deﬁne constitutes interpretability considering people explain reasoning processes behind complicated visual classiﬁcation tasks turns classiﬁcation natural images domain experts often direct attention different parts image explain parts image important reasoning process question whether construct network architectures deep learning also network must make decisions reasoning parts image explanations real posthoc recent attempt chen colleagues building architectures append special prototype layer end network training prototype layer ﬁnds parts training images act prototypes class instance bird classiﬁcation prototype layer might pick prototypical head blue jay prototypical feathers blue jay etc network also learns similarity figure image authors indicating parts test image left similar prototypical parts training examples test image classiﬁed left similar prototypes middle column heatmaps show part test image similar prototype right included copies test image right easier see part bird heatmaps referring similarities prototypes test image determine predicted class label image image predicted sparrow top prototype seems comparing bird head prototypical head sparrow second prototype considers throat bird third looks feathers last seems consider abdomen leg test image prototypes image constructed alina barnett metric parts images thus testing new test image needs evaluated network ﬁnds parts test image similar prototypes learned training shown figure ﬁnal class prediction network based weighted sum similarities prototypes sum evidence throughout image particular class explanations given network prototypes weighted similarities explanations actual computations model posthoc explanations network called look like reasoning process considers whether part image looks like prototype training prototype network easy training ordinary neural network tricks developed regular deep learning yet developed prototype network however far prototype networks trained approximately accurate original black box deep neural networks derived prototype layer added discussion interpretability speciﬁc domains let ﬁnish short discussion challenges interpretability speciﬁc domains mentioning vast numbers papers imbued interpretability methodology interpretability mentioned title papers often body text almost impossible create review article interpretability machine learning statistics without missing overwhelming majority clear review articles interpretability explainability make sense create normally reviews measures despite fact many accuracy area roc curve partial auc sensitivity speciﬁcity discounted cumulative gain many measures accuracy performance clear reviews interpretability make sense reviews yet ﬁnd even single recent review recognized chasm interpretability explainability let discuss brieﬂy examples work interpretability would covered recent review articles yet valuable contributions interpretability respective domains gallagher analyze electrical spatiotemporal dynamics understand depression vulnerability ﬁnd interpretable patterns low dimensional space dimension reduction interpretable dimensions important theme interpretable machine learning problems residing applied statistics often interpretable embed physics domain wang create models recovery curves prostatectomy patients whose signal uncertainty obey speciﬁc constraints order realistic constraints uncertainty predictions make models interpretable setup recent fico explainable challenge exempliﬁed blind belief myth tradeoff speciﬁc domain namely credit scoring entrants instructed create black box predict credit default explain model afterwards however performance difference interpretable models explainable models fico data globally interpretable model fico recognition prize competition case organizers judges expected interpretable model able constructed thus ask entrants try construct model model additive model known form interpretable model see also additive models used medical data additive models could optimized using similar techniques introduced challenge technical reason accurate interpretable models might exist many domains accurate interpretable models could possibly exist many different domains really possible many aspects nature simple truths waiting discovered machine learning although would intriguing make kind occham argument favor technical argument function classes particular rashomon sets argument ﬂeshed formally related different notation ﬂat minima nice example given hand rashomon set argument consider data permit large set reasonably accurate predictive models exist set accurate models large often contains least one model interpretable model thus interpretable accurate unpacking argument slightly given data set deﬁne rashomon set set reasonably accurate predictive models say within given accuracy best model accuracy boosted decision trees data ﬁnite data could admit many models predict differently large rashomon set suspect happens often practice sometimes many different machine learning algorithms perform similarly dataset despite different functional forms random forests neural networks support vector machines long rashomon set contains large enough set models diverse predictions probably contains functions approximated well simpler functions rashomon set also contain simpler functions said another way uncertainty arising data leads rashomon set larger rashomon set probably contains interpretable models thus interpretable accurate models often exist theory holds expect see interpretable models exist across domains interpretable models may hard ﬁnd optimization least reason might expect models exist many diverse yet good models means algorithms may stable algorithm might choose one model small change algorithm dataset may yield completely different still accurate model necessarily bad thing fact availability diverse good models means domain experts may ﬂexibility choosing model ﬁnd interpretable appendix discusses slightly detail conclusion commentary shift focus even slightly basic assumption underlying work explainable black box necessary accurate predictions considered document success document encourage policy makers accept black box models without signiﬁcant attempts interpretable rather explainable models would even better make people aware current challenges right interpretable machine learning allow mechanism demand effort made ensuring safety trust machine learning models decisions succeed efforts possible black box models continue permitted safe use since deﬁnition constitutes viable explanation unclear even strong regulations right explanation undermined explanations continue problems combining black box model predictions information outside database continued miscalculations black box model inputs may continue lead poor decisions throughout criminal justice system incorrect safety guidance air quality disasters incomprehensible loan decisions widespread societal problems acknowledgments would like thank fulton wang tong wang chaofan chen oscar alina barnett tom dietterich margo seltzer elaine angelino nicholas elizabeth mannshart maya gupta several others helped thought processes various ways particularly berk ustun ron parr rob holte father stephen rudin went considerable efforts provide thoughtful comments discussion would also like thank two anonymous reviewers suggestions improved manuscript would like acknowledge funding laura john arnold foundation nih nsf darpa lord foundation north carolina laboratory references wexler computer program keeps jail computers harming criminal justice new york times june mcgough bad sacramento air exactly google results appear odds reality say sacramento bee august varshney alemzadeh safety machine learning systems decision sciences data products big data freitas comprehensible classiﬁcation models position paper acm sigkdd explorations newsletter mar kodratoff comprehensibility manifesto kdd nugget newsletter huysmans dejaeger mues vanthienen baesens empirical evaluation comprehensibility decision table tree rule based predictive models decision support systems rüping learning interpretable models universität dortmund gupta cotter pfeifer oevodski canini mangylov monotonic calibrated interpolated tables journal machine learning research lou caruana gehrke hooker accurate intelligible models pairwise interactions proceedings acm sigkdd international conference knowledge discovery data mining kdd acm miller magical number seven plus minus two limits capacity processing information psychological review cowan magical mystery four working memory capacity limited current directions psychological science wang wang wiens learning credible models proceedings acm sigkdd international conference knowledge discovery data mining kdd acm rudin please stop explaining black box models high stakes decisions proceedings neurips workshop critiquing correcting trends machine learning holte simple classiﬁcation rules perform well commonly used datasets machine learning fayyad smyth data mining knowledge discovery databases magazine chapman data mining guide spss agrawal bernstein bertino davidson dayal franklin challenges opportunities big data white paper prepared computing community consortium committee computing research association available defense advanced research projects agency broad agency announcement explainable artiﬁcial intelligence xai published august available hand classiﬁer technology illusion progress statist sci rudin passonneau radeva dutta ierome isaac process predicting manhole events manhattan machine learning rudin ustun optimized scoring systems toward trust machine learning healthcare criminal justice interfaces special issue daniel wagner prize excellence operations research practice chen lin rudin shaposhnik wang wang interpretable model globally consistent explanations credit risk proceedings neurips workshop challenges opportunities financial services impact fairness explainability accuracy privacy mittelstadt russell wachter explaining explanations proceedings fairness accountability transparency fat flores lowenkamp bechtel false positives false negatives false analyses rejoinder machine bias software used across country predict future criminals federal probation september angwin larson mattu kirchner machine bias propublica available larson mattu kirchner angwin analyzed compas recidivism algorithm propublica rudin wang coker age secrecy unfairness recidivism prediction arxiv applied statistics nov checkermallow canis lupus winstonii siberian husky public domain image brennan dieterich ehret evaluating predictive validity compas risk needs assessment system criminal justice behavior january zeng ustun rudin interpretable classiﬁcation models recidivism prediction journal royal statistical society series statistics society tollenaar van der heijden pgm method predicts recidivism best comparison statistical machine learning data mining predictive models journal royal statistical society series statistics society angelino alabi seltzer rudin certiﬁably optimal rule lists categorical data journal machine learning research mannshardt naess air quality usa signiﬁcance oct zech variable generalization performance deep learning model detect pneumonia chest radiographs study plos med chang rudin cavaretta thomas chou quality rankings machine learning september goodman flaxman regulations algorithmic right explanation magazine wachter mittelstadt russell counterfactual explanations without opening black box automated decisions gdpr harvard journal law technology quinlan programs machine learning vol morgan kaufmann breiman friedman stone olshen classiﬁcation regression trees crc press auer holte maass theory applications agnostic small decision trees machine learning proceedings san francisco morgan kaufmann wang rudin falling rule lists proceedings machine learning research artiﬁcial intelligence statistics aistats chen rudin optimization approach learning falling rule lists proceedings machine learning research artiﬁcial intelligence statistics aistats burgess factors determining success failure parole illinois committee law parole springﬁeld ustun rudin optimized risk scores proceedings acm sigkdd international conference knowledge discovery data mining kdd ustun rudin supersparse linear integer models optimized medical scoring systems machine learning carrizosa morales binarized support vector machines informs journal computing sokolovska chevaleyre zucker provable algorithm learning interpretable scoring systems proceedings machine learning research artiﬁcial intelligence statistics aistats ustun world health organization adult disorder screening scale jama psychiatry chen tao barnett rudin looks like deep learning interpretable image recognition neural information processing systems neurips malley sparrow public domain image sparrow public domain image schmierer sparrow public domain image schmierer sparrow public domain image schmierer sparrow public domain image liu chen rudin deep learning reasoning prototypes neural network explains predictions proceedings aaai conference artiﬁcial intelligence aaai gallagher factor analysis proceedings advances neural information processing systems neurips curran associates wang rudin mccormick gore modeling recovery curves application prostatectomy biostatistics available lou caruana gehrke intelligible models classiﬁcation regression proceedings knowledge discovery databases kdd acm semenova parr rudin study rashomon curves volumes new perspective generalization model simplicity machine learning progress razavian prediction type diabetes claims data analysis risk factors big data ustun spangher liu actionable recourse linear classiﬁcation acm conference fairness accountability transparency fat wei varshney malioutov interpretable boolean rule learning classiﬁcation proceedings icml workshop human interpretability machine learning dash günlük wei boolean decision rules via column generation conference neural information processing systems neurips wang rudin liu klampﬂ macneille bayesian framework learning rule sets interpretable classiﬁcation journal machine learning research rijnbeek kors finding short accurate decision rule disjunctive normal form exhaustive search machine learning jul goh rudin box drawings learning imbalanced data proceedings acm sigkdd conference knowledge discovery data mining kdd murdoch singh kumbier interpretable machine learning deﬁnitions methods applications arxiv statistical machine learning jan two types black box black box models ﬁrst type complicated human comprehend black box models second type proprietary models types consequences two types black box different related instance black box model complicated proprietary least know variables uses also know model form could use attempt analyze different parts model black box model proprietary complicated evidence compas model may even access query order study proprietary model sparse risk could easily thus incentive make proprietary models complicated order preserve secrecy performance comparisons problems meaningful structured covariates machine learning algorithms tend perform similarly algorithm clearly dominating others variation due tuning parameters single algorithm often higher variation algorithms lack single dominating algorithm structured data arguably ﬁeld machine learning focuses image speech recognition whose data represented raw features pixels sound ﬁles ﬁelds choice algorithm impacts performance even complex domains medical records reported studies logistic regression identical performance deep neural networks dominating algorithm rashomon set argument discussed would suggest interpretable models might perform well unfortunately culture publication within machine learning favors selective reporting algorithms selectively chosen datasets papers often rejected small performance gains reported algorithms encourages omission accurate baselines comparison well omission datasets method perform well encourages authors poorly tune parameters baseline methods equivalently place effort tuning parameters author method creates illusion large performance differences algorithms even performance differences truly exist counterfactual explanations argued counterfactual explanations see way black boxes provide useful information preserving secrecy global model counterfactual explanations also called inverse classiﬁcation state change features sufﬁcient necessary prediction switch another class reduced debt increased savings would qualiﬁed loan applied important recourse certain types decisions meaning user could take action reverse decision several problems argument counterfactual explanations sufﬁcient loan applications instance would want counterfactual explanation provide lowest cost action user take according user cost metric see example counterfactual reasoning product rankings words let say one counterfactual explanation available ﬁrst explanation reduced debt increased savings would qualiﬁed loan applied second explanation gotten job pays per week would qualiﬁed loan case explanation shown user easiest one user actually accomplish however unclear advance explanation would easier user accomplish credit example perhaps easier user save money rather get job vice versa order determine explanation lowest cost user would need elicit cost information user cost information generally difﬁcult obtain worse cost information could actually change user attempts follow policy provided counterfactual explanation turns harder user thought get salary increase reason unclear counterfactual explanations would sufﬁce high stakes decisions additionally counterfactual explanations black boxes many pitfalls discussed throughout paper interpretable models provide explanations possible create global model perhaps complicated one explanations given individual sparse words even global model would take several pages text write prediction given individual simple calculate perhaps requiring conditions let consider case credit risk prediction assume need justify client would grant loan would need justify would deny loan let consider disjunctive normal form model collection instance model might deny loan credit history short least one bad past trade least bad past trades least one recent delinquency high percentage delinquent trades even hundreds conjunctions within model one needs shown client conjunction true conjunction deﬁning reason client would denied loan words client least one recent delinquency high percentage delinquent trades regardless aspects credit history could shown simple explanation would deﬁning reason loan application would denied disjunctive normal form models called various names well decision rules rule sets associative substantial work able generate models past years models globally interpretable locally interpretable meaning global model consists small number conjunctions see many types models would provide explanations instance falling rule lists provide shorter explanations decisions important instance falling rule list predicting patient mortality would use logical conditions categorize whether patient group use several additional logical conditions determine group patient falls algorithm stability common criticism decision trees stable meaning small changes training data lead completely different trees giving guidance tree choose fact problem happen linear models highly correlated features happen even basic least squares correlations features lead different models precisely levels performance correlated features lack stability happens algorithms strongly regularized hypothesize instability learning algorithm could rashomon effect mentioned earlier many different good predictive models adding regularization algorithm increases stability also limits ﬂexibility user choose element rashomon set would desirable applications models purely predictive causal criminal recidivism use age prior criminal history predict future crime assumption model represents outcomes actually generated importance variables model reﬂect causal relationship variables outcomes thus without additional guidance domain expert way proceed choose single best model among set models perform similarly discussed regularization act additional input view lack algorithmic stability advantage rather disadvantage lack stability indeed caused large rashomon effect means domain experts add constraints model customize without losing accuracy words many people criticize methods decision trees stable view strength interpretability decision trees many equally accurate trees domain expert pick one interpretable note researchers working interpretability agree general sentiment advantages instability
