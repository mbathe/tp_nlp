unfairness algorithm distilling harms automated overviewanalysis personal data used improve services advance research combat discrimination however analysis also create valid concerns differential treatment individuals harmful impacts vulnerable communities concerns amplified automated uses sensitive data race gender familial status impacts protected classes affects individuals eligibility housing employment core services seeking identify harms important appreciate context interactions individuals companies benefits provided automated frameworks fallibility human recent discussions highlighted legal ethical issues raised use sensitive data hiring policing benefits determinations marketing purposes conversations become mired definitional challenges make progress towards solutions difficult easy ways navigate issues stakeholders hold frank discussions promote fairness encourage responsible data use combat facilitate discussions future privacy forum fpf attempted identify articulate categorize types harm may result automated inform effort fpf reviewed leading books articles advocacy pieces topic algorithmic discrimination distilled harms potential mitigation strategies identified literature two charts hope suggest revisions identify challenges help improve document contacting lsmith addition presenting document consideration ftc informational injury workshop anticipate useful assessing fairness transparency accountability artificial intelligence well methodologies assess impacts rights freedoms general data protection regulation chart potential mitigation setsthis chart uses fpf taxonomy categorize harms groups sufficiently similar could amenable mitigation solve prevent broad swath harms require range tools perspectives attempts benefit categorization identified harms five groups similar harms groups include individual harms illegal individual harms simply unfair corresponding illegal analog harms corresponding individual illegal analog individual harms unfair lack corresponding illegal analog harms lack corresponding individual illegal analog chart includes description mitigation strategies best positioned address group ample debate whether lawful decisions included chart fair unfair ethical unethical absent societal consensus harms may ripe legal remedies chart potential harms automated chart groups harms identified literature four broad buckets opportunity economic loss social detriment loss depict various spheres life automated cause injury also notes whether harm manifests individuals collectives illegal simply hope identifying categorizing harms begin process empower seeking solutions mitigate harms believe clear articulation harms help focus attention energy potential mitigation strategies reduce risks algorithmic discrimination attempted include harms articulated literature chart presume establish harms pose greater lesser risks individuals society filter algorithms promote familiar news informationstereotype assumption computed decisions inherently unbiasedindividual harmscollective societal harmsillegalunfair network varied exposure opportunity evaluation based know employment filtering job candidates race filtering candidates work proximity leads excluding minoritiesinsurance social benefit discriminationhousing discriminationeducation denial opportunity student certain ability presenting ads colleges higher termination rate benefit eligibility religious increasing auto insurance prices landlord relies search results suggesting criminal history matching algorithm less likely provide suitable housing minoritiescredit denying credit residents specified neighborhoods redlining presenting certain credit offers members certain groupsdifferential pricing goods raising online prices based membership protected presenting product discounts based ethnic affinity differential access insurance benefitsdifferential access housingdifferential access educationdifferential access creditloss opportunity economic loss social detrimentnarrowing choice presenting ads based solely past clicks differential access goods services confirmation image search results ceo results teacher dignitary emotional distress due bias decision based incorrect datapotential harms automated narrowing choice groups loss libertydifferential access job opportunities increased use predictive policing police minority neighborhoods moredisproportionate incarceration groups higher rates based historic policing dataconstraints emotional dignitary social impacts increased surveillanceconstraints constrained conceptions career prospects based search resultsindividual use recidivism scores determine prison sentence length legal status uncertain potential mitigation setsharmsmitigation toolsindividual harms discriminationinsurance social benefit discriminationhousing discriminationeducation discriminationcredit discriminationdifferential pricingindividual methods ensure proxies used protected classes data amplify historical design carefully consider whether use protected status inputs trigger manual policies use data identify discriminationinsurance social benefit discriminationhousing discriminationeducation discriminationcredit discriminationdifferential pricingindividual harms illegal analog employment discriminationindividual incarceration network bubblesdignitary harmsconstraints biasconstraints suspicionindividual harms without illegal analog narrowing processes index concerns ethical frameworks best practices monitor evaluate policies consider whether appropriate expect industry identify enforce normsfilter bubblesstereotype reinforcementconfirmation biasincreased surveillance harms without illegal analog narrowing choice processes index concerns ethical frameworks best practices monitor evaluate policies include tools like dpias measure impact enable rights explanation existing law defines impermissible outcomes often specifically protected classesdescription individual harms could considered illegal involved protected classes case individual impacts legal rules mitigation may difficult undesirable absent defined set societal normsdifferential access insurance benefitsdifferential access housingdifferential access educationdifferential access creditdifferential access goods harms illegal analog differential access job opportunitiesdisproportionate policies consider offline analogies whether appropriate industry identify mitigategroup level impacts legally prohibited though related individual impacts could illegal group level impacts legal rules societal agreement constitutes harmeconomic losssocial stigmatizationloss libertykeyloss opportunity working definitions harmsautomated decision direct output indirect result automated program analyzing individual aggregate data includes algorithms evolve via machine learning examples category represent harms illegal several civil rights laws generally protect core race gender age discrimination disparate treatment disparate examples category represent actions typically legal nonetheless trigger notions unfairness like illegal category examples may differently classified depending legal societal harms category represents overall negative effects society chiefly collective rather individual nature loss opportunity group broadly describes harms occurring within domains workplace housing social support systems healthcare loss group broadly describes harms primarily cause financial injury discrimination marketplace goods services social detriment group broadly describes harms one sense self self worth community standing relative liberty group broadly describes harms constrain one physical freedom definitions mitigationindividual harms harms category american law defines outcomes legally permissible harms typically become legally cognizable impact legally protected classes manner defined impermissible existing law notably disparate impact may relevant illegality regardless intent harms illegal analog individual harms category involve protected classes could considered illegal protected classes implicated example price discrimination based race could illegal fair credit reporting act civil rights act price discrimination based computer operating system user protected law nonetheless automated enables growing number personalized distinctions may consider distinctions unfair harms illegal analog category impacts group level may legally prohibited individual impacts could illegal different circumstances rules may prohibit disparate treatment protected classes differential treatment groups legally protected may considered illegal example systematically failing hire people certain race may illegal systematically failing hire apple computer users red sox fans protected law though may consider without illegal analog category applies impacts individuals legal rules narrowing choice network bubbles may harms newly enabled growth technology platforms others constraints bias constraints suspicion challenges analog world decades harms without illegal analog category includes collective outcomes legal rules prior group narrowing choice groups filter become frequent due increased reliance algorithmic personalization techniques stereotype reinforcement old time compounded volume information available online confirmation bias increased surveillance groups challenges society decades since inception reviewed literaturethe alphabetized list captures literature fpf reviewed date effort welcome suggestions materials review lsmith reike let hype social media scores distract equalfuture acquisti christina fong experiment hiring discrimination via online social network presented privacy law scholars conference lange perspective algorithmic personalization presented fed trade comm privacycon conference king marko mrkonich big data risk employment discrimination tutt fda algorithms admin hannak bias online freelance marketplaces evidence taskrabbit presented workshop data algorithmic transparency neil weaponsofmathdestruction sandvig auditing algorithms research methods detecting discrimination internet platforms presented int comm ass conference data discrimination converting critical concerns productive inquiry solove taxonomy privacy penn rev keats citron frank pasquale scored society due process automated predictions ofthepresident bigdata seizingopportunities preservingvalues ofthepresident bigdata reportonalgorithmicsystems opportunity andcivilrights bigdata toolforinclusionorexclusion jan pasquale danielle keats citron promoting innovation preventing discrimination policy goals scored society jeremy ashkan soltani websites vary prices deals based users information wallst kroll accountable algorithms penn rev kulshrestha quantifying search bias investigating sources bias political searches social media presented workshop data algorithmic transparency crawford jason schultz big data due process toward framework redress predictive privacy harms sweeney discrimination online delivery commc nsoftheass nofcomputingmachinery rainie jana anderson pros cons algorithm age pewresearchcenter maccarthy student privacy harm context int lrev ofinfo madden michele gilman karen levy alice marwick privacy poverty big data matrix vulnerabilities poor americans rev forthcoming mar garcia keep turning racist monster wired hardt eric price nathan srebro equality opportunity supervised learning presented conference neural info processing sys eslami reasoning invisible algorithms news feed presented ass computing machinery special interest interaction zafar fairness beyond disparate treatment disparate impact learning classification without disparate mistreatment presented int world wide web conference byrnes expect algorithms biased mit technologyreview opentech dataanddiscrimination collectedessays gangadharan teneand jules polonetsky big data privacy user control age analytics tech intell robertgellman thescoringofamerica howsecretconsumerscoresthreatenyourprivacyandyourfuture worldprivacyforum kim discrimination work william maryl swire lessons fair lending law fair marketing big data machine bias investigative series wachter brent mittelstadt luciano floridi right explanation automated decision making exist general data protection regulation barocas andrew selbst big data disparate impact civilrights bigdata andouralgorithmicfuture
