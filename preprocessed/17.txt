Strasbourg , 17 December 2020 CAHAI ( 2020 ) 23 AD HOC COMMITTEE ON ARTIFICIAL INTELLIGENCE ( CAHAI ) Feasibility Study www.coe.int/cahai 2 1 . GENERAL INTRODUCTION 1 . The Council of Europe is the continent 's leading human rights organisation and the guardian of the rights of some 830 million Europeans . Throughout the transformations of our society since 1949 , the Council of Europe has constantly ensured that human rights , democracy an d the rule of law guide development , including technological development , and some of its legal instruments have become recognised European or world standards , reconciling innovation and regulation for the benefit of human beings1 . 2 . Specifically , in the di gital domain , the advances of the last decades have fundamentally transformed society by providing new tools for communication , information consumption , public administration , education , and many other facets of daily life . Thanks to the detection of patte rns and trends in large datasets using statistical methods , algorithmic systems now offer the possibility to recognise images or sound , streamline services or products and achieve huge efficiency gains in the performance of complex tasks . These services an d products , commonly referred to as `` artificial intelligence '' ( AI2 ) have the potential to promote human prosperity and individual and societal well -being by enhancing progress and innovation . Member States agree that economic prosperity is an important obj ective of public policies and consider innovation as one of its key components . At the same time , concerns are rising in respect of harm resulting from different types of AI applications and their potential negative impact on human beings and society . Disc rimination , the advent of a surveillance society , the weakening of human agency , information distortion , electoral interference , digital exclusion and potentially harmful attention economy , are just some of the concrete concerns that are being expressed . 3 . It is therefore crucial that the Council of Europe ’ s standards on human rights , democracy and the rule of law are effectively anchored in appropriate legislative frameworks by member States . While the existing general international and regional human right s instruments , including the European Convention on Human Rights ( ECHR ) , remain applicable in all areas of life , including online and offline and regardless of the technology , a Council of Europe legal response , aimed at filling legal gaps3 in existing leg islation and tailored to the specific challenges raised by AI systems should be developed , based on broad multi -stakeholder consultations . This has already happened in the past with innovative industrial processes such as pharmaceuticals , biomedicine or the automotive industry . Moreover , such a legal response could also foster and influence AI technologies in line with the above -mentioned standards . 4 . Therefore , on 11 September 2019 , the Committee of Ministers mandated an Ad hoc Committee on Artificial Intelligence ( CAHAI ) to examine , on the basis of broad multi -stakeholder consultations , the feasibility and potential elements of a legal framework for the development , design and application of artificial intelligence , based on Council of Europe standards in the field of human rights , democracy and the rule of law . This feasibility study takes into account such standards for the design , development and application of AI in the field of human rights , democracy and the rule of law , as well as existing relevant i nternational - universal and 1 See in this regard the Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data ( `` Convention 108 ” , ETS No . 108 ) and its Protocol ( “ Convention 108 + ” , CETS No . 223 ) ; the Convention for the Protection of Human Rights and Dignity of the Human Being with regard to the Application of Biology and Medicine , ETS No . 164 ( “ Oviedo Convention ” ) ; the Convention on Cybercrime , ETS No . 185 ( `` Budapest Convention ” ) ; the Convention on Elaboration of a European Pharmacopeia , ETS No . 50 . 2 Section 2 further clarifies the use of this term for the purpose of the Feasibility Study . To avoid any form of anthropomorphising and to include all technologies falling under the umbrella term of “ AI ” , the terms “ AI systems , ” , “ AI applica tions ” , “ AI solutions ” will be generally preferred in this feasibility study to refer to algorithmic systems based , indifferently , on machine learning , deep learning , rule -based systems such as expert systems or any other form of computer programming and d ata processing . The notion of “ algorithmic systems ” is to be understood as defined in the appendix to Recommendation CM/Rec ( 2020 ) 1 of the Committee of Ministers , as `` applications which , often using mathematical optimisation techniques , perform one or more tasks such as collecting , grouping , cleaning , sorting , classifying and deriving data , as well as selecting , prioritising , making recommendations and taking decisions . By relying on one or more algorithms to perform their tasks in the environments where the y are implemented , algorithmic systems automate activities to enable the creation of scalable , real -time services ” . 3 As further specified in paragraph 5.4 of this feasibility study . 3 regional - legal instruments . It also takes into account work carried out by other bodies of the Council of Europe as well as work in progress within other regional and international organisations ( in particular within the Unit ed Nations – including UNESCO , OHCHR , ITU , WIPO and the WHO – the European Union , OECD , OSCE , G7/G20 , the World Bank , and the World Economic Forum ) . Finally , this study takes into account a gender perspective and the building of cohesive societies and the promotion and protection of the rights of vulnerable people , including persons with disabilities and minors . 2 . SCOPE OF APPLICATION OF A COUNCIL OF EUROPE LEGAL FRAMEWOR K ON ARTIFICIAL INTE LLIGENCE 5 . To date , there is no single definition of AI accepted by the scientific community . The term , which has become part of everyday language , covers a wide variety of sciences , theories and techniques of which the aim is to have a machine reproduce the cognitiv e capacities of a human being . The term can therefore cover any automation resulting from this technology , as well as precise technologies such as machine learning or deep learning based on neural networks . 6 . Similarly , the various international organisatio ns that have worked on AI have also not found a consensus on the definition of AI . The independent High -Level Expert Group on AI mandated by the European Commission has therefore published a comprehensive document on the definition of AI4 . The European Com mission 's AI Watch Observatory has also conducted a very thorough study on an operational definition and taxonomy of AI5 . The OECD Council Recommendation on AI includes a preamble defining AI systems , the life cycle of an AI system , AI knowledge , AI actors and stakeholders6 . UNESCO has produced a preliminary study referring to `` AI based machines '' and `` cognitive computing '' 7 as well as a draft Recommendation on the Ethics of Artificial Intelligence defining AI systems as “ technological systems which have the capacity to process information in a way that resembles intelligent behaviour , and typically includes aspects of reason ing , learning , perception , prediction , planning or control ” 8 . 7 . As regards the non -binding instruments that have been published on this topic by the Council of Europe so far , no uniform definition of AI has been used . The Recommendation of the Committee of M inisters to member States on the impact of algorithmic systems on human rights9 defines the notion of `` algorithmic systems '' as covering a broad range of AI applications . The Declaration of the Committee of Ministers on the Manipulation Capabilities of Algo rithmic Processes10 does not include definitions and uses various concepts such as `` technologies '' , `` data -based systems '' , `` machine learning tools '' , depending on the specific issues to be considered . The Commissioner for Human Rights11 , the Consultative Commit tee of the Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data ( T -PD12 ) and the European Commission for the Efficiency of Justice ( CEPEJ13 ) use a relatively similar generic definition referring to a set of scienc es , theories and techniques . 4 AI HLEG , A Definition of AI : Main Capabilities and Disciplines , April 2019 . 5 AI Watch , Joint Research Centre , Defining Artificial Intelligence : towards an operational definition and taxonomy of artifici al intelligence , February 2020 . 6 OECD , Council Recommendation on Artificial Intelligence , June 2019 . 7 UNESCO , Preliminary study on the technical and legal aspects relating to the desirability of a standard -settin g instrument on the ethics of artificial intelligence , March 2019 8 UNESCO , First draft of the Recommendation on Ethics of Artificial Intelligence , September 2020 9 Council of Europe , Committee of Ministers , Recommendation CM/Rec ( 2020 ) 1 of the Committee of Ministers to member States on the human rights impacts of algorithmic sy stems , April 2020 . 10 Committee of Ministers , Declaration of the Committee of Ministers on the manipulative capabilities of algorithmic processes , February 2019 . 11 Commissioner for Human Rights , Unboxing AI : 10 steps to protect human rights - Recommendation of the Commissioner for Human Rights , May 2019 . 12 Consultative Committee of the Convention for the protection of individuals with regard to the Automatic Processing of Personal Data , Guidelines on Artificial Intelligence and Data Protection , January 2019 . 13 CEPEJ , European Ethical Charter for the use of artificial intelligence in judicial systems an d their environment , December 2018 . 4 8 . In sum , it can be concluded that the term “ AI ” is used as a “ blanket term ” for various computer applications based on different techniques , which exhibit capabilities commonly and currently associated with human intelligence . T hese techniques can consist of formal models ( or symbolic systems ) as well as data -driven models ( learning -based systems ) typically relying on statistical approaches , including for instance supervised learning , unsupervised learning and reinforcement learn ing . AI systems act in the physical or digital dimension by recording their environment through data acquisition , analysing certain structured or unstructured data , reasoning on the knowledge or processing information derived from the data , and on that bas is decide on the best course of action to reach a certain goal . They can be designed to adapt their behaviour over time based on new data and enhance their performance towards a certain goal . 9 . Whereas the CAHAI members , participants and observers have also indicated different approaches on the ( need for a ) definition of AI resulting from different legal traditions and cultures , a consensus has been found on the need to approach AI systems in a technologically neutral ( i.e . regardless of the underlying techn ology being used ) way , comprising all the various automated decision -making technologies that fall under this umbrella term , including their broader socio -technical context . Furthermore , a balance should be sought between a definition that may be too preci se from a technical point of view and might thus be obsolete in the short term , and a definition that is too vague and thus leaves a wide margin of interpretation , potentially resulting in a non -uniform application of the legal framework.14 10 . As a result , a f uture Council of Europe legal framework on AI should adopt a simplified and technologically neutral definition of its purpose , covering those practices or application cases where the development and use of AI systems , or automated decision -making systems m ore generally , can impact on human rights , democracy and the rule of law , and taking into account all of the systems ’ socio -technical implications .15 In this feasibility study , a broad definition is applied in order to ensure that the diversity of the chal lenges raised by the development and use of AI systems are adequately identified . In subsequent stages of the CAHAI ’ s work , this definition may need to be further refined in light of the form and scope of a potential legal instrument , so as to clarify whic h human behaviour related to the development and use of algorithm driven processes more generally is targeted . 14 A few CAHAI members pointed to the importance of delineating more clearly the definition of AI that should be used for the purpose of the legal framework . This delineation – which will also help setting out the scope of the legal framework – should be carefully considered by the CAHAI ’ s Legal Framework Group and could also be part of the envisaged stakeholder consultations . 15 It is worth noting in this respect that other legal instruments of the Council of Europe rela ting to scientific fields , such as the Convention on Human Rights and Biomedicine ( `` Oviedo Convention '' , ETS No . 164 ) , do not define its subject matter either . The `` Convention 108 ” , as its modernised version ( Convention 108+ ) , defines the concept of `` data p rocessing '' , without mentioning specific technical objects such as algorithms , and link such concept to the notion of `` personal data '' , thus making it possible to determine whether or not a processing operation falls within its scope . Any new regulation of t he Council of Europe should not contravene the existing Council of Europe instruments such as the Convention on Human Right and Biomedicine , Convention 108 and 108+ . 5 3 . OPPORTUNITIES AND RISKS ARISING FROM THE DESIGN , DEVELOPMENT AND APPLICATION OF ARTIFICIAL INTELLIGENCE ON HUMAN RIGHTS , THE RULE OF LAW AND DEMOCRACY . 1 . Introduction 11 . As noted in various Council of Europe documents , including reports recently adopted by the Parliamentary Assembly ( PACE ) 16 , AI systems are substantially transforming individual lives and have a profound impact on the fabric of society and the functioning of its institutions . Their use has the capacity to generate substantive benefits in numerous domains , such as healthcare , transport , education and public administration , generating promising opportunities for humanity at large . At the same time , the development and use of AI systems also entails substantial risks , in particular in relation to interference with human rights , democracy and the rule of law , the core elements upon which our European societies are built . 12 . AI systems should be seen as “ socio -technical systems ” , in the sense that the impact of an AI system – whatever its underlying technology – depends not only on the system ’ s design , but also on the way in which the system is developed and used within a broader environ ment , including the data used , its intended purpose , functionality and accuracy , the scale of deployment , and the broader organisational , societal and legal context in which it is used .17 The positive or negative consequences of AI systems depend also on th e values and behaviour of the human beings that develop and deploy them , which leads to the importance of ensuring human responsibility . There are , however , some distinct characteristics of AI systems that set them apart from other technologies in relation to both their positive and negative impact on human rights , democracy and the rule of law.18 13 . First , the scale , connectedness and reach of AI systems can amplify certain risks that are also inherent in other technologies or human behaviour . AI systems can analyse an unprecedented amount of fine -grained data ( including highly sensitive personal data ) at a much faster pace than humans . This ability can lead AI systems to be used in a way that perpetuates or amplifies unjust bias19 , also based on new discrimina tion grounds in case of so called “ proxy discrimination ” .20 The increased prominence of proxy discrimination in the context of machine learning may raise interpretive questions about the distinction between direct and indirect discrimination or , indeed , the adequacy of this distinction as it is traditionally understood . Moreover , AI systems are subject to statistical error rates . Even if the error rate of a system applied to millions of people is close to zero , thousands of people can still be adversely impa cted due to the scale of deployment and interconnectivity of the systems . On the other side , the scale and reach of AI systems also imply that they can be used to mitigate certain risks and biases that are also inherent in other technologies or human behav iour , and to monitor and reduce human error rates . 16 See e.g . the reports of the Parliamentary Assembly of the Council of Europe , in particu lar on the need for democratic governance of AI ; the role of AI in policing and criminal justice systems ; preventing discrimination caused by AI ; ethical and legal frameworks for the research and development of neurotechnology ; AI and health care ; consequences of AI on labour markets ; and legal aspects of ‘ autonomous vehicles ’ . See also the Recommendation by the Comm issioner for Human Rights , “ Unboxing artificial intelligence : 10 measures to protect human rights ” ; the Committee of Ministers ’ Recommendation Rec/CM ( 2020 ) 1 , 17 See also Recommendation CM/Rec ( 2020 ) 1 of the Committee of Ministers to Member States on the hu man rights impacts of algorithmic systems . 18 These three factors are interacting and mutually reinforcing . Given the rapid evolvement of the technology and its unforeseen uses in future , this list is not conclusive but subject to constant development . 19 `` Unjust bias '' means a violation of the right to equality and non -discrimination in a context specific application of AI technology . 20 See e.g . the CoE study by F. Zuiderveen Borgesius , Discrimination , artificial intelligence , and algorithmic decision -mak ing , 2018 ; Affinity Profiling and Discrimination by Association in Online Behavioural Advertising , Wachter 2020 . 6 14 . Second , the complexity or opacity of many AI systems ( in particular in the case of machine learning applications ) can make it difficult for humans , including system developers , to understand or trace the s ystem ’ s functioning or outcome . This opacity , in combination with the involvement of many different actors at different stages during the system ’ s lifecycle , further complicates the identification of the agent ( s ) responsible for a potential negative outcom e , hence reducing human responsibility and accountability . 15 . Third , certain AI systems can re-calibrate themselves through feedback and reinforcement learning . However , if an AI system is re -trained on data resulting from its own decisions which contains un just biases , errors , inaccuracies or other deficiencies , a vicious feedback loop may arise which can lead to a discriminatory , erroneous or malicious functioning of the system and which can be difficult to detect . 2 . Opportunities arising from AI 16 . AI systems can have a highly positive impact across society . As a key driver for socio -economic development globally , they can contribute to alleviating some of the world ’ s problems and achieving the UN Sustainable Development Goals21 . AI systems can optimise agricult ural processes , revolutionise transportation and urban living , help mitigate the effects of climate change or predict natural disasters and facilitate greater access to information and knowledge . 17 . Indeed , AI systems can provide intelligent capabilities in many areas that are of value to individuals and society at large , and given their efficiency and large scale effects , be used to help overcome some of the barriers posed by the limited availability of human cognitive and decision -making capability . They ca n significantly improve the efficiency of existing industry practices , assist in the development of new industrial applications , and enhance their safety . AI systems can also lead to the creation of new services , products , markets and industries , which can significantly increase the well -being of citizens and society at large and be used to support socially beneficial applications and services . AI solutions can also enhance cyber security as they can be used to detect malicious behaviour and automate first response to ( low -level ) cyber -attacks . 18 . One of the most significant attributes of AI systems is their potential impact on human health and healthcare systems . This includes the improvement of medical diagnosis and treatment , the improvement of foetal health , as well as the advanced prediction and monitoring of epidemics and chronic diseases . Some opportunities generated by AI systems can also be observed within the response to the COVID -19 pandemic . AI systems are deployed to study the virus , accelerate medi cal research , develop vaccines , detect and diagnose infections , predict the virus ’ evolution , and to rapidly exchange information . 19 . Also , in other domains , AI systems can transform the scope of and manner in which research is conducted , and can be used to a dvance and expediate scientific discoveries that benefit society at large . Beyond research , AI systems can also be used to enhance educational opportunities by enabling personalised learning approaches and increasing the availability of education on a wide r scale . 20 . Finally , AI systems can foster and strengthen human rights more generally , and contribute to the effective application and enforcement of human rights standards . This can , for instance , be achieved by detecting biased ( human or automated ) decision s , monitoring representation patterns of different people or groups ( for example women in the media ) or analysing discriminatory structures in organisations . Where used responsibly , they can also enhance the rule of law and democracy , by improving the effi ciency of administrative procedures and helping public authorities being more responsive to the public ’ s needs , while freeing up time to tackle other complex and important issues . AI systems can also help public actors better identify the needs and concern s of the public , as well as to inform analyses and decisions , contributing to the development of more effective policies . 21 See “ The role of artificial intelligence in achieving the Sustainable Development Goals ” , Nature , https : //www.nature.com/articles/s41467 -019-14108 -y . 7 3 . Impact on Human Rights , Democracy and the Rule of Law 21 . Despite these benefits , the increasing use of AI systems in all areas of private and public life also carries significant challenges for human rights , democracy and the rule of law22 . Examples of known cases for each are discussed below . Respect for human rights is an essential component of democracy and the rule of law . Therefore , the review of the challenges posed by AI systems specifically to democracy and the rule of law is closely entwined with the impact of AI systems on human rights . 3.3.1 Impact on Human Rights 22 . The development and use of AI systems has an impact on a wide range of human rights.23 The main issues are briefly set out below , focusing in particular on the rights set out by the European Convention on Human Rights ( `` ECHR '' ) , its Protocols and the European Social Charter ( `` ESC '' ) . Liberty and Security ; Fair Trial ; No Puni shment without Law ; Effective remedy ( Art . 5 , 6 , 7 , 13 ECHR ) 23 . The above -mentioned risks raised by the use of AI systems to facilitate or amplify unjust bias can pose a threat to the right to liberty and security combined with the right to a fair trial ( Art . 5 , 6 , 7 ECHR ) when these systems are used in situations where physical freedom or personal security is at stake ( such as justice and law enforcement ) . For instance , some AI systems used to predict recidivism rely on characteristics that the suspect shares with others ( such as address , income , nationality , debts , employment ) , which raises concerns as regards maintaining an individualised approach to sentencing and other fundamental aspects of the right to a fair trial.24 In addition , an AI system ’ s opacity m ay render it impossible to understand the reasoning behind its outcomes , hence making it difficult or impossible to ensure the full respect of the principle of equality of arms , to challenge the decision , seek effective redress or have an effective remedy . If applied responsibly and with prudence , however , certain AI applications can also make the work of justice and law enforcement professionals more efficient and hence have a positive impact on these rights . This necessitates further efforts to build the capacities of judicial actors in their knowledge and understanding of AI systems and their application . Private and Family Life ; Physical , Psychological and Moral Integrity ( Art . 8 ECHR ) 24 . Art . 8 ECHR encompasses the protection of a wide range of aspects of our private lives , which can be divided into three broad categories namely : ( i ) a person ’ s ( general ) privacy , ( ii ) a person 's physical , psychological and moral integrity and ( iii ) a person 's identity and autonomy.25 Various AI applications can impact these categories . This occurs most notably when personal data is processed ( for instance to identify or surveil individuals ) , but it can also occur without the processing of personal data . Examples of invasive AI applications include in particular systems that t rack the faces or other biometrical data of individuals , such as micro 22 See the report prepared by Catelijne Muller ( CAHAI ( 2020 ) 06 -fin ) , The Impact of Artificial Intelligence on Human Rights , Democracy and th e Rule of Law . 23 See e.g . Study by FRA ( EU Agency for Fundamental Rights ) on Facial recognition technology and fundamental rights ( 2019 ) : https : //fra.europa.eu/sites/default/files/fra_uploads/fra -2019 -facial -recognition -technology -focus -paper -1_en.pdf . See also the new FRA report , “ Getting the Future Right – Artificial Intelligence and Fundamental Rights in the EU ” , Luxembourg : Publications Office of the European Union , 14 December 2020 , https : //fra.europa.eu/en/publication/2020/artificial -intelligence -and-fundamental -rights . 24 The problematic use of AI systems ( such as the COMPAS system used in the US ) was demonstrated by several studies , including the Dartmouth study on the accuracy , fairness , and limits of predicting recidivism by Julia Dressel and Hany Farid , Science Adva nces 17 Jan 2018 , Vol . 4 , no . 1 , DOI : 10.1126/sciadv.aao5580 . At the same time , when using more responsible approaches , some studies indicated that AI systems can also help improve predictions . See e.g . The limits of human predictions of recidivism , Zhiyua n Lin , Jongbin Jung , Sharad Goel and Jennifer Skeem , Science Advances ,14 Feb 2020 , Vol . 6 , no . 7 , DOI : 10.1126/sciadv.aaz0652 . 25 See Guide on Article 8 of the ECHR , Council of Europe . 8 expressions , gait , ( tone of ) voice , heart rate or temperature data.26 Beyond identification or authentication purposes , such data can also be used to assess , predict and influence a pers on ’ s behaviour , and to profile or categorise individuals for various purposes and in different contexts , from predictive policing to insurance rates.27 There is also ample evidence that the use of biometric recognition technology can lead to discrimination , notably on the basis of skin colour and/or sex , when bias in the algorithm or underlying dataset is insufficiently addressed.28 25 . Furthermore , AI -based tracking techniques can be used in a way which broadly affects 'general ' privacy , identity and autonomy an d which can make it possible to constantly watch , follow , identify and influence individuals , thereby also affecting their moral and psychological integrity . As a result , people might feel inclined to adapt their behaviour to a certain norm , which in turn also raises the issue of the balance of power between the state or private organisation using tracking and surveillance technologies on the one hand , and the tracked ( group of ) individuals on the other.29 The indiscriminate on - and offline tracking of all a spects of people ’ s lives ( through online behaviour , location data , data from smart watches and other Internet -of-Things ( IoT ) applications , such as health trackers , smart speakers , thermostats , cars , etc . ) , can have the same impact on the right to privacy , including psychological integrity . A right to privacy implies a right to a private space free from AI -enabled surveillance as necessary for personal development and democracy . 30 Freedom of expression ; Freedom of assembly and association ( Art . 10 , 11 E CHR ) 26 . The use of AI systems - both online and offline - can impact individuals ’ freedom of expression and access to information , as well as the freedom of assembly and association.31 AI applications can be used to intervene in the media space with high effic iency , and substantively alter human interactions . The internet and social media platforms have shown huge potential for people organising themselves to exercise their right to peaceful assembly and association . At the same time , the use of AI -driven surve illance can jeopardise these rights by automatically tracking and identifying those ( groups of ) individuals or even excluding them from participating in social protests.32 Moreover , the personalised tracking of individuals – in virtual and real life – may h amper these rights by diminishing the protection of ‘ group anonymity ’ . This can lead to individuals no longer partaking in peaceful demonstrations , and more generally refraining from openly expressing their opinions , watching certain media or reading certa in books or newspapers . 27 . Furthermore , the use of AI systems can affect the right to receive and impart information and ideas when used in online ( social ) media and news curations to pre -sort or display content according to personal interests or 26 The case law of the European Court of Human Rights ( ECtHR ) makes cle ar that the capture , storage and processing of such information , even briefly , impacts art . 8 ECHR . 27 It can be noted that no sound scientific evidence exists corroborating that a person 's inner emotions or mental state can be accurately 'read ' from a per son 's face or other biometric data . See also the study by Barrett , L. F. , Adolphs , R. , Marsella , S. , Martinez , A. M. , & Pollak , S. D. ( 2019 ) . 28 See e.g . the MIT Study by Joy Buolamwini ( 2018 ) : https : //news.mit.edu/2018/study -finds -gender -skin-type -bias-artificial intelligence -systems -0212 ; the US National Institute of Standards and Technology study on face recognition ( 2019 ) : https : //nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8280.pdf ; Study by FRA ( EU Agency for Fundamental Rights ) on Facial recognition technology and fundamental rights ( 2019 ) , pages 26 -27 : https : //fra.eur opa.eu/sites/default/files/fra_uploads/fra -2019 -facial -recognition -technology -focus -paper -1_en.pdf . 29 Study by Catelijne Muller , CAHAI ( 2020 ) 06 -fin , para 18 ; Examined Lives : Informational Privacy and the Subject as Object , Julie E. Cohen , 2000 . 30 The chi lling effect describes the inhibition or discouragement of the legitimate exercise of a right . Studies have shown that , once people know they are being surveyed , they start to behave and develop differently . Staben , J . ( 2016 ) . Der Abschreckungseffekt auf d ie Grundrechtsausübung : Strukturen eines verfassungsrechtlichen Arguments . Mohr Siebeck . 31 For the impact of AI on freedom of expression , see UNESCO , 2019 , Steering AI and Advanced ICTs for Knowledge Societies : A Rights , Openness , Access , and Multi -stakeholder Perspective at http s : //unesdoc.unesco.org/ark : /48223/pf0000372132 . 32 Algorithms and Human Rights , Study on the human rights dimensions of automated data processing techniques and possible regulatory implications , Council of Europe , 2018 ; Study by FRA ( EU Agency for Fundamen tal Rights ) on Facial recognition technology and fundamental rights ( 2019 ) , pages 26 -27 : https : //fra.europa.eu/sites/default/files/fra_uploads/fra -2019 facial -recognition -technology -focus -paper -1_en.pdf . 9 habits . This can also reinforce outdated social norms , including gender -based stereotypes and fuel polarisation and extremism through creating ‘ echo chambers ’ and ‘ filter bubbles ’ .33 Search engines , recommendation systems and news aggregators are often non -transparent and unaccountable , both concerning the data they use to select or prioritise content , but also as concerns the purpose of the specific selection or prioritisation34 which they can use for financial and political interest promotion . AI systems are routinely used to select and prioritise content that keeps people on the platform as long as possible , irrespective of whether the content is objective , factually true , dive rse or relevant . Furthermore , content is increasingly being “ faked ” by producing synthetic media footage , e.g . by mimicking real people ’ s appearance or voice using so called “ deep fakes ” . Such technology is already able to manipulate or generate visual and audio content with an unprecedented potential to deceive and to blur the line between real and fake content . This significantly affects the capacity of individuals to form and develop opinions freely , to receive and impart information and ideas , which mig ht lead to an erosion of our information society.35 Apart from that , online platforms are increasingly turning to AI systems to identify , flag , downrank and remove content which breaches their terms of service . Inaccuracies of the AI systems can lead to the consequence that legitimate content – protected by the right to freedom of expression – is flagged or removed in error . This is particularly difficult for content that requires understanding of nuance and context , related to areas such as hate speech and disinformation . Finally , as the online platforms have claimed audiences and advertising revenue , some traditional news media have struggled to survive . The threats to the viability of news media , connected with the consumption of news and information throu gh online platforms , presents a risk to a free , independent and pluralistic media ecosystem . Equality and Non -Discrimination ( Art . 14 ECHR , Protocol 12 ) 28 . The impact of the use of AI systems on the prohibition of discrimination and the right to equal treatment is one of the most widely reported upon . As noted above , AI systems can be deployed to detect and mitigate human bias . At the same time , the use of AI systems can also enable the perpetuation and amplification of biases and stereotypes36 , sexism , racism , ageism , discrimination based on various grounds and other unjust discrimination ( including based on proxies or intersectional37 grounds ) , which creates a new challenge to non discrimination and equal treatment . 29 . The risk of discrimination can arise in multiple ways , for instance due to biased training data ( e.g . when the data -set is not sufficiently representative or inaccurate ) , due to a biased design of the algorithm or its optimisation function ( e.g . due to the conscious or unconscious stereotypes or biases of developers ) , due to exposure to a biased environment once it is being used , or due to a biased use of the AI system . For instance , in light of past legal or factual discriminations against women , historical data bases can lack sufficiently ge nder balanced data . When such a data base is subsequently used by AI systems , this can lead to equally biased decisions and hence perpetuate unjust discrimination . The same holds true for traditionally vulnerable , excluded or marginalised groups more gener ally . In addition , the gaps in representation of the above mentioned groups in the AI sector might further amplify this risk .38 Measures to ensure gender balance in the AI workforce and to improve diversity in terms of ethnic/social origin could help mitigate some of those risks . 33 See e.g . a study by Carnegie Mellon researchers : https : //www.theguardian.com/technology/2015/jul/08/women -less-likely ads-high -paid -jobs-google -study 34 Burrell , J . ( 2016 ) . How the machine ‘ thinks ’ : Understanding opacity in machine learning algorithms . Big Data & Society , 3 ( 1 ) , 2053951715622512 . 35 UN Report of the Special Rapporteur on the promotion and protection of the right to freedom of opinion and expression , A/73/348 . Its effects on democracy are further discussed below . 36 This can most notably include gender stereotypes , which are “ preconceived ideas whereby males and females are arbitrarily assigned characteristics and roles determined and limited by their sex. ” See in this regard : https : //rm.coe.int/prems -093618 -gbr-gender -equality -strategy -2023 -web -a5/16808b47e1 37 See also footnote 15 4 in this regard . 38 According to the study conducted by AI Now Institute `` Discriminating systems , Gender , Race , and Power in AI ” in 2019 , women comprised only 15 % of AI research staff at Facebook and 10 % at Google . For black workers , the picture is even worse . 10 30 . In addition , when the transparency of AI systems ’ decision -making processes is not ensured , and when mandatory reporting or auditability requirements are not in place , the existence of such biases can easily remain undetected or even be obscured39 , and thus marginalise the social control mechanisms that typically govern human behaviour.40 Social and Economi c Rights ( Art . 2 , 3 , 5 , 11 , 12 , 13 and 20 ESC ) 31 . AI systems can have major benefits when used for hazardous , heavy , exhausting , unpleasant , repetitive or boring work . However , the wide adoption of AI systems in all domains of our lives also creates new risks to social and economic rights . AI systems are increasingly used to monitor and track workers , distribute work without human intervention and assess and predict worker potential and performance in hiring and firing situations . In some situations , this can also have detrimental consequences for workers ’ right to decent pay , as their pay can be determined by algorithms in a way that is irregular , inconsistent and insufficient.41 Furthermore , AI systems can also be used to detect and counter the unionisation of workers . These applications can jeopardise the right to just , safe and healthy working conditions , dignity at work as well as the right to organise . The discrimination capacity of AI systems that assess and predict the performance of job applications or w orkers can also undermine equality , including gender equality , in matters of employment and occupation . 32 . In addition , AI systems can , for instance , be used in the context of social security decisions , in which case the right guaranteed by Article 12 of the European Social Charter – stating that all workers and their dependents have the right to social security – can be impacted . Indeed , AI systems are increasingly relied on in social welfare administration , and the decisions taken in that context can signifi cantly impact individuals ’ lives . Similar issues arise where AI systems are deployed in the context of education or housing allocation administrations . 33 . Moreover , whenever AI systems are used to automate decisions regarding the provision of healthcare and medical assistances , such use can also impact the rights enshrined in Articles 11 and 13 of the Charter , which respectively state that everyone has the right to benefit from measures that enable the enjoyment of the highest possible standard of health attai nable , and that anyone without adequate resources has the right to social and medical assistance . AI systems can , for instance , be utilised to determine patients ’ access to health care services by analysing patients ’ personal data , such as their health car e records , lifestyle data and other information . It is important that this occurs in line with not only the right to privacy and personal data protection , but also with all the social rights laid down in the aforementioned Charter , the impact on which has so far received less attention than the impact on civil and political rights . 3.3.2 Impact on Democracy 34 . The development and use of AI systems can also impact the functioning of democratic institutions and processes , as well as the social and political beh aviour of citizens and civil society at large.42 Where designed , deployed and used responsibly , AI systems can improve the quality of governance , for instance by enhancing For example , only 2.5 % of Google ’ s workforce is black , while Facebook and Microsoft are each at 4 % . Given decades of concern and investment to redress this imbalance , the current state of the field is alarming . 39 This has , for instance , been shown by the low number of complaints to responsible authorities , including national equality bodies ( NEB ) , but also court cases . 40 In the field of anti -discrimination legislation , specific rules on the sharing of the burden of proof are typically used with the aim of compensating for such in -transparency . 41 This problem can be linked with a more general issue related to the gig economy or crowdsourcing platform model ( from delivery services to data enrichment services ) , which is generally based on the “ worker as an independent contractor ” model ( instead of a typically more protected employee ) , wh ereby workers often lack access to unemployment benefits , sick leave , holidays and overall social benefits . 42 For further details on the impact of AI systems on democracy , see the report for the Parliamentary Assembly of the Council of Europe on the “ Need for democratic governance of artificial intelligence ” ( Doc . 15150 ) . 11 the accountability , responsiveness and efficiency of public institutions , helping to fight corruption and fostering pluralism . They c an help broaden the space for diverse democratic representation and debate by decentralising information systems and communication platforms . Moreover , they can improve the way citizens and civil society at large receive and collect information about polit ical processes and help them participate therein remotely by facilitating political expression and providing feedback channels with political actors . At the same time , AI systems can also be used in ways that ( un ) intentionally hamper democracy.43 35 . A functio ning democracy relies on open social and political discourse , as well as the absence of improper voter influence or manipulation . As indicated above , AI technologies can be used to interfere in the online ( social ) media space for private financial or polit ical gain rather than with the public interest in mind.44 While propaganda and manipulation are not new , AI -based tools have amplified their scale and reach , and facilitated rapid iteration to strengthen their capabilities to influence people . They enable l arge scale yet targeted disinformation campaigns , through coordinated inauthentic behaviour , for instance through deep fake content , fake accounts , the illegal micro -targeting of voters and the polarisation of public debate . Moreover , they can threaten to undermine the human agency and autonomy required for meaningful voter decisions , which are at the heart of the creation of legitimate institutions.45 As a consequence , certain uses of AI can undermine confidence in democratic institutions and hinder the ele ctoral process . 36 . More generally , the concentration of power in the hands of a few private platforms with limited regulation so far , while these platforms have de facto become part of the public sphere , can amplify these risks . Furthermore , public -private c ollaborations on the use of AI in sensitive fields , such as law enforcement or border control , can blur the boundaries between the interests and responsibilities of democratic states on the one hand , and of private corporations on the other . This raises inter alia questions as regards the accountability of public institutions for decisions taken through AI solutions provided by private actors.46 37 . Finally , AI ’ s impact on the human rights set out above can more generally have a negative impact on democracy . AI systems can for instance be used by governments to control citizens , e.g . by automatically filtering and ranking information ( which can amount to censorship ) , or by using AI -enabled ( mass ) surveillance . Such use of AI systems can undermine democratic value s , curb the free will of the people , and erode political freedoms – such as freedom of expression , association and assembly . 38 . So far , public institutions that resorted to the use of AI systems have predominantly done so in order to support standardised adm inistrative decisions . However , the prospective reliance on AI by public institutions to inform or take policy decisions , would be very problematic if it would replace a dialogue between the majority and the minority or if it would not be subjected to demo cratic debate . In addition , growing reliance on AI systems could substantially affect the nature of state powers ( legislative , executive and judiciary ) and alter the balance between them . 43 Maja Brkan , ‘ Artificial Intelligence and Democracy ’ : Delphi - Interdisciplinary Review of Emerging Technologies 2 , no . 2 ( 2019 ) : 66 –71 , https : //doi.org/10.21552/delphi/2019/2/4 . 44 While automated content filtering technologies can help to restrict the display of unlawful or otherwise problematic content , in some situations its use can also restrict discussions on gender equality or hate -speech related concerns , as was for instance noted in the study of the European Parliament on Online Content Moderation ( 2020 ) , https : //www.europarl.europa.eu/RegData/etudes/STUD/2020/652718/IPOL_STU ( 2020 ) 652718_EN.pdf , p 59 . 45 See also the Report on Personal Data Processing by and for Political Campaigns : The Application of the Council of Europe ’ s Modernised Convention 108 ” by Colin J. Bennett , https : //rm.coe.int/t -pd-2020 -02rev -political -campaigns -en-clean -cjb/1680a01fc3 . 46 Regarding the obligations of private sector companies to respect human rights , see “ The UN Guiding Principles on Business and Human Rights outline the obligations of private sector companies to respect human rights ” : https : //www.ohchr.org/documents/publications/guidingprinciplesbusinesshr_en.pdf . 12 3.3.3 Impact on the Rule of Law 39 . In addition to impacting human rights and democracy , AI systems can also affect the rule of law.47 The rule of law prescribes that all public authorities act within the constraints set out by law , in accordance with the principles of democracy and human rights , and under the control of i ndependent and impartial courts . When used responsibly , AI systems can be used to increase the efficiency of governance , including legal institutions such as the courts48 , as well as law enforcement and public administrations.49 Furthermore , AI systems can help agencies to identify corruption within public entities,50 as well as detect and defend against cyberattacks.51 40 . The rule of law requires respect for principles such as legality , transparency , accountability , legal certainty , non-discrimination , equality and effective judicial protection – which can be at risk when certain decisions are delegated to AI systems . In addition , AI systems can also negatively affect the process of law -making and the application of the law by judges.52 Concerns have also been exp ressed on the possible negative effects of some AI applications used in judicial systems or connected areas53 . Such use could pose a challenge to the right to a fair trial enshrined in Article 6 of the ECHR54 , of which components such as the right to an inde pendent and impartial judiciary , the right to a lawyer or the principle of equality of arms in judicial proceedings are key elements that are also essential for the effective implementation of the rule of law . 41 . Moreover , companies face increased pressure , including through regulation , to take decisions on the legality of content that is shown on their platform . Since social media platforms have become the new “ public square ” , their own terms of service essentially set the rules of how freedom of expression manifests itself online , but with fewer safeguards than in more traditional public settings . It is , however , essential that states can and do continue to fulfil their responsibility for the protection of the rule of law . 3.4 A CONTEXTUAL AND RISK -BASED AP PROACH TO GOVERN AI 42 . The above demonstrates that some applications of AI systems pose a range of risks to human rights , democracy and the rule of law . These , risks , however , depend on the application context , technology and stakeholders involved . To counter any stifling of socially beneficial AI innovation , and to ensure that the benefits of this technology can be reaped fully while adequately tackling its risks , the CAHAI recommends that a future Council of Europe legal framework on AI should pursue a risk -based approach targeting the specific application 47 See for instance Mireille Hildebrandt , ‘ Algorithmic Regulation and the Rule of Law ’ , Philosophical Transactions of the Royal Society A : Mathe matical , Physical and Engineering Sciences 376 , no . 2128 , 2018 , 20170355. https : //doi.org/10.1098/rsta.2017.0355 . 48 AI systems can support legal professionals ’ work , for instance by assisting with complex tasks like analysing and structuring information on legal cases and legal documents , transcribing the minutes of court proceedings , promoting automated document classification hence eliminating a lot of processing time for the courts , civil registr ies and territorial offices , or providing legal information via chatbots . 49 Danaher , J . ( 2016 ) . The Threat of Algocracy : Reality , Resistance and Accommodation . Philosophy & Technology , 29 ( 3 ) , 245 – 268 . 50 West , J. , & Bhattacharya , M. ( 2016 ) . Intelligent fi nancial fraud detection : A comprehensive review . Computers & Security , 57 , 47 –66 . Hajek , P. , & Henriques , R. ( 2017 ) . Mining corporate annual reports for intelligent detection of financial statement fraud – A comparative study of machine learning methods . K nowledge -Based Systems , 128 , 139 –152 . 51 Taddeo , M. , & Floridi , L. ( 2018a ) . Regulate artificial intelligence to avert cyber arms race . Nature , 556 ( 7701 ) , 296 –298 . 52 By favouring the emergence of quantitative trends of analysis of judicial decisions , the traditional process of application of the law by the judge could be jeopardised . See the CEPEJ European Ethical Charter on the use of AI in judicial systems and their environment , §35 . See e.g . G. Buchholtz , “ Artificial Intelligence and Legal Tech : Challen ges to the Rule of Law ” in T. Wischmeyer , T. Rademacher ( eds . ) , Regulating Artificial Intelligence , Springer ( 2020 ) . 53 See the CEPEJ European Ethical Charter on the use of AI in judicial systems and their environment , which refers specifically to risks arising from systems of anticipation of judicial decisions in civil , administrative and commercial matters , from ris kassessment systems in criminal matters , an d from the use of AI systems without appropriate safeguards in the framework of non -judicial alternative dispute resolution . Among those risks the CEPEJ notes the risks of “ performative effect ” and of delegation of responsibility , and of lack of transparen cy of judicial decision -making . 54 As well the right to an effective remedy enshrined in Article 13 ECHR . 13 context.55 This means not only that the risks posed by AI systems should be assessed and reviewed on a systematic and regular basis , but also that any mitigating measures , that are further elaborated under C hapter 7 , should be specifically tailored to these risks . In addition to the risk -based approach , where relevant , a precautionary56 approach , including potential prohibitions , should be considered.57 This can , for instance , be the case where a certain AI sys tem in a specific context poses a significant level of risk coupled with a high level of uncertainty as to the harm ’ s reversibility . Such approach can help ensure that the specific risks posed by the development and use of AI are tackled , all the while sec uring that the benefits generated by this innovative technology can be reaped and thereby enhance individual and societal well -being . 43 . AI applications that promote , strengthen and augment the protection of human rights , democracy and the rule of law , shou ld be fostered . However , where based on a context -specific risk assessment it is found that an AI application can pose “ significant ” or unknown risks to human rights , democracy or the rule of law , and no appropriate mitigation measures exist within existin g legal frameworks to adequately mitigate these risks , states should consider the introduction of additional regulatory measures or other restrictions for the exceptional and controlled use of the application and , where essential , a ban or moratorium ( red lines ) .58 Building an international agreement on problematic AI uses and red lines can be essential to anticipate objections around competitive disadvantages and to create a clear and fair level playing field for AI developers and deployers.59 Examples of ap plications that might fall under red lines are remote biometric recognition systems – or other AI -enabled tracking applications – that risk leading to mass surveillance or to social scoring , or AI -enabled covert manipulation of individuals , each of which s ignificantly impact individuals ’ autonomy as well as fundamental democratic principles and freedoms . Exceptional use of such technologies should be specifically foreseen by law , necessary in a democratic society and proportionate to the legitimate aim , and permissibly only in controlled environments and ( if applicable ) for limited periods of time . On the other hand – where a certain application of an AI system does not pose any risk to human rights , democracy or the rule of law – it should be exempted from any additional regulatory measures60 . When assessing the risk posed by an AI system , a relevant issue to consider is whether the use of an AI system might result in a higher risk as compared to not using AI . 44 . A contextual and periodical assessment of the risks arising from the development and use of AI is necessary , in light of the context -specific nature of the benefits and risks related to the application of AI . As a transversal technology , the same AI technology can be used for different purposes and in different contexts , and the positive or negative consequences of the technology will depend heavily thereon . 55 This would also be in line with the approach taken by the European Union in its White Paper on AI February 2020 , https : //ec.europa.eu/info/sites/info/files/commission -white -paper -artificial -intelligence -feb2020_en.pdf 56 The precautionary approach is typically used in the context of innovations with a ( significant ) potential for causing harm when extensive scientific knowledge on the matter is ( still ) lacking . For more background , see for instance a Communication from the European Commission on the precautionary principle , Brussels , 2.2.2000 , COM ( 2000 ) 1 final , https : //eur lex.europa.eu/legal -content/EN/TXT/ ? uri=celex % 3A52000DC0001 57 In this regard , reference can be made to Chapter 7.2 , whic h indicates how Member States can implement a risk -based approach to AI governance . This can be given further consideration by the CAHAI in its future work . 58 One of the intentions of building international agreement on red lines is to prevent competitive disadvantages . Red Lines in the form of moratoria could in some instances be overcome when provisions can be set out to secure appropriate methods to develop trustworthy ( legal , ethical and robust AI ) , for instance where prior evaluation , continuous monit oring , certification procedures or standardised development processes can ensure appropriate guarantees to safeguard human rights , democracy and the rule of law . 59 See for instance Pekka Ala -Pietilä and Nathalie Smuha , ‘ A Framework for Global Cooperation on Artificial Intelligence and its Governance ( September 2020 ) ’ , https : //ssrn.com/abstract=3696519 . 60 Indeed , it is also possible that the development and use of a particular AI system in a certain context does not necessarily impact – whether positively or negatively – human rights , democracy or the rule of law . 14 4 . THE COUNCIL OF EUROPE 'S WORK IN THE FIELD OF ARTIFICIAL INTELL IGENCE TO DATE 45 . The significant impact of information technologies on human righ ts , democracy and the rule of law has led the Council of Europe to develop relevant binding and non -binding mechanisms , which complement and reinforce one another . They will be examined below , along with the case law on new technologies of the European Cou rt of Human Rights . 4.1 . Work in the field of protection of personal data 46 . `` Convention 10861 '' , modernised by an amending protocol in 201862 ( `` Convention 108+ '' ) , sets global standards on the rights to privacy and data protection of individuals , regardless of technological evolutions . In particular , it requires that the processing of special categories of data ( sensitive data ) 63 only be allowed where appropriate safeguards are enshrined in law , complementing those of the Convention , and creates a right for everyone to know that their personal data are processed and for which purpose , with a right of rectification where data are processed contrary to the Convention ’ s provisions . The amending protocol added new principles , such as transparency ( Article 8 ) , proportionality ( Article 5 ) , accountability ( Article 10 ) , impact assessments ( Article 10 ) and respect for privacy by design ( Article 10 ) . As rega rds the rights of individuals , the right not to be subject to a decision significantly affecting him or her based solely on an automated processing of data without having his or her views taken into consideration64 , and the right to obtain knowledge of the reasoning underlying the processing of data , where the results of the processing are applied , have been introduced ( Article 9 ) . Those new rights are of particular importance in relation to the profiling of individuals and automated decision -making65 . 47 . While not specific to AI applications , the legal framework built around Convention 108 remains fully applicable to AI technology as soon as the processed data fall within the scope of the Convention . Guidelines and a report in 2019 specified the guiding princip les to be applied , both for legislators and decision makers and for developers , manufacturers and service providers66 . A Council of Europe legal instrument on AI applications would therefore have to take full account of this acquis to supplement it ( i.e . fo cusing on outstanding gaps of protection ) , for instance by including in its scope such processing operations that do not only involve personal data , by extending its scope to the prevention of harm to other human rights , and by including societal ( and not only individual ) harm . 4.2 . Work in the field of cybercrime 48 . Various uses of AI systems may entail major risks in the field of cybercrime and are already much used to perpetrate such crimes , from automated and coordinated distributed denial of service attacks to scanning 61 Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data ( `` Convention 108 ” , ETS No . 108 , done at Strasbourg , on the 28th of January 1981 . By November 2020 , Convention 108 had 55 Parties , including all the mem ber States of the Council of Europe . 62 Protocol amending the Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data , CETS No . 223 , done at Strasbourg , on the 10th of October 2018 . 63 See Article 6 of Convention 108+ for the full list of sensitive data . 64 The Convention specifies that an individual can not exercise this right if the automated decision is authorised by a law to which the controller is subject and which also lays down suitable measures to safeguard the data subject ’ s rights and freedoms and legitimate interests . 65 See in this respect Recommendation ( 2010 ) 13 on the protection of individuals with regard to automatic processing of personal data in the context of profiling , and its explanatory memorandum . The Committee of Convention 108 is currently working on updating this important Recommendation . 66 Consultative Committee of the Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data , Guidelines on Artificial In telligence and Data Protection , January 2019 and Consultative Committee of the Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data , Report on Artificial Intelligence ( Artificial Intelligence and Data Protection : Challenges and Possible Solutions ) , January 201 9 ; Consulta tive Committee of the Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data , Guidelines on the Protection of Individuals with regard to the Processing of Personal Data in a World of Big Data , January 2017 . 15 systems for vulnerabilities , social engineering and identity theft , and autonomous cybercrime by machines . The Convention on Cybercrime ( `` Budapest Convention '' ) is an important instrument for criminalising offences against and by means o f computers , for procedural powers to investigate cybercrime and secure electronic evidence in relation to any crime subject to rule of law safeguards , and for effective international co operation.67 A new Protocol to the Budapest Convention on enhanced co -operation on cybercrime and electronic evidence is in preparation and may become available in 2021.68 The Budapest Convention and its provisions are fully applicable to acts carried out or facilitated by AI systems . In addition , general Council of Europe T reaties in the anti -criminal and anti -terrorism fields69 may be applicable to offences committed using AI technology too . 4.3 . Work in the field of algorithmic systems 49 . The Committee of Ministers adopted a Declaration on the Manipulation Capabilities of Algorithmic Processes70 in February 2019 and a Recommendation on the Human Rights Impacts of Algorithmic Systems71 in April 2020 . Studies and reports on the human rights ’ dimensions of automated data processing techniques72 and accountability and AI73 have also been developed by specialised committees and expert bodies , while the development of an instrument ( in the form of a recommendation ) on the impacts of digital technologies on freedom of expression74 is underway . 4.4 Work in the field of justice 50 . The European Commission for the Efficiency of Justice ( CEPEJ ) adopted in December 2018 the European Ethical Charter for the use of artificial intelligence in judicial systems75 which sets five key principles ( respect of fundamental rights , non -discriminatio n , quality and security , transparency , impartiality and fairness , `` under the control '' of the user ) for the use of AI systems in this field . The CEPEJ is currently studying the advisability and feasibility of a certification or labelling framework for artif icial intelligence products used in judicial systems . The European Committee on Legal Co -operation ( CDCJ ) is preparing guidelines to ensure the compatibility of these mechanisms with Articles 6 and 13 of the Convention on Human Rights . The European 67 Convention on Cybercrime , ETS No . 185 . See also a Guidance Note adopted in December 2014 . 68 See also the Additional Protocol to the Convention on Cybercrime , concerning the criminalisation of acts of a racist and xenophobic nature committed through computer systems , ETS No . 189 . 69 Such as , for example , t he Council of Europe Convention on the Prevention of Terrorism , the European Convention on the Suppression of Terrorism and the European Convention on Mutual Assistance in Criminal Matters , with their relevant protocols . 70 Committee of Ministers , Declaration on the manipulation capabilities of algorithmic processes - Decl ( 13/02/2019 ) 1 , 13 February 201 9 The Declaration draws , inter alia , member States ' attention to `` properly assess the need for stricter regulatory or other measures to ensure appropriate and democratically legitimate oversight of the design , development , deployment and use of algorithmic tools , with a view to implementing effective protection against unfair practices and abuses of economic power '' . 71 Committee of Ministers , Recommendation to member States on the human rights impacts of algorithmic systems - CM/Rec ( 2020 ) 1 , 8 April 2 020 The Recommendation , for its part , invites member States to `` review their legislative frameworks and policies , as well as their own practices with regard to the ongoing acquisition , design , development and deployment of algorithmic systems to ensure tha t they are in line with the guidelines set out in the Appendix to this Recommendation '' . 72 See the study produced by the Committee of experts on Internet Intermediaries ( MSI -NET ) under the authority of the Steering Committee on the Media and the Informatio n Society ( CDMSI ) on the human rights ’ dimensions of automated data processing techniques and possible regulatory implications DGI ( 2017 ) 12 . 73 MSI-AUT , Accountability and AI : Study on the impact of advanced digital technologies ( including artificial intelligence ) on the notion of accountability , from a human rights perspective - DGI ( 2019 ) 05 , September 201 9 74 See the ongoing work of the Committee of Experts on Freedom of Expression and Digital Technologies ( MSI -DIG ) 75 CEPEJ , European Ethical Charter on the use o f artificial intelligence in judicial systems and their environment - CEPEJ ( 2018 ) 14 , December 2018 16 Committ ee on Crime Problems ( CDPC ) is currently studying the topic of AI and criminal law and may propose the creation of a new specialised legal instrument76 . 4.5 Work in the field of Good Governance and elections 51 . The European Committee on Democracy and Governance ( CDDG ) is preparing a study on the impact of digital transformation – including AI – on democracy and governance . The study looks at the impact of AI on elections , civil participation , and democratic oversight . In the chapter devoted to governan ce , it maps the use of AI by public administrations in Europe and analyses its use through the lens of the 12 Principles of Good Democratic Governance . 52 . The Venice Commission has also published a report on digital technologies and elections77 as well as “ Principles on a human -rights compliant use of digital technologies in electoral processes ” . 4.6 Work in the field of gender equality and non -discrimination 53 . The Committee of Ministers Recommendation CM/Rec ( 2019 ) 1 on preventing and combating sexism recommends member States to integrate a gender equality perspective in all policies , programmes and research in relation to artificial intelligence , to avoid the pot ential risks of perpetuating sexism and gender stereotypes , and to examine how artificial intelligence could help eliminating gender gaps and sexism . 54 . Work is underway in the field of equality and non -discrimination78 , following the comprehensive study comm issioned by ECRI on “ discrimination , artificial intelligence and algorithmic decision making ” 79 . 55 . The European Commission against Racism and Intolerance ( ECRI ) monitors discrimination cases related to AI and algorithmic decision -making ( ADM ) , which falls un der its mandate and , where appropriate , makes relevant recommendations to address legislative or other gaps in order to prevent direct or indirect AI - and ADM -driven discrimination . 4.7 Work in the field of education and culture 56 . The Committee of Minister s ’ Recommendation on developing and promoting digital citizenship education80 invites member States to adopt regulatory and policy measures on digital citizenship education , assess their impact at regular intervals , provide or facilitate the provision of ap propriate initial and in -service education and training on digital citizenship education to teachers and other professionals in education , to name but a few recommended measures . Building on this , the Committee of Ministers mandated the Steering Committee for Education Policy and Practice ( CDPPE ) to explore the implications of artificial intelligence and other emerging technologies for education generally , and more specifically for their use in education . A Committee of Ministers Recommendation addresses sp ecifically the rights of the child in the digital environment81 . Moreover , the Committee of Convention 108 also adopted Guidelines on Children ’ s Data Protection in an Education setting82 . 76 See CDPC ( 2020 ) 3Rev , Feasibility Study on a future Council of Europe instrument on a future Council of Europe instrument on AI and criminal law . 77 Venice Commission , Principles for a fundamental rights -compliant use of digital technologies in electoral processes - CDLAD ( 2020 ) 037 , 11 December 2020 78 See also ECRI 25th anniversary conference and its Roadmap on Effective Equality ( September 2019 ) . 79 See the study commissioned by ECRI on : ‘ Discrimination , artificial intelligence and algorithmic decision making ’ ( 2018 ) , https : //rm.coe.int /discrimination -artificial -intelligence -and-algorithmic -decision -making/1680925d73 , written by independent expert Frederik Zuiderveen Borgesius . See in this regard also §81 of this study . 80 Committee of Ministers ’ Recommendation on developing and promoting digital citizenship education , CM/Rec ( 2019 ) 10 . 81 See the Committee of Ministers Recommendation ( 2018 ) 7 on Guidelines to re spect , protect and fulfil the rights of the child in the digital environment . 82 Consultative Committee on the Convention for the protection of individuals with regard to automatic processing of personal data , Convention 108 , Guidelines : Children ’ s Data Protec tion in an Education setting , November 2020 . 17 57 . Different activities have taken place since October 2018 concerning AI and art , creativity and cultural heritage , that demonstrated the increasing impact of AI systems on these three areas and highlighting the need for a direct involvement of creative and cultural professionals in AI systems ’ developments and re lated policies . In addition , Eurimages published a study on the impact of predictive technologies and AI on the audio -visual sector , including possible specific measures to be put in place to guarantee freedom of expression and cultural diversity83 . 4.8 The work of the Parliamentary Assembly of the Council of Europe 58 . The Parliamentary Assembly of the Council of Europe ( PACE ) adopted , on 28 April 2017 , a Recommendation on `` Technological convergence , artificial intelligence and human rights84 '' . On 22 October 2020 , the PACE adopted 7 reports , focusing on : the need for democratic governance of AI ; the role of AI in policing and criminal justice systems ; discrimination caused by AI ; threats to fundamental freedoms ; medical , legal and ethical challenges in the fie ld of health care ; consequences on labour markets ; and legal aspects of ‘ autonomous vehicles ’ . The reports were accompanied by Recommendations to the Committee of Ministers and Resolutions . 59 . Of significant relevance in the context of this feasibility study is the PACE report on the need for democratic governance of artificial intelligence . This report proposed , in particular , that the Committee of Ministers supports the drafting of a legally binding instrument governing AI applications , possibly in the form of a Council of Europe Convention85 . 4.9 The work of The Congress of Local and Regional Authorities of the Council of Europe 60 . The Congress of Local and Regional Authorities of the Council of Europe has in recent years worked in various ways on issues relat ed to artificial intelligence . Recently , the members of the Governance Committee have held an exchange of views of a report which is currently being prepared : “ Smart cities : the challenges for democracy ” , which will be issued in the second half of 2021 . 4.10 The work of the Commissioner for Human Rights 61 . In May 2019 , the Commissioner for Human Rights issued a Recommendation “ Unboxing artificial intelligence : 10 measures to protect human rights86 '' . It proposes a series of practical recommendations to nationa l authorities on 10 main areas for action : human rights impact assessment ; public consultations ; human rights standards in the private sector ; information and transparency ; independent monitoring ; non -discrimination and equality ; data protection and privac y ; freedom of expression , freedom of assembly and association , and the right to work ; avenues for redress ; and promoting knowledge and understanding of AI . 4.11 The work of the Council of Europe in the field of y outh 62 . The Council of Europe Youth Strategy 2030 refers to AI under the strategic priority “ young people ’ s access to rights ” with special emphasis on “ improving institutional responses to emerging issues affecting young people ’ s rights and their transition to adulthood , such as , [ ... ] artificial int elligence , digital space [ … ] ” . On this basis , and grounded in the CM/Rec ( 2016 ) 7 on Young People 's Access to Rights , the youth department will continue promoting and supporting a co -ordinated approach to improving young people ’ s access to rights across all relevant policy areas , including the field of AI governance , and will continue promoting AI literacy and equipping young people with skills , competences and knowledge needed to participate in AI governance and benefit from developing technologies . 83 Eurimages , Study on the impact of predictive technologies and AI on the audiovisual sector , including possible specific measures to be put in place to ensure freedom of expression and cultural diversity , December 2019 84 Recommendation 2102 ( 2017 ) 85 Parliamentary Assembly of the Council of Europe , Political Affairs and Democracy Committee , Report on the need for democratic governance of artificial intelligence , Doc . 15150 , 24 September 2020 86 Commissioner for Human Rights , Recommendation `` Unboxing AI : 10 steps to protect human rights '' , May 2019 18 4.12 The case law of the European Court of Human Rights relating to information technology 63 . The European Court of Human Rights ( ECtHR ) has not yet developed any specific case law on AI systems , hence the CAHAI could not rely on any ECtHR decisions specifically o n AI technology . At the moment there are no known relevant cases pending before the Court either . 64 . Existing case law in connection with this topic concerns algorithms in general and violations of Article 8 of the Convention ( privacy ) or Article 10 ( freedom of expression ) and , in a more indirect way , Article 14 ( non discrimination ) on cases dealing with e.g . mass surveillance87 , the editorial responsibility of platforms88 and electoral interference89 . 65 . In Sigurður Einarsson and others v. Iceland90 , a prosecuting authority used statistical data processing techniques to process large amounts of information and establish evidence in an economic and financial case . The question raised in this case concerned access by the defence to the data from which in criminating evidence was inferred . 66 . Other decisions of the Court have dealt with the consequences of algorithmic mechanisms used to prevent the commission of infringements . In 2006 , the Court stated in its Weber and Saravia v. Germany judgment91 that any po tential abuse of the state 's supervisory powers was subject to adequate and effective safeguards and that , in any event , Germany had a relatively wide margin of appreciation in the matter . 67 . With regard to mass surveillance of the population using algorithm s , which could potentially include AI tools , two potentially relevant cases are pending before the Grand Chamber : Centrum För Rättvisa v. Sweden92 and Big Brother Watch and others v. the United Kingdom93 . The last hearings in these cases took place on 10 Jul y 2019 . 5 . MAPPING OF INSTRUMENT S APPLICABLE TO ARTI FICIAL INTELLIGENCE 1. International legal instruments applicable to artificial intelligence 68 . General international and regional human rights instruments , including the ECHR , the International Bill of Human Rights and the EU Charter of Fundamental Rights , are applicable in all areas of life , including online and offline and regardless of the techno logy used . They are therefore also applicable in the context of AI systems . The question is , however , whether these instruments , separately or applied together , can sufficiently meet the challenges posed by AI systems and ensure adherence to the Council of Europe ’ s standards on human rights , democracy and the rule of law throughout their life cycle . Currently , no international legal instrument exists that specifically applies to the challenges raised by AI systems – or by automated decision making more gene rally – for democracy , human rights and the rule of law in a comprehensive way . There are , however , a number of international legal instruments that partially deal with certain aspects pertaining to AI systems indirectly . 69 . In this regard , the CAHAI took no te , during its 2nd plenary meeting , of the analysis of relevant international binding instruments made by an independent consultant.94 This analysis was based on a review of binding and 87 ECtHR , Big Brother Watch and others v. the United Kingdom , 13 September 2018 ( Chamber judgment ) - case referred to the Grand Chamber in February 2019 88 ECtHR , Delfi AS v. Estonia , 16 June 2015 ( Grand Chamber ) 89 ECtHR Court , Magyar Kétfarkú Kutya Párt v. Hungary , 23 January 2018 - case referred to the Grand Chamber in May 2018 90 ECtHR , Sigurður Einarsson and Others v. Iceland , 4 June 2019 ( 2nd section ) 91 ECtHR , Dec. 29 June 2006 , Weber and Saravia v. Germany , no . 54934/00 92 ECtHR , De c. 19 June 2018 , Centrum För Rättvisa v. Sweden , no . 35252/08 referred back to the Grand Chamber in February 2019 - hearing held on 10 July 2019 93 ECtHR , Dec. 13 September 2018 , Big Brother Watch and others v. the United Kingdom , nos . 58170/13 , 62322/14 and 24960/15 referred back to the Gr and Chamber in February 2019 - hearing held on 10 July 2019 94 See CAHAI ( 2020 ) 08 -fin , Analysis of internationally legally binding instruments , report by Alessandro Mantelero , University of Turin . 19 non-binding instruments in four core areas ( data protection , health , de mocracy and justice ) and was complemented by an overview of the Council of Europe ’ s instruments in other fields . It noted that various international legal instruments already exist to safeguard human rights more generally95 , to safeguard the rights of speci fic groups in light of vulnerabilities that are also relevant in an AI context96 , and to safeguard specific human rights that can be impacted by AI . The latter encompass , for instance , the right to non -discrimination97 and the right to the protection of priv acy and personal data98 , particularly in the context of automated personal data processing . 70 . Of particular importance is the Protocol amending the original Convention for the Protection of Individuals with Regard to Automatic Processing of Personal Data ( Co nvention 108+ ) , which was already mentioned above . This protocol not only modernised the 1980 landmark instrument but also enabled full consistency with the EU General Data Protection Regulation99 . It introduced , for instance , requirements of transparency a nd accountability , as well as protective rights for data subjects who are subjected to automated decision -making processes . This Protocol has not entered into force yet.100 71 . Furthermore , in addition to horizontally applicable instruments , a number of inter national legal instruments deal with specific sectors or domains that may indirectly pertain to AI or automated decision -making processes . These instruments cover areas as diverse as cybercrime101 , ( bio ) medicine102 and aviation.103 Finally , some legal instrument s concern procedural rights – such as transparency104 and access to justice105 – that might be helpful to monitor and safeguard the protection of substantive rights , or to address aspects relating to liability for certain harms.106 72 . The CAHAI acknowledges that t hese different legal instruments are relevant in the context of AI regulation . However , the CAHAI also supports the conclusions drawn in the analysis that these instruments do not always provide adequate safeguards to the challenges raised by AI systems . T his will be the subject of further analysis under sub -section 5.4 below . 73 . The growing need for a more comprehensive and effective governance framework to address the new challenges and opportunities raised by AI has been acknowledged by a number of intergov ernmental actors at international level . To date , most of these initiatives have been limited to non -binding recommendations.107 It 95 Such as e.g . the European Convention on Human Rights ( ET S No . 5 ) and its Protocols ; the European Social Charter ( ETS No . 163 ) ; the International Bill of Human Rights ; and the EU Charter of Fundamental Rights . 96 See e.g . the Convention on the Rights of the Child and the Convention on the Rights of Persons with Disabilities . See also th e European Charter for Regional or Minority Languages ( ETS No . 148 ) which can indirectly help ensure attention to minority language s when developing AI -applications . 97 See e.g . the International Convention on the Elimination of All Forms of Racial Discrimination , the Convention on the Elimination of All Forms of Discrimination against Women , and the Convention on Cybercrime and its Additional Protocol . 98 See e.g . the Convention for the Protection of Individuals with Regard to Automatic Processing of Personal Data ( ETS No . 108 ) , the EU General Data Protection Regulation ( 2016/679 ) and the EU Law Enforcement Directive ( 2016/680 ) . 99 The General Data Protection Regulation ( EU ) 2016/679 ( GDPR ) . 100 The Protocol will only enter into force when ratified , accepted or approved by all Parties to Treaty ETS 108 , or on 11 Octobe r 2023 if there are 38 Parties to the Protocol at this date . 101 See e.g . the Convention on Cybercrime ( ETS No . 185 ) . As regards the EU , see e.g . the Cybersecurity Act ( Regulation 2019/881 ) and the NIS Directive ( 2016/1148 ) . 102 See e.g . the Convention on Human Rights and Biomedicine ( ETS No . 164 ) and its Additional Prot ocols ( ETS 186 , 195 , 203 ) . See also the EU ’ s Medical Device Regulation ( 2017/745 ) and Regulation on in vitro diagnostic medical devices ( 2017/746 ) . 103 See e.g . the Chicago Convention on International Civil Aviation . 104 See e.g . the Council of Europe Conven tion on Access to Official Documents ( ETS No . 205 ) . 105 See e.g . the European Convention on the Exercise of Children 's Rights ( ETS No . 160 ) and the European Convention on Mutual Assistance in Criminal Matters ( ETS No . 30 ) . 106 See for instance the European C onvention on Products Liability in regard to Personal Injury and Death ( ETS No . 91 – not yet in force ) and the European Union ’ s Product Liability Directive ( Council Directive 85/374/EEC of 25 July 1985 ) and Machinery Directive ( Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 ) . 107 For instance , the OECD adopted a Council Recommendation on AI listing a number of ethical principles ( see https : //l egalinstruments.oecd.org/en/instruments/OECD -LEGAL -0449 ) , which provided inspiration for the human -centered 20 is worth mentioning that the European Commission has announced the preparation of a legislative proposal to tackle fundamental rights challenges related to ensuring trustworthy AI , which is scheduled for publication in the first quarter of 2021.108 2 . Ethics Guidelines applicable to artificial intelligence 74 . In recent years , private companies , academic and public -sector organisations have issued principles , guidelines and other soft law instruments for the ethical use of AI109 . In this regard , the CAHAI took note , during its 2nd plenary meeting , of the mapping work by two independent consultants110 who reviewed 116 documents on “ ethical AI ” , primarily developed in Europe , North America and Asia . This mapping revealed that current AI ethics guidelines converge on some generic principles , but – to the extent they give practical guidance – they tend to sharply disagree over the details of what should be done in practice . Notably as regards transparency , the most frequently identified principle , it was not clear whether transparency should be achieved through publishing source code , rendering the algorithmic training data accessible or auditable ( while considering the applicable data protection laws ) or through some other means . Resolving the challenge of applying these principles in practice and considering potential interdependencies and trade -offs with other desirable properties was hence cons idered an important issue to be addressed by policy makers . 75 . According to the mapping , compared to the rest of the world , soft law documents produced within Council of Europe ’ s member States appear to place greater emphasis on the ethical principles of solidarity , trust and trustworthiness , and refer more sporadically to the principles of beneficence and dignity . The principles of privacy , justice and fairness showed the least variation across Council of Europe ’ s member States , observers and the rest of the world , and hence the highest degree of cross -geographical and cross -cultural stability . 76 . In terms of key policy implications , it was noted that ethics guidelines are useful tools to exert some influence on public decision making over AI and to steer its development towards social good . However , it was also underlined that soft law approaches can not substitute mandatory governance . In some instances , due to the fact that the interests of those developing and commercialising the technology and those who mi ght suffer negative consequences thereof are not always fully aligned , there is a particular risk that self -regulation by private actors can bypass or avoid mandatory governance by ( inter ) governmental authorities . Soft law instruments and self -regulation i nitiatives can however play an important role in complementing mandatory governance , especially where the interests of the different actors are more aligned and where no substantive risk of negative effects on human rights , democracy and the rule of law is present.111 77 . The CAHAI agrees with the general findings of the mapping study and finds that the common principles identified in the study on relevant ethics guidelines could be part of CAHAI ’ s reflections on the development of a legal framework on AI . Resp ect for human rights , which was mentioned in just over half of the soft law documents reviewed , should be the focus of any future legal instrument on AI based on the Council of Europe ’ s AI principles endorsed by G20 in a Ministerial Statement in June 2019 ( see https : //www.mo fa.go.jp/files/000486596.pdf ) . Also UNESCO is preparing a ( non -binding ) Recommendation on ethical AI ( see UNESCO , First Draft of the Recommendation on the Ethics of Artificial Intelligence , September 2020 , https : //unesdoc.unesco.org/ark : /48223/pf0000373434 . ) While UNESCO ’ s current draft mention AI ’ s impact on human rights and the rule of law , it does not focus on AI ’ s challenges to democracy . 108 The European Commission particularly emphasises risks for fundamental rights , safety and the effective functioning of the liability regime . See the EC White Paper on Artificial Intelligence , published in February 2020 , https : //ec.europa.eu/info/sites/info/files/commission -white -paper -artificial -intelligence -feb2020_en.pdf . 109 Amongst recent initiatives feature the Ethics Guidelines fo r Trustworthy AI published in April 2019 by the Independent High Level Expert Group on Artificial Intelligence , set up by the European Commission , and its “ Assessment List for Trustworthy AI ” ( ALTAI ) for self -assessment published in July 2020 . 110 See CAHAI ( 2020 ) 07 -fin , AI Ethics Guidelines : European and Global Perspectives , report prepared by Marcello Ienca and Effy Vayena . 111 Effective mandatory governance requires , however , an instrument that is signed and ratified by enough States so as to ensure a cro ss-border level playing field , especially in light of the cross -border nature of AI products and services . 21 standards . In addition , the mapping study could be used as a practical foundation for implementing ethical frameworks in member States in a harmonised fashion . 3 . Overview of national instruments , policies and strategies related to artificial intelligence 78 . The analysis of the electronic consultation carried out among CAHAI memb ers , observers and participants on this issue112 indicated that four member States have adopted specific legal frameworks on specific AI systems concerning the testing and use of autonomous cars and enterprises . Two member States are developing legal framewo rks on the use of AI systems in the fields of recruitment and automated decision making by public authorities . 79 . Domestic ethics charters and soft law documents appear to be more widespread and cover issues such as robotics , facial recognition , the use of “ ethical AI ” in the public service and in electoral processes , and the use of personal and non -personal data . In one-member State , a voluntary AI certification programme was launched . Two member States have formally endorsed international or European non -binding AI ethics frameworks . Twelve member and four observer States have adopted one or more of the above -mentio ned instruments . Different types of institutions such as national councils , committees , public institutions specialised in AI and government entities have been responsible for their development . 80 . Strategies and policies on AI systems have been put in place in thirty member and four observer States . Built on multi -annual action plans , accompanied in some cases by ambitious funding programmes , they pursue the objectives of increasing trust in this technology and promoting its uptake , strengthening skills for its design and development , supporting research and boosting business development . States have very often involved experts from the public and private sectors , as well as academia , in the preparation of these plans . In most cases , AI systems are the subjec t of targeted strategies , whilst in other cases they have been integrated into broader sector policies concerning the economy and digital technologies . The development and use of AI systems has also been considered in sectorial strategies on agriculture , e -justice , public services , health , environment , education , security and defence , mobility and data . 81 . Finally , the need to promote the development of AI systems in line with ethical requirements and international human rights standards has been underlined i n seven national strategies . 4 . Advantages , disadvantages and limitations of existing international and national instruments and ethical guidelines on artificial intelligence 82 . The above overview has shown that a number of more broadly applicable provisions al ready extend to the development and use of AI systems . In the absence of an international legally -binding instrument focused on AI ’ s challenges , significant efforts have been put into interpreting existing legal provisions in the light of AI , and in formul ating non -binding rules to contextualise the principles embedded in existing instruments.113 However , the fact that existing legal instruments have been adopted prior to the wide -spread use of AI systems often tends to reduce their effectiveness to provide a n adequate and specific response to the challenges brought by AI systems , as they are not tailored to its specificities . For instance , a study on “ Discrimination , artificial intelligence , and algorithmic decision -making ” commissioned by ECRI has highlighted that , though existing internat ional and domestic legal instruments in the field of non -discrimination do apply to the use of AI systems and can in some instances already provide some level of protection , they still have some limitations.114 The 112 See the latest updates in the document CAHAI ( 2020 ) 09 rev 2 , on the electronic consultation of CAHAI members , observers and participants , which i ncludes replies until 30 September 2020 . 113 See for instance T -PD ( 2019 ) 01 Guidelines on Artificial Intelligence and Data Protection ; CEPEJ . 2019 . European Ethical Charter on the use of artificial intelligence ( AI ) in judicial systems and their environment . 114 The study examines existing instruments in the context non -discrimination and states that many of these can already provide some protection against AI -driven discrimination . However , it also points to their limitations , as AI also paves the way for new types of unfair differentiation that escape current laws , suggesting the need for additional ( sectoral ) regulation to protect fairness and human rights in the context of AI . These limitations concern , for instance , the fact that these instrume nts 22 independent expert ’ s analysis prepared for the CAHAI regarding the impact of AI on human rights , democracy and the rule of law also described the adverse effects on other human rights,115 and the Council of Europe study on AI and responsibility specifically highlighted the limits of existing human ri ghts provisions to secure comprehensive protection.116 83 . Furthermore , despite being overlapping and mutually reinforcing , the number and diversity of instruments render it difficult to interpret and apply them to the AI context in a consistent and comprehensive manner , leading to uneven protection levels . While certain soft law instruments ( e.g . ethics guidelines ) set out more tailored principles on the development and use of AI systems , these are non -binding and can be limited in their effectivenes s with regards to the respect of human rights , democracy and the rule of law , as their implementation entirely relies on the goodwill of those involved . Furthermore , ethics guidelines do not have the same universal dimension as human rights -based standards and are characterised by a variety of theoretical approaches117 , which limits their utility . The CAHAI therefore notes that , while there is no legal vacuum as regards AI regulation , a number of substantive and procedural legal gaps nevertheless exist , as no ted here below.118 84 . First , the rights and obligations formulated in existing legal instruments tend to be articulated broadly or generally , which is not problematic as such , yet can in some instances raise interpretation difficulties in the context of AI . In addition , they do not explicitly address some AI -specific issues , thereby hampering their effective application to the challenges raised throughout the life cycle of an AI system.119 It has been indicated that a translation or concretisation of existing hum an rights to the context of AI systems120 , through more do not a pply if an AI system invents new classes which do not correlate with protected characteristics under such instruments ( i.e . gender or ethnicity ) , to differentiate between people . Such differentiation can nevertheless be unfair ( e.g . AI -driven price discrim ination could lead to certain groups in society consistently paying more ) . In another study authored by the same expert , it is suggested that legislators should “ consider introducing a general discrimination clause ” which can serve as a safety net for those gaps , and which people can “ directly invoke before national courts , in relation to discrimination by both public authorities and private bodies ” ( rather than adding new grounds or new exemptions to existing closed nondiscrimination laws . See J. Gerards and F. Zuiderveen Borgesius , ‘ Protected grounds and the system of non -discrimination law in the context of algorithmic decision -making and artificial intelligence ’ , November 2020 , SSRN : https : //papers.ssrn.com/sol3/papers.cfm ? abstract_id=3723873 . Other studies also point to the limitations of current non discrimination law , and particularly to the fact that , even if such laws apply , the comple xity and opacity of AI systems ’ decision -making process render it virtually impossible for those unjustly discriminated to be aware thereof , and to challenge it before court . It has thus been suggested that non -discrimination law ought to be strengthened b y measures that may “ include the introduction and broadening of mechanisms of collective action in antidiscrimination lawsuits , AI ‘ audits ’ or eve n additional competences of antidiscrimination and/or data protection agencies ” to bridge this knowledge gap a nd ensure the effective enforcement of the right to non -discrimination . See A. Tischbirek ‘ Artificial Intelligence and Discrimination : Discriminating Against Discriminatory Systems ’ in T. Wischmeyer , T. Rademacher ( eds . ) , Regulating Artificial Intelligence , Springer , 2020 . 115 See CAHAI ( 2020 ) 06 -fin , report prepared by Catelijne Muller . 116 Council of Europe study DGI ( 2019 ) 05 , Rapporteur Karen Yeung , Responsibility and AI , September 2019 , https : //rm.coe.int/responsability -and-ai-en/168097d9c5 . The new FRA study also outlines the impact of AI on several human rights and proposes certain measures to tackle existing limitations ( “ Getting the Future Right – Artificial Intelligence and Fundament al Rights in the EU ” , 14 December 2020 , https : //fra.europa.eu/en/publication/2020/artificial -intelligence -andfundamental -rights ) . 117 As pointed out in the independent expert ’ s report CAHAI ( 2020 ) 08 -fin , cited above . 118 This conclusion is also reached by the European Commission in its inception impact assessment on a potential EU regulation on AI , stating that : “ While the potential harms above are not pe r se new or otherwise necessarily tied to AI only , the preliminary analysis of the Commission in the White Paper indicates that a number of specific , significant risks are at stake when it comes to AI and are not adequately covered by existing legislation ” , https : //ec.europa.eu/info/law/better regulation/have -your -say/initiatives/12527 -Requirements -for-Artificial -Intellige nce . Note in this regard also the text adopted by the Standing Committee , acting on behalf of the Assembly , on 22 October 2020 ( see Doc . 15150 , report of the Committee on Political Affairs and Democracy ) . 119 This has , for instance , been highlighted by the abovementioned FRA report on AI and human rights ( footnote 114 ) , which in this regard particularly noted that those developing and using AI are often unsure about the applicability of existing laws with respect to AI . 120 As it is done by European General D ata Protection Regulation with regard to the protection of personal data . 23 specific provisions , could help remedy this issue.121 This could be done by specifying more concretely what falls under a broader human right and how it could be invoked by those subjected to AI systems . For instance , the right to a fair trial could be further concretised in terms of a right to challenge and get insight into any evidence based on an AI system.122 This could also be done by deriving specific obligations that should be complied with or require ments that should be met by those who develop or deploy AI systems . For instance , from the right to non-discrimination could be derived a due diligence obligation to analyse and mitigate , throughout AI systems ’ life cycle , the risk of unjust bias . Without such concretisation of existing rights in the context of AI applications , and of clear obligations upon developers and deployers of AI systems to ensure respect of those rights , individuals may fail to obtain the full and effective protection thereof.123 The CAHAI believes that the Council of Europe ’ s standards on human rights , democracy and the rule of law could provide an appropriate basis for the elaboration of more specific provisions to secure effective protection against the risks posed by the practical application of certain AI systems . 85 . Secondly , a number of essential principles that are relevant for the protection of human rights , democracy and the rule of law in the context of AI , are currently not explicitly legally assured . These gaps concern , for instance , the necessity to ensure sufficient human control and oversight124 over AI applications , to ensure their technical robustness , and to secure their effective transparency125 and explainability126 , in particular when they produce legal or other significant effects on individuals , and regardless of whether personal data is involved . A lack of sufficiently comprehensive legal provisions in existing legal instruments to safeguard these principles was pointed out in several studies.127 Importantly , saf eguarding these principles is also a necessary precondition to safeguard substantive rights , given the opacity of some AI systems and of the human choices to design and use 121 See CAHAI ( 2020 ) 06 -fin and CAHAI ( 2020 ) 08 -fin , cited above . See also Karen Yeung , Andrew Howes , and Ganna Pogrebna ( University of Birmingham ) , ‘ AI Governance by Human Rights –Cent ered Design , Deliberation , and Oversight : An End to Ethics Washing ’ , in The Oxford Handbook on Ethics of AI ( eds . M. D. Dubber , F. Pasquale , and S. Das ) , 2020 , DOI : 10.1093/oxfordhb/9780190067397.013.5 ; Nathalie A. Smuha ( KU Leuven ) , ‘ Beyond a Human Rights -Based Approach to AI Governance : Promise , Pitfalls , Plea ’ , in Philosophy and Technology , 2020 , https : //doi.org/10.1007/s13347 -020-00403 -w. 122 Such concretisation could not only clarify for legal subjects the rights they have in the context of AI systems , but would al so ensure foreseeability of the substantive dimensions covered and equality before the law when the right to a fair trial is bei ng applied by judges . Without such concretisation , the risk exists that not all judges would interpret the broader right as implying this more concrete right , hence leading to the unequal protection of individuals . 123 Moreover , as indicated above , those who are responsible to apply and interpret existing rights ( for instance judges ) may lack guidance as to how to do so in the context of AI , potentially leading to unequal / inadequate standards of protection depending on the jurisdiction . 124 This could also i nclude provisions to minimise the risks that may arise through the unqualified tampering or interference with AI systems . 125 As regards the transparency of automated decision -making processes , it should be noted that the limited protection offered by Con vention 108+ only applies to the processing of personal data , while AI systems can negatively affect individuals and societies also based on non -personal data . Moreover , the right not to be subjected to solely automated decision -making is currently formula ted very restrictively , as it only applies when it can be proven that an individual is significantly impacted by the decision , and if it can be proven that the decision was taken ‘ solely ’ by an AI system . Hence , the risk exists that a ver y limited review b y a human being – even if subjected to automation bias , severe time pressure or lack of information when reviewing the decision – makes this right moot . 126 Convention 108+ and the EU General Data Protection Regulation do not contain an explicit right to a n explanation for the data subject , and it is highly contested whether and to which extent this right can be implicitly read therein . See e.g . Sand ra Wachter et al. , ‘ Why a Right to Explanation of Automated Decision -Making Does Not Exist in the General Dat a Protection Regulation ’ , 7 International Data Privacy Law 2 , 76 -99 ( 2017 ) , doi:10.1093/idpl/ipx005 . See however also Andrew D. Selbst and Julia Powles , Meaningful information and the right to explanation , International Data Privacy Law 7 ( 4 ) , November 2017 , p233 –242 , https : //doi.org/10.1093/idpl/ipx022 . 127 See CAHAI ( 2020 ) 06 -fin and CAHAI ( 2020 ) 08 -fin , cited above . See also Automating Society Report 2020 , AlgorithmWatch , https : //automatingsociety.algorithmwatch.org/report2020/european -union/ ; Moreover , see also the European Commission White Paper on AI , 19 February 2020 , COM ( 2020 ) 65 final , at p 9 : “ A key result of the f eedback process is that , while a number of the requirements are already reflected in existing legal or regulatory regimes , those regarding transparency , traceability and human oversight are not specifically covered under current legislation in many economi c sectors ” . 24 them.128 Without the transparency or explainability of an impactful AI -enabled decisi on , it can not be assessed whether a right – such as the right to non -discrimination – is actually ensured . Moreover , it hinders an individual ’ s capability to challenge the decision . The existence of asymmetries of information between those negatively impac ted by AI systems and those developing and using them also stresses the need to reinforce mechanisms of responsibility , accountability and redress , and to render AI systems traceable and auditable .129 If these gaps are not filled , for instance by securing th e protection of these principles through the establishment of concrete rights and obligations , those negatively impacted – as well as other stakeholders , including regulators and law enforcers – will not be able to assess the existence of ( human ) rights in fringements . 86 . Current instruments also lack sufficient attention to the steps that developers and deployers of AI systems should take to ensure the effectiveness of these systems whenever they can impact on human rights , democracy or the rule of law130 , and to ensure that AI developers and deployers have the necessary competences or professional qualifications to do so . Moreover , the societal dimension of AI ’ s risks that surpasses the impact on individuals , such as the impact on the electoral process and the democratic institutions or the legal system , is not yet sufficiently considered . While a number of national and international mechanisms allow individuals to seek redress before a court when a human right is breached in the context of AI , this mechanism is currently underdeveloped as regards an interference with democracy or the rule of law , which concern broader societal issues . Their protection necessitates public oversight over the responsible design , development and use of AI systems whenever such risks exist , by setting out clear obligations or requirements to this end.131 87 . These legal gaps can also lead to uncertainty for stakeholders , and in particular AI developers , deployers and users , who lack a predictable and sound legal framework in which AI syste ms can be designed and implemented . This uncertainty risks hampering beneficial AI innovation , and can hence stand in the way of reaping the benefits provided by AI for citizens and society at large . A comprehensive legal framework for AI systems , guided b y a risk-based approach , can help provide the contours in which beneficial innovation can be stimulated and enhanced , and AI ’ s benefits can be optimised , while ensuring – as well as maximising – the protection of human rights , democracy and the rule of law via effective legal remedies . 128 It is only when the traceability of AI is ensured , for instance through the documentation or logging of relevant information , that a system can be audited and that it can be verified to which extent it may for instance infringe the right to nondiscrimination . Furthermore , the lack of explanation of the decision -making process hinders the possibility for individuals to challenge a decision and seek redress . In this regard , the European Commission White Paper on AI noted more generally , at p12 , that “ the specific characteristics of many AI technologies , including opacity ( ‘ black box -effect ’ ) , complexity , unpredictability and partially autonomous behaviour , may make it hard to verify compliance with , and may hamper the effective enforcement of , rules of existing EU law meant to protect fundamental rights ” . This also applies to the human rights provisions in other existing legal instruments , as they are currently not tailored to the specific challenges raised by AI . However , several examples of “ supplementary models ” and other methods for understanding how a decision has been reached do exist . Supplementary models are becoming more common , as is the use of more interpretable AI systems ( see https : //ico.org.uk/for -organisations/guide -to-data -protection/key -data -protection -themes/explaining -decisions -made with -ai/ ) . 129 In this regard , it can be noted that public administrations require even greater levels of accountability than the private sector . At the same time , it should be acknowledged that the distinction between public and private sector involvement is sometimes blurred , as public sector entities often heav ily rely on private actors for the development , procurement and use of AI systems and data sets . 130 In domains that materially impact human life , such as medicine , society relies on sound instruments to ensure that the technologies and human agents involved are effective at both meeting the intended objective ( e.g . : curing ailments ) and at avoiding negative side -effects ( e.g . : putting patients at undue risk ) . When AI systems can impact human rights , democracy or the rule of law ( for example : in legal , judici al , or law enforcement environments ) similar instruments are necessary . Facial recognition systems , for example , if used in law enforcement , should be generally effective at accurately identifying individuals ( the intended objective in a given law enforcem ent action ) , and have reasonably uniform accuracy across races ( to uphold human rights and the rule of law ) . 131 Recent technical progress has been made in this space . Several examples of “ supplementary models ” and other methods for understanding how a deci sion has been reached do exist . Supplementary models are becoming more common , as is the use of more interpretable AI systems , see https : //ico.org.uk/for -organisations/guide -to-data -protection/key -data -protection themes/explaining -decisions -made -with -ai/ . 25 88 . Finally , the various gaps in existing legal instruments - as well as the fragmented approach of applying these instruments to the context of AI across Europe - also raise uncertainty as regards the manner in which the transbo undary nature of the impact generated by the development and use of AI applications can be tackled . A lack of common norms at international level might hamper the cross -border trade of AI products and services , as the lack of shared norms and a clear level playing field can stand in the way of mutual trust , hence potentially also preventing that the benefits of AI applications can travel across national borders.132 89 . Based on the above , it can be concluded that a regulatory approach to AI should aim to address those gaps . Beyond existing legal frameworks , such an approach could contain binding provisions to safeguard human rights , democracy and the rule of law in the context of AI , to ensure a more comprehensive level of protection regardless of the sector conce rned and be complemented with sector -specific rules where appropriate.133 This can be done by clarifying or broadening the scope of existing rights and/or obligations and mandating the protection of additional principles or requirements to this end . In addit ion to such a binding approach , consideration can also be given to the elaboration of sector -specific guidance and ethical guidelines for issues that are only or particularly relevant in a given field or application.134 In this regard , reference can be made to Chapter 8 of this Feasibility Study , which sets out various options for a Council of Europe legal framework for the design , development and application of AI . 90 . As mentioned in the CAHAI progress report , the work undertaken by the CAHAI provides an oppor tunity to contribute and complement other international initiatives in this area ( e.g . by the OECD , the European Union , UNESCO and the United Nations in general , with whom coordination and synergies are being sought on a regular basis135 ) by enacting an inst rument based on the Council of Europe ’ s standards on human rights , the rule of law and democracy , as part of a global legal mechanism for the regulation of digital technologies . In this regard , the CAHAI underlined that part of the added value that the Cou ncil of Europe can provide when elaborating a legal instrument on AI is that , besides the protection of human rights , it can also address the societal and environmental challenges posed by AI systems to democracy and the rule of law.136 Developing a legally -binding instrument based on Council of Europe standards – should this option be supported by the Committee of Ministers – would contribute to making the Council of Europe initiative unique among other international initiatives , which either focus on elabor ating a different type of instrument or have a different scope or background . It is important to keep in mind the specific nature of regional standards , and to engage the full spectrum of Council of Europe ’ s competence when performing this work . 5 . Internatio nal legal instruments , ethical guidelines and private actors 91 . Council of Europe instruments are typically addressed to the member States rather than to private actors . Nevertheless , private actors can be addressed indirectly , by virtue of the rights granted to , and obligations assumed by , states under such instruments . States have a duty137 to ensure that private actors respect human 132 This is particularly important in the case of small countries that are extremely dependent of their neighbors ’ regulation . Fo r small countries with limited AI development capabilities , a transboundary regulation or a common ground of regulation principles would be particularly useful . 133 In this regard , it can be noted that many AI systems can be repurposed for use in other sectors . Therefore , an approach that sets out certain safeguards across sectors can be desirable , potentially coupled with complementa ry safeguards or guidelines that are more sector -specific where needed . 134 In this regard , the CAHAI recognized the context -specificity of certain risks . The wide -scale use of AI -based remote biometric identification , for instance , does not raise the same impact on human rights as the use of an AI -based system to recommend a song . 135 During its second plenary meeting , the CAHAI heard updates from the FRA , the European Union , the OECD , the United Nations High Level Panel on Digital Co -operation and UNESCO . See the report of the second plenary meeting of the CAHAI , paragraphs 78 -84 . 136 It can be noted that , while the European Commission White Paper on AI focuses on the impact of AI on fundamental rights , it does not explicitly address AI ’ s impact on democrac y and the rule of law . 137 2011 UN Guiding Principles on Business and Human Rights : States have a duty to protect against human rights abuse within their territory and/or jurisdiction by third parties , including business enterprises . 26 rights by implementing and enforcing them in their national laws and policies , and by making sure that effective legal re medies through either judicial or non -judicial mechanisms are available at national level . Additionally , private actors , in line with the UN Guiding Principles on Business and Human Rights , have a corporate responsibility to respect human rights across the ir operations , products and services.138 92 . A number of international instruments directly focus on the need for businesses to comply with human rights and ensure responsible technological research and innovation.139 Over the past years , private actors have shown a strong interest in advancing the responsible development and use of AI systems , acknowledging not only the opportunities but also the risks raised thereby . Private actors have not only contributed to the proliferation of AI ethics guidelines , but some a lso explicitly argued in favour of a regulatory framework to enhance legal certainty in this domain.140 93 . Should a regulatory approach that combines a binding instrument with soft law tools be supported by the CAHAI , private actors , civil society organisation s , academia and other stakeholders would have an important role not only in assisting states in the development of a binding legal instrument , but also in contributing to the development of sectorial soft law instruments that can complement as well as aid in the implementation of the binding provisions in a context -specific manner ( for instance through sectorial guidelines , certifications and technical standards ) . An effective regulatory framework for AI systems will require close co -operation between all s takeholders , from states and public entities who must secure public oversight , private actors who can contribute their knowledge and secure socially beneficial AI innovation , and civil society organisations who can represent the interests of the public at large , including those underrepresented or from disadvantaged backgrounds . The CAHAI acknowledges that the Council of Europe is uniquely positioned to lead this effort and – by building further on existing frameworks – to guide the alignment of AI systems with its standards on human rights , democracy and the rule of law . 6 . MAIN CONCLUSIONS OF T HE MULTI -STAKEHOLDER CONSULTA TIONS 94 . The multi -stakeholder consultation is planned to take place in 2021 , under the aegis of the Working Group on Consultations and Outreach ( CAHAI -COG ) , which is currently working in close co -operation with the CAHAI PDG to determine the scope , the target groups and the modalities of the consultation , based on the indications previously provided by the CAHAI . The CAHAI will take a dec ision on these issues during its third plenary meeting in December 2020 . The findings of the consultation , which could feed the work of elaboration of the main elements of a legal framework that the CAHAI is mandated to develop , will be first reviewed by t he CAHAI and then presented to the Committee of Ministers as part of the reporting process of CAHAI activities . 138 See Recommendation CM/Rec ( 2020 ) 1 of the Committee of Ministers to member States on the human rights impacts of algorithmic system ) , https : //rm.coe.int/09000016809e1154 . See also Recommendation CM/Rec ( 201 6 ) 3 of the Committee of Ministers to Member States on human rights and business , at https : //rm.coe.int/human -rights -and-business recommenda tion-cm-rec-2016 -3-of-the-committe/16806f2032 . 139 Most notably the UN Guiding Principles on Business and Human Rights mentioned above , particularly principles 18 and 19 . See also the OECD Due Diligence Guidelines for Multinational Enterprises and the OECD Due Diligence Guidelines for Responsible Business Conduct . 140 Besides the statements of individual companies , such as e.g . Microsoft ( https : //blogs.microsoft.com/on -theissues/2018/07/13/facial -recognition -technology -the-need -for-public -regulation -and-corporate -responsibility/ ) or IBM ( https : //www.ibm.com/blogs/policy/ai -precision -regulation/ ) , the Policy Recommendations of the European Commission ’ s High -Level Expert Group on AI , including over 20 companies , ask to consider the adoption of new legislation , e.g . at p. 40 : “ For AI -systems deployed by the private sector that have the potential to have a significant impact on human lives , for example by interfering with an individual ’ s fundamental rights at any stage of the AI system ’ s life cycle and for safety -critical applications , consider the need to introduce : a mandatory obligation to conduct a trustworthy AI assessment ( including a fundamental rights impact assessment which also covers for example the rights of children , the rights of individuals in relat ion to the state , and t he rights of persons with disabilities ) and stakeholder consultation including consultation with relevant authorities ; traceability , auditability and ex -ante oversight requirements ; and an obligation to ensure appropriate by default and by design procedure s to enable effective and immediate redress in case of mistakes , harms and/or other rights infringement ” . It also stresses the need for legal certainty . 27 7 . MAIN ELEMENTS OF A LE GAL FRAMEWORK FOR TH E DESIGN , DEVELOPMENT AND APPL ICATION OF ARTIFICIA L INTELLIGENCE 7.1 Key values , rights and principles141 deriving - in a bottom -up perspective - from sectoral approaches and ethical guidelines ; in a top -down perspective - from the requirements of human rights , democracy and the rule of law . 95 . In line with the CAHAI ’ s mandate , a legal framework on AI should ensure that the design , development , and application of this technology is based on Council of Europe standards on human rights , democracy and the rule of law . Following a risk -based approach , it should provide an enabling regulatory setting in which beneficial AI innovation can flourish , all the while addressing the risks set out in Chapter 3 , and the substantive and procedural legal gaps identified in Chapter 5 , to ensure both its relevance and effectiveness amidst existing instruments . 96 . This can be done by formulating key principles that must be secured in the context of AI and , on that basis , identifying concrete rights that individuals can invoke ( whether existing rights , newly tailored ri ghts to the challenges and opportunities raised by AI , or further clarifications of existing rights ) as well as requirements that developers and deployers of AI systems should meet.142 The potential introduction of new rights and obligations in a future legal instrument should occur in a manner that is necessary , useful and proportionate to the goal to be achieved , namely the protection of potential adverse effects of the development and use of AI systems on human rights , democracy and the ru le of law , and in a manner that is mindful of a balance of the various legitimate interests at stake . Furthermore , where appropriate , exceptions to existing and new rights should be in accordance with the law and necessary in a democratic society in the in terests of national security , public safety or other legitimate public interests . 97 . In what follows , the main principles143 are described that are considered essential to respect in the context of AI systems , including the concrete rights and obligations attac hed thereto , and that could be potentially considered for inclusion in a future Council of Europe legal instrument on AI . While these principles , rights and requirements are described in a horizontally applicable manner , as noted above , they could be combi ned with a sector -specific approach that provides ( more detailed ) contextual requirements in the form of soft law instruments , such as sectoral guidelines or assessment lists . 1 . Human dignity 98 . Human dignity is the foundation of all human rights . It recognise s that all individuals are inherently worthy of respect by mere virtue of their status as human beings . Human dignity , as an absolute right144 , is inviolable . Hence , even when a human right is restricted – for instance when a balance of rights and interests must be made – human dignity must always be safeguarded . In the context of AI , this means that the design , development and use of AI systems must respect the dignity of the human beings interacting therewith or impacted thereby . Humans should be treated as moral subjects , and not as mere objects that are categorised , scored , predicted or manipulated . 99 . AI applications can be used to foster human dignity and empower individuals , yet their use can also challenge it and ( un ) intentionally run counter to it . To sa feguard human dignity , it is essential that human beings are aware 141 Due to the increasing speed of innovation and technology development , member States , via their Unive rsities , engineering schools or any other means , should promote , train and coach AI developers and deployers about all these principles that are related to AI ethics and regulation principles that are mentioned , among many others , in this document in order to keep up with this fast pace . 142 The list of rights impacted by the development and use of AI systems as mentioned in this chapter should by no means be considered as an exhaustive list . 143 The principles in this chapter are derived from the identified principles in the CAHAI ( 2020 ) 07 -fin report of M. Ienca and E. Vayena and from further CAHAI discussions . They are not stated in any specific order . 144 While the right to human dignity is not explicitly included in the European Convention on Human Rights , it has been reco gnised as implicitly enshrined therein by the European Court of Human Rights on multiple occasions ( see in this regard also Antoine Buyse , The Role of Human Dignity in ECHR Case -Law , October 2016 , http : //echrblog.blogspot.com/2016/10/the -role-of-human -dignity -in-echr -case.html . ) This right is also explicitly enshrined in Article 1 of the Charter of Fundamental Rights of the European Union , and i s acknowledged in the Universal Declaration of Human Rights . 28 of the fact that they are interacting with an AI system and are not misled in this regard . Moreover , they should in principle be able to choose not to interact with it , and to not be subjec t to a decision informed or made by an AI system whenever this can significantly impact their lives , especially when this can violate rights related to their human dignity . Furthermore , the allocation of certain tasks may need to be reserved for humans rat her than machines given their potential impact on human dignity . More generally , AI systems should be developed and used in a way that secures and promotes the physical and mental integrity of human beings . ❖ Key substantive rights : o The right to human dignity , the right to life ( Art . 2 ECHR ) , and the right to physical and mental integrity . o The right to be informed of the fact that one is interacting with an AI system rather than with a human being145 , in particular when the risk of confusion arises and can affect human dignity . o The right to refuse interaction with an AI system whenever this can adversely impact human dignity . ❖ Key obligations : o Member States should ensure that , where tasks risk violating human dignity if carried out by machines rather than human beings , these tasks are reserved for humans . o Member States should require AI deployers to inform human beings of the fact that they are interacting with an AI system rather than with a human being whenever confusion may arise 2 . Prevention of harm to human rights , democracy and the rule of law 100 . AI systems can be used in security and protection systems to help minimise the risk of harm to individuals , to the environment and even to other systems . At the same time , AI systems can also be used in a mann er that harms individuals , societies and the environment . The prevention of harm is a fundamental principle that should be upheld , in both the individual and collective dimension , especially when such harm concerns the negative impact on human rights , demo cracy and the rule of law . The physical and mental integrity of human beings must be adequately protected , with additional safeguards for persons and groups who are more vulnerable . Particular attention must also be paid to situations where the use of AI s ystems can cause or exacerbate adverse impacts due to asymmetries of power or information , such as between employers and employees , businesses and consumers or governments and citizens . 101 . Importantly , beyond the impact of AI systems on individuals , preventi ng harm also entails consideration of the natural environment and all living beings , and the manner in which the AI systems can have an adverse impact thereon . After all , individuals rely on a safe and healthy natural environment in order to live . Attentio n must also be given to the safety and security of AI systems , including safeguards for their technical robustness , reliability , and measures that prevent the risk of adversarial attacks or malicious uses . 102 . In light of the above , member States should ensure that adequate safeguards are put in place to minimise and prevent harm stemming from the development and use of AI , whether this concerns physical , psychological , economic , environmental , social or legal harm . The above -mentioned safeguards are particular ly important in the context of public procurement procedures as well as in the design of the electronic public procurement 145 This has also been recommended by the Council of Europe Guidelines on AI and Data Protection , https : //rm.coe.int/guidelines -on-artificial -intelligence -and-data -protection/168091f9d8 . 29 systems . When implementing measures to prevent harm , member States should pursue a risk -based approach . Moreover , where relevant give n the specific circumstances , for instance in case of a high level of uncertainty coupled with a high level of risk , a precautionary approach , including potential prohibitions , should be taken . Finally , member States could also consider the use of AI -based safeguards to minimise and prevent harm stemming from the actions of humans . ❖ Key substantive rights : o The right to life ( Art . 2 ECHR ) , and the right to physical and mental integrity . o The right to the protection of the environment , and the right to sustainability of the community and biosphere . ❖ Key obligations : o Member States should ensure that developers and deployers of AI systems take adequate measures to minimise any physical or men tal harm to individuals , society and the environment . ▪ This could , for instance , be done by ensuring that potentially harmful AI systems operate based on an opt -in instead of an opt -out model . Where this is not possible , clear instructions should be provid ed on how individuals can opt -out from the system ’ s use and on which alternative non -AI driven methods are available . o Member States should ensure the existence of adequate ( by design ) safety , security and robustness requirements and compliance therewith by developers and deployers of AI systems . ▪ These requirements should include , inter alia , resilience to attacks , accuracy and reliability , and the necessity to ensure data quality and integrity . Moreover , AI systems should be duly tested and verified prior to their use as well as throughout the entire life cycle of the AI system including by means of periodical reviews to minimise such risks . o Member States should ensure that AI systems are developed and used in a sustainable manner , with full respect for applicable environmental protection standards . o Where relevant , member States could fos ter the use AI systems to avoid and mitigate harm from the actions of human beings and of other technological systems , while safeguarding the standards of human rights , democracy and the rule of law . o Member states could also consider fostering AI solution s that protect and support human integrity , and that can help to solve environmental challenges . 30 3 . Human Freedom and Human Autonomy 103 . Human freedom and autonomy are core values which are reflected in various human rights of the ECHR . In the context of AI , they refer to the ability of humans to act self -determinedly , by deciding in an informed and autonomous manner on the use of AI systems and on the consequences thereof on themselves and others . This also includes the decisions if , when and how to use AI sy stems . As noted in Chapter 3 , human freedom and autonomy can be impacted by the use of AI in different ways , such as by AI -driven ( mass ) surveillance or targeted manipulation – whether by public or private entities – for instance through the use of remote biometric recognition or online tracking . 104 . In general , AI systems should not be used to subordinate , coerce , deceive , manipulate or condition humans , but rather to complement and augment their capabilities . Human oversight mechanisms must be established , ensuring that human intervention is possible whenever needed to safeguard human rights , democracy and the rule of law . As noted in Chapter 5 , the establishment of adequate human oversight mechanisms is not yet secured by law . The extent and frequency of ov ersight should be tailored to the specific AI application context146 and the autonomy of such human interventions should be preserved147 . It must , however , be ensured that when human intervention is required , this occurs by someone with the truly autonomous ab ility to override the system ’ s decision148 ( without hindrance of automation bias or lack of time for review ) .149 ❖ Key substantive rights : o The right to liberty and security ( Art . 5 ECHR ) . o The right to human autonomy and self -determination . The right not to be subject to a decision based solely on automated processing when this produces legal effects on or similarly significantly affects individuals.150 o The right to effectively contest and challenge decisions informed and/or made by an AI system and demand that s uch decision be reviewed by a person ( right to opt out ) . o The right to decide freely to be excluded from AI -enabled manipulation , individualised profiling and predictions , also in case of non -personal data processing . o The right to have the opportunity , when it is not excluded by competing legitimate overriding grounds , to choose to have contact with a human being rather than a robot . ❖ Key obligations : 146 See in this regard the distinction made between a human -on-the-loop ( HOL ) , human -in-the-loop ( HIL ) and human -incommand approach ( HIC ) in the Ethics Guidelines for Trustworthy AI at p. 16 , accessible at : https : //ec.europa.eu/newsroom/dae/docu ment.cfm ? doc_id=60419 . 147 E.g . by ensuring – where appropriate and feasible – that the person , who intervenes should not know the decision taken by the machine . 148 Regarding the overreliance on the solutions provided by AI applications and fears of chall enging decisions suggested by AI applications , which risk altering the autonomy of human intervention in decision -making processes see also the T-PD Guidelines on AI and Data Protection ( T-PD ( 2019 ) 01 ) where it is said that “ The role of human intervention i n decision -making processes and the freedom of human decision makers not to rely on the result of the recommendations provided using AI should therefore be preserved. ” 149 Care must be taken that to ensure that the ‘ human in the loop ’ does not become a moral or legal ‘ crumple zone ’ , which can be used to describe how responsibility for an action may be misattributed to a human actor who had limited control over the behaviour of an automated or autonomous system . 150 It can be noted that a similar right exi sts in Convention 108+ , but the protection it affords its less comprehensive ( Article 9 ( a ) : ‘ the right not to be subject to a decision significantly affecting him or her based solely on an automated processing of data without having his or her views taken into consideration ’ ) . For instance , it does not apply in situations falling outside the Convention ’ s scope , such as where an individuals ’ personal data has not been processed , while an AI system can also impact individuals without processing their persona l data . 31 o Any AI -enabled manipulation , individualised profiling and predictions involving the processing of personal data must comply with the obligations set out in the Council of Europe Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data . Member States should effectively implement the modernised version of the Convention ( “ Convention 108+ ” ) to better address AI -related issue . o Member States should require AI developers and deployers to establish human oversight mechanisms that safeguard human autonomy , in a manner that is tailored to the specific risks arising from the contex t in which the AI system is developed and used : ▪ An adequate level of human involvement should be ensured in the operation of AI systems , based on a contextual risk assessment taking into account the system ’ s impact on human rights , democracy and the rule of law . ▪ Whenever necessary and possible , based o n a thorough risk assessment , a qualified human being should be able to disable any AI system or change its functionality . ▪ Those developing and operating AI systems should have the adequate competences or qualifications to do so , to ensure appropriate ove rsight that enables the protection of human rights , democracy and the rule of law . ▪ To protect the physical and mental integrity of human beings , AI deployers should strive to avoid the use of ‘ attention economy ’ models that can limit human autonomy . o Member States should require AI developers and deployers to duly and timely communicate options for redress . 4 . Non -Discrimination , Gender Equality151 , Fairness and Diversity 105 . As noted in Chapter 3 , the use of AI systems can negatively impact the right to non-discrimination and the right to equal treatment and equality . Various studies have pointed to the fact that the use of these systems can perpetuate and amplify discriminatory or unjust biases and harmful stereotypes , which has an adverse impact not onl y on the individuals subjected to the technology , but on society as a whole.152 Indeed , reliance on unjustly biased AI systems could increase inequality , thereby threatening the social cohesion and equality required for a thriving democracy . 106 . While the right to non -discrimination and equality is already set forth in numerous international legal instruments , as noted in Chapter 5 , it requires contextualisation to the specific challenges raised by AI so that its protection can be secured . In particular , the inc reased prominence of proxy discrimination in the context of machine learning may raise interpretive questions about the distinction between direct and indirect discrimination or , indeed , the adequacy of this distinction as it is traditionally understood . S imilarly , there may be interpretive questions about the meaning of traditional justifiability standards for discrimination in the context of machine learning . Special attention should be given to the impact of the use of AI systems on gender 151 As defined by the Council of Europe , “ Gender equality entails equal rights for women and men , girls and boys , as well as the same visibility , empowerment , responsibility and participation , in all spheres of public and private life . It also implies equal access to and distribution of resources between women and men . ” https : //rm.coe.int/prems -093618 -gbr-gender -equality strategy -2023 -web -a5/16808 b47e1 152 See e.g . the CoE study by F. Zuiderveen Borgesius , Discrimination , artificial intelligence , and algorithmic decision -making , 2018 , at : https : //rm.coe.int/discrimination -artificial -intelligence -and-algorithmic -decision -making/1680925d73 ; Joy Buolamwini , Timnit Gebru ; Proceedings of the 1st Conference on Fairness , Accountability and Transparency , PMLR 81:77 91 , 2018 . See also CAHAI -PDG 1st meeting report , p.5 . 32 equality , give n the risk that gender -based discrimination , gender stereotypes and sexism might be ( inadvertently ) perpetuated thereby . Caution is also needed for the potential amplification of discrimination against those who are marginalised and in vulnerable situation s more generally , including discrimination based on racial , ethnic or cultural origin and racism which might be perpetuated by AI153 . The current lack of diversity among the people developing and making decisions in the AI sector is a source of concern , and diverse representation in consultative processes regarding AI system applications in sensitive areas should be encouraged . This would help prevent and mitigate adverse human rights impacts , notably in relation to equality and non -discrimination . It is equa lly important to consider duly the risk of intersectional discrimination arising from the use of AI systems154 , as well as treatment based on differentiation grounds or erroneous associations that might not be covered by Article 14 ECHR.155 ❖ Key substantive rights : o The right to non -discrimination and the right to equal treatment . ▪ The right to non -discrimination ( on the basis of the protected grounds set out in Article 14 of the ECHR and Protocol 12 to the ECHR ) , including intersectional discrimination . ▪ AI s ystems can also give rise to unjust treatment based on new types of differentiation that are not traditionally protected . 156 ▪ This right must be ensured in relation to the entire lifecycle of an AI system ( design , development , implementation and use ) , as wel l as to the human choices around the AI system ’ s use , whether used in the public or private sector . ❖ Key obligations : o Member States are obliged to ensure that the AI systems they deploy do not result in unlawful discrimination , harmful stereotypes ( including but not limited to gender stereotypes ) and wider social inequality , and should therefore apply the highest level of scrutiny when using or promoting the use of AI systems in sensitive public policy areas , including but not limited to law enforcement , justice , asylum and migration , health , social security and employment . o Member States should include non -discrimination and promotion of equality requirements in public procurement processes for AI systems , and ensure that the systems are independently audited for discriminatory effects prior to deployment . AI 153 In the case of AI natural language processors and language -based assistants , this is particularly important for minorities ' languages that can be discriminated if only the most common languages are used . 154 Intersectional discrimination takes place on the basis of several personal grounds or characteristics that operate and interact with each other at the same time in such a way as to be inseparable . Current AI systems are particularly susceptible to such discrimination as they merely look for correlations between different features . A Council of Europe legal framework should take a special interest in this issue , as intersectional discrimination is rarely covered by national discrimination law whic h tends to focus on one di scrimination ground at a time . 155 See e.g . the example in the CoE Study on AI and discrimination cited above , at p.35 : “ Suppose an AI system finds a correlation between ( i ) using a certain web browser and ( ii ) a greater willingness to pay . An online shop c ould charge higher prices to people using that browser . Such practices remain outside the scope of non -discrimination law , as a browser type is not a protected characteristic . ” 156 Some experts have suggested that , rather than expanding the list of unjust d ifferentiation grounds with new grounds , a catch -all provision could be more appropriate to fill this specific legal gap . See J. Gerards and F. Zuiderveen Borgesius , ‘ Protected grounds and the system of non -discrimination law in the context of algorithmic decision -making and artificial intelligence ’ , Nov. 2020 , https : //papers.ssrn.com/sol3/papers.cfm ? abstract_id=3723873 . 33 systems should be duly tested and verified prior to their use as well as throughout the entire life cycle of the AI system including by means of periodical audits and reviews . o Member States should impose requirements to effectively counter the potential discriminatory effects of AI systems deployed by both the public a nd private sectors and protect individuals from the negative consequences thereof . Such requirements should be proportionate to the risks involved . ▪ These requirements should cover the entire lifecycle of an AI system and should concern , inter alia , filling existing gender data gaps157 , the representativeness , quality and accuracy of data sets , the design and optimisation function of algorithms , the use of the system , and adequate testing and evaluation processes to verify and mitigate the risk of discriminati on . ▪ The transparency and auditability of AI systems must be ensured to enable the detection of discrimination throughout the lifecycle of an AI system ( see below ) . o Member States should encourage diversity and gender balance in the AI workforce and perio dic feedback from a diverse range of stakeholders . Awareness of the risk of discrimination , including new types of differentiation , and bias in the context of AI should be fostered . o Member States should encourage the deployment of AI systems where they could effectively counter existing discrimination in human and machine -based decision -making . 5 . Principle of Transparency and Explainability of AI systems 107 . Ascertaining whether an AI system impacts human rights , democracy and the rule of law can be rendered difficult or impossible when there is no transparency about whether a product or service uses an AI system , and if so , based on which criteria it operates . Further , without such information , a decision informed or taken by an AI system can not be effectively contested , nor can the system be improved or fixed when causing harm . Transparency can thus ensure the enforcement of other principles and rights , inclu ding the right to an effective remedy if they are violated , which includes the right to challenge an AI -informed decision and to seek redress . Therefore , the principles of transparency and explainability158 are essential in the context of AI , especially when a system can impact human rights , democracy or the rule of law . As noted in Chapter 5 , however , these principles are not yet adequately protected in existing legal instruments . 108 . Transparency entails that AI processes are rendered traceable , for instance by documenting or logging them , and that meaningful information is provided on the system ’ s capabilities , limitations and purpose . This information must be tailored to the context and intended audience . Me mber States should define procedures that enable the independent and effective audit of AI systems , allowing for a meaningful assessment of their impact . Those affected by a decision solely or significantly informed or made by an AI system should be notifi ed 157 This could also include the mandatory use of intersectional training data sets , the creation of intersectional benchmarks and the introduction of intersectional audits . The basis to assess whether these requirements are met can also be the results produced by the AI system , which means that access to the training , test and evaluation as such is not always necessary . This requires , however , suitable procedures to enable the meaningful review of the system ’ s results in terms of e.g . representativeness , accuracy and quality . 158 The implementation of these principles needs to occur in a manner that balances them against other legitimate interests , such as national security and intellectual property rights . 34 and promptly provided with the aforementioned information . Moreover , they should receive an explanation159 of how decisions that impact them are reached . While an explanation as to why a system has generated a particular output is not always possible,160 in such a case , the system ’ s auditability161 should be ensured . While business secrets and intellectual property rights must be respected , they must be balanced against other legitimate interests . Public authorities must be able to audit AI systems when there are sound indication of non compliance to verify compliance with existing legislation . Technical burdens of transparency and explainability must not unreasonably restrict market opportunities , especially where risks to human rights , democracy and rule of l aw are less prominent . A risk -based approach should hence be taken , and an appropriate balance should be found to prevent or minimise the risk of entrenching the biggest market players and / or crowding out and , in so doing , decreasing innovative socially beneficial research and product development . ❖ Key substantive rights : o The right to be promptly informed that a decision which produces legal effects or similarly significantly impacts an individual ’ s life is informed or made by an AI system.162 o The right to a meaningful explanation of how such AI system functions , what optimisation logic it follows , what type of data it uses , and how it affects one ’ s interests , whenever it generates legal effects or similarly impacts individuals ’ lives . The explanation must be tailored to the context , and provided in a manner that is useful and comprehensible for an individual , allowing individuals to effectively protect their rights . o The right of a user of an AI system to be assisted by a human being when an AI system is use d to interact with individuals , in particular in the context of public services . ❖ Key obligations : o Member States should require developers and deployers of AI systems to provide adequate communication : ▪ Users should be clearly informed of their right to be assisted by a human being whenever using an AI system that can impact their rights or similarly significantly affect them , particularly in the context of public services , and of how to request such assi stance . o Whenever the use of AI systems risks negatively affecting human rights , demo cracy and the rule of law , Member States should impose requirements on AI developers and deployers regarding traceability and the provision of information : 159 While different kinds of explanations might be possible , it is importa nt to ensure an explanation that is tailored to the specific context and audience . Such explanation should at least provide the necessary elements to allow an individual to understand and challenge a decision that has been informed or made by an AI system , and that affects his or her legal position or his or her life in a substantive manner . 160 It should be noted that , in some situations , a higher standard of explainability can only be obtained by reducing the system ’ s performance and accuracy . 161 This me ans that ( independent ) auditors should be able to assess and evaluate the various steps that were taken to design , develop , train and verify the system , encompassing both the system ’ s algorithms and the data -sets that were used , so as to ensure the system ’ s alignment with human rights , democracy and the rule of law . The most appropriate auditing mechanisms will depend on the context and application . 162 Exceptions to this right should be foreseen by law , to safeguard legitimate public interests such as natio nal security , where this is necessary in a democratic society and abiding by the principle of proportionality . 35 ▪ Persons with a legitimate interest ( e.g . consumers , citizens , supervisory authorities or others ) should have easy access to contextually relevant information on AI systems . ▪ This information should be comprehensible and accessible and could , inter alia , include the types of decisions or situations subject to automated processing , criteria relevant to a decision , information on the data used , a description of the method of the data collection . A description of the system ’ s potential legal or other effects sh ould be accessible for review/audit by independent bodies with necessary competences.163 ▪ Specific attention should be paid if children or other vulnerable groups are subjected to interaction with AI systems . o Member States should impose requirements on AI dev elopers and deployers regarding documentation : ▪ AI systems that can have a negative impact on human rights , democracy or the rule of law should be traceable and auditable . The data sets and processes that yield the AI system ’ s decisions , including those of data gathering , data labelling and the algorith ms used , should be documented , hence enabling the ex post auditability of the system . ▪ Qualitative and effective documentation procedures should be established . o Member States should make public and accessible all relevant information on AI systems ( inclu ding their functioning , optimisation functioning , underlying logic , type of data used ) that are used in the provision of public services , while safeguarding legitimate interests such as public security or intellectual property rights , yet securing the full respect of human rights . 6 . Data protection and the right to privacy 109 . The right to privacy is part of the right to private and family life under Article 8 of the ECHR and is afforded specific protection in the context of the automatic processing of personal data in Convention 108 . It is also fundamental to the enjoyment of o ther human rights . Thus , the design , development , training , testing , use and evaluation of AI systems that rely on the processing of personal data must fully secure a person ’ s right to respect for private and family life , including the “ right to a form of informational self -determination ” in relation to their data . Individuals should be able to exercise control over their data . Consent – while not the only legal basis to process personal data – is central in this regard . However , in order to be valid , conse nt needs to be informed , specific , freely given and unambiguous ( if not “ explicit ” when the processing concerns sensitive data ) . Situations of asymmetry of power or information can affect the freely given requirement of consent , hence implying certain limi tations to its protective function in certain situations and the need for a more appropriate legal basis for the processing in those situations . 110 . Member States should effectively implement the modernised Council of Europe Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data ( “ Convention 108+ ” ) as well as other binding international instruments on data protection and privacy . Not all AI systems process personal data . But even where AI systems are not designed to process personal data and instead rely on anonymised , anonymous , or non-personal data , the line between personal data and non -personal data is increasingly blurred . The interplay between personal and non -personal data must hence be further examined , to clo se any potential legal gaps in protection . Machine learning systems in particular can infer personal information about individuals , including 163 Without prejudice to existing rights and obligations in this regard enshrined in Convention 108 . 36 sensitive data , from anonymised or anonymous data , or even from data about other people . In this regard , special c onsideration must be given to protecting people against inferred personal data.164 111 . Finally , regardless of the benefits that the use of a particular AI system could bring , any interference with the exercise of the right to privacy in particular by a public a uthority shall be in accordance with the law , especially with potentially colliding fundamental rights , and necessary in a democratic society . To establish whether a particular infringement on this right is “ necessary in a democratic society ” , the European Court of Human Rights has clarified that “ necessary ” does not have the flexibility of such expressions as “ useful ” , “ reasonable ” , or “ desirable ” , but instead implies the existence of a “ pressing social need ” for the interference in question.165 It is for na tional authorities to make the initial assessment of the pressing social need that the use of an AI system could meet in each case , subject to review by the Court . National authorities are encouraged to consult a wide range of stakeholders in the context o f this assessment and ensure its periodic review . 164 See , for instance , S. Wachter and B. Mittelstadt , A Right to Reasonable Inferences : Re -Thinking Data Protection Law in the Age of Big Data and AI , Columbia Business Law Review , 2019 ( 2 ) . This also raises the question of what precisely can be considered as an individual ’ s “ own ” data that should be protected , and the extent to which such protection may need to encompass the raw data , the analysed data , as well as the inferences that can be drawn on the basis of ( personal ) data . 165 European Court of Human Rights , ‘ Guide on Article 8 of the Europea n Convention on Human Rights ’ ( 2020 ) , p12 . available at : https : //www.echr.coe.int/documents/guide_art_8_eng.pdf . 37 Key substantive rights and obligations : ❖ Key substantive right : o The right to respect for private and family life , and the protection of personal data ( Art . 8 ECHR ) . o The right to physical , psychological and moral integrity in light of AI -based profiling and affect recognition . o All the rights enshrined in Convention 108 and in its modernised version , and in particular with regard to AI -based profiling and location tracking . ❖ Key obligations : o Member States m ust ensure that the right to privacy and data protection are safeguarded throughout the entire lifecycle of AI systems that they deploy , or that are deployed by private actors . The processing of personal data at any stage , including data sets , of an AI sys tem ’ s lifecycle must be based on the principles set out under the Convention 108+ ( including fairness and transparency , proportionality , lawfulness of the processing , quality of data , right not to be subject to purely automated decisions and other rights o f the data subject , data security , accountability , impact assessments and privacy by design ) . o Member States should take particular measures to effectively protect individuals from AI driven mass surveillance , for instance through remote biometric recogniti on technology or other AI -enabled tracking technology , as this is not compatible with the Council of Europe ’ s standards on human rights , democracy and the rule of law . In this regard , as mentioned in Chapter 3 , where necessary and appropriate to protect h uman rights , states should consider the introduction of additional regulatory measures or other restrictions for the exceptional and controlled use of the application and , where essential , a ban or moratorium . o When procuring or implementing AI systems , mem ber States should assess and mitigate any negative impact thereof on the right to privacy and data protection as well as on the broader right to respect for private and family life , by particularly considering the proportionality of the system ’ s invasivene ss in light of the legitimate aim it should fulfil , as well as its necessity to achieve it . o Member states should consider the development and use of AI applications that can harness the beneficial use of ( personal ) data where it can contribute to the prom otion and protection of human rights , such as the right to life ( for instance in the context of AI -driven evidence based medicine ) . In doing so , they must ensure the fulfilment of all human rights , and in particular the right to privacy and data protection by ensuring full compliance with the Council of Europe Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data and effectively implementing the modernised version of the Convention ( “ Convention 108+ ” ) . o Given the importance of data in the context of AI , Member states should put in place appropriate safeguards for transborder data flows to ensure that data protection rules are not circumvented , in accordance with Convention 108 and its modernised version . 38 7 . Accountability and responsibility 112 . Persons – including p ublic and private organisations – designing , developing , deploying or evaluating AI systems must take responsibility for these systems , and they should be held accountable whenever legal norms based on the principles mentioned above are not respected or any unjust harm occurs to end -users or to others . This means that appropriate mechanisms must be put in place to ensure that AI systems , both before and after their development , deployment and use , comp ly with the Council of Europe ’ s standards on human rights , democracy and the rule of law . Member states should take appropriate measures to ensure this , for instance by imposing civil or criminal liability when the design , development and use of AI applica tions infringes human rights , or negatively affect the democratic process and the rule of law . It is essential that potential negative impacts of AI systems can be identified , assessed , documented and minimised , and that those who report on such negative impacts ( e.g . whistle -blowers ) are protected . Based on a risk -based approach , effective public oversight and control mechanisms must be guaranteed , to ensure that AI developers and deployers act in compliance with relevant legal requirements , while allowing for intervention by state authorities when it does not happen . 113 . In turn , Member states must ensure that those who might be negatively impacted by AI systems have an effective and accessible remedy against the developers or deployers of AI systems who are responsible . The availability of such remedy should be clearly communicated to them , with special attention to those who are marginalised or in vulnerable situations . Effective remedies should involve redress for any harm suffered , and may include measures under civil , administrative , or , where appropriate , criminal law . Moreover , because AI has a myriad of applications , remedies need to be tailored towards those different applications . This should include the obligation to terminate unlawful conduct and gu arantees of non -repetition , as well as the obligation to redress the damage caused , and compliance with the general rules about the sharing and reversal of the burden of proof in anti -discrimination legislation.166 166 In this regard , see §11 of ECRI ’ s General Policy Recommendation No . 7 and §§ 29 and following of its explanatory memorandum , accessible at : https : //www.coe.int/en/web/european -commission -against -racism -andintole rance/recommendation -no.7 . 39 ❖ Key substantive rights o The right to an effective remedy ( Art . 13 ECHR ) . o This should also include the right to effective and accessible remedies whenever the development or use of AI systems by private or public entities causes unjust harm or breaches an individual ’ s legally protected rights . ❖ Key obligations o Member States must ensure that effective remedies are available under respective national jurisdictions , including for civil and criminal responsibility , and that accessible redress mechanisms are put in place for individuals whose rights are negatively impacted by the development or use of AI applications . o In this regard , they could also consider the introduction of class actions in the context of harm caused by the use of AI systems and ensure that the general rules about the sharing and reversal of the burden of proof in antidiscrimination legislation are applied . o Member States should establish public oversight mechanisms for AI systems that may breach legal norms in the sphere of human rights , democracy or the rule of law . o Member States should ensure that developers and deployers of AI systems : ▪ identify , document and report on potential negative impacts of AI systems on human rights , democracy and the rule of law ; ▪ put in place adequate mitigation measures to ensure responsibility and acco untability for any caused harm . o Member States should put in place measures to ensure that public authorities are always able to audit AI systems used by private actors167 , so as to assess their compliance with existing legislation and to hold private actors accountable . 8 . Democracy 114 . In order to properly address the risks to democracy highlighted in Chapter 3 , effective , transparent and inclusive democratic oversight mechanisms are needed to ensure that the democratic decision -making processes and the related values of pluralism , acces s to information , and autonomy are safeguarded in the context of AI , as well as economic and social rights that may be negatively affected thereby . 115 . Where relevant and reasonably possible , member States should ensure a meaningful participatory approach and the involvement of different stakeholders ( from civil society , the private sector , academia and the media ) in the decision -making processes concerning the deployment of AI systems in the public sector , with special attention to the inclusion of under -repr esented and vulnerable individuals and groups , which is key to ensuring trust in the technology and its acceptance by all stakeholders . 116 . The use of AI systems can also influence electoral processes negatively by inter alia reinforcing information disorder ( e.g . through dissemination of disinformation and misleading content , as well as coordinated inauthentic behaviour ) which can affect the principles of free and fair elections and unlawfully interference in 167 As was already noted above , while business secrets and intellectual property rights must be respected , they must be balanced against other legitimate interests . 40 the equality of opportunities and the freedom of vo ters to form an opinion . It is crucial to ensure that electoral processes are in conformity with Council of Europe and other applicable international standards . 117 . The use of AI systems can render public institutions more efficient , yet at the potential cost of less transparency , human agency and oversight . Furthermore , public authorities often depend on private actors to procure and deploy AI -systems , which creates a risk of further eroding public trust , as it exacerbates the challenges of accountability , independent oversight and public scrutiny that can be amplified by the use of non -transparent AI systems . An appropriate governance framework should hence enable AI developers and deployers to act responsibly and in compliance with relevant legal requirements , while allowing for proper remedies and intervention by state authorities when this does not happen . 118 . Including criteria such as equality , fairness , accountabil ity and transparency in AI -related public procurement processes is key168 , and introducing legal safeguards to this end can serve two purposes . Firstly , it ensures that governments strictly only use systems that are compatible with human rights , democracy an d the rule of law . Secondly , it also creates economic incentives for the private sector to develop and use systems that comply with these standards . Since the use of AI systems in public services should be held to higher standards of transparency and accou ntability , public authorities should not acquire AI systems from third parties that do not comply with legal information obligations as regards their AI systems , or are unwilling to waive information restrictions ( e.g . confidentiality or trade secrets ) whe re such restrictions impede the process of ( i ) carrying out human rights impact assessments ( including carrying out external research/review169 ) and ( ii ) making these assessments available to the public . ❖ Key substantive rights o The right to freedom of expression , freedom of assembly and association ( Art . 10 and 11 ECHR ) . o The right to vote and to be elected , the right to free and fair elections , and in particular universal , equal and free suffrage , including equality of opportunities and the freedom of voters to form an opinion . In this regard , individuals should not to be subjected to any deception or manipulation . o The right to ( diverse ) information , free discourse and access to plurality of ideas and perspectives . o The right to good governance . ❖ Key obligations o Member States should take adequate measures to counter the use or misuse of AI systems for unlawful interference in electoral processes , for personalised political targeting without adequate transparency , responsibility and accountability mecha nisms , or more generally for shaping voters ’ political behaviours or to manipulate public opinion in a manner that can breach legal norms safeguarding human rights , democracy and the rule of law . o Member States should adopt strategies and put in place measu res for fighting disinformation and identifying online hate speech to ensure fair informational plurality . 168 This has also been recommended by the Council of Europe Guidelines on AI and Data Protection ( Section III ) , https : //rm.coe.int/guidelines -on-artificial -intelligence -and-data -protection/168091f9d8 . 169 With particular focus on the system ’ s impact on marginalised communities . 41 o Member States should subject the public procurement of AI systems to adequate oversight mechanisms : ▪ Member States should subject their public procurement processes to legally binding requirements that ensure the responsible use of AI in the public sector by safeguarding compliance with the above -mentioned principles , including transparency , fairness , responsibility and accountability . o Member Sta tes should subject the use of AI systems in the public sector to adequate oversight mechanisms : ▪ This can also include providing redress to ombudspersons and the courts . ▪ Member states should also secure oversight over how AI systems are being used in indiv idual public sector organisations and intervene and coordinate where appropriate to safeguard their alignment with human rights , democracy and the rule of law . ▪ Member States should ensure that , when the public sector is utilising AI systems , this happens w ith the involvement of people with appropriate competences from a wide range of fields , including also public administration and political science , to ensure that there is a thorough understanding of the potential implications for the governance of the adm inistrative state and the citizen -state relationship . o Member States should make public and accessible all relevant information on AI systems ( including their functioning , optimisation functioning , underlying logic , type of data used ) that are used in the provision of public services , while safeguarding legitimate interests such as public security . o Member States should put in place measures to increase digital literacy and skills in all segments of the population . Their educational curricula should adjust t o promote a culture of responsible innovations that respects human rights , democracy and the rule of law . o Member States should foster the use of AI solutions and other tools that can : ▪ strengthen the informational autonomy of citizens , improve the way they collect information about political processes and help them participate therein ; ▪ help fight corruption and economic crime , and that enhance the legitimacy and functioning of democratic institutions . This can contribute to the positive impact of AI systems within the democratic sphere and enhance trust ; ▪ help in the provision of public services . ▪ In doing so , they should always safeguard respect for human rights , democracy and the rule of law . 9 . Rule of Law 119 . The use of AI systems can increase the efficiency of judicial systems , but as noted in Chapter 3 , it can also create significant challenges for the rule of law . According to the European Ethical Charter on the use of AI in the judicial systems and their environment170 , when AI tools are used to resolve a dispu te , or when they are used 170 The analysis of the CEPEJ concerns the challenges arising from the use of AI systems also in the field of online dispute resolution and law enforcement . 42 as tool to assist in judicial decision making or to give guidance to the public , it is essential to ensure that they do not undermine the guarantees of the right of access to a judge and the right to a fair trial . 120 . In particular , this means that the principle of equality of arms and respect for the adversarial process must be safeguarded . Moreover , the use of AI systems should not undermine judicial independence and impartiality . To ensure this , the CEPEJ has underli ned the importance of securing the quality and security of judicial decisions and data , as well as the transparency , impartiality and fairness of data processing methods . In addition , safeguards for the accessibility and explainability of data processing m ethods , including the possibility of external audits , must likewise be introduced . Member States should therefore subject the use of AI systems within the judicial system to thorough checks and ensure their compliance with all the above principles . 121 . Whenev er a legal dispute arises in the context of the use of an AI system – whether by the private or public sector – persons who file a claim related to a violation or harm caused through the use of an AI system must have access to relevant information in the p ossession of the defendant or a third party that is necessary to allow for an effective remedy . Access to relevant information by parties in judicial proceeding is also critical when AI systems have been used to support judicial decision -making , as this re presents an important condition for preserving the equality of arms between the parties . This may include , where relevant , training and testing data , information on how the AI system was used , meaningful and understandable information on how the AI system reached a recommendation , decision or prediction , and details of how the AI system ’ s outputs were interpreted and acted on . In this regard , a fair balance must be sought between the various legitimate interests of the parties involved , which can include co nsiderations of national security in case of a publicly used AI systems , for instance , as well as intellectual property and other rights , all the while ensuring the full protection of human rights . Moreover , individuals who seek redress for alleged violati ons of human rights in the context of AI systems should not be held to a higher standard of proof171 . 171 Recall also that general rules about the sharing and reversal of the burden of proof in the anti -discrimination legislation should in principle apply in such cases . 43 ❖ Key substantive rights o The right to a fair trial and due process ( Art . 6 ECHR ) . This should also include the possibility to get insight into and challenge a n AI-informed decision in the context of law enforcement or justice , including the right to review of such decision by a human . o The right to judicial independence and impartiality , and the right to legal assistance . o The right to an effective remedy ( Art . 1 3 ECHR ) , also in case of unlawful harm or breach an individual ’ s human rights in the context of AI systems . ❖ Key obligations o Member States must ensure that AI systems used in the field of justice and law enforcement are in line with the essential requirements of the right to a fair trial . To this end , they should pay due regard to the need to ensure the quality and security of j udicial decisions and data , as well as the transparency , impartiality and fairness of data processing methods . Safeguards for the accessibility and explainability of data processing methods , including the possibility of external audits , should be introduce d to this end . o Member States must ensure that effective remedies are available and that accessible redress mechanisms are put in place for individuals whose rights are violated through the development or use of AI systems in contexts relevant to the rule of law . o Member States should provide meaningful information to individuals on the use of AI systems in the public sector whenever this can significantly impact individuals ’ lives . Such information must especially be provided when AI systems are used in the field of justi ce and law enforcement , both as concerns the role of AI systems within the process , and the right to challenge the decisions informed or made thereby . o Member States should ensure that use of AI systems does not interfere with the decision making power of j udges or judicial independence and that any judicial decision is submitted to human oversight . 7.2 Role and responsibilities of member States and private actors in the development of applications complying with these requirements 122 . AI systems can affect human rights , democracy and the rule of law when being developed and used by private and public actors alike . As noted in Chapter 5 , in addition to an obligation to protect human rights in the public sphere , member States may also hav e the positive obligation to ensure that private actors respect human rights standards . Moreover , several international frameworks also stipulate that private actors must respect human rights ( such as the UN Guiding Principles on Business and Human Rights ) . 123 . In the section above , the obligations of member States to ensure conformity with the Council of Europe ’ s standards on human rights , democracy and the rule of law in the context of AI systems were already pointed out . More generally , national authoritie s should carry out evidence -based assessments of domestic legislation to verify its compliance with – and ability to protect – human rights and adopt measures to fill potential legal gaps . Moreover , they should establish control mechanisms and ensure effec tive judicial remedies for redress whenever the development and use of AI leads to violations of law . To this end , national oversight authorities 44 should be able to audit and assess the functioning of ( public or private ) AI systems , particularly when indica tions of non -compliance exist . Such oversight should complement oversight obligations in the context of existing legislation , including data protection law ( the accountability principle , impact assessment172 , prior consultation with supervisory authorities , etc ) to increase transparency . There may be limited circumstances where , due to concerns around privacy or intellectual property , a certain degree of confidentiality may need to be maintained . 124 . It should be noted that many public actors procure AI systems from private actors . They hence rely on private actors to obtain relevant data to deploy AI systems , and to access the underlying infrastructure on which AI systems can operate . Accordingly , given their essential role in this field , private actors have a r esponsibility to ensure that their systems are developed and used in line with the above principles , rights and requirements . As the interests of commercial private actors and of individuals and society are not always aligned , a legal structure that would oblige private actors to comply with specific rights and requirements in the context of AI may be appropriate , especially when the risk exists that private actors and individual interests are divergent . Moreover , this would secure access to justice should they fail to meet these obligations.173 125 . As noted above , when member States take measures to safeguard the listed principles , rights and requirements in the context of AI , a risk -based approach – complemented with a precautionary approach where needed – is recommended . Such approach acknowledges that not all AI systems pose an equally high level of risk , and that regulatory measures should take this into account . Moreover , it requires that the risks posed by AI systems to human rights , democracy and the rule of law , are assessed on a systematic basis and that mitigating measures are specifically tailored thereto . 126 . When implementing a risk -based approach and assessing the type of regulatory intervention needed to mitigate risks , member States can be guided by a number of factors that are commonly used in risk -impact assessments . These risk -factors include , for instance , the potential extent of the adverse effects on human rights , democracy and the rule of law ; the likelihood or probability that an adverse impact occurs ; the scale and ubiquity of such impact ; its geographical reach ; its temporal extension ; and the extent to which the potential adverse effects are reversible . In addition , a number of AI -specific factors that can influence the risk level ( such as th e application ’ s level of automation , the underlying AI technique , the availability of testing mechanisms , the level of opacity ) can also be considered . 7.3 Liability for damage caused by artificial intelligence 127 . The development and use of AI systems raises new challenges in terms of safety and liability . Views differ , however , as to whether existing liability regimes should apply , or whether specific regimes should be developed for the context of AI . Nevertheless , it can be noted that the widespread use of A I systems may raise some challenges to the interpretation and implementation of existing liability legislation . For example , the Council of Europe Convention on Products Liability with regard to Personal Injury and Death ( ETS No . 91 – not yet in force ) 174 only applies to AI systems that are considered to be movable products ( hardware ) rather than software , and only applies to AI systems offered as a product rather than a service . Therefore , a clarification that stand -alone software can be qualified as a produ ct within the meaning of existing product liability law might be advisable . The opacity of some AI systems , coupled with the asymmetry of information between AI developers and producers on the one hand and individuals who may be negatively impacted by AI s ystems on the other hand , may in certain cases make it difficult for the latter to meet the standard of proof required to 172 As further specified in Chapter 9 . See in this regard also the FRA study which outlines the need for human right impact assessments , “ Getting the Future Right – Artificial Intelligence and Fundamental Rights in the EU ” , 14 December 2020 , https : //fra.europa.eu/en/publication/2020/artificial -intelligence -and-fundamental -rights 173 C. Muller , p. 16 ; this means going beyond merely referring to the Recommendation CM/Rec ( 2016 ) 3 on human rights and business of the Committee of Ministers of the Council of Europe ( and the UN Guiding Principles on Business and Human Rights . 174 The same is true for its EU counterpart , the Product Liability Directive ( 85/374/EEC ) , which is one of the reasons why the European Commission ’ s White Pa per on AI includes attention for the need to address issues around AI and liability . 45 support a claim for damages . However , in general , the existing assignment of the burden of proof can bring about appropriate and reaso nable solutions with regard to AI systems . 128 . If the Committee of Ministers decides to address the question of liability in a future legal framework at the level of the Council of Europe , the CAHAI recommends that the following aspects be considered :  A proper and balanced liability regime is important for both consumers and manufacturers , and can contribute to legal certainty .  It is essential to guarantee the same level of protection to persons harmed through the use of an AI system as those harmed through the use of traditional technologies .  Liability for any unjust harm should be able to arise from any unjust harm occurring throughout the entire life cycle of the AI system .  A distinction may need to be drawn as regards the allocation of liability in business -to-consumer contexts and business -to-business contexts . Liability among business agents could , for instance , be more suitable to address through contractual stipulations rather than through the adoption of a specific liability regime .  The issue of trans -border responsibility should be taken into account . This is particularly relevant when , for instance , a company using an AI system is registered in one state , the developer of that system in another state , and a user that suffers harm habitually resides in a third state .  The rules for liability may be supplemented , in some sector specific applications , by industry ( voluntary ) ethical codes of conduct which would serve the purpose of enhancing public trust in sensitive areas of AI .  The extent to which priva te actors ensure and invest in due diligence mechanisms can be a relevant factor when considering the liability of private actors and the burden of proof.175 8 . POSSIBLE OPTIONS FOR A COUNCIL OF EUROPE LEGAL FRAMEWOR K FOR THE DESIGN , DEVELOPMENT AND APPL ICATION OF ARTIFICIA L INTELLIGENCE BASED O N HUMAN RIGHTS , DEMOCRACY AND THE RU LE OF LAW 129 . In order to fill the gaps in legal protection identified in Chapter 5 , a number of different options for a legal framework are available within the Council of Europe , including binding and non -binding legal instruments . These instruments , and their advanta ges and disadvantages , are outlined below . Whereas the previous chapter focused on the substance of the legal framework , this chapter focuses on its format . 8.1 Modernising existing binding legal instruments 130 . A first option that could be considered is to am end existing binding legal instruments , to complement and/or adapt them in light of the particularities of AI systems . 131 . An additional protocol to the ECHR could be adopted to enshrine new or adapt existing human rights in relation to AI systems.176 These cou ld be drawn from Chapter 7 above177 . It is not unlikely that , under the dynamic and evolutive interpretation adopted by the ECtHR , existing ECHR rights , such as the right to private life , freedom of thought and of expression , and the right to non -discriminat ion could be interpreted so as to include some of the aforementioned rights . The advantage of an additional protocol , however , would be that the recognition of certain rights in relation to AI would not depend on the submission of a relevant case to the Eu ropean Court of 175 Given the ongoing work at the European Union regarding a potential mandatory EU system of due diligence for companies , it could be useful to ensure alignment therewith if this point were to be considered . See the European Commission ’ s Study on due diligence requirements through the supply chain of January 2020 , available at : https : //op.europa.eu/en/publication -detail/ -/publication/8ba0a8fd -4c83 -11ea -b8b7 -01aa75ed71a1/language -en 176 This would happen in close cooperation with relevant steering committees and in particular the Steering Committee for Huma n Rights ( CDDH ) , 177 See also CAHAI ( 2020 ) 06 -fin , §77 . 46 Human Rights ( ECtHR ) , and hence , would offer more clarity and legal certainty ( also avoiding possible criticism of the ECtHR for interpreting Convention rights too expansively ) . While the adoption of an additional protocol to the ECHR would affirm , in the strongest possible manner , the member States ’ commitment to protecting key substantive human rights , the rule of law and democracy , in relation to AI -systems , it would not be an appropriate instrument to lay down specific requirements or ob ligations178 . It should also be noted that additional protocols to the ECHR are binding only upon those States that ratify them , which may result in only some member States being bound and a fragmentary oversight by the ECtHR . Moreover , the ECtHR is already overburdened with its current caseload and should therefore not be burdened with additional issues , the decision on which requires technical knowledge not necessarily available there . 132 . Modernising existing instruments , such as the Budapest Convention on Cyb ercrime ( CETS No.185 ) or “ Convention 108+ ” , could be another possibility . An important adva ntage of this approach – compared to drafting a new convention ( see below ) – is that existing networks for monitoring and enforcement ( like in the case of Convention 108+ the national data protection independent authorities , whose scope of regulatory activ ities could be expanded to artificial intelligence ) could be mobilised . The drawback of this approach , however , in addition to the length and complexity of adoption , lies with the limited scope of the existing instruments , which necessitates multiple inter ventions in order to tackle the various concerns discussed in previous chapters . Modernising “ Convention 108+ ” , for example , would not capture all concerns in relation to AI systems , given its ( current ) specific focus on the protection of individuals , and the processing of personal data ; at the same time , it should be noted that many of the high level principles so far identified to face the challenges raised by AI systems ( e.g . accountability , transparency , automated decisions ) are to some extent already i ncluded in Convention 108+.179 Moreover , since “ Convention 108+ ” was concluded in 2018 , it might be difficult to modernise it again in the short term180 . 133 . The two concerns expressed for each option could be addressed by combining both ideas , i.e . of an additio nal protocol to the ECHR with modernising ( certain ) existing instruments , like “ Convention 108+ ” . Whereas the first would lay down overarching principles and values , the latter could further elaborate States ’ obligations and establish an effective network of independent competent authorities to ensure the effective implementation of those safeguards . These authorities could deal with acts or omissions of States as regards AI systems and engage the State ’ s responsibility under the Convention under some circu mstances . The lengthy character of a combined process remains however an issue , against the background of the fast -paced rollout of AI systems . 8.2 Adoption of a new binding legal instrument : Convention or Framework Convention 134 . A second option to be considered is the adoption of a new and separate binding legal instrument , which could take the form of a convention or framework convention . It is worth noting that both conventions and framework conventions are multilateral treaties , they have the same legal nature and are both subject to the usual rules for international treaties as set out in the Vienna Convention on the Law of Treaties ( 1969 ) . Moreover , both may include a system of governance ( see for more details Chapter 9 . 4 ) and can be complemented by additiona l protocols . The difference between the two is that a convention regulates a specific matter or area in a more concrete way , typically by creating rights and obligations , whereas a framework convention rather sets out broader principles and areas for actio n which have been agreed between States Parties . 135 . A framework convention typically only foresees a general duty for State Parties to undertake certain actions , achieve certain objectives , or to recognise certain rights . without attributing such rights dire ctly to natural or 178 Such as those mentioned under Chapter 7 of this feasibility study . 179 There is a need to take into account the regulatory developments in other international fora , such as the EU , as the limitations of the EU General Data Protection Regulation ( which are parallel to the limitations of Convention 108+ ) in the context of AI sy stems , have led the EU to propose new regulation in this field . A regulatory proposal is expected in Q1 2021 . 180 It is also noted in this respect that entry into force of amendments or of an amending protocol requires the acceptance , approval or ratifica tion of all or a certain number of the Parties to the Convention , which may be a lengthy process . 47 legal persons . Hence , the national ratification of a framework convention would not suffice for natural and legal persons to be able to invoke certain rights , and additional legislative action by the individual states would be needed . Th ere is consequently a considerable margin of discretion for States as to how they implement the broader principles and objectives . 136 . A convention could more comprehensively regulate the design , development and application of AI systems or of algorithmic deci sion making in general , building further on this Feasibility Study and on Recommendation CM/Rec ( 2020 ) 1.181 It could list certain rights and obligations that could help safeguard the protection of human rights , democracy and the rule of law in the context of AI systems , and thereby offer legal protection to both natural and legal persons once it has entered into force . It could stress the importance of a speedy accession by the maximum number of Parties to facilitate the formation of a comprehensive legal regi me for AI systems as specified under the convention , and it would urge member States and other Parties to the convention to initiate the process under their national law leading to ratification , approval or acceptance of the Convention . It is worth noting that , in October 2020 , the Parliamentary Assembly of the Council of Europe ( PACE ) recommended “ that the Committee of Ministers support the elaboration of a “ legally binding instrument ” governing artificial intelligence , possibly in the form of a convention ” . PACE further recommended that the Committee of Ministers ensures that “ such a legally binding instrument is based on a comprehensive approach , deals with the whole life cycle of AI -based systems , is addressed to all stakeholders , and includes mechanisms to ensure ” its implementation.182 137 . The added value would be to get a specific legally binding instrument on the design , development and application of AI based on the Council of Europe ’ s standards on human rights , rule of law and democracy . It would harmoni se rules and obligations across states , and thereby also enhance trust in cross -border AI products and services , in light of agreement regarding the manner in which AI systems should be designed , developed and applied . Successful examples of such innovativ e legal frameworks developed by the Council of Europe in the past include the Convention for the Protection of Individuals with regard to Automatic Processing of Perso nal Data ( CETS No . 108 ) , and the Budapest Convention on Cybercrime ( CETS No.185 ) . 138 . At the same time , it may be considered premature to attempt to draft a convention containing detailed legal obligations in relation to AI systems . An overly prescriptive and rigid approach could lead to a rejection of the instrument and a lack of willingne ss on the part of States to sign , express their consent to be bound by and actually implement the convention in practice . Conversely , wide approval of a convention containing overly rigid rules could stymie innovation and curtail research into the develop ment and deployment of new technologies and cutting -edge solutions to existing problems , many of which could save lives and benefit society as a whole . 139 . However , it is important to note that a concrete set of internationally binding rules would provide leg al certainty to State and private actors alike , while strongly protecting the rights of individuals and establishing clear principles of responsibility between the actors involved in the use of AI systems . Furthermore , the latter concerns could be addresse d by ensuring that the rights and obligations that are set out in the convention are not overly prescriptive or detailed . Alternatively , it could also be addressed by adopting a framework convention on AI , which would provide for broad core principles and values to be respected as regards the design , development and application of AI to be enshrined in a binding instrument , in line with Council of Europe ’ s standards and leave a broad margin of discretion to States parties in their respective implementation . Under the so -called “ framework convention and protocol approach ” , parties could agree on a more general treaty – a framework convention – and when in the future they wish to do so , decide to elaborate more detailed protocols 181 Council of Europe , Committee of Ministers , Recommendation CM/Rec ( 2020 ) 1 of the Committee of Ministers to member States on the human rights impacts of algo rithmic systems , April 2020 . 182 See Recommendation 2181 ( 2020 ) and Resolution 2341 ( 2020 ) , “ Need for democratic governance of artificial intelligence ” , available at : https : //pace.coe.int/en/news/8059/establishing -a-legally -binding -instrument -for-democratic -governance -ofai . 48 or other instruments to enact specific provisions . This regulatory technique , which has a number of benefits compared to single “ piecemeal ” treaties in international law , could be particularly appropriate in the field of AI . In this context , it should be carefully considered whether su ch a structure based on a more general treaty and the possible elaboration of additional specific instruments ( such as protocols ) would increase the complexity of the resulting legal framework and make compliance more challenging.183 140 . A framework convention would require the states to agree mutually on the scope of the legal framework and the procedure to be complied with to offer effective safeguards in the design , development and application of AI systems , based on Council of Europe ’ s standards . It could c ontain the commonly agreed upon core principles and rules for AI research , development and implementation , in the interests of human society . It could also contain specific rules on procedural safeguards , preventive measures , jurisdiction , international co -operation . For instance , it could include provisions to allow for the exchange of information , or for already existing independent competent authorities like the ones dedicated to data protection or competition supervision at the national level to be mobi lised . A framework convention could also set forth the rules and procedures necessary for States to implement it . 141 . An existing example of such a framework convention at Council of Europe level is the Framework Convention for the protection of national minor ities ( FCNM ) . The FCNM is a legally binding instrument under international law and provides for a monitoring system , but the word “ framework ” highlights the scope for member States to translate the Conv ention ’ s provisions to their specific country situation through national legislation and appropriate governmental policies . Another example , albeit not officially carrying the term `` framework '' in its title , is the Convention for the Protection of Human Rig hts and Dignity of the Human Being with regard to the Application of Biology and Medicine , in short , the Convention on Human Rights and Biomedicine ( the so -called `` Oviedo Convention `` ) . This Convention was adopted in 1997 to respond to potential misuses of scientific advances in the field of biology and medicine . It draws on the principles established by the ECHR and aims at protecting the dignity and identity of all human beings . It sets out fundamental principles applicable to daily medical practice and also deals specifically with biomedical research , genetics and transplantation of organ and tissues . It is further elaborated and complemented by additional protocols on specific subjects , for instance , on the prohibition of cloning human beings . 142 . Irrespective of the choice of a convention or framework convention , the addressees would be States and state bodies , in particular the Council of Europe ’ s member States , but also possibly other St ates . This would of course constitute considerable added value and could significantly contribute to the global reach and effectiveness of the instrument184 . The strength of treaties lies in their formality and the fact that they are legally binding on those States which have expressed their consent to be bound by them . States becoming parties to a convention incur legal obligations which are enforceable under international law . 143 . The foregoing does not rule out the important role that other stakeholders could play in the implementation of specific regulations implemented at the national level on the basis of broad international commitments . In particular , they should take up a prom inent role in the design of co -regulatory mechanisms by which States would , in close interaction with all stakeholders , give further shape to their international commitments . 144 . Although no prediction could be made as to how long it would take to negotiate a nd agree upon a convention , and for it to enter into force ( this could range from a couple of months to several years , depending on the nature and complexity of the subject matter , but also on the political will of member States ) , a potential drawback of this option lies with the process of entering into force of an international treaty . There is no legal obligation for 183 In this regard , it should be considered that any additional binding Council of Europe instruments , such as proto cols to a convention , would also need to be ratified . 184 In general , non -member States of the Council of Europe can accede to Council of Europe conventions through an invitation by the Committee of Ministers and after having obtained the unanimous agreemen t of the Parties to the Convention . Some of these conventions have become global standards . For instance , “ Convention 108 ” counts 55 States parties , whereas the “ Budapest Convention ” has 64 States parties . 49 member States to ratify , approve or accept a new convention even if they have voted in favour of its adoption or have signed it , and there is no way of obliging member States to speed up their internal procedures for its ratification , acceptance or approval . There is therefore no way to guarantee that any or all member States will express their consent to be bound by a convention . Furthermore , without building broader buy -in from international actors ( e.g . observer States ) , even the option of a convention or framework convention would risk creating divergence and , hence , the fragmentation of international regulatory approaches . 8.3 Non -binding legal instruments 1 . Council of Europe 145 . A distinction should be made between non -binding ( or soft law ) instruments at the level of the Council of Europe and at the national level . The former are already relied upon in several sectors ( cf . Chapter 4 ) , but could be complemented with a general soft law instrument , such as a recommendation or declaration , that consolidates common principles . Such a soft law instrument could operate as a stand -alone document or complement a binding instrument to further operationalise its provisions . Other options inc lude the drafting of guidance documents to increase the understanding of the relationship between the protection of human rights , democracy and rule of law , and AI ( e.g . by providing information about case law of the ECtHR ) , and to thereby contribute to st rengthening protection at national level . Such ‘ manuals ’ or ‘ guides ’ could be developed through broad multi -stakeholder consultation with governments , private companies , civil society organisations and representatives of the technical community and academi a . These documents should be evolving , updated periodically , and fleshed out collaboratively in light of new developments . Precedents include the Manual on Human Rights and the Environment and the Guide to Human Rights for Internet Users . 2 . Member State level 146 . Soft law mechanisms approved by national competent authorities could be encouraged by a Council of Europe legal framework , to further operationalise it and demonstrate compliance . These soft law instruments could consist of approved guidelines , codes of conduct , labelling mechanisms , marks and seals , as well as certification mechanisms . Whereas soft -law measures can not , due to their non -binding character , meet the objectives of ensuring that A I applications protect and advance human rights , democracy and the rule of law , they can make important contributions thereto . The advantages of a soft law approach include flexibility , adaptability , immediacy of implementation , broader appeal , and capacit y to be reviewed and amended quickly 147 . Private actors , including academic institutions and standard -setting bodies , can help ensure that such soft -law instruments are practically effective . Organisations developing and deploying AI can incorporate soft -law instruments into their governance structure , procurement process , operation , and auditing practices ( as they already do with many standards and certifications related , for example , to security ) . In addition , rating agencies could potentially also play a rol e , for instance by providing an annual ranking of private organisations complying with soft -law requirements based on sound evidence . 148 . However , it should be stressed that while self -regulation might be a complementary method of implementing certain princip les and rules , it can not substitute the positive obligations that member States have under the ECHR to effectively protect and safeguard human rights , democracy and the rule of law in relation to AI . Voluntary , self -regulatory and ethics -based approaches lack effective mechanisms of enforcement , responsibility and accountability , and should therefore , on their own , not be considered as a sufficient and effective means to regulate AI that has an impact on human rights , democracy or the rule of law . Moreover , certification mechanisms are not immune to errors and mistakes . Hence , for these mechanisms to be effective , a number of conditions should be fulfilled . 8.4 Other type of support to member States such as identification of best practices 149 . There are numerous ways in which best practices can be identified or encouraged ( many of which are familiar to or implemented by member States or economic actors ) . For example , amongst other options , a European 50 Benchmarking Institute could be a highly effe ctive , efficient , and trustworthy source of identification , definition , and consensus around the underlying evidence that should guide sound best practices.185 Such evidence can , in turn , serve as the basis for a wide range of best practices that can be effi ciently and effectively propagated by sound technical standards and certifications . The added value of such an Institute in respect of other standard setting organisations such as the ISO and the IEC would have nonetheless to be carefully considered . Coope ration with standard -setting organisations can be fostered more generally . 150 . In addition , a uniform model or tool developed at the level of the Council of Europe for a human rights , democracy and rule of law impact assessment could be extremely helpful in ha rmonising member States ’ implementation of legal standards and common values in relation to AI systems . As concerns practical mechanisms that can help to both implement and enforce the legal framework , we refer to Chapter 9 of this feasibility study . 8.5 . Possible complementarity between the horizontal and cross -cutting elements that could form part of a conventional -type instrument and the vertical and sectoral work that could give rise to specific instruments of a different nature . 151 . Chapter 4 has described the Council of Europe ’ s sectorial work on AI systems , which is expected to develop further in the coming years and which is feeding , in a complementary way , the horizontal , cross -cutting dimension of the work of the CAHAI . The horizontal elements which co uld form part of a convention -type instrument would help finetune sectorial work and provide impetus to the development of specific instruments in areas where the analysis of the impact of AI systems and of the required policy responses is advancing . A potential horizontal binding legal instrument could include explicit references to the existing or future instruments in the different areas of work of the Council of Europe . 152 . Another mechanism which could potentially be considered to ensure complementarity c ould be the setting up of a joint ( voluntary or mandatory ) certification scheme/body , comparable to the one existing in the pharmaceutical sector ( the Council of Europe European Directorate for the Quality of Medicines ( EDQM ) and HealthCare and its Pharmacopoeia ) . Such joint certification mechanism/body could , for instance , be tasked with providing more detailed guidelines regarding human rights , democracy and rule of law impact assessments and c ommon quality standards at European level . Moreover , it could be responsible for supporting the implementation and monitoring the application of quality standards for AI systems ( which voluntarily or mandatorily adhere to the certification scheme ) , just li ke EDQM does for safe medicines and their safe use . 153 . In conclusion , given the evolving nature and the challenges posed by AI , an appropriate legal framework would likely need to consist of a combination of binding and non -binding legal instruments , that complement one another . A binding horizontal instrument , i.e . a convention or framework convention , could consolidate general common principles that would be contextualised to apply to the risk inherent AI environment and include more concrete provisions to safeguard the rights , principles and obligations iden tified in Chapter 7 . It could serve as a basis for relevant national legislation in this field , in a harmonised manner , and can foster best practices for AI regulation more generally . This instrument , which could include appropriate follow -up mechanisms an d processes , could be combined with additional ( binding or non -binding ) sectoral Council of Europe instruments establishing further sector specific principles and detailed requirements on how to address specific sectoral challenges of AI . This combination would allow the required level of guidance to private actors who wish to undertake self -regulatory initiatives to be provided . 154 . This approach would also allow for the flexibility required for technological development , as revisions to the vertical instrume nts could be undertaken with relatively less formality and complexity . 185 Reference can , for instance , be made to the work that is being undertaken in this field by the US National Institute of Standards and Technology ( NIST ) , e.g . at : https : //www.nist.gov/speech -testimony/facial -recognition -technology -frt-0 . 51 9 . POSSIBLE PRACTICAL AN D FOLLOW -UP MECHANISMS TO ENS URE COMPLIANCE AND E FFECTIVENESS OF THE LEGAL FRAMEWORK 9.1 The Role of Compliance Mechanisms 155 . The ultimate effectiveness of any legal framework will depend on the breadth of its adoption and compliance . Practical mechanisms ( such as impact assessments , lifecycle auditing , and monitoring , certification methods , and sandboxes ) are one way of driving such compliance and of helping member States to understand and monitor adherence to the legal framework . Such mechanisms confer further benefits beyond compliance , for example by increasing transparency around the use of AI and creating a common framework for promoting trust . 156 . Any Council of Europe legal framework should formulate the abstract requirement to develop compliance mechanisms at a general level as well as what principles need to be fulfilled by any practical mechanisms to ensure compliance . It w ould be for state parties to decide how to enforce this through their legislative framework , including which practical mechanisms they choose to make mandatory or which actors or institutions they empower to provide independent , expert , and effective overs ight . This would enable implementation to account for the existing roles of local institutions , regulatory culture , and legal requirements . Rather than mandating a single solution , this approach would further enable the creation of an AI assurance ecosyste m , which would create the potential for diverse participation and the emergence of novel and innovative approaches to compliance . That said , collaboration between state parties should be considered paramount to protect against the risk of diverging approac hes and the resulting fragmentation of markets . 157 . Compliance mechanisms might be used to assess the design of an AI -enabled system , as well as its operational processes , contextual implementation and use case . On the question of when AI systems that have an impact on human rights , democracy and the rule of law should be subject to such assessment , the CAHAI agreed on the fundamental importance of ex ante assessment and continuous assessment at various milestones throughout the AI project lifecycle , including after initial deployment and use . Compliance mechanisms should also evolve over time to account for the evolving nature of the system . To ensure that impact assessments can be used efficiently , particular attention should be paid to their comprehensibility and accessibility to all relevant actors . Legal safeguards should ensure that compliance mechanisms are not used by organisations to shield themselves from potential liability claims associated with their conduct . 158 . The ongoing assessment approach presents three salient advantages . First , it allows for a better understanding of the implications of any AI system ( throughout its design , development , and deployment ) . Second , it facilitates decision making to reconsider future unforeseen uses of an AI system . Th ird , it monitors changes in the behaviour of the model ex post ( which is particularly crucial in e.g . reinforcement learning contexts and dynamic learning systems ) . In particular , the procurement and use of pre -built AI -enabled solutions and technical adva ncements such as transfer learning applications presents challenges that need to be considered . 9.2 The Role of Different Actors 159 . As outlined above , each member State should ensure national regulatory compliance with any future legal framework . Different actors should contribute in a complementary way to bring about a new culture of AI applications that are compliant with the legal framework ’ s principles and local regulations to generate adequate incentives for compliance and oversight incentives , either a s assurers , developers , or operators and users . 9.2.1 Assurers of systems 160 . Member States should also be responsible for identify and empower independent actors to provide oversight . These independent actors should represent and be accountable to clearly id entified stakeholder groups affected by practical applications of AI , and could be , as appropriate , an expert committee , academics , sectoral regulators or private sector auditors . Where they do not exist already , member States might consider setting up ind ependent oversight bodies equipped with appropriate and adequate inter -disciplinary expertise , competencies , and resources to carry out their oversight function . Such bodies might be equipped with 52 intervening powers and be required to report , for instance to a national parliament or to other public bodies , and publish reports about their activities regularly186 . They might also resolve disputes on behalf of citizens or consumers . For example , states could extend the mandate of existing human rights institutions , equality bodies , ombudsmen institutions or other oversight bodies , or they can create new ones in order to assess and resolve any complaints or appeals as a complement to binding judicial mechanisms . It is unreasonable to expect that any such entity could cover all AI -based products and systems , and so consideration as to scope would be important . If new entities are created , their mandates should not overlap or enter in conflict with previously existing entities whose oversight functions woul d also include AI systems if their specific usage is part of their mandate . It is also important to acknowledge the important role of existing ( national ) human rights institutions , equality bodies187 and ombudsman institutions , whose structures will remain r elevant to provide effective oversight within their mandate on AI -related issues . 161 . Many AI systems are deployed across multiple jurisdictions . It is vital for adequate oversight to share information among the member States . Mechanisms of information sharing and reporting about AI systems could be included in each State ’ s regulatory framework ( e.g . information on certified AI systems , banned AI applications or the current status of a specific AI application ) . Private sector actors could also play a rol e in assuring systems . 162 . In addition to auditing services , ( voluntary or mandatory ) certification schemes can support a legal framework and promote an active role for the private sector to prevent and manage the risks of adverse human rights impacts associat ed with AI systems . Indeed , more generally , certification mechanisms are highly versatile and can provide evidence -based instruments upon which governance regimes can be flexibly developed to meet the needs of different domains and the allowances of nation al regulatory regimes . Standards and certifications can be developed for all stages of AI development and operations and may engage all agents involved in order to implement certain requirements . Procurement practices of intergovernmental organisations and of national public sector entities can contribute to their adoption . When duly implemented , they can help empower ordinary citizens by serving as the “ currency of trust ” that both experts and non -experts can relate to ( as with nutritional labels or car sa fety crash -tests ) . The underlying evidence sought by such standards and certifications can also be used to spur , accelerate , and reward innovation through open , recurring , AI -innovation benchmarking initiatives . 163 . Within certification schemes , professional t raining could include the legal framework as part of the training curricula . In broader terms , universities and civil society could be part of education policy to disseminate , research and instruct on AI ’ s legal framework and technical developments . This a pproach would also confer further benefits in a global market economy . 164 . Furthermore , professional certification at the level of developers and of systems may be another strategy for assuring that AI is used in line with the Council of Europe standards of h uman rights , democracy and the rule of law . This certification mechanism could be similar to already existing certification mechanisms in various countries for certain professions . 9.2.2 Developers of systems 186 See the Recommendation of the Council of Europe Human Rights Commissioner on “ Unboxing AI : 10 steps to protect human rights ” . 187 In its revised General Policy Recommendation No . 2 , ECRI asks member states to adeq uately empower and resource equality bodies to effectively address issues of equality and non -discrimination . This also extends to discrimination arising due to the use of AI , by insisting on equality bodies ’ role in investigating specific cases , counselli ng victims , carrying out litigation , raising awareness with public and private organisations using AI and among the general public about the potential risks related to the use of AI systems . In addition , Equinet ’ s new 2020 report highlights the important r ole and potential of ( national ) equality bodies in AI context , accessible at : https : //equineteurope.org/2020/equinet -report regulating -for-an-equal -ai-a-new -role-for-equality -bodies/ 53 165 . Actors building AI -enabled systems ( both priva te and public sector ) should consider actions they can take to increase compliance with a future legal framework . For example , policies can be adopted to increase the visibility of where such technologies are being deployed , in particular by publishing pub lic sector contracts , or by establishing public registers188 or notification systems ) or developing norms and standardised tools for internal audit and self -certification ( all the while acknowledging the limitations of this approach ) . Liability consideration s should also be taken into account . 9.2.3 Operators and Users of systems 166 . Operators and users of AI could generate demand for AI applications that comply with the future legal framework . This is particularly true of the public sector and its relative procurement power . The promotion of trust carriers , such as certification mecha nisms or labels on AI systems ' lifecycles , and periodic auditing and reporting , are market responses pushed by operators and users of AI systems ’ preferences and expectations . When operators and users of AI systems become better informed of their rights an d redress mechanisms , the transaction cost of oversight is significantly reduced . 9.3 Examples of Types of Compliance Mechanism 167 . There are many contexts where organisations are already required to meet standards or regulations , such as , for example , financ ial services and healthcare . Each of these has evolved into ecosystems of services that allow organisations to prove to themselves , their customers , and regulators that they met a required standard . Different mechanisms will work best in different contexts , depending on existing infrastructure , sectoral mechanisms and institutions . It should also be considered , within a risk -based approach , which components of an AI system can be subject to compliance , for example , the training data used , the algorithm cons truction , the weighting of different inputs or the accuracy of any outputs . Inclusive participatory processes should be conducted to establish the relevant regulatory and enforcement mechanisms in each case . 168 . A future legal framework might specify that prac tical mechanisms adhere to a set of principles that promote the framework ’ s core values . These might include :  Dynamic ( not static ) : assessment ex ante and at various points throughout the AI project lifecycle to account for choices made during the design , development and deployment processes and any changes in the application -behaviour of dynamic learning models .  Technology adaptive : to support the fut ure-proofing of any compliance mechanisms .  Differentially accessible : understandable to experts and non -experts , in turn simplifying the process of any potential appeals and redress .  Independent : conducted , or overseen , by an independent party .  Evidence -based : supported on evidence produced by technical standards and certifications . For example , including data collected through best practices such as borderless , standardization or key metrics developed , for instance through benchmarking . 169 . Any mechanisms n eed to be implementable in practice and account for existing governance infrastructure and technical limitations . The practical mechanisms outlined below should therefore be considered as a toolkit that presents ample opportunity for further regulatory inn ovation and refinement : ( 1 ) Human rights due diligence , including human rights impact assessments189 – Conducting human rights due diligence is a requirement of companies to meeting their responsibility to respect human rights as set 188 Such registers already exist in the Netherlands and in the UK : https : //algoritmeregister.amsterdam.nl/ ; https : //ai.hel.fi/en/ai -register/ . 189 See also Consultative Committee of the Convention for the protection of individuals with regard to the Automatic Processing of Personal Data , Guidelines on Artificial Intelligence and Data Protection , January 2019 , and Consultative Committee of the Convent ion for the protection of individuals with regard to the Automatic Processing of Personal Data , Guidelines on the Protection of Individuals with regard to the Processing of Personal Data in a World of Big Data , January 2017 . See in this 54 out in the United Nations Guiding Principles on Business and Human Rights ( UNGPs ) . Companies should identify , prevent , mitigate and account for adverse human rights impacts stemming from their activities . Human rights due diligence should include assessing actual and potent ial human rights impacts , integrating and acting upon the findings , tracking responses , and communicating how impacts are addressed190 . Human rights impact assessments should be part of the wider human rights due diligence process where identified risks and impacts are effectively mitigated and addressed , and should be part of an ongoing assessment process rather than being a static exercise . Moreover , The Council of Europe ’ Recommendation on the human rights impact of algorithmic systems has recommended that public and private organisations perform a human rights impact assessment . These assessments might explicitly validate conformity with principles outlined in a future legal framework . In specific contexts , 'integrated impact assessments ' might be deemed m ore appropriate to reduce the administrative burden on development teams ( bringing together , for example , human rights , data protection , transparency , accountability , competence , and equalities considerations ) . When conducting a human rights impact assessm ent , a holistic approach should be taken , whereby all relevant civil , political , social , cultural and economic rights are considered . A uniform model and guidance developed at the Council of Europe level for a human rights , democracy and rule of law impact assessment , or an integrated impact assessment , could be helpful to validate conformity with the principles outlined in a future Council of Europe legal framework . ( 2 ) Certification & Quality Labelling – Ex ante obligations , administered by recognised bodies and independently reviewed , would help build trust . A distinction could be made between standards and certifications that can apply to ( i ) products / AI systems or ( ii ) organisations developing or using AI systems . An expiration date would ensure systems are re -reviewed regularly . Such schemes could be made voluntary ( e.g . for systems that pose low risk ) or mandatory ( e.g . for systems that pose higher risks ) , depending on the maturity of the ecosystem . Legal s afeguards must ensure certifications are not used by companies to shield themselves from potential liability claims associated with their conduct , or to gain an unfair competitive advantage . The certification process should subject to regulation regarding auditors ' qualifications , the standards adopted , and how conflicts of interests are managed . The certification process should strive for continuous improvement and be responsive to complaints . 191 Ongoing multi stakeholder standards development work would su pport this led by standard -setting bodies . ( 3 ) Audits – Regular independent assessments or audits of AI -enabled systems by experts or accredited groups is also a mechanism that should be exercised throughout the lifecycle of every AI -enabled system that can negatively affect human rights , democracy and the ru le of law to verify their integrity , impact , robustness , and absence of bias . Audits will facilitate a move towards more transparent and accountable use of AI -enabled systems . Audits could certify organisations as a whole , rather than just specific use cas es . ( 4 ) Regulatory Sandboxes192 – Regulatory sandboxes , particularly those that enable closer regulatory support , present an agile and safe approach to testing new technologies and could be used in order to regard also the new study of the FRA , referred to in footnote 114 above , which elaborates on the need to introduce human rights impact assessments in the context of AI systems . Of relevance is also the ongoing work at the level of the European Union on a potential mandatory EU system of human rights and environmental due diligence . See in this regard the European Commission ’ s Study on due diligence requirements through the supply chain of January 2020 , available at : https : //op.europa.eu/en/publication -detail/ -/publication/8ba0a8fd -4c83 -11ea -b8b7 -01aa75ed71a1/language -en . 190 See UNGP 17 , https : //www.ohchr.org/documents/publications/guidingprinciplesbusinesshr_en.pdf ; OHCHR B -Tech “ Key characteristics of Business Respect for Human Rights ” , https : //www.ohchr.org/Documents/Issues/Business/B -Tech/key characteristics -business -respect.pdf . 191 Where new certification mechanisms are created , it is important to consider existing initiatives in this area , such a s the ongoing work of CEPEJ as regards a specific certification for AI systems in the legal sector , as well as the various certific ation types and labels established in the EU , for instance . 192 Sandboxes shall be understood as concrete frameworks which , by providing a structured context for experimentation , enable in a real -world environment the testing of innovative technologies , products , services or approaches especially in the 55 strengthen innovative capacity in the field of AI.193 Sandboxes could be of particular use where a timely , possibly limited market introduction appears warranted for public welfare reasons , e.g . in extraordinary crises such as a pandemic , or in cases where current legal frameworks have not been tested in pr actice that could lead to constrained innovation . It is important that the establishment of regulatory sandboxes occurs within an appropriate legal framework that protects human rights . Cross -jurisdictional sandboxes present further opportunities for colla boration , building on the model of the Global Financial Innovation Network194 . ( 5 ) Continuous , automated monitoring – Automated systems can be deployed in parallel to AI -enabled systems to continuously monitor and asses its operation to guarantee compliance of established norms , for instance where AI systems carry a significant risk . 170 . Mandating practical mechanisms to enforce compliance should be considered only one part of a broader package of initiatives required to drive change . Member States could reinforce compliance mechanisms with several initiatives . For example , to invest in digital literacy , skilling up and building competencies and capacities of developers , policymakers and wider society to understand the human rights implications of AI -enabl ed systems ; to drive the widespread adoption of norms such as open access to source code ; or engaging with human rights civil society organisations as key stakeholders at various stages of development195 . 171 . This more comprehensive work to develop best practic es and norms within existing legal and regulatory regimes should be accompanied by ongoing discourse , collaboration , and best practice sharing between actors at national and international level . Centres of expertise would be well placed to facilitate colla boration on innovative solutions to inter -sectoral regulation projects196 . 9.4 Follow -up mechanisms 172 . In addition to the above -mentioned practical mechanisms , the CAHAI has taken note of the variety of follow -up mechanisms and processes , as well as measures for international co -operation which are envisaged under the Council of Europe ’ s legal instruments , of which the features vary according to the type and contents of such instruments . 173 . As regards follow -up mechanisms and processes , the CAHAI noted that they can include , for instance , the appointment of one or more entities – such as independent expert gr oups , conventional committees , standing committees , consultative committees and committees of parties197 – that can be in charge of tasks such as monitoring the implementation of a given convention , facilitating the effective use and implementation of a conv ention , and exchanging information and good practices on significant legal , policy or technological developments pertaining to a given area . In addition , an observatory could be established to track the implementation and impact of a potential Council of E urope legal framework on AI . It could also monitor the societal consequences of the uptake of AI systems on human rights , democracy and the rule of law , and keep an overview of contributions made by other stakeholders in this field . 174 . As to potential measu res of international co -operation , these could include the appointment of points of contact or the creation of networks among the state parties to advance mutual assistance and co -operation in criminal or civil matters . context of digitalisation for a limited time and generally in a limited part of a sector or area under regulatory supervision of the respective authority ensuring that appropriate safeguards are in place . 193 At the same time , it should be ensured that the protection of human rights – and even more so when it con cerns absolute human rights – remains secured . 194 https : //www.fca.org.uk/firms/innovation/global -financial -innovation -network . 195 CAHAI ( 2020 ) 21 rev PDG contributions p.45 -46 . 196 CAHAI ( 2020 ) 21 rev PDG contributions p.32 -33 . 197 The Committee of the parties could be entrusted with the collection and sharing information on legislation and best practices in a given field . 56 175 . While identifying precise solutions would be too premature at this stage , and bearing in mind that the concrete features of follow -up mechanisms and processes will depend on the nature and substantive elements of the chosen legal instrument ( s ) , the CAHAI recommends that a future legal fram ework on AI includes appropriate follow -up mechanisms and processes , as well as measures for international co -operation , in line with the Council of Europe ’ s legal standards and practice . This is of key importance to guarantee the effectiveness of the main principles , rights and obligations set out in Chapter 7 at international level , and to complement the practical and oversight measures described earlier in this chapter , which can be implemented at domestic level . 10 . FINAL CONSIDERATIONS 176 . This study has confirmed that AI systems can provide major opportunities for individual and societal development as well as for human rights , democracy and the rule of law . At the same time , it also confirmed that AI systems can have a negative impact on s everal human rights protected by the ECHR and other Council of Europe instruments , as well as on democracy and the rule of law . The study has noted that no international legal instrument specifically tailored to the challenges posed by AI exists , and that there are gaps in the current level of protection provided by existing international and national instruments . The study has identified the principles , rights and obligations which could become the main elements of a future legal framework for the design , development and application of AI , based on Council of Europe standards , which the CAHAI has been entrusted to develop . 177 . An appropriate legal framework will likely consist of a combination of binding and non -binding legal instruments , that complement each o ther . A binding instrument , a convention or framework convention , of horizontal character , could consolidate general common principles – contextualised to apply to the AI environment and using a risk -based approach – and include more granular provisions in line with the rights , principles and obligations identified in this feasibility study . Any binding document , whatever its shape , should not be overly prescriptive so as to secure its future -proof nature . Moreover , it should ensure that socially beneficial AI innovation can flourish , all the while adequately tackling the specific risks posed by the design , development and application of AI systems . 178 . This instrument could be combined with additional binding or non -binding sectoral Council of Europe instrument s to address challenges brought by AI systems in specific sectors . This combination would also allow legal certainty for AI stakeholders to be enhanced , and provide the required legal guidance to private actors wishing to undertake self -regulatory initiati ves . Moreover , by establishing common norms at an international level , transboundary trust in AI products and services would be ensured , thereby guaranteeing that the benefits generated by AI systems can travel across national borders . It is important that any legal framework includes practical mechanisms to mitigate risks arising from AI systems , as well as appropriate follow -up mechanisms and processes and measures for international co -operation . 179 . The Committee of Ministers is invited to take note of this feasibility study and to instruct the CAHAI to focus its work on the elaboration of the specific elements of a n appropriate legal framework . This could include a binding legal instrument , as well as non -binding instruments as appropriate , in parallel with progress that can be made on sectoral instruments .
