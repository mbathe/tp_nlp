{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmLPXIMjXePB",
    "outputId": "a61343bc-17b4-402a-8d1f-d80654b53ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: spacy in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (75.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/paul/.cache/pypoetry/virtualenvs/tp-nlp-R8Mk3SIc-py3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m36m0:00:02\u001b[0m:04\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdiBqoijvvAY"
   },
   "source": [
    "# 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGovKGK7wBvg",
    "outputId": "16f9dffb-e77b-43cb-cf5b-67b9a480c37d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle spaCy chargé avec succès.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle RoBERTa chargé avec succès.\n",
      "Erreur : Le fichier à l'emplacement ./src/docfile/txt2/5.txt n'a pas été trouvé.\n",
      "\n",
      "Extrait du texte nettoyé (500 premiers caractères) :\n",
      "\n",
      "\n",
      "Nombre de phrases extraites : 0\n",
      "\n",
      "Aucune phrase n'a été extraite.\n",
      "Le texte après nettoyage est vide. Impossible de continuer.\n",
      "\n",
      "Résultats des Méthodes de Similarité :\n",
      "Méthode 1 - Phrase la plus pertinente : \n",
      "Méthode 2 - Phrase la plus centrale : \n",
      "Méthode 3 - Phrase la plus pertinente (hybride) : \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, n_grams=2, min_n_gram_freq=1, stop_words=None):\n",
    "        self.n_grams = n_grams\n",
    "        self.min_n_gram_freq = min_n_gram_freq\n",
    "        if stop_words is None:\n",
    "            self.stop_words = set()\n",
    "        else:\n",
    "            self.stop_words = set(stop_words)\n",
    "\n",
    "        # Charger le modèle spaCy\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "            #print(\"Modèle spaCy chargé avec succès.\")\n",
    "        except OSError:\n",
    "            #print(\n",
    "                \"Erreur : Le modèle spaCy 'en_core_web_sm' n'est pas installé. Installation en cours...\")\n",
    "            import subprocess\n",
    "            import sys\n",
    "            subprocess.check_call(\n",
    "                [sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "            #print(\"Modèle spaCy installé et chargé avec succès.\")\n",
    "\n",
    "        # Charger le modèle et le tokenizer RoBERTa\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "        #print(\"Modèle RoBERTa chargé avec succès.\")\n",
    "\n",
    "    def nettoyer_texte(self, text):\n",
    "        \"\"\"\n",
    "        Nettoie le texte en supprimant les hashtags, mentions, emails, URLs, et caractères non pertinents.\n",
    "\n",
    "        :param text: Texte brut à nettoyer\n",
    "        :return: Texte nettoyé\n",
    "        \"\"\"\n",
    "        # Regex pour supprimer les hashtags, mentions, emails, URLs et caractères non pertinents\n",
    "        text = re.sub(\n",
    "            r'(#\\S+|@\\S+|\\S*@\\S*\\s?|http\\S+|[^A-Za-z0-9\\'\\- \\.\\,\\!\\?\\;]+)', \"\", text)\n",
    "\n",
    "        # Suppression des espaces multiples et des lignes vides\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def segmenter_phrases(self, text):\n",
    "        \"\"\"\n",
    "        Segmente le texte nettoyé en phrases en utilisant spaCy.\n",
    "\n",
    "        :param text: Texte nettoyé\n",
    "        :return: Liste de phrases\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        phrases = [sent.text.strip() for sent in doc.sents]\n",
    "        return phrases\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Encode une phrase en utilisant RoBERTa.\n",
    "\n",
    "        :param sentence: Phrase à encoder\n",
    "        :return: Encodage de la phrase sous forme de numpy array (hidden_size,)\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze(0).numpy()\n",
    "\n",
    "    def calculer_similarites(self, phrases, text_encoding):\n",
    "        \"\"\"\n",
    "        Calcule la similarité cosinus entre chaque phrase et le texte entier.\n",
    "\n",
    "        :param phrases: Liste de phrases\n",
    "        :param text_encoding: Encodage du texte entier (hidden_size,)\n",
    "        :return: Liste de tuples (phrase, similarité)\n",
    "        \"\"\"\n",
    "        similarities = []\n",
    "        for phrase in phrases:\n",
    "            if not phrase:  # Ignorer les phrases vides\n",
    "                continue\n",
    "            phrase_encoding = self.encode_sentence(phrase)\n",
    "            similarity = cosine_similarity(\n",
    "                [text_encoding], [phrase_encoding]).item()\n",
    "            similarities.append((phrase, similarity))\n",
    "        return similarities\n",
    "\n",
    "    def calculer_similarites_entre_phrases(self, phrases_encodées):\n",
    "        \"\"\"\n",
    "        Calcule la similarité cosinus entre chaque paire de phrases.\n",
    "\n",
    "        :param phrases_encodées: Liste d'encodages de phrases (list of numpy arrays, each (hidden_size,))\n",
    "        :return: Matrice de similarité (n_phrases, n_phrases)\n",
    "        \"\"\"\n",
    "        if not phrases_encodées:\n",
    "            return np.array([])\n",
    "\n",
    "        # Empiler les encodages en une matrice 2D (n_phrases, hidden_size)\n",
    "        phrases_encodées_stacked = np.vstack(phrases_encodées)\n",
    "        similarity_matrix = cosine_similarity(phrases_encodées_stacked)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def process_document(self, content):\n",
    "        \"\"\"\n",
    "        Traite un document individuel en nettoyant le texte, en segmentant en phrases,\n",
    "        et en calculant les similarités selon trois méthodes.\n",
    "\n",
    "        :param content: Contenu textuel du document à traiter\n",
    "        :return: Dictionnaire contenant les résultats des trois méthodes\n",
    "        \"\"\"\n",
    "        # 1. Nettoyage du texte\n",
    "        text_propre = self.nettoyer_texte(content)\n",
    "        #print(\"\\nExtrait du texte nettoyé (500 premiers caractères) :\")\n",
    "        #print(text_propre[:500])\n",
    "\n",
    "        # 2. Segmentation en phrases\n",
    "        phrases = self.segmenter_phrases(text_propre)\n",
    "        #print(f\"\\nNombre de phrases extraites : {len(phrases)}\")\n",
    "        if phrases:\n",
    "            #print(\"\\nQuelques phrases extraites :\")\n",
    "            for i, phrase in enumerate(phrases[:10]):\n",
    "                #print(f\"Phrase {i+1} :\", phrase)\n",
    "        else:\n",
    "            #print(\"\\nAucune phrase n'a été extraite.\")\n",
    "\n",
    "        # 3. Encodage du texte entier\n",
    "        if text_propre:\n",
    "            text_encoding = self.encode_sentence(text_propre)\n",
    "        else:\n",
    "            #print(\"Le texte après nettoyage est vide. Impossible de continuer.\")\n",
    "            return {\n",
    "                \"method1\": \"\",\n",
    "                \"method2\": \"\",\n",
    "                \"hybrid\": \"\"\n",
    "            }\n",
    "\n",
    "        # Méthode 1 : Comparer chaque phrase avec le texte entier\n",
    "        similarites_avec_texte = self.calculer_similarites(\n",
    "            phrases, text_encoding)\n",
    "        if similarites_avec_texte:\n",
    "            phrase_most_relevant_method1 = max(\n",
    "                similarites_avec_texte, key=lambda x: x[1])[0]\n",
    "            #print(\"\\nMéthode 1 - La phrase la plus pertinente est :\",\n",
    "                  phrase_most_relevant_method1)\n",
    "        else:\n",
    "            phrase_most_relevant_method1 = \"\"\n",
    "            #print(\"\\nMéthode 1 - Aucune phrase pertinente trouvée.\")\n",
    "\n",
    "        # Méthode 2 : Comparer les phrases entre elles\n",
    "        if phrases:\n",
    "            phrases_encodées = [self.encode_sentence(\n",
    "                phrase) for phrase in phrases if phrase]\n",
    "            similarity_matrix = self.calculer_similarites_entre_phrases(\n",
    "                phrases_encodées)\n",
    "            if similarity_matrix.size > 0:\n",
    "                centralite = similarity_matrix.sum(axis=1)\n",
    "                indice_centrale = centralite.argmax()\n",
    "                phrase_most_relevant_method2 = phrases[indice_centrale]\n",
    "                #print(\"Méthode 2 - La phrase la plus centrale est :\",\n",
    "                      phrase_most_relevant_method2)\n",
    "            else:\n",
    "                phrase_most_relevant_method2 = \"\"\n",
    "                #print(\"Méthode 2 - Aucune similarité trouvée entre les phrases.\")\n",
    "        else:\n",
    "            phrase_most_relevant_method2 = \"\"\n",
    "            #print(\"Méthode 2 - Aucune phrase trouvée pour comparaison.\")\n",
    "\n",
    "        # Méthode 3 : Approche Hybride\n",
    "        # Étape 1 : Comparer chaque phrase avec le texte entier et sélectionner les top N\n",
    "        N = 5\n",
    "        top_phrases_method1 = sorted(\n",
    "            similarites_avec_texte, key=lambda x: x[1], reverse=True)[:N]\n",
    "        phrases_top_method1 = [phrase for phrase, score in top_phrases_method1]\n",
    "        if phrases_top_method1:\n",
    "            #print(\n",
    "                f\"\\nMéthode 3 - Top {N} phrases les plus pertinentes selon Méthode 1 :\")\n",
    "            for i, phrase in enumerate(phrases_top_method1, 1):\n",
    "                #print(f\"Top Phrase {i} :\", phrase)\n",
    "        else:\n",
    "            #print(\n",
    "                \"\\nMéthode 3 - Aucune phrase pertinente trouvée pour l'approche hybride.\")\n",
    "\n",
    "        # Étape 2 : Comparer ces top phrases entre elles pour trouver la plus centrale\n",
    "        if phrases_top_method1:\n",
    "            phrases_top_encodées = [self.encode_sentence(\n",
    "                phrase) for phrase in phrases_top_method1]\n",
    "            similarity_matrix_top = self.calculer_similarites_entre_phrases(\n",
    "                phrases_top_encodées)\n",
    "            if similarity_matrix_top.size > 0:\n",
    "                centralite_top = similarity_matrix_top.sum(axis=1)\n",
    "                indice_centrale_top = centralite_top.argmax()\n",
    "                phrase_most_relevant_hybride = phrases_top_method1[indice_centrale_top]\n",
    "                #print(\"Méthode 3 - La phrase la plus pertinente (hybride) est :\",\n",
    "                      phrase_most_relevant_hybride)\n",
    "            else:\n",
    "                phrase_most_relevant_hybride = \"\"\n",
    "                #print(\"Méthode 3 - Aucune similarité trouvée parmi les top phrases.\")\n",
    "        else:\n",
    "            phrase_most_relevant_hybride = \"\"\n",
    "            #print(\"Méthode 3 - Aucune phrase pertinente trouvée pour l'approche hybride.\")\n",
    "\n",
    "        # Retourner les résultats sous forme de dictionnaire\n",
    "        return {\n",
    "            \"method1\": phrase_most_relevant_method1,\n",
    "            \"method2\": phrase_most_relevant_method2,\n",
    "            \"hybrid\": phrase_most_relevant_hybride\n",
    "        }\n",
    "\n",
    "\n",
    "# Exemple d'utilisation de la classe DocumentProcessor\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialiser le processeur de document\n",
    "    processor = DocumentProcessor(\n",
    "        n_grams=2, min_n_gram_freq=1, stop_words=['example', 'stopword'])\n",
    "\n",
    "    # Charger le contenu du fichier\n",
    "    file_path = './src/docfile/txts2/5.txt'  # Remplace par ton chemin de fichier\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        #print(\"Fichier chargé avec succès.\")\n",
    "    except FileNotFoundError:\n",
    "        #print(f\"Erreur : Le fichier à l'emplacement {\n",
    "              file_path} n'a pas été trouvé.\")\n",
    "        content = \"\"\n",
    "\n",
    "    # Traiter le document\n",
    "    resultats = processor.process_document(content)\n",
    "\n",
    "    # Afficher les résultats\n",
    "    #print(\"\\nRésultats des Méthodes de Similarité :\")\n",
    "    #print(f\"Méthode 1 - Phrase la plus pertinente : {resultats['method1']}\")\n",
    "    #print(f\"Méthode 2 - Phrase la plus centrale : {resultats['method2']}\")\n",
    "    #print(\n",
    "        f\"Méthode 3 - Phrase la plus pertinente (hybride) : {resultats['hybrid']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tp-nlp-R8Mk3SIc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
