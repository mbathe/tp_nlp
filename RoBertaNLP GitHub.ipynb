{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmLPXIMjXePB",
    "outputId": "a61343bc-17b4-402a-8d1f-d80654b53ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.2)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdiBqoijvvAY"
   },
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGovKGK7wBvg",
    "outputId": "16f9dffb-e77b-43cb-cf5b-67b9a480c37d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle spaCy chargé avec succès.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle RoBERTa chargé avec succès.\n",
      "Fichier chargé avec succès.\n",
      "\n",
      "Extrait du texte nettoyé (500 premiers caractères) :\n",
      "14 1541-16720926.00 2009 IEEE IEEE INtEllIgENt systEMs Published by the IEEE Computer SocietyHISTORIES AND FUTURES Editors Robert R. Hoffman, Jeffrey M. Bradshaw, and Kenneth M. Ford, Institute for Human and Machine Cognition, COMPUTING that the robot had complex perception and reasoning skills equivalent to a child and that robots were subservient to humans. Although the laws were simple and few, the stories attempted to demonstrate just how diffi cult they were to apply in various real-world s\n",
      "\n",
      "Nombre de phrases extraites : 252\n",
      "\n",
      "Quelques phrases extraites :\n",
      "Phrase 1 : 14 1541-16720926.00 2009 IEEE IEEE INtEllIgENt systEMs Published by the IEEE Computer SocietyHISTORIES AND FUTURES Editors Robert R. Hoffman, Jeffrey M. Bradshaw, and Kenneth M. Ford, Institute for Human and Machine Cognition, COMPUTING that the robot had complex perception and reasoning skills equivalent to a child and that robots were subservient to humans.\n",
      "Phrase 2 : Although the laws were simple and few, the stories attempted to demonstrate just how diffi cult they were to apply in various real-world situations.\n",
      "Phrase 3 : In most situations, although the robots usually behaved logically, they often failed to do the right thing, typically because the particular context of application required subtle adjustments of judgment on the part of the robot for example, determining which law took priority in a given situation, or what constituted helpful or harmful behavior.\n",
      "Phrase 4 : The three laws have been so successfully inculcated into the public consciousness through entertainment that they now appear to shape societys expectations about how robots should act around humans.\n",
      "Phrase 5 : For instance, the media frequently refer to humanrobot interaction in terms of the three laws.\n",
      "Phrase 6 : Theyve been the subject of serious blogs, events, and even scientifi c publications.\n",
      "Phrase 7 : The Sin-gularity Institute organized an event and Web site, Three Laws Unsafe, to try to counter public expectations of robots in the wake of the movie I, Robot .\n",
      "Phrase 8 : Both the philosophy 1 and AI2 communities have discussed ethical considerations of ro-bots in society using the three laws as a reference, with a recent discussion in IEEE Intelligent Systems .\n",
      "Phrase 9 : 3\n",
      "Phrase 10 : Even medical doctors have considered robotic surgery in the context of the three laws.4With few notable exceptions,5,6 t h e r e h a s b e e n relatively little discussion of whether robots, now or in the near future, will have suffi cient perceptual and reasoning capabilities to actually follow the laws.\n",
      "\n",
      "Méthode 1 - La phrase la plus pertinente est : 14 1541-16720926.00 2009 IEEE IEEE INtEllIgENt systEMs Published by the IEEE Computer SocietyHISTORIES AND FUTURES Editors Robert R. Hoffman, Jeffrey M. Bradshaw, and Kenneth M. Ford, Institute for Human and Machine Cognition, COMPUTING that the robot had complex perception and reasoning skills equivalent to a child and that robots were subservient to humans.\n",
      "Méthode 2 - La phrase la plus centrale est : A quick review of the computer vi sion literature shows that scientists continue to struggle with many fun damental perceptual processes.\n",
      "\n",
      "Méthode 3 - Top 5 phrases les plus pertinentes selon Méthode 1 :\n",
      "Top Phrase 1 : 14 1541-16720926.00 2009 IEEE IEEE INtEllIgENt systEMs Published by the IEEE Computer SocietyHISTORIES AND FUTURES Editors Robert R. Hoffman, Jeffrey M. Bradshaw, and Kenneth M. Ford, Institute for Human and Machine Cognition, COMPUTING that the robot had complex perception and reasoning skills equivalent to a child and that robots were subservient to humans.\n",
      "Top Phrase 2 : He uses the situations as a foil to explore issues such as the ambiguity and cultural dependence of lan- guage and behaviorfor example, whether what appears to be cruel in the short run can actually become a kindness in the longer term;social utilityfor instance, how different indi- viduals roles, capabilities, or backgrounds are valuable in different ways with respect to each other and to society; andthe limits of technologyfor example, the im- possibility of assuring timely, correct actions in all situations and the omnipresence of trade-offs.\n",
      "Top Phrase 3 : 17The Alternative Three Laws of Responsible Robotics To address the difficulties of apply ing Asimovs three laws to the cur rent generation of robots while re specting the laws general intent, we suggest the three laws of responsible robotics.\n",
      "Top Phrase 4 : Following the defi nitions in Moral Machines Teaching Robots Right from Wrong , 7 Asimovs laws are based on functional morality, which as-sumes that robots have suffi cient agency and cognition to make moral decisions.\n",
      "Top Phrase 5 : Both the philosophy 1 and AI2 communities have discussed ethical considerations of ro-bots in society using the three laws as a reference, with a recent discussion in IEEE Intelligent Systems .\n",
      "Méthode 3 - La phrase la plus pertinente (hybride) est : Following the defi nitions in Moral Machines Teaching Robots Right from Wrong , 7 Asimovs laws are based on functional morality, which as-sumes that robots have suffi cient agency and cognition to make moral decisions.\n",
      "\n",
      "Résultats des Méthodes de Similarité :\n",
      "Méthode 1 - Phrase la plus pertinente : 14 1541-16720926.00 2009 IEEE IEEE INtEllIgENt systEMs Published by the IEEE Computer SocietyHISTORIES AND FUTURES Editors Robert R. Hoffman, Jeffrey M. Bradshaw, and Kenneth M. Ford, Institute for Human and Machine Cognition, COMPUTING that the robot had complex perception and reasoning skills equivalent to a child and that robots were subservient to humans.\n",
      "Méthode 2 - Phrase la plus centrale : A quick review of the computer vi sion literature shows that scientists continue to struggle with many fun damental perceptual processes.\n",
      "Méthode 3 - Phrase la plus pertinente (hybride) : Following the defi nitions in Moral Machines Teaching Robots Right from Wrong , 7 Asimovs laws are based on functional morality, which as-sumes that robots have suffi cient agency and cognition to make moral decisions.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, n_grams=2, min_n_gram_freq=1, stop_words=None):\n",
    "        self.n_grams = n_grams\n",
    "        self.min_n_gram_freq = min_n_gram_freq\n",
    "        if stop_words is None:\n",
    "            self.stop_words = set()\n",
    "        else:\n",
    "            self.stop_words = set(stop_words)\n",
    "\n",
    "        # Charger le modèle spaCy\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "            print(\"Modèle spaCy chargé avec succès.\")\n",
    "        except OSError:\n",
    "            print(\"Erreur : Le modèle spaCy 'en_core_web_sm' n'est pas installé. Installation en cours...\")\n",
    "            import subprocess\n",
    "            import sys\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "            print(\"Modèle spaCy installé et chargé avec succès.\")\n",
    "\n",
    "        # Charger le modèle et le tokenizer RoBERTa\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "        print(\"Modèle RoBERTa chargé avec succès.\")\n",
    "\n",
    "    def nettoyer_texte(self, text):\n",
    "        \"\"\"\n",
    "        Nettoie le texte en supprimant les hashtags, mentions, emails, URLs, et caractères non pertinents.\n",
    "\n",
    "        :param text: Texte brut à nettoyer\n",
    "        :return: Texte nettoyé\n",
    "        \"\"\"\n",
    "        # Regex pour supprimer les hashtags, mentions, emails, URLs et caractères non pertinents\n",
    "        text = re.sub(r'(#\\S+|@\\S+|\\S*@\\S*\\s?|http\\S+|[^A-Za-z0-9\\'\\- \\.\\,\\!\\?\\;]+)', \"\", text)\n",
    "\n",
    "        # Suppression des espaces multiples et des lignes vides\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def segmenter_phrases(self, text):\n",
    "        \"\"\"\n",
    "        Segmente le texte nettoyé en phrases en utilisant spaCy.\n",
    "\n",
    "        :param text: Texte nettoyé\n",
    "        :return: Liste de phrases\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        phrases = [sent.text.strip() for sent in doc.sents]\n",
    "        return phrases\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Encode une phrase en utilisant RoBERTa.\n",
    "\n",
    "        :param sentence: Phrase à encoder\n",
    "        :return: Encodage de la phrase sous forme de numpy array (hidden_size,)\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze(0).numpy()\n",
    "\n",
    "    def calculer_similarites(self, phrases, text_encoding):\n",
    "        \"\"\"\n",
    "        Calcule la similarité cosinus entre chaque phrase et le texte entier.\n",
    "\n",
    "        :param phrases: Liste de phrases\n",
    "        :param text_encoding: Encodage du texte entier (hidden_size,)\n",
    "        :return: Liste de tuples (phrase, similarité)\n",
    "        \"\"\"\n",
    "        similarities = []\n",
    "        for phrase in phrases:\n",
    "            if not phrase:  # Ignorer les phrases vides\n",
    "                continue\n",
    "            phrase_encoding = self.encode_sentence(phrase)\n",
    "            similarity = cosine_similarity([text_encoding], [phrase_encoding]).item()\n",
    "            similarities.append((phrase, similarity))\n",
    "        return similarities\n",
    "\n",
    "    def calculer_similarites_entre_phrases(self, phrases_encodées):\n",
    "        \"\"\"\n",
    "        Calcule la similarité cosinus entre chaque paire de phrases.\n",
    "\n",
    "        :param phrases_encodées: Liste d'encodages de phrases (list of numpy arrays, each (hidden_size,))\n",
    "        :return: Matrice de similarité (n_phrases, n_phrases)\n",
    "        \"\"\"\n",
    "        if not phrases_encodées:\n",
    "            return np.array([])\n",
    "\n",
    "        # Empiler les encodages en une matrice 2D (n_phrases, hidden_size)\n",
    "        phrases_encodées_stacked = np.vstack(phrases_encodées)\n",
    "        similarity_matrix = cosine_similarity(phrases_encodées_stacked)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def process_document(self, content):\n",
    "        \"\"\"\n",
    "        Traite un document individuel en nettoyant le texte, en segmentant en phrases,\n",
    "        et en calculant les similarités selon trois méthodes.\n",
    "\n",
    "        :param content: Contenu textuel du document à traiter\n",
    "        :return: Dictionnaire contenant les résultats des trois méthodes\n",
    "        \"\"\"\n",
    "        # 1. Nettoyage du texte\n",
    "        text_propre = self.nettoyer_texte(content)\n",
    "        print(\"\\nExtrait du texte nettoyé (500 premiers caractères) :\")\n",
    "        print(text_propre[:500])\n",
    "\n",
    "        # 2. Segmentation en phrases\n",
    "        phrases = self.segmenter_phrases(text_propre)\n",
    "        print(f\"\\nNombre de phrases extraites : {len(phrases)}\")\n",
    "        if phrases:\n",
    "            print(\"\\nQuelques phrases extraites :\")\n",
    "            for i, phrase in enumerate(phrases[:10]):\n",
    "                print(f\"Phrase {i+1} :\", phrase)\n",
    "        else:\n",
    "            print(\"\\nAucune phrase n'a été extraite.\")\n",
    "\n",
    "        # 3. Encodage du texte entier\n",
    "        if text_propre:\n",
    "            text_encoding = self.encode_sentence(text_propre)\n",
    "        else:\n",
    "            print(\"Le texte après nettoyage est vide. Impossible de continuer.\")\n",
    "            return {\n",
    "                \"method1\": \"\",\n",
    "                \"method2\": \"\",\n",
    "                \"hybrid\": \"\"\n",
    "            }\n",
    "\n",
    "        # Méthode 1 : Comparer chaque phrase avec le texte entier\n",
    "        similarites_avec_texte = self.calculer_similarites(phrases, text_encoding)\n",
    "        if similarites_avec_texte:\n",
    "            phrase_most_relevant_method1 = max(similarites_avec_texte, key=lambda x: x[1])[0]\n",
    "            print(\"\\nMéthode 1 - La phrase la plus pertinente est :\", phrase_most_relevant_method1)\n",
    "        else:\n",
    "            phrase_most_relevant_method1 = \"\"\n",
    "            print(\"\\nMéthode 1 - Aucune phrase pertinente trouvée.\")\n",
    "\n",
    "        # Méthode 2 : Comparer les phrases entre elles\n",
    "        if phrases:\n",
    "            phrases_encodées = [self.encode_sentence(phrase) for phrase in phrases if phrase]\n",
    "            similarity_matrix = self.calculer_similarites_entre_phrases(phrases_encodées)\n",
    "            if similarity_matrix.size > 0:\n",
    "                centralite = similarity_matrix.sum(axis=1)\n",
    "                indice_centrale = centralite.argmax()\n",
    "                phrase_most_relevant_method2 = phrases[indice_centrale]\n",
    "                print(\"Méthode 2 - La phrase la plus centrale est :\", phrase_most_relevant_method2)\n",
    "            else:\n",
    "                phrase_most_relevant_method2 = \"\"\n",
    "                print(\"Méthode 2 - Aucune similarité trouvée entre les phrases.\")\n",
    "        else:\n",
    "            phrase_most_relevant_method2 = \"\"\n",
    "            print(\"Méthode 2 - Aucune phrase trouvée pour comparaison.\")\n",
    "\n",
    "        # Méthode 3 : Approche Hybride\n",
    "        # Étape 1 : Comparer chaque phrase avec le texte entier et sélectionner les top N\n",
    "        N = 5\n",
    "        top_phrases_method1 = sorted(similarites_avec_texte, key=lambda x: x[1], reverse=True)[:N]\n",
    "        phrases_top_method1 = [phrase for phrase, score in top_phrases_method1]\n",
    "        if phrases_top_method1:\n",
    "            print(f\"\\nMéthode 3 - Top {N} phrases les plus pertinentes selon Méthode 1 :\")\n",
    "            for i, phrase in enumerate(phrases_top_method1, 1):\n",
    "                print(f\"Top Phrase {i} :\", phrase)\n",
    "        else:\n",
    "            print(\"\\nMéthode 3 - Aucune phrase pertinente trouvée pour l'approche hybride.\")\n",
    "\n",
    "        # Étape 2 : Comparer ces top phrases entre elles pour trouver la plus centrale\n",
    "        if phrases_top_method1:\n",
    "            phrases_top_encodées = [self.encode_sentence(phrase) for phrase in phrases_top_method1]\n",
    "            similarity_matrix_top = self.calculer_similarites_entre_phrases(phrases_top_encodées)\n",
    "            if similarity_matrix_top.size > 0:\n",
    "                centralite_top = similarity_matrix_top.sum(axis=1)\n",
    "                indice_centrale_top = centralite_top.argmax()\n",
    "                phrase_most_relevant_hybride = phrases_top_method1[indice_centrale_top]\n",
    "                print(\"Méthode 3 - La phrase la plus pertinente (hybride) est :\", phrase_most_relevant_hybride)\n",
    "            else:\n",
    "                phrase_most_relevant_hybride = \"\"\n",
    "                print(\"Méthode 3 - Aucune similarité trouvée parmi les top phrases.\")\n",
    "        else:\n",
    "            phrase_most_relevant_hybride = \"\"\n",
    "            print(\"Méthode 3 - Aucune phrase pertinente trouvée pour l'approche hybride.\")\n",
    "\n",
    "        # Retourner les résultats sous forme de dictionnaire\n",
    "        return {\n",
    "            \"method1\": phrase_most_relevant_method1,\n",
    "            \"method2\": phrase_most_relevant_method2,\n",
    "            \"hybrid\": phrase_most_relevant_hybride\n",
    "        }\n",
    "\n",
    "\n",
    "# Exemple d'utilisation de la classe DocumentProcessor\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialiser le processeur de document\n",
    "    processor = DocumentProcessor(n_grams=2, min_n_gram_freq=1, stop_words=['example', 'stopword'])\n",
    "\n",
    "    # Charger le contenu du fichier\n",
    "    file_path = '/content/5.txt'  # Remplace par ton chemin de fichier\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        print(\"Fichier chargé avec succès.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur : Le fichier à l'emplacement {file_path} n'a pas été trouvé.\")\n",
    "        content = \"\"\n",
    "\n",
    "    # Traiter le document\n",
    "    resultats = processor.process_document(content)\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(\"\\nRésultats des Méthodes de Similarité :\")\n",
    "    print(f\"Méthode 1 - Phrase la plus pertinente : {resultats['method1']}\")\n",
    "    print(f\"Méthode 2 - Phrase la plus centrale : {resultats['method2']}\")\n",
    "    print(f\"Méthode 3 - Phrase la plus pertinente (hybride) : {resultats['hybrid']}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
