{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.code.tools.metrics import calculate_metrics\n",
    "import os\n",
    "from src.code.TFIDF.pre_processing import Processing\n",
    "from src.code.bert.bert import Bert\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from nltk.tokenize import sent_tokenize\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 784/784 [00:22<00:00, 35.63it/s]\n",
      "100%|██████████| 784/784 [00:03<00:00, 228.76it/s]\n",
      "100%|██████████| 784/784 [00:00<00:00, 216522.97it/s]\n",
      "100%|██████████| 784/784 [00:00<00:00, 853447.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai​ ​now​ ​2017​ ​report  authors   alex​ ​campolo, ​ ​new​ ​york​ ​university  madelyn​ ​sanfilippo, ​ ​new​ ​york​ ​university  meredith​ ​whittaker, ​ ​google​ ​open​ ​research,​ ​new​ ​york​ ​university, ​ ​and​ ​ai​ ​now  ​ ​kate ​ ​crawford, ​ ​microsoft​ ​research,​ ​new​ ​york​ ​university, ​ ​and​ ​ai​ ​now  editors   andrew​ ​selbst,​ ​yale​ ​information​ ​society​ ​project​ ​and​ ​data​ ​&​ ​society  solon​ ​barocas, ​ ​cornell​ ​university  table​ ​of​ ​contents   recommendations 1  executive​ ​summary 3  introduction 6  labor​ ​and​ ​automation 7  research ​ ​by​ ​sector​ ​and​ ​task 7  ai​ ​and​ ​the​ ​nature​ ​of​ ​work 9  inequality​ ​and​ ​redistribution 1​3  bias​ ​and​ ​inclusion 1​3  where​ ​bias​ ​comes ​ ​from 1​4  the​ ​ai​ ​field​ ​is​ ​not​ ​diverse 1​6  recent​ ​developments ​ ​in​ ​bias​ ​research 1​8  emerging ​ ​strategies​ ​to​ ​address​ ​bias 20  rights​ ​and​ ​liberties 21  population ​ ​registries ​ ​and​ ​computing​ ​power 2​2  corporate ​ ​and​ ​government​ ​entanglements 2​3  ai​ ​and​ ​the​ ​legal​ ​system 2​6  ai​ ​and​ ​privacy 2​8  ethics​ ​and​ ​governance 30  ethical​ ​concerns ​ ​in​ ​ai 30  ai​ ​reflects​ ​its​ ​origins 31  ethical​ ​codes 3​2  challenges ​ ​and​ ​concerns ​ ​going​ ​forward 3​4  conclusion 3​6  this work is licensed under a creative commons attribution-noderivatives 4.0 international license\n",
      "ai​ ​now​ ​2017​ ​report 1  recommendations  these​ ​recommendations​ ​reflect​ ​the ​ ​views​ ​and​ ​research​ ​of​ ​the ​ ​ ​​ai​ ​now​ ​institute​ ​at​ ​new​ ​york  university.​ ​we ​ ​thank​ ​the​ ​experts​ ​who​ ​contributed​ ​to​ ​the​ ​​ai ​ ​now ​ ​2017 ​ ​symposium ​ ​and  workshop ​​ ​for​ ​informing​ ​these​ ​perspectives, ​ ​and​ ​our​ ​research​ ​team​ ​for​ ​helping​ ​shape​ ​the​ ​ ​ai  now ​ ​2017 ​ ​report.   1.core​ ​public​ ​agencies,​ ​such​ ​as​ ​those​ ​responsible​ ​for​ ​criminal​ ​justice,​ ​healthcare, welfare,​ ​and ​ ​education ​ ​(e.g​ ​“high ​ ​stakes”​ ​domains) ​ ​should ​ ​no ​ ​longer​ ​use​ ​“black​ ​box” ai​ ​and ​ ​algorithmic​ ​systems.​​ ​this​ ​includes​ ​the​ ​unreviewed​ ​or​ ​unvalidated​ ​use ​ ​of pre-trained​ ​models,​ ​ai​ ​systems​ ​licensed​ ​from​ ​third​ ​party​ ​vendors, ​ ​and​ ​algorithmic processes​ ​created​ ​in-house.​ ​the ​ ​use​ ​of​ ​such​ ​systems​ ​by​ ​public​ ​agencies​ ​raises​ ​serious due​ ​process​ ​concerns, ​ ​and​ ​at​ ​a​ ​minimum​ ​they​ ​should​ ​be ​ ​available​ ​for​ ​public​ ​auditing, testing,​ ​and​ ​review, ​ ​and​ ​subject​ ​to​ ​accountability​ ​standards. 2.before​ ​releasing​ ​an​ ​ai​ ​system,​ ​companies​ ​should ​ ​run​ ​rigorous​ ​pre-release​ ​trials​ ​to ensure​ ​that​ ​they​ ​will​ ​not​ ​amplify​ ​biases​ ​and ​ ​errors​​ ​​due​ ​to ​ ​any​ ​issues​ ​with ​ ​the​ ​training data,​ ​algorithms,​ ​or​ ​other​ ​elements​ ​of​ ​system​ ​design.​​ ​as​ ​this​ ​is​ ​a​ ​rapidly​ ​changing​ ​field, the​ ​methods​ ​and​ ​assumptions​ ​by​ ​which​ ​such​ ​testing​ ​is​ ​conducted, ​ ​along​ ​with​ ​the results,​ ​should​ ​be ​ ​openly​ ​documented​ ​and​ ​publicly​ ​available, ​ ​with​ ​clear​ ​versioning​ ​to accommodate ​ ​updates​ ​and​ ​new​ ​findings. 3.after​ ​releasing​ ​an ​ ​ai​ ​system,​ ​companies​ ​should ​ ​continue​ ​to ​ ​monitor​ ​its​ ​use​ ​across different​ ​contexts​ ​and​ ​communities.​​ ​the​ ​methods​ ​and​ ​outcomes​ ​of​ ​monitoring​ ​should be​ ​defined​ ​through​ ​open, ​ ​academically​ ​rigorous​ ​processes, ​ ​and​ ​should​ ​be ​ ​accountable to​ ​the​ ​public.​ ​particularly​ ​in​ ​high​ ​stakes​ ​decision-making​ ​contexts, ​ ​the​ ​views​ ​and experiences​ ​of​ ​traditionally​ ​marginalized​ ​communities​ ​should​ ​be ​ ​prioritized. 4.more​ ​research ​ ​and ​ ​policy​ ​making​ ​is​ ​needed ​ ​on ​ ​the​ ​use​ ​of​ ​ai​ ​systems​ ​in ​ ​workplace management​ ​and ​ ​monitoring,​ ​including​ ​hiring​ ​and ​ ​hr.​​ ​this​ ​research​ ​will​ ​complement the​ ​existing​ ​focus​ ​on​ ​worker​ ​replacement​ ​via​ ​automation.​ ​specific​ ​attention​ ​should​ ​be given​ ​to​ ​the ​ ​potential​ ​impact​ ​on​ ​labor​ ​rights​ ​and​ ​practices, ​ ​and​ ​should​ ​focus​ ​especially on​ ​the​ ​potential​ ​for​ ​behavioral​ ​manipulation​ ​and​ ​the ​ ​unintended​ ​reinforcement​ ​of​ ​bias in​ ​hiring​ ​and​ ​promotion. 5.develop​ ​standards​ ​to ​ ​track​ ​the​ ​provenance,​ ​development,​ ​and ​ ​use​ ​of​ ​training​ ​datasets throughout​ ​their​ ​life​ ​cycle.​ ​​this​ ​is​ ​necessary​ ​to​ ​better​ ​understand​ ​and​ ​monitor​ ​issues​ ​of bias​ ​and​ ​representational​ ​skews.​ ​in​ ​addition​ ​to​ ​developing​ ​better​ ​records​ ​for​ ​how​ ​a training​ ​dataset​ ​was​ ​created​ ​and​ ​maintained,​ ​social​ ​scientists​ ​and​ ​measurement researchers​ ​within​ ​the ​ ​ai​ ​bias​ ​research​ ​field​ ​should​ ​continue ​ ​to​ ​examine​ ​existing​ ​training datasets, ​ ​and​ ​work ​ ​to​ ​understand​ ​potential​ ​blind​ ​spots​ ​and​ ​biases​ ​that​ ​may​ ​already​ ​be at​ ​work. \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 2          6. expand ​  ​ ai ​  ​ bias ​  ​ research ​  ​ and ​  ​ mitigation ​  ​ strategies ​  ​ beyond ​  ​ a ​  ​ narrowly ​  ​ technical    approach. ​ ​  ​ bias ​  ​ issues ​  ​ are ​  ​ long ​  ​ term ​  ​ and ​  ​ structural, ​  ​ and ​  ​ contending ​  ​ with ​  ​ them    necessitates ​  ​ deep ​  ​ interdisciplinary ​  ​ research. ​  ​ technical ​  ​ approaches ​  ​ that ​  ​ look ​  ​ for ​  ​ a    one-time ​  ​ “fix” ​  ​ for ​  ​ fairness ​  ​ risk ​  ​ oversimplifying ​  ​ the ​  ​ complexity ​  ​ of ​  ​ social ​  ​ systems. ​  ​ within    each ​  ​ domain ​  ​ – ​  ​ such ​  ​ as ​  ​ education, ​  ​ healthcare ​  ​ or ​  ​ criminal ​  ​ justice ​  ​ – ​  ​ legacies ​  ​ of ​  ​ bias ​  ​ and    movements ​  ​ toward ​  ​ equality ​  ​ have ​  ​ their ​  ​ own ​  ​ histories ​  ​ and ​  ​ practices. ​  ​ legacies ​  ​ of ​  ​ bias    cannot ​  ​ be ​  ​ “solved” ​  ​ without ​  ​ drawing ​  ​ on ​  ​ domain ​  ​ expertise. ​  ​ addressing ​  ​ fairness    meaningfully ​  ​ will ​  ​ require ​  ​ interdisciplinary ​  ​ collaboration ​  ​ and ​  ​ methods ​  ​ of ​  ​ listening ​  ​ across    different ​  ​ disciplines.       7. strong ​  ​ standards ​  ​ for ​  ​ auditing ​  ​ and ​  ​ understanding ​  ​ the ​  ​ use ​  ​ of ​  ​ ai ​  ​ systems ​  ​ “in ​  ​ the ​  ​ wild”    are ​  ​ urgently ​  ​ needed. ​ ​  ​ creating ​  ​ such ​  ​ standards ​  ​ will ​  ​ require ​  ​ the ​  ​ perspectives ​  ​ of ​  ​ diverse    disciplines ​  ​ and ​  ​ coalitions. ​  ​ the ​  ​ process ​  ​ by ​  ​ which ​  ​ such ​  ​ standards ​  ​ are ​  ​ developed ​  ​ should ​  ​ be    publicly ​  ​ accountable, ​  ​ academically ​  ​ rigorous ​  ​ and ​  ​ subject ​  ​ to ​  ​ periodic ​  ​ review ​  ​ and ​  ​ revision.        8. companies, ​  ​ universities, ​  ​ conferences ​  ​ and ​  ​ other ​  ​ stakeholders ​  ​ in ​  ​ the ​  ​ ai ​  ​ field ​  ​ should    release ​  ​ data ​  ​ on ​  ​ the ​  ​ participation ​  ​ of ​  ​ women, ​  ​ minorities ​  ​ and ​  ​ other ​  ​ marginalized ​  ​ groups    within ​  ​ ai ​  ​ research ​  ​ and ​  ​ development. ​ ​  ​ many ​  ​ now ​  ​ recognize ​  ​ that ​  ​ the ​  ​ current ​  ​ lack ​  ​ of    diversity ​  ​ in ​  ​ ai ​  ​ is ​  ​ a ​  ​ serious ​  ​ issue, ​  ​ yet ​  ​ there ​  ​ is ​  ​ insufficiently ​  ​ granular ​  ​ data ​  ​ on ​  ​ the ​  ​ scope ​  ​ of    the ​  ​ problem, ​  ​ which ​  ​ is ​  ​ needed ​  ​ to ​  ​ measure ​  ​ progress. ​  ​ beyond ​  ​ this, ​  ​ we ​  ​ need ​  ​ a ​  ​ deeper    assessment ​  ​ of ​  ​ workplace ​  ​ cultures ​  ​ in ​  ​ the ​  ​ technology ​  ​ industry, ​  ​ which ​  ​ requires ​  ​ going    beyond ​  ​ simply ​  ​ hiring ​  ​ more ​  ​ women ​  ​ and ​  ​ minorities, ​  ​ toward ​  ​ building ​  ​ more ​  ​ genuinely    inclusive ​  ​ workplaces.        9. the ​  ​ ai ​  ​ industry ​  ​ should ​  ​ hire ​  ​ experts ​  ​ from ​  ​ disciplines ​  ​ beyond ​  ​ computer ​  ​ science ​  ​ and    engineering ​  ​ and ​  ​ ensure ​  ​ they ​  ​ have ​  ​ decision ​  ​ making ​  ​ power. ​ ​  ​​  ​ as ​  ​ ai ​  ​ moves ​  ​ into ​  ​ diverse    social ​  ​ and ​  ​ institutional ​  ​ domains, ​  ​ influencing ​  ​ increasingly ​  ​ high ​  ​ stakes ​  ​ decisions, ​  ​ efforts    must ​  ​ be ​  ​ made ​  ​ to ​  ​ integrate ​  ​ social ​  ​ scientists, ​  ​ legal ​  ​ scholars, ​  ​ and ​  ​ others ​  ​ with ​  ​ domain    expertise ​  ​ that ​  ​ can ​  ​ guide ​  ​ the ​  ​ creation ​  ​ and ​  ​ integration ​  ​ of ​  ​ ai ​  ​ into ​  ​ long-standing ​  ​ systems    with ​  ​ established ​  ​ practices ​  ​ and ​  ​ norms.        10. ethical ​  ​ codes ​  ​ meant ​  ​ to ​  ​ steer ​  ​ the ​  ​ ai ​  ​ field ​  ​ should ​  ​ be ​  ​ accompanied ​  ​ by ​  ​ strong ​  ​ oversight    and ​  ​ accountability ​  ​ mechanisms. ​ ​  ​ more ​  ​ work ​  ​ is ​  ​ needed ​  ​ on ​  ​ how ​  ​ to ​  ​ substantively ​  ​ connect    high ​  ​ level ​  ​ ethical ​  ​ principles ​  ​ and ​  ​ guidelines ​  ​ for ​  ​ best ​  ​ practices ​  ​ to ​  ​ everyday ​  ​ development    processes, ​  ​ promotion ​  ​ and ​  ​ product ​  ​ release ​  ​ cycles.       \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 3       executive ​  ​ summary    artificial ​  ​ intelligence ​  ​ (ai) ​  ​ technologies ​  ​ are ​  ​ in ​  ​ a ​  ​ phase ​  ​ of ​  ​ rapid ​  ​ development, ​  ​ and ​  ​ are ​  ​ being    adopted ​  ​ widely. ​  ​ while ​  ​ the ​  ​ concept ​  ​ of ​  ​ artificial ​  ​ intelligence ​  ​ has ​  ​ existed ​  ​ for ​  ​ over ​  ​ sixty ​  ​ years,    real-world ​  ​ applications ​  ​ have ​  ​ only ​  ​ accelerated ​  ​ in ​  ​ the ​  ​ last ​  ​ decade ​  ​ due ​  ​ to ​  ​ three ​  ​ concurrent    developments: ​  ​ better ​  ​ algorithms, ​  ​ increases ​  ​ in ​  ​ networked ​  ​ computing ​  ​ power ​  ​ and ​  ​ the ​  ​ tech    industry’s ​  ​ ability ​  ​ to ​  ​ capture ​  ​ and ​  ​ store ​  ​ massive ​  ​ amounts ​  ​ of ​  ​​  ​ data.        ai ​  ​ systems ​  ​ are ​  ​ already ​  ​ integrated ​  ​ in ​  ​ everyday ​  ​ technologies ​  ​ like ​  ​ smartphones ​  ​ and ​  ​ personal    assistants, ​  ​ making ​  ​ predictions ​  ​ and ​  ​ determinations ​  ​ that ​  ​ help ​  ​ personalize ​  ​ experiences ​  ​ and    advertise ​  ​ products. ​  ​ beyond ​  ​ the ​  ​ familiar, ​  ​ these ​  ​ systems ​  ​ are ​  ​ also ​  ​ being ​  ​ introduced ​  ​ in ​  ​ critical    areas ​  ​ like ​  ​ law, ​  ​ finance, ​  ​ policing ​  ​ and ​  ​ the ​  ​ workplace, ​  ​ where ​  ​ they ​  ​ are ​  ​ increasingly ​  ​ used ​  ​ to    predict ​  ​ everything ​  ​ from ​  ​ our ​  ​ taste ​  ​ in ​  ​ music ​  ​ to ​  ​ our ​  ​ likelihood ​  ​ of ​  ​ committing ​  ​ a ​  ​ crime ​  ​ to ​  ​ our    fitness ​  ​ for ​  ​ a ​  ​ job ​  ​ or ​  ​ an ​  ​ educational ​  ​ opportunity.        ai ​  ​ companies ​  ​ promise ​  ​ that ​  ​ the ​  ​ technologies ​  ​ they ​  ​ create ​  ​ can ​  ​ automate ​  ​ the ​  ​ toil ​  ​ of ​  ​ repetitive    work, ​  ​ identify ​  ​ subtle ​  ​ behavioral ​  ​ patterns ​  ​ and ​  ​ much ​  ​ more. ​  ​ however, ​  ​ the ​  ​ analysis ​  ​ and    understanding ​  ​ of ​  ​ artificial ​  ​ intelligence ​  ​ should ​  ​ not ​  ​ be ​  ​ limited ​  ​ to ​  ​ its ​  ​ technical ​  ​ capabilities. ​  ​ the    design ​  ​ and ​  ​ implementation ​  ​ of ​  ​ this ​  ​ next ​  ​ generation ​  ​ of ​  ​ computational ​  ​ tools ​  ​ presents ​  ​ deep    normative ​  ​ and ​  ​ ethical ​  ​ challenges ​  ​ for ​  ​ our ​  ​ existing ​  ​ social, ​  ​ economic ​  ​ and ​  ​ political    relationships ​  ​ and ​  ​ institutions, ​  ​ and ​  ​ these ​  ​ changes ​  ​ are ​  ​ already ​  ​ underway. ​  ​ simply ​  ​ put, ​  ​ ai ​  ​ does    not ​  ​ exist ​  ​ in ​  ​ a ​  ​ vacuum. ​  ​ we ​  ​ must ​  ​ also ​  ​ ask ​  ​ how ​  ​ broader ​  ​ phenomena ​  ​ like ​  ​ widening ​  ​ inequality,    an ​  ​ intensification ​  ​ of ​  ​ concentrated ​  ​ geopolitical ​  ​ power ​  ​ and ​  ​ populist ​  ​ political ​  ​ movements ​  ​ will    shape ​  ​ and ​  ​ be ​  ​ shaped ​  ​ by ​  ​ the ​  ​ development ​  ​ and ​  ​ application ​  ​ of ​  ​ ai ​  ​ technologies.       building ​  ​ on ​  ​ the ​  ​ inaugural ​  ​ 2016 ​  ​ report, ​  ​​ the ​  ​ ai ​  ​ now ​  ​ 2017 ​  ​ report ​ ​  ​ addresses ​  ​ the ​  ​ most ​  ​ recent    scholarly ​  ​ literature ​  ​ in ​  ​ order ​  ​ to ​  ​ raise ​  ​ critical ​  ​ social ​  ​ questions ​  ​ that ​  ​ will ​  ​ shape ​  ​ our ​  ​ present ​  ​ and    near ​  ​ future. ​  ​ a ​  ​ year ​  ​ is ​  ​ a ​  ​ long ​  ​ time ​  ​ in ​  ​ ai ​  ​ research, ​  ​ and ​  ​ this ​  ​ report ​  ​ focuses ​  ​ on ​  ​ new    developments ​  ​ in ​  ​ four ​  ​ areas: ​  ​ labor ​  ​ and ​  ​ automation, ​  ​ bias ​  ​ and ​  ​ inclusion, ​  ​ rights ​  ​ and ​  ​ liberties,    and ​  ​ ethics ​  ​ and ​  ​ governance. ​  ​ we ​  ​ identify ​  ​ emerging ​  ​ challenges ​  ​ in ​  ​ each ​  ​ of ​  ​ these ​  ​ areas ​  ​ and    make ​  ​ recommendations ​  ​ to ​  ​ ensure ​  ​ that ​  ​ the ​  ​ benefits ​  ​ of ​  ​ ai ​  ​ will ​  ​ be ​  ​ shared ​  ​ broadly, ​  ​ and ​  ​ that    risks ​  ​ can ​  ​ be ​  ​ identified ​  ​ and ​  ​ mitigated.       labor ​  ​ and ​  ​ automation ​ : ​  ​ popular ​  ​ media ​  ​ narratives ​  ​ have ​  ​ emphasized ​  ​ the ​  ​ prospect ​  ​ of    mass ​  ​ job ​  ​ loss ​  ​ due ​  ​ to ​  ​ automation ​  ​ and ​  ​ the ​  ​ widescale ​  ​ adoption ​  ​ of ​  ​ robots. ​  ​ such ​  ​ serious    scenarios ​  ​ deserve ​  ​ sustained ​  ​ empirical ​  ​ attention, ​  ​ but ​  ​ some ​  ​ of ​  ​ the ​  ​ best ​  ​ recent ​  ​ work    on ​  ​ ai ​  ​ and ​  ​ labor ​  ​ has ​  ​ focused ​  ​ instead ​  ​ on ​  ​ specific ​  ​ sectors ​  ​ and ​  ​ tasks. ​  ​ while ​  ​ few ​  ​ jobs ​  ​ will    be ​  ​ completely ​  ​ automated ​  ​ in ​  ​ the ​  ​ near ​  ​ term, ​  ​ researchers ​  ​ estimate ​  ​ that ​  ​ about ​  ​ a ​  ​ third    of ​  ​ workplace ​  ​ tasks ​  ​ can ​  ​ be ​  ​ automated ​  ​ for ​  ​ the ​  ​ majority ​  ​ of ​  ​ workers. ​  ​ new ​  ​ policies ​  ​ such    as ​  ​ the ​  ​ universal ​  ​ basic ​  ​ income ​  ​ (ubi) ​  ​ are ​  ​ being ​  ​ designed ​  ​ to ​  ​ address ​  ​ concerns ​  ​ about    job ​  ​ loss, ​  ​ but ​  ​ these ​  ​ need ​  ​ much ​  ​ more ​  ​ study.        an ​  ​ underexplored ​  ​ area ​  ​ that ​  ​ needs ​  ​ urgent ​  ​ attention ​  ​ is ​  ​ how ​  ​ ai ​  ​ and ​  ​ related    algorithmic ​  ​ systems ​  ​ are ​  ​ already ​  ​ changing ​  ​ the ​  ​ balance ​  ​ of ​  ​ workplace ​  ​ power. ​  ​ machine    learning ​  ​ techniques ​  ​ are ​  ​ quickly ​  ​ being ​  ​ integrated ​  ​ into ​  ​ management ​  ​ and ​  ​ hiring  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 4    decisions, ​  ​ including ​  ​ in ​  ​ the ​  ​ so-called ​  ​ gig ​  ​ economy ​  ​ where ​  ​ technical ​  ​ systems ​  ​ match    workers ​  ​ with ​  ​ jobs, ​  ​ but ​  ​ also ​  ​ across ​  ​ more ​  ​ traditional ​  ​ white ​  ​ collar ​  ​ industries. ​  ​ new    systems ​  ​ make ​  ​ promises ​  ​ of ​  ​​  ​ flexibility ​  ​ and ​  ​ efficiency, ​  ​ but ​  ​ they ​  ​ also ​  ​ intensify ​  ​ the    surveillance ​  ​ of ​  ​ workers, ​  ​ who ​  ​ often ​  ​ do ​  ​ not ​  ​ know ​  ​ when ​  ​ and ​  ​ how ​  ​ they ​  ​ are ​  ​ being    tracked ​  ​ and ​  ​ evaluated, ​  ​ or ​  ​ why ​  ​ they ​  ​ are ​  ​ hired ​  ​ or ​  ​ fired. ​  ​ furthermore, ​  ​ ai-assisted    forms ​  ​ of ​  ​ management ​  ​ may ​  ​ replace ​  ​ more ​  ​ democratic ​  ​ forms ​  ​ of ​  ​ bargaining ​  ​ between    workers ​  ​ and ​  ​ employers, ​  ​ increasing ​  ​ owner ​  ​ power ​  ​ under ​  ​ the ​  ​ guise ​  ​ of ​  ​ technical    neutrality.       bias ​  ​ and ​  ​ inclusion ​ : ​  ​ one ​  ​ of ​  ​ the ​  ​ most ​  ​ active ​  ​ areas ​  ​ of ​  ​ critical ​  ​ ai ​  ​ research ​  ​ in ​  ​ the ​  ​ past    year ​  ​ has ​  ​ been ​  ​ the ​  ​ study ​  ​ of ​  ​ bias, ​  ​ both ​  ​ in ​  ​ its ​  ​ more ​  ​ formal ​  ​ statistical ​  ​ sense ​  ​ and ​  ​ in ​  ​ the    wider ​  ​ legal ​  ​ and ​  ​ normative ​  ​ senses. ​  ​ at ​  ​ their ​  ​ best, ​  ​ ai ​  ​ systems ​  ​ can ​  ​ be ​  ​ used ​  ​ to ​  ​ augment    human ​  ​ judgement ​  ​ and ​  ​ reduce ​  ​ both ​  ​ our ​  ​ conscious ​  ​ and ​  ​ unconscious ​  ​ biases. ​  ​ however,    training ​  ​ data, ​  ​ algorithms, ​  ​ and ​  ​ other ​  ​ design ​  ​ choices ​  ​ that ​  ​ shape ​  ​ ai ​  ​ systems ​  ​ may    reflect ​  ​ and ​  ​ amplify ​  ​ existing ​  ​ cultural ​  ​ assumptions ​  ​ and ​  ​ inequalities. ​  ​ for ​  ​ example,    natural ​  ​ language ​  ​ processing ​  ​ techniques ​  ​ trained ​  ​ on ​  ​ a ​  ​ corpus ​  ​ of ​  ​ internet ​  ​ writing ​  ​ from    the ​  ​ 1990s ​  ​ may ​  ​ reflect ​  ​ stereotypical ​  ​ and ​  ​ dated ​  ​ word ​  ​ associations—the ​  ​ word    “female” ​  ​ might ​  ​ be ​  ​ associated ​  ​ with ​  ​ “receptionist.” ​  ​ if ​  ​ these ​  ​ models ​  ​ are ​  ​ used ​  ​ to ​  ​ make    educational ​  ​ or ​  ​ hiring ​  ​ decisions, ​  ​ they ​  ​ may ​  ​ reinforce ​  ​ existing ​  ​ inequalities, ​  ​ regardless    of ​  ​ the ​  ​ intentions ​  ​ or ​  ​ even ​  ​ knowledge ​  ​ of ​  ​ system’s ​  ​ designers.        those ​  ​ researching, ​  ​ designing ​  ​ and ​  ​ developing ​  ​ ai ​  ​ systems ​  ​ tend ​  ​ to ​  ​ be ​  ​ male, ​  ​ highly    educated ​  ​ and ​  ​ very ​  ​ well ​  ​ paid. ​  ​ yet ​  ​ their ​  ​ systems ​  ​ are ​  ​ working ​  ​ to ​  ​ predict ​  ​ and    understand ​  ​ the ​  ​ behaviors ​  ​ and ​  ​ preferences ​  ​ of ​  ​ diverse ​  ​ populations ​  ​ with ​  ​ very ​  ​ different    life ​  ​ experiences. ​  ​ more ​  ​ diversity ​  ​ within ​  ​ the ​  ​ fields ​  ​ building ​  ​ these ​  ​ systems ​  ​ will ​  ​ help    ensure ​  ​ that ​  ​ they ​  ​ reflect ​  ​ a ​  ​ broader ​  ​ variety ​  ​ of ​  ​ viewpoints.        rights ​  ​ and ​  ​ liberties ​ : ​  ​ the ​  ​ application ​  ​ of ​  ​ ai ​  ​ systems ​  ​ in ​  ​ public ​  ​ and ​  ​ civil ​  ​ institutions ​  ​ is    challenging ​  ​ existing ​  ​ political ​  ​ arrangements, ​  ​ especially ​  ​ in ​  ​ a ​  ​ global ​  ​ political ​  ​ context    shaped ​  ​ by ​  ​ events ​  ​ such ​  ​ as ​  ​ the ​  ​ election ​  ​ of ​  ​ donald ​  ​ trump ​  ​ in ​  ​ the ​  ​ united ​  ​ states. ​  ​ a    number ​  ​ of ​  ​ governmental ​  ​ agencies ​  ​ are ​  ​ already ​  ​ partnering ​  ​ with ​  ​ private ​  ​ corporations    to ​  ​ deploy ​  ​ ai ​  ​ systems ​  ​ in ​  ​ ways ​  ​ that ​  ​ challenge ​  ​​  ​ civil ​  ​ rights ​  ​ and ​  ​ liberties. ​  ​ for ​  ​ example,    police ​  ​ body ​  ​ camera ​  ​ footage ​  ​ is ​  ​ being ​  ​ used ​  ​ to ​  ​ train ​  ​ machine ​  ​ vision ​  ​ algorithms ​  ​ for ​  ​ law    enforcement, ​  ​ raising ​  ​ privacy ​  ​ and ​  ​ accountability ​  ​ concerns. ​  ​ ai ​  ​ technologies ​  ​ are ​  ​ also    being ​  ​ deployed ​  ​ in ​  ​ the ​  ​ very ​  ​ legal ​  ​ institutions ​  ​ designed ​  ​ to ​  ​ safeguard ​  ​ our ​  ​ rights ​  ​ and    liberties, ​  ​ with ​  ​ proprietary ​  ​ risk ​  ​ assessment ​  ​ algorithms ​  ​ already ​  ​ being ​  ​ used ​  ​ to ​  ​ help    judges ​  ​ make ​  ​ sentencing ​  ​ and ​  ​ bail ​  ​ decisions, ​  ​ potentially ​  ​ amplifying ​  ​ and ​  ​ naturalizing    longstanding ​  ​ biases, ​  ​ and ​  ​ rendering ​  ​ them ​  ​ more ​  ​ opaque ​  ​ to ​  ​ oversight ​  ​ and ​  ​ scrutiny.        privacy ​  ​ rights ​  ​ represent ​  ​ a ​  ​ particularly ​  ​ sensitive ​  ​ challenge ​  ​ for ​  ​ current ​  ​ ai ​  ​ applications,    especially ​  ​ in ​  ​ domains ​  ​ like ​  ​ healthcare, ​  ​ where ​  ​ ai ​  ​ is ​  ​ being ​  ​ used ​  ​ to ​  ​ help ​  ​ make    diagnoses. ​  ​ for ​  ​ ai ​  ​ to ​  ​ deliver ​  ​ on ​  ​ its ​  ​ promises, ​  ​ it ​  ​ requires ​  ​ large ​  ​ amounts ​  ​ of ​  ​ data, ​  ​ which    likely ​  ​ means ​  ​ an ​  ​ increase ​  ​ in ​  ​ data ​  ​ collection, ​  ​ both ​  ​ its ​  ​ scale ​  ​ and ​  ​ granularity. ​  ​ without    contextual ​  ​ knowledge, ​  ​ informed ​  ​ consent, ​  ​ and ​  ​ due ​  ​ processes ​  ​ mechanisms, ​  ​ these    systems ​  ​ can ​  ​ create ​  ​ risks ​  ​ that ​  ​ threaten ​  ​ and ​  ​ expose ​  ​ already ​  ​ vulnerable ​  ​ populations.        ethics ​  ​ and ​  ​ governance ​ : ​  ​ the ​  ​ areas ​  ​ of ​  ​ ethics ​  ​ and ​  ​ governance ​  ​ attempt ​  ​ to ​  ​ address    many ​  ​ of ​  ​ the ​  ​ challenges ​  ​ and ​  ​ opportunities ​  ​ identified ​  ​ above. ​  ​ we ​  ​ track ​  ​ the ​  ​ growing  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 5    interest ​  ​ in ​  ​ ethical ​  ​ codes ​  ​ of ​  ​ conduct ​  ​ and ​  ​ principles, ​  ​ while ​  ​ noting ​  ​ that ​  ​ these ​  ​ need ​  ​ to    be ​  ​ tied ​  ​ more ​  ​ closely ​  ​ to ​  ​ everyday ​  ​ ai ​  ​ design ​  ​ and ​  ​ development. ​  ​ the ​  ​ military ​  ​ use ​  ​ of    artificial ​  ​ intelligence ​  ​ takes ​  ​ on ​  ​ a ​  ​ special ​  ​ urgency ​  ​ in ​  ​ the ​  ​ case ​  ​ of ​  ​ lethal ​  ​ autonomous    weapons ​  ​ systems.       there ​  ​ are ​  ​ multiple ​  ​ signs ​  ​ of ​  ​ progress ​  ​ in ​  ​ the ​  ​ development ​  ​ of ​  ​ professional ​  ​ and ​  ​ legal    ethical ​  ​ codes ​  ​ to ​  ​ govern ​  ​ the ​  ​ design ​  ​ and ​  ​ application ​  ​ of ​  ​ ai ​  ​ technologies. ​  ​ however, ​  ​ in    the ​  ​ face ​  ​ of ​  ​ rapid, ​  ​ distributed, ​  ​ and ​  ​ often ​  ​ proprietary ​  ​ ai ​  ​ development ​  ​ and    implementation, ​  ​ such ​  ​ forms ​  ​ of ​  ​ soft ​  ​ governance ​  ​ face ​  ​ real ​  ​ challenges. ​  ​ among ​  ​ these    are ​  ​ problems ​  ​ of ​  ​ coordination ​  ​ among ​  ​ different ​  ​ ethical ​  ​ codes, ​  ​ as ​  ​ well ​  ​ as ​  ​ questions    around ​  ​ enforcement ​  ​ mechanisms ​  ​ that ​  ​ would ​  ​ go ​  ​ beyond ​  ​ voluntary ​  ​ cooperation ​  ​ by    individuals ​  ​ working ​  ​ in ​  ​ research ​  ​ and ​  ​ industry. ​  ​ new ​  ​ ethical ​  ​ frameworks ​  ​ for ​  ​ ai ​  ​ need ​  ​ to    move ​  ​ beyond ​  ​ individual ​  ​ responsibility ​  ​ to ​  ​ hold ​  ​ powerful ​  ​ industrial, ​  ​ governmental ​  ​ and    military ​  ​ interests ​  ​ accountable ​  ​ as ​  ​ they ​  ​ design ​  ​ and ​  ​ employ ​  ​ ai.        the ​  ​ following ​  ​ report ​  ​ develops ​  ​ these ​  ​ themes ​  ​ in ​  ​ detail, ​  ​ and ​  ​ reflects ​  ​ on ​  ​ the ​  ​ latest ​  ​ academic    research. ​  ​ ai ​  ​ is ​  ​ already ​  ​ with ​  ​ us, ​  ​ and ​  ​ we ​  ​ are ​  ​ now ​  ​ faced ​  ​ with ​  ​ important ​  ​ choices ​  ​ on ​  ​ how ​  ​ it ​  ​ will    be ​  ​ designed ​  ​ and ​  ​ applied. ​  ​ most ​  ​ promisingly, ​  ​ the ​  ​ approaches ​  ​ described ​  ​ in ​  ​ this ​  ​ report    demonstrate ​  ​ that ​  ​ there ​  ​ is ​  ​ growing ​  ​ interest ​  ​ in ​  ​ developing ​  ​ ai ​  ​ that ​  ​ is ​  ​ attuned ​  ​ to ​  ​ underlying    issues ​  ​ of ​  ​ fairness ​  ​ and ​  ​ equality.         \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 6    introduction    in ​  ​ july ​  ​ of ​  ​ 2016, ​  ​ kate ​  ​ crawford ​  ​ and ​  ​ meredith ​  ​ whittaker ​  ​ co-chaired ​  ​ the ​  ​ first ​  ​​ ai ​  ​ now    symposium ​ ​  ​ in ​  ​ collaboration ​  ​ with ​  ​ the ​  ​ obama ​  ​ white ​  ​ house’s ​  ​ office ​  ​ of ​  ​ science ​  ​ and    technology ​  ​ policy ​  ​ and ​  ​ the ​  ​ national ​  ​ economic ​  ​ council. ​  ​ the ​  ​ event ​  ​ brought ​  ​ together ​  ​ experts    and ​  ​ members ​  ​ of ​  ​ the ​  ​ public ​  ​ to ​  ​ discuss ​  ​ the ​  ​ near-term ​  ​ social ​  ​ and ​  ​ economic ​  ​ impacts ​  ​ of    artificial ​  ​ intelligence ​  ​ (ai). ​  ​ ai ​  ​ systems ​  ​ are ​  ​ already ​  ​ being ​  ​ integrated ​  ​ in ​  ​ social, ​  ​ political ​  ​ and  1  economic ​  ​ domains, ​  ​ and ​  ​ the ​  ​ implications ​  ​ can ​  ​ be ​  ​ complex ​  ​ and ​  ​ unpredictable. ​  ​ the    now-annual ​  ​​ ai ​  ​ now ​  ​ symposium ​ ​  ​ focuses ​  ​ on ​  ​ ai’s ​  ​ core ​  ​ social ​  ​ implications, ​  ​ bringing ​  ​ together    leading ​  ​ experts ​  ​ from ​  ​ across ​  ​ sectors ​  ​ and ​  ​ disciplines ​  ​ with ​  ​ the ​  ​ aim ​  ​ of ​  ​ better ​  ​ understanding    how ​  ​ ai ​  ​ systems ​  ​ are ​  ​ already ​  ​ working ​  ​ in ​  ​ the ​  ​ world.    the ​  ​​ ai ​  ​ now ​  ​ 2016 ​  ​ symposium ​ ​  ​ identified ​  ​ instances ​  ​ where ​  ​ ai ​  ​ challenged ​  ​ current ​  ​ thinking    about ​  ​ professional ​  ​ responsibilities, ​  ​ decision-making ​  ​ and ​  ​ accountability. ​  ​ following ​  ​ this, ​  ​​ the    ai ​  ​ now ​  ​ 2016 ​  ​ report ​ ​  ​ reflected ​  ​​  ​ expert ​  ​ discussion ​  ​ and ​  ​ provided ​  ​ recommendations ​  ​ for ​  ​ future    research ​  ​ and ​  ​ policy ​  ​ interventions.   2  the ​  ​​ ai ​  ​ now ​  ​ 2017 ​  ​ symposium ​ ​  ​ deepened ​  ​ this ​  ​ examination ​  ​ of ​  ​ the ​  ​ near-term ​  ​ social ​  ​ and    economic ​  ​ implications ​  ​ of ​  ​ ai, ​  ​ and ​  ​ the ​  ​ accompanying ​  ​ report ​  ​ provides ​  ​ an ​  ​ overview ​  ​ of ​  ​ the ​  ​ key    issues ​  ​ that ​  ​ the ​  ​​ 2017 ​  ​ symposium ​  ​​ addressed. ​  ​ these ​  ​ are: ​  ​ 1) ​  ​​ labor ​  ​ and ​  ​ automation ​ , ​  ​ 2) ​  ​​ bias    and ​  ​ inclusion ​ , ​ ​  ​ 3) ​  ​ rights ​  ​ and ​  ​ liberties ​ ​  ​ and ​  ​ 4) ​  ​​ ethics ​  ​ and ​  ​ governance ​ . ​  ​ in ​  ​ selecting ​  ​ these ​  ​ four    themes, ​  ​ we ​  ​ are ​  ​ building ​  ​ on ​  ​ the ​  ​ 2016 ​  ​ report ​  ​ and ​  ​ introducing ​  ​ new ​  ​ areas ​  ​ of ​  ​ concern, ​  ​ with  3  close ​  ​ attention ​  ​ to ​  ​ developments ​  ​ that ​  ​ have ​  ​ occurred ​  ​ in ​  ​ the ​  ​ last ​  ​ 12 ​  ​ months.     the ​  ​ first ​  ​ section ​  ​ on ​  ​​ labor ​  ​ and ​  ​ automation ​ ​  ​ considers ​  ​ the ​  ​ need ​  ​ for ​  ​ a ​  ​ more ​  ​ granular,    skills-based, ​  ​ and ​  ​ sectoral ​  ​ approach ​  ​ to ​  ​ understanding ​  ​ ai ​  ​ and ​  ​ automation’s ​  ​ impacts ​  ​ on ​  ​ labor    practices. ​  ​ while ​  ​ big ​  ​ questions ​  ​ about ​  ​ what ​  ​ implications ​  ​ automation ​  ​ and ​  ​ ai ​  ​ have ​  ​ for ​  ​ labor    overall ​  ​ are ​  ​ still ​  ​ wide ​  ​ open, ​  ​ there ​  ​ are ​  ​ also ​  ​ important ​  ​ questions ​  ​ about ​  ​ the ​  ​ distinct ​  ​ roles ​  ​ that    automation ​  ​ and ​  ​ ai ​  ​ will ​  ​ play ​  ​ within ​  ​ specific ​  ​ industries, ​  ​ sectors ​  ​ and ​  ​ tasks ​  ​ - ​  ​ particularly ​  ​ how ​  ​ it    will ​  ​ be ​  ​ used ​  ​ as ​  ​ a ​  ​ tool ​  ​ of ​  ​ employee ​  ​ hiring, ​  ​ firing ​  ​ and ​  ​ management. ​  ​ the ​  ​ second ​  ​ section    focuses ​  ​ on ​  ​​ bias ​  ​ and ​  ​ inclusion ​ , ​  ​ a ​  ​ growing ​  ​ concern ​  ​ among ​  ​ those ​  ​ looking ​  ​ at ​  ​ the ​  ​ design ​  ​ and    social ​  ​ implications ​  ​ of ​  ​ ai ​  ​ decision-making ​  ​ systems. ​  ​ here, ​  ​ we ​  ​ address ​  ​ the ​  ​ problem ​  ​ of    diversity ​  ​ and ​  ​ inclusion ​  ​ within ​  ​ the ​  ​ ai ​  ​ industry ​  ​ itself. ​  ​ we ​  ​ also ​  ​ share ​  ​ new ​  ​ technical ​  ​ advances    1  as ​  ​ ai ​  ​ pioneers ​  ​ stuart ​  ​ russell ​  ​ and ​  ​ peter ​  ​ norvig ​  ​ point ​  ​ out, ​  ​ the ​  ​ history ​  ​ of ​  ​ artificial ​  ​ intelligence ​  ​ has ​  ​ not ​  ​ produced ​  ​ a ​  ​ clear    definition ​  ​ of ​  ​ ai, ​  ​ but ​  ​ can ​  ​ be ​  ​ seen ​  ​ as ​  ​ variously ​  ​ emphasizing ​  ​ four ​  ​ possible ​  ​ goals: ​  ​ “systems ​  ​ that ​  ​ think ​  ​ like ​  ​ humans, ​  ​ systems ​  ​ that    act ​  ​ like ​  ​ humans, ​  ​ systems ​  ​ that ​  ​ think ​  ​ rationally, ​  ​ systems ​  ​ that ​  ​ act ​  ​ rationally.” ​  ​ in ​  ​ this ​  ​ report ​  ​ we ​  ​ use ​  ​ the ​  ​ term ​  ​ ai ​  ​ to ​  ​ refer ​  ​ to ​  ​ a    broad ​  ​ assemblage ​  ​ of ​  ​ technologies, ​  ​ from ​  ​ early ​  ​ rule-based ​  ​ algorithmic ​  ​ systems ​  ​ to ​  ​ deep ​  ​ neural ​  ​ networks, ​  ​ all ​  ​ of ​  ​ which ​  ​ rely ​  ​ on    an ​  ​ array ​  ​ of ​  ​ data ​  ​ and ​  ​ computational ​  ​ infrastructures. ​  ​ these ​  ​ technologies ​  ​ span ​  ​ speech ​  ​ recognition, ​  ​ language ​  ​ translation,    image ​  ​ recognition, ​  ​ predictions ​  ​ and ​  ​ determinations ​  ​ - ​  ​ tasks ​  ​ that ​  ​ have ​  ​ traditionally ​  ​ relied ​  ​ on ​  ​ human ​  ​ capacities ​  ​ across ​  ​ the ​  ​ four    goals ​  ​ russell ​  ​ and ​  ​ norvig ​  ​ identify. ​  ​ while ​  ​ ai ​  ​ is ​  ​ not ​  ​ new, ​  ​ recent ​  ​ developments ​  ​ in ​  ​ the ​  ​ ability ​  ​ to ​  ​ collect ​  ​ and ​  ​ store ​  ​ large ​  ​ quantities    of ​  ​ data, ​  ​ combined ​  ​ with ​  ​ advances ​  ​ in ​  ​ computational ​  ​ power ​  ​ have ​  ​ led ​  ​ to ​  ​ significant ​  ​ breakthroughs ​  ​ in ​  ​ the ​  ​ field ​  ​ over ​  ​ the ​  ​ last ​  ​ ten    years. ​  ​ stuart ​  ​ j. ​  ​ russell ​  ​ and ​  ​ peter ​  ​ norvig, ​  ​ artificial ​  ​ intelligence: ​  ​ a ​  ​ modern ​  ​ approach, ​  ​ englewood ​  ​ cliffs, ​  ​ nj: ​  ​ prentice ​  ​ hall,    1995: ​  ​ 27    2  ai ​  ​ now, ​  ​ “the ​  ​ ai ​  ​ now ​  ​ report: ​  ​ the ​  ​ social ​  ​ and ​  ​ economic ​  ​ implications ​  ​ of ​  ​ artificial ​  ​ intelligence ​  ​ technologies ​  ​ in ​  ​ the ​  ​ near-term,”    (2016) ​  ​​ https://ainowinstitute.org/ai_now_2016_report.pdf ​ .    3 ​  ​​  ​​  ​​  ​​  ​​ ibid.  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 7    that ​  ​ help ​  ​ to ​  ​ better ​  ​ understand ​  ​ and ​  ​ mitigate ​  ​ biases ​  ​ that ​  ​ ai ​  ​ systems ​  ​ may ​  ​ perpetuate ​  ​ and    even ​  ​ amplify ​  ​ due ​  ​ to ​  ​ biased ​  ​ training ​  ​ data, ​  ​ faulty ​  ​ algorithms ​  ​ or ​  ​ other ​  ​ factors. ​  ​ the ​  ​ third    section, ​  ​ on ​  ​​ rights ​  ​ and ​  ​ liberties ​ , ​  ​ begins ​  ​ by ​  ​ recognizing ​  ​ the ​  ​ recent ​  ​ rise ​  ​ of ​  ​ political    authoritarianism, ​  ​ and ​  ​ asks ​  ​ about ​  ​ the ​  ​ role ​  ​ of ​  ​ ai ​  ​ systems ​  ​ in ​  ​ either ​  ​ supporting ​  ​ or ​  ​ eroding    citizens’ ​  ​ rights ​  ​ and ​  ​ liberties ​  ​ in ​  ​ areas ​  ​ like ​  ​ criminal ​  ​ justice, ​  ​ law ​  ​ enforcement, ​  ​ housing, ​  ​ hiring,    lending ​  ​ and ​  ​ other ​  ​ domains. ​  ​ the ​  ​ last ​  ​ section, ​  ​ on ​  ​​ ethics ​  ​ and ​  ​ governance ​ , ​  ​ connects ​  ​ ai ​  ​ as ​  ​ we    see ​  ​ it ​  ​ today ​  ​ with ​  ​ the ​  ​ history ​  ​ of ​  ​ ai ​  ​ research ​  ​ and ​  ​ development. ​  ​ it ​  ​ also ​  ​ looks ​  ​ at ​  ​​ whose    concerns ​  ​ are ​  ​ ultimately ​  ​ reflected ​  ​ in ​  ​ the ​  ​ ethics ​  ​ of ​  ​ ai, ​  ​ and ​  ​ how ​  ​ ethical ​  ​ codes ​  ​ and ​  ​ other    strategies ​  ​ could ​  ​ be ​  ​ developed ​  ​ in ​  ​ a ​  ​ time ​  ​ of ​  ​ political ​  ​ volatility.    we ​  ​ are ​  ​ in ​  ​ the ​  ​ early ​  ​ stages ​  ​ of ​  ​ a ​  ​ long-term ​  ​ discussion, ​  ​ and ​  ​ accordingly, ​  ​ there ​  ​ are ​  ​ as ​  ​ many    new ​  ​ questions ​  ​ as ​  ​ there ​  ​ are ​  ​ answers ​  ​ to ​  ​ the ​  ​ old ​  ​ ones. ​  ​ we ​  ​ hope ​  ​ this ​  ​ report ​  ​ provides ​  ​ a    productive ​  ​ grounding ​  ​ in ​  ​ the ​  ​ extraordinary ​  ​ challenges ​  ​ and ​  ​ opportunities ​  ​ of ​  ​ the ​  ​ current    moment, ​  ​ and ​  ​ helps ​  ​ spur ​  ​ research ​  ​ and ​  ​ inquiry ​  ​ into ​  ​ the ​  ​ social ​  ​ and ​  ​ economic ​  ​ implications ​  ​ of    the ​  ​ turn ​  ​ to ​  ​ ai. ​  ​ .     labor ​  ​ and ​  ​ automation    the ​  ​ editors ​  ​ of ​  ​​ nature ​  ​​ have ​  ​ argued ​  ​ that ​  ​ we ​  ​ need ​  ​ to ​  ​ match ​  ​ technical ​  ​ ai ​  ​ research ​  ​ funding    with ​  ​ “solid, ​  ​ well-funded ​  ​ research ​  ​ to ​  ​ anticipate ​  ​ the ​  ​ scenarios ​  ​ [ai] ​  ​ could ​  ​ bring ​  ​ about, ​  ​ and ​  ​ to    study ​  ​ possible ​  ​ political ​  ​ and ​  ​ economic ​  ​ reforms ​  ​ that ​  ​ will ​  ​ allow ​  ​ those ​  ​ usurped ​  ​ by ​  ​ machinery    to ​  ​ contribute ​  ​ to ​  ​ society.” ​  ​ the ​ ​  ​ ai ​  ​ now ​  ​ labor ​  ​ primer ​ ​  ​ described ​  ​ how ​  ​ forms ​  ​ of ​  ​ automation  4  based ​  ​ on ​  ​ machine ​  ​ learning ​  ​ and ​  ​ robotics ​  ​ have ​  ​ the ​  ​ potential ​  ​ to ​  ​ both ​  ​ increase ​  ​ the    productivity ​  ​ of ​  ​ labor ​  ​ and ​  ​ to ​  ​ exacerbate ​  ​ existing ​  ​ inequalities ​  ​ in ​  ​ the ​  ​ distribution ​  ​ of ​  ​ wealth.  5  in ​  ​ an ​  ​ economic ​  ​ context ​  ​ characterized ​  ​ by ​  ​ both ​  ​ low ​  ​ productivity ​  ​ growth ​  ​ and ​  ​ historically ​  ​ high    levels ​  ​ of ​  ​ inequality, ​  ​ it ​  ​ will ​  ​ be ​  ​ important ​  ​ to ​  ​ find ​  ​ ways ​  ​ to ​  ​ use ​  ​ ai ​  ​ to ​  ​ promote ​  ​ equality ​  ​ and    shared ​  ​ prosperity.  6  while ​  ​ there ​  ​ is ​  ​ still ​  ​ considerable ​  ​ attention ​  ​ focused ​  ​ on ​  ​ large, ​  ​ structural ​  ​ changes ​  ​ in ​  ​ labor    markets ​  ​ and ​  ​ on ​  ​ the ​  ​ economy ​  ​ as ​  ​ a ​  ​ whole, ​  ​ new ​  ​ research ​  ​ has ​  ​ been ​  ​ focusing ​  ​ on ​  ​ specific    industries ​  ​ and ​  ​ the ​  ​ impact ​  ​ of ​  ​ ai ​  ​ systems ​  ​ on ​  ​ particular ​  ​ tasks ​  ​ within ​  ​ a ​  ​ profession. ​  ​ this ​  ​ section    describes ​  ​ new ​  ​ developments ​  ​ in ​  ​ ai’s ​  ​ application ​  ​ within ​  ​ various ​  ​ labor ​  ​ sectors, ​  ​ and ​  ​ suggests    directions ​  ​ that ​  ​ research ​  ​ could ​  ​ productively ​  ​ explore ​  ​ in ​  ​ the ​  ​ future.    research ​  ​ by ​  ​ sector ​  ​ and ​  ​ task    at ​  ​ the ​  ​ beginning ​  ​ of ​  ​ 2017, ​  ​ the ​  ​ mckinsey ​  ​ global ​  ​ institute ​  ​ (mgi) ​  ​ released ​  ​ a ​  ​ report ​  ​ looking ​  ​ at    specific ​  ​ workplace ​  ​​ tasks ​ ​  ​ and ​  ​ whether ​  ​ they ​  ​ were ​  ​ more ​  ​ or ​  ​ less ​  ​ susceptible ​  ​ to ​  ​ automation,    specifically ​  ​ those ​  ​ involving ​  ​ “predictable ​  ​ physical” ​  ​ activities ​  ​ and ​  ​ those ​  ​ involving ​  ​ data    collection ​  ​ or ​  ​ processing. ​  ​ while ​  ​ relatively ​  ​ few ​  ​ current ​  ​ jobs ​  ​ can ​  ​ be ​  ​ totally ​  ​ automated ​  ​ with    4  “anticipating ​  ​ artificial ​  ​ intelligence,” ​  ​​ nature ​ ​  ​ 532, ​  ​ no. ​  ​ 7600 ​  ​ (april ​  ​ 28, ​  ​ 2016): ​  ​ 413, ​  ​ doi:10.1038/532413a.     5  “labor ​  ​ and ​  ​ ai” ​  ​ (new ​  ​ york, ​  ​ ny: ​  ​​ ai ​  ​ now ​ , ​  ​ july ​  ​ 7, ​  ​ 2016), ​  ​​ https://ainowinstitute.org/ai_now_2016_primers.pdf ​ .    6  jason ​  ​ furman, ​  ​ “is ​  ​ this ​  ​ time ​  ​ different? ​  ​ the ​  ​ opportunities ​  ​ and ​  ​ challenges ​  ​ of ​  ​ artificial ​  ​ intelligence,” ​  ​ expanded ​  ​ remarks ​  ​ from    the ​  ​​ ai ​  ​ now ​ ​  ​ expert ​  ​ workshop, ​  ​ july ​  ​ 7, ​  ​ 2016, ​  ​ new ​  ​ york ​  ​ university,    https://obamawhitehouse.archives.gov/sites/default/files/page/files/20160707_cea_ai_furman.pdf ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 8    today’s ​  ​ technology, ​  ​ mgi ​  ​ estimates ​  ​ that ​  ​ 60 ​  ​ percent ​  ​ of ​  ​ all ​  ​ occupations ​  ​ have ​  ​ the ​  ​ potential ​  ​ for    about ​  ​ a ​  ​ third ​  ​ of ​  ​ their ​  ​ activities ​  ​ to ​  ​ be ​  ​ automated. ​  ​ in ​  ​ a ​  ​ similar ​  ​ vein, ​  ​ analysts ​  ​ in ​  ​ deloitte’s  7  human ​  ​ capital ​  ​ division ​  ​ predict ​  ​ a ​  ​ future ​  ​ where ​  ​ human ​  ​ skills ​  ​ will ​  ​ be ​  ​ “augmented” ​  ​ through    “collaboration” ​  ​ with ​  ​ machines ​  ​ capable ​  ​ of ​  ​ performing ​  ​ routine ​  ​ tasks.  8  to ​  ​ prepare ​  ​ for ​  ​ these ​  ​ changes, ​  ​ it ​  ​ will ​  ​ be ​  ​ essential ​  ​ that ​  ​ policymakers ​  ​ have ​  ​ access ​  ​ to ​  ​ robust    data ​  ​ on ​  ​ how ​  ​ advances ​  ​ in ​  ​ machine ​  ​ learning, ​  ​ robotics ​  ​ and ​  ​ the ​  ​ automation ​  ​ of ​  ​ perceptual    tasks ​  ​ are ​  ​ changing ​  ​ the ​  ​ nature ​  ​ and ​  ​ organization ​  ​ of ​  ​ work, ​  ​ and ​  ​ how ​  ​ these ​  ​ changes ​  ​ manifest    across ​  ​ different ​  ​ roles ​  ​ and ​  ​ different ​  ​ sectors. ​  ​ this ​  ​ data ​  ​ will ​  ​ be ​  ​ necessary ​  ​ for ​  ​ any ​  ​ robust ​  ​ policy    proposal. ​  ​ however, ​  ​ a ​  ​ recent ​  ​ report ​  ​ from ​  ​ the ​  ​ national ​  ​ academies ​  ​ of ​  ​ sciences, ​  ​ engineering,    and ​  ​ medicine ​  ​ identifies ​  ​ a ​  ​ lack ​  ​ of ​  ​ such ​  ​ data, ​  ​ finding ​  ​ existing ​  ​ federal ​  ​ statistical ​  ​ data ​  ​ limited ​  ​ in    its ​  ​ capacity ​  ​ to ​  ​ answer ​  ​ these ​  ​ questions. ​  ​ the ​  ​ report ​  ​ recommends ​  ​ new ​  ​ multidisciplinary ​  ​ and    qualitative ​  ​ research ​  ​ methods ​  ​ to ​  ​ capture ​  ​ present ​  ​ and ​  ​ future ​  ​ transformations ​  ​ in ​  ​ work.   9  a ​  ​ series ​  ​ of ​  ​ economic ​  ​ studies ​  ​ have ​  ​ begun ​  ​ to ​  ​ investigate ​  ​ the ​  ​ effects ​  ​ of ​  ​ robots ​  ​ on ​  ​ labor    markets ​  ​ from ​  ​ an ​  ​ empirical ​  ​ perspective. ​  ​​  ​ a ​  ​ 2015 ​  ​ paper ​  ​ by ​  ​ george ​  ​ graetz ​  ​ and ​  ​ guy ​  ​ michaels    used ​  ​ new ​  ​ data ​  ​ from ​  ​ the ​  ​ international ​  ​ federation ​  ​ of ​  ​ robots ​  ​ to ​  ​ estimate ​  ​ changes ​  ​ in    productivity ​  ​ and ​  ​ employment ​  ​ due ​  ​ to ​  ​ robot ​  ​ adoption, ​  ​ finding ​  ​ increases ​  ​ in ​  ​ productivity ​  ​ and    slightly ​  ​ lowered ​  ​ working ​  ​ hours ​  ​ for ​  ​ low ​  ​ and ​  ​ middle-skilled ​  ​ workers. ​  ​ using ​  ​ the ​  ​ same ​  ​ data,  10  daron ​  ​ acemoglu ​  ​ and ​  ​ pascual ​  ​ restrepo ​  ​ analyzed ​  ​ developments ​  ​ in ​  ​ labor ​  ​ markets ​  ​ across ​  ​ the    united ​  ​ states ​  ​ from ​  ​ 1990 ​  ​ to ​  ​ 2007. ​  ​ they ​  ​ estimated ​  ​ that ​  ​ the ​  ​ number ​  ​ of ​  ​ jobs ​  ​ lost ​  ​ due ​  ​ to    robots ​  ​ during ​  ​ this ​  ​ period ​  ​ ranged ​  ​ from ​  ​ 360,000 ​  ​ to ​  ​ 670,000, ​  ​ and ​  ​ that ​  ​ this ​  ​ trend ​  ​ could    accelerate ​  ​ with ​  ​ a ​  ​ more ​  ​ intensive ​  ​ adoption ​  ​ of ​  ​ automation ​  ​ across ​  ​ sectors. ​ ​  ​ model  11  assumptions ​  ​ play ​  ​ an ​  ​ important ​  ​ role ​  ​ in ​  ​ these ​  ​ empirical ​  ​ analyses ​  ​ and ​  ​ will ​  ​ need ​  ​ to ​  ​ be  12  continually ​  ​ tested ​  ​ against ​  ​ employment ​  ​ data. ​  ​ to ​  ​ this ​  ​ end, ​  ​ management ​  ​ professor ​  ​ and    former ​  ​ senior ​  ​ economist ​  ​ at ​  ​ the ​  ​ white ​  ​ house ​  ​ council ​  ​ of ​  ​ economic ​  ​ advisers ​  ​ robert ​  ​ seamans    argues ​  ​ that ​  ​ even ​  ​ more ​  ​ fine-grained, ​  ​ company-level ​  ​ data ​  ​ will ​  ​ be ​  ​ necessary ​  ​ to ​  ​ understand    whether ​  ​ ai ​  ​ and ​  ​ automation ​  ​ systems ​  ​ are ​  ​ replacing ​  ​ or ​  ​ complementing ​  ​ human ​  ​ workers.   13  7  ibid., ​  ​ 5-6.    8  jeff ​  ​ schwartz, ​  ​ laurence ​  ​ collins, ​  ​ heather ​  ​ stockton, ​  ​ darryl ​  ​ wagner ​  ​ and ​  ​ brett ​  ​ walsh, ​  ​ “the ​  ​ future ​  ​ of ​  ​ work: ​  ​ the ​  ​ augmented    workforce,” ​  ​ (deloitte ​  ​ human ​  ​ capital, ​  ​ february ​  ​ 28, ​  ​ 2017),    https://dupress.deloitte.com/dup-us-en/focus/human-capital-trends/2017/future-workforce-changing-nature-of-work.html    9  national ​  ​ academies ​  ​ of ​  ​ sciences, ​  ​ engineering ​  ​ and ​  ​ medicine, ​  ​ “information ​  ​ technology ​  ​ and ​  ​ the ​  ​ u.s. ​  ​ workforce: ​  ​ where ​  ​ are    we ​  ​ and ​  ​ where ​  ​ do ​  ​ we ​  ​ go ​  ​ from ​  ​ here?,” ​  ​ (washington, ​  ​ dc: ​  ​ the ​  ​ national ​  ​ academies ​  ​ press, ​  ​ 2017),    https://www.nap.edu/read/24649/ ​ .    10 ​  ​​  ​​  ​​ ​  ​ georg ​  ​ graetz ​  ​ and ​  ​ guy ​  ​ michaels, ​  ​ “robots ​  ​ at ​  ​ work,” ​  ​ iza ​  ​ discussion ​  ​ paper ​  ​ (institute ​  ​ for ​  ​ the ​  ​ study ​  ​ of ​  ​ labor ​  ​ (iza), ​  ​ march ​  ​ 2015),    http://econpapers.repec.org/paper/izaizadps/dp8938.htm.    11  daron ​  ​ acemoglu ​  ​ and ​  ​ pascual ​  ​ restrepo, ​  ​ “robots ​  ​ and ​  ​ jobs: ​  ​ evidence ​  ​ from ​  ​ us ​  ​ labor ​  ​ markets,” ​  ​ working ​  ​ paper ​  ​ (cambridge    ma: ​  ​ national ​  ​ bureau ​  ​ of ​  ​ economic ​  ​ research, ​  ​ march ​  ​ 2017), ​  ​ doi:10.3386/w23285.    12 ​  ​​  ​​ ​  ​​  ​ for ​  ​ instance, ​  ​ economists ​  ​ at ​  ​ the ​  ​ economic ​  ​ policy ​  ​ institute ​  ​ argue ​  ​ that ​  ​ restrepo ​  ​ and ​  ​ acemoglu’s ​  ​ estimates ​  ​ of ​  ​ unemployment    were ​  ​ localized ​  ​ and ​  ​ that ​  ​ the ​  ​ media ​  ​ distorted ​  ​ their ​  ​ conclusions ​  ​ regarding ​  ​ job ​  ​ loss ​  ​ while ​  ​ also ​  ​ ignoring ​  ​ productivity ​  ​ increases.    see: ​  ​ lawrence ​  ​ mishel ​  ​ and ​  ​ bivens, ​  ​ “the ​  ​ zombie ​  ​ robot ​  ​ argument ​  ​ lurches ​  ​ on: ​  ​ there ​  ​ is ​  ​ no ​  ​ evidence ​  ​ that ​  ​ automation ​  ​ leads ​  ​ to    joblessness ​  ​ or ​  ​ inequality” ​  ​ (washington, ​  ​ dc: ​  ​ economic ​  ​ policy ​  ​ institute, ​  ​ may ​  ​ 24, ​  ​ 2017),    http://www.epi.org/publication/the-zombie-robot-argument-lurches-on-there-is-no-evidence-that-automation-leads-to-job  lessness-or-inequality/.    13  robert ​  ​ seamans, ​  ​ “we ​  ​ won’t ​  ​ even ​  ​ know ​  ​ if ​  ​ a ​  ​ robot ​  ​ takes ​  ​ your ​  ​ job,” ​  ​​ forbes ​ , ​  ​ january ​  ​ 11, ​  ​ 2017,    http://www.forbes.com/sites/washingtonbytes/2017/01/11/we-wont-even-know-if-a-robot-takes-your-job/ ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 9    ai ​  ​ and ​  ​ the ​  ​ nature ​  ​ of ​  ​ work    while ​  ​ the ​  ​ displacement ​  ​ of ​  ​ entire ​  ​ occupations, ​  ​ such ​  ​ as ​  ​ taxi ​  ​ or ​  ​ truck ​  ​ drivers, ​  ​ is ​  ​ clearly ​  ​ an  14  important ​  ​ concern, ​  ​ ai ​  ​ is ​  ​ also ​  ​ transforming ​  ​ a ​  ​ wide ​  ​ range ​  ​ of ​  ​ occupations ​  ​ and ​  ​ roles. ​  ​ across    sectors, ​  ​ automated ​  ​ management ​  ​ and ​  ​ hiring ​  ​ technologies ​  ​ are ​  ​ being ​  ​ introduced, ​  ​ promising    to ​  ​ increase ​  ​ worker ​  ​ productivity ​  ​ and ​  ​ flexibility, ​  ​ but ​  ​ also ​  ​ exposing ​  ​ workers ​  ​ to ​  ​ new ​  ​ forms ​  ​ of    monitoring, ​  ​ manipulation ​  ​ and ​  ​ control. ​  ​ this ​  ​ changes ​  ​ labor ​  ​ processes ​  ​ and ​  ​ power ​  ​ relations.    further ​  ​ research ​  ​ on ​  ​ this ​  ​ topic ​  ​ is ​  ​ needed ​  ​ to ​  ​ address ​  ​ how ​  ​ ai ​  ​ is ​  ​ transforming ​  ​ the ​  ​ nature ​  ​ of    work ​  ​ itself, ​  ​ and ​  ​ how ​  ​ these ​  ​ transformations ​  ​ are ​  ​ manifesting ​  ​ for ​  ​ specific ​  ​ occupations ​  ​ within    specific ​  ​ sectors.     luke ​  ​ stark ​  ​ and ​  ​ alex ​  ​ rosenblat’s ​  ​ research ​  ​ with ​  ​ uber ​  ​ drivers ​  ​ suggests ​  ​ one ​  ​ model ​  ​ for ​  ​ this    approach. ​  ​ by ​  ​ listening ​  ​ to ​  ​ drivers, ​  ​ they ​  ​ identified ​  ​ algorithmic ​  ​ forms ​  ​ of ​  ​ management ​  ​ used ​  ​ by    the ​  ​ company. ​  ​ while ​  ​ its ​  ​ driver ​  ​ platform, ​  ​ which ​  ​ acts ​  ​ as ​  ​ a ​  ​ kind ​  ​ of ​  ​ remote ​  ​ management  15  console, ​  ​ helps ​  ​ make ​  ​ more ​  ​ efficient ​  ​ use ​  ​ of ​  ​ driver ​  ​ time ​  ​ in ​  ​ this ​  ​ digital ​  ​ “matching ​  ​ market,”  16  the ​  ​ platform ​  ​ also ​  ​ exposes ​  ​ fundamental ​  ​ informational ​  ​ asymmetries ​  ​ between ​  ​ worker ​  ​ and    platform ​  ​ owner. ​  ​ for ​  ​ example, ​  ​ drivers ​  ​ have ​  ​ about ​  ​ 15 ​  ​ seconds ​  ​ to ​  ​ accept ​  ​ ride ​  ​ requests ​  ​ via ​  ​ the    platform, ​  ​ and ​  ​ are ​  ​ not ​  ​ shown ​  ​ the ​  ​ rider’s ​  ​ destination. ​  ​ with ​  ​ drivers ​  ​ in ​  ​ the ​  ​ dark, ​  ​ they ​  ​ don’t    know ​  ​ when ​  ​ they ​  ​ will ​  ​ accept ​  ​ short, ​  ​ unprofitable ​  ​ fares. ​  ​ meanwhile, ​  ​ uber ​  ​ furthers ​  ​ its ​  ​ own    goal ​  ​ of ​  ​ providing ​  ​ near-instantaneous ​  ​ service ​  ​ to ​  ​ all ​  ​ prospective ​  ​ riders. ​  ​ because ​  ​ uber  17  designs ​  ​ the ​  ​ platform ​  ​ and ​  ​ can ​  ​ change ​  ​ it ​  ​ at ​  ​ will, ​  ​ conflicts ​  ​ of ​  ​ interest ​  ​ between ​  ​ worker ​  ​ and    platform ​  ​ owner ​  ​ are ​  ​ systematically ​  ​ settled ​  ​ in ​  ​ favor ​  ​ of ​  ​ uber ​  ​ via ​  ​ the ​  ​ platform ​  ​ itself, ​  ​ not    collective ​  ​ bargaining ​  ​ or ​  ​ other ​  ​ processes ​  ​ that ​  ​ allow ​  ​ for ​  ​ worker ​  ​ participation. ​  ​ this ​  ​ flatly    contradicts ​  ​ any ​  ​ argument ​  ​ that ​  ​ the ​  ​ platform ​  ​ is ​  ​ “neutral.” ​  ​ it ​  ​ will ​  ​ be ​  ​ interesting ​  ​ to ​  ​ see ​  ​ what    comes ​  ​ of ​  ​ the ​  ​ recent ​  ​ new ​  ​ york ​  ​ administrative ​  ​ law ​  ​ judge’s ​  ​ ruling, ​  ​ which ​  ​ classified ​  ​ uber    drivers ​  ​ as ​  ​ “employees” ​  ​ under ​  ​ new ​  ​ york ​  ​ law, ​  ​ contrary ​  ​ to ​  ​ uber’s ​  ​ claims ​  ​ otherwise.  18  of ​  ​ course, ​  ​ asymmetrical ​  ​ forms ​  ​ of ​  ​ workplace ​  ​ management ​  ​ and ​  ​ control ​  ​ long ​  ​ predate ​  ​ ai.  19  the ​  ​ task ​  ​ for ​  ​ researchers ​  ​ is ​  ​ to ​  ​ determine ​  ​ specifically ​  ​ what ​  ​ makes ​  ​ ai-powered ​  ​ asymmetries    different ​  ​ from ​  ​ other ​  ​ forms ​  ​ of ​  ​ monitoring, ​  ​ such ​  ​ as ​  ​ taylorist ​  ​ scientific ​  ​ management ​  ​ and ​  ​ the  20  audit ​  ​ culture ​  ​ of ​  ​ total ​  ​ quality ​  ​ control. ​  ​​ one ​  ​ clear ​  ​ difference ​  ​ is ​  ​ ai’s ​  ​ reliance ​  ​ on ​  ​ workplace  21  surveillance ​  ​ and ​  ​ the ​  ​ data ​  ​ it ​  ​ produces, ​  ​ and ​  ​ thus ​  ​ the ​  ​ normalization ​  ​ of ​  ​ workplace ​  ​ surveillance    14  truckers, ​  ​ like ​  ​ ride-sharing ​  ​ drivers, ​  ​ are ​  ​ also ​  ​ subject ​  ​ to ​  ​ data-driven ​  ​ forms ​  ​ of ​  ​ surveillance ​  ​ and ​  ​ control. ​  ​ e.g. ​  ​ karen ​  ​ e. ​  ​ c. ​  ​ levy,    “the ​  ​ contexts ​  ​ of ​  ​ control: ​  ​ information, ​  ​ power, ​  ​ and ​  ​ truck-driving ​  ​ work,” ​  ​​ the ​  ​ information ​  ​ society ​ ​  ​ 31, ​  ​ no. ​  ​ 2 ​  ​ (march ​  ​ 15, ​  ​ 2015):    160–74, ​  ​ doi:10.1080/01972243.2015.998105.    15  alex ​  ​ rosenblat ​  ​ and ​  ​ luke ​  ​ stark, ​  ​ “algorithmic ​  ​ labor ​  ​ and ​  ​ information ​  ​ asymmetries: ​  ​ a ​  ​ case ​  ​ study ​  ​ of ​  ​ uber’s ​  ​ drivers,”    international ​  ​ journal ​  ​ of ​  ​ communication ​  ​ 10 ​  ​ (july ​  ​ 27, ​  ​ 2016): ​  ​ 3758-3784,    http://ijoc.org/index.php/ijoc/article/view/4892/1739 ​ .    16  eduardo ​  ​ m. ​  ​ azevedo ​  ​ and ​  ​ e. ​  ​ glen ​  ​ weyl, ​  ​ “matching ​  ​ markets ​  ​ in ​  ​ the ​  ​ digital ​  ​ age,” ​  ​ science ​  ​ 352, ​  ​ no. ​  ​ 6289 ​  ​ (may ​  ​ 27, ​  ​ 2016):    1056–57, ​  ​​ http://science.sciencemag.org/content/352/6289/1056 ​ .    17  rosenblat ​  ​ and ​  ​ stark, ​  ​ “algorithmic ​  ​ labor ​  ​ and ​  ​ information ​  ​ asymmetries,” ​  ​ 3762.    18  dana ​  ​ rubenstein, ​  ​ “state ​  ​ labor ​  ​ judge ​  ​ finds ​  ​ uber ​  ​ an ​  ​ ‘employer’,” ​  ​​ politico ​ , ​  ​ may ​  ​ 13, ​  ​ 2017,    http://www.politico.com/states/new-york/albany/story/2017/06/13/state-labor-court-finds-uber-an-employer-112733 ​ .    19 ​  ​​  ​​  ​​ ifeoma ​  ​ ajunwa, ​  ​ kate ​  ​ crawford ​  ​ and ​  ​ jason ​  ​ schultz, ​  ​ “limitless ​  ​ worker ​  ​ surveillance,” ​ california ​  ​ law ​  ​ review ​ ​  ​ 105, ​  ​ no. ​  ​ 3 ​  ​ , ​  ​ 2017.    20  hugh ​  ​ g. ​  ​ j ​  ​ aitken, ​  ​​ taylorism ​  ​ at ​  ​ watertown ​  ​ arsenal; ​  ​ scientific ​  ​ management ​  ​ in ​  ​ action ​ , ​  ​ 1908-1915. ​  ​ (cambridge: ​  ​ harvard    university ​  ​ press, ​  ​ 1960).    21  marilyn ​  ​ strathern, ​  ​​ audit ​  ​ cultures: ​  ​ anthropological ​  ​ studies ​  ​ in ​  ​ accountability, ​  ​ ethics ​  ​ and ​  ​ the ​  ​ academy ​ ​  ​ (london: ​  ​ routledge,    2000).  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 10    practices. ​  ​ such ​  ​ systems ​  ​ provide ​  ​ employers ​  ​ with ​  ​ expansive ​  ​ and ​  ​ often ​  ​ invasive ​  ​ data ​  ​ about    the ​  ​ workplace ​  ​ behaviors ​  ​ of ​  ​ their ​  ​ employees. ​  ​ it ​  ​ is ​  ​ this ​  ​ data ​  ​ that ​  ​ ai-powered ​  ​ management    systems ​  ​ rely ​  ​ on ​  ​ to ​  ​ generate ​  ​ insights. ​  ​ as ​  ​ ai-driven ​  ​ management ​  ​ becomes ​  ​ more ​  ​ common, ​  ​ so    will ​  ​ the ​  ​ data ​  ​ collection ​  ​ and ​  ​ worker ​  ​ surveillance ​  ​ practices ​  ​ on ​  ​ which ​  ​ it ​  ​ relies. ​  ​ worryingly, ​  ​ this    employee ​  ​ monitoring ​  ​ is ​  ​ not ​  ​ necessarily ​  ​ limited ​  ​ to ​  ​ the ​  ​ workplace, ​  ​ and ​  ​ can ​  ​ spill ​  ​ into ​  ​ private    life, ​  ​ such ​  ​ as ​  ​ with ​  ​ fitness ​  ​ trackers, ​  ​ ubiquitous ​  ​ productivity ​  ​ apps, ​  ​ or ​  ​ company-issued    smartphones ​  ​ equipped ​  ​ with ​  ​ monitoring ​  ​ features.     while ​  ​ we ​  ​ might ​  ​ assume ​  ​ this ​  ​ would ​  ​ be ​  ​ held ​  ​ in ​  ​ check ​  ​ by ​  ​ privacy ​  ​ laws ​  ​ and ​  ​ existing ​  ​ policy,    ifeoma ​  ​ ajunwa, ​  ​ kate ​  ​ crawford ​  ​ and ​  ​ jason ​  ​ schultz ​  ​ published ​  ​ a ​  ​ study ​  ​ of ​  ​ existing ​  ​ legal    frameworks, ​  ​ assessing ​  ​ if ​  ​ there ​  ​ are ​  ​ any ​  ​ meaningful ​  ​ limits ​  ​ on ​  ​ workplace ​  ​ surveillance. ​  ​ they    found ​  ​ very ​  ​ few, ​  ​ some ​  ​ of ​  ​ which ​  ​ are ​  ​ already ​  ​ under ​  ​ threat ​  ​ from ​  ​ the ​  ​ trump ​  ​ administration.  22  this ​  ​ degree ​  ​ of ​  ​ 24/7 ​  ​ surveillance ​  ​ has ​  ​ the ​  ​ potential ​  ​ to ​  ​ transform ​  ​ key ​  ​ features ​  ​ of ​  ​ prior    management ​  ​ systems, ​  ​ potentially ​  ​ in ​  ​ ways ​  ​ workers ​  ​ won’t ​  ​ be ​  ​ aware ​  ​ of ​  ​ or ​  ​ have ​  ​ a ​  ​ say ​  ​ in.    employers ​  ​ could ​  ​ easily ​  ​ use ​  ​ machine ​  ​ learning ​  ​ techniques ​  ​ to ​  ​ identify ​  ​ behavioral ​  ​ patterns    both ​  ​ during ​  ​ and ​  ​ outside ​  ​ of ​  ​ work ​  ​ hours, ​  ​ and ​  ​ then ​  ​ exploit ​  ​ these ​  ​ one-sided ​  ​ insights ​  ​ to    increase ​  ​ profits ​  ​ and ​  ​ manipulate ​  ​ behaviors, ​  ​ with ​  ​ potentially ​  ​ negative ​  ​ effects ​  ​ for ​  ​ workers.     uber’s ​  ​ platform ​  ​ demonstrates ​  ​ how ​  ​ workers ​  ​ are ​  ​ directly ​  ​ and ​  ​ indirectly ​  ​ manipulated ​  ​ in    service ​  ​ of ​  ​ instant ​  ​ customer ​  ​ gratification. ​  ​ the ​  ​ company ​  ​ wants ​  ​ to ​  ​ keep ​  ​ up ​  ​ the ​  ​ number ​  ​ of    available ​  ​ cars, ​  ​ even ​  ​ during ​  ​ times ​  ​ of ​  ​ low ​  ​ demand ​  ​ when ​  ​ drivers ​  ​ make ​  ​ less ​  ​ money. ​  ​ to ​  ​ address    this, ​  ​ the ​  ​ ride-sharing ​  ​ company ​  ​ drew ​  ​ on ​  ​ behavioral ​  ​ economic ​  ​ research ​  ​ about ​  ​ the    psychological ​  ​ tendency ​  ​ of ​  ​ taxi ​  ​ workers ​  ​ to ​  ​ set ​  ​ round ​  ​ earnings ​  ​ goals ​  ​ and ​  ​ stop ​  ​ working ​  ​ when    they ​  ​ reach ​  ​ them. ​  ​ uber, ​  ​ with ​  ​ access ​  ​ to ​  ​ vast ​  ​ real-time ​  ​ data ​  ​ about ​  ​ driver ​  ​ activities, ​  ​ can  23  quickly ​  ​ test ​  ​ such ​  ​ theories, ​  ​ using ​  ​ machine ​  ​ learning ​  ​ to ​  ​ identify ​  ​ exploitable ​  ​ behavioral    patterns, ​  ​ even ​  ​ at ​  ​ an ​  ​ individual ​  ​ level. ​  ​ uber ​  ​ discovered ​  ​ that ​  ​ drivers ​  ​ quickly ​  ​ abandon ​  ​ mental    income ​  ​ targets ​  ​ in ​  ​ favor ​  ​ of ​  ​ working ​  ​ at ​  ​ times ​  ​ of ​  ​ high ​  ​ demand. ​  ​ to ​  ​ combat ​  ​ this ​  ​ tendency, ​  ​ uber    sent ​  ​ tailored ​  ​ nudge ​  ​ messages ​  ​ to ​  ​ drivers ​  ​ indicating ​  ​ when ​  ​ they ​  ​ are ​  ​ close ​  ​ to ​  ​ revenue ​  ​ target  24  during ​  ​ times ​  ​ when ​  ​ it ​  ​ was ​  ​ advantageous ​  ​ for ​  ​ uber ​  ​ to ​  ​ keep ​  ​ its ​  ​ drivers ​  ​ on ​  ​ the ​  ​ road. ​  ​ until ​  ​ a  25  recent ​  ​ feature ​  ​ in ​  ​​ the ​  ​ new ​  ​ york ​  ​ times ​ , ​  ​ drivers ​  ​ were ​  ​ unaware ​  ​ that ​  ​ they ​  ​ were ​  ​ subjects ​  ​ in ​  ​ a    large ​  ​ behavioral ​  ​ experiment ​  ​ that ​  ​ sought ​  ​ to ​  ​ modify ​  ​ their ​  ​ actions ​  ​ to ​  ​ benefit ​  ​ the ​  ​ company’s    goals. ​  ​ given ​  ​ the ​  ​ opacity ​  ​ of ​  ​ these ​  ​ systems, ​  ​ there ​  ​ may ​  ​ be ​  ​ many ​  ​ more ​  ​ such ​  ​ experiments ​  ​ that    22  ifeoma ​  ​ ajunwa, ​  ​ kate ​  ​ crawford ​  ​ and ​  ​ jason ​  ​ schultz, ​  ​ “limitless ​  ​ worker ​  ​ surveillance,” ​ california ​  ​ law ​  ​ review ​ ​  ​ 105, ​  ​ no. ​  ​ 3 ​  ​ (june ​  ​ 1,    2017).    23  colin ​  ​ camerer, ​  ​ linda ​  ​ babcock, ​  ​ george ​  ​ loewenstein ​  ​ and ​  ​ richard ​  ​ thaler, ​  ​ “labor ​  ​ supply ​  ​ of ​  ​ new ​  ​ york ​  ​ city ​  ​ cab ​  ​ drivers: ​  ​ one    day ​  ​ at ​  ​ a ​  ​ time,” ​  ​ the ​  ​ quarterly ​  ​ journal ​  ​ of ​  ​ economics ​  ​ 112, ​  ​ no. ​  ​ 2 ​  ​ (may ​  ​ 1, ​  ​ 1997): ​  ​ 407–41, ​  ​ doi:10.1162/003355397555244.    24  the ​  ​ use ​  ​ of ​  ​ “nudge” ​  ​ as ​  ​ a ​  ​ more ​  ​ technical, ​  ​ policy-oriented ​  ​ term ​  ​ has ​  ​ emerged ​  ​ out ​  ​ of ​  ​ work ​  ​ in ​  ​ the ​  ​ decision ​  ​ and ​  ​ choice ​  ​ sciences,    most ​  ​ influentially ​  ​ that ​  ​ of ​  ​ behavioral ​  ​ economist ​  ​ richard ​  ​ thaler ​  ​ and ​  ​ the ​  ​ legal ​  ​ scholar ​  ​ cass ​  ​ sunstein, ​  ​ who ​  ​ headed ​  ​ the ​  ​ obama    administration’s ​  ​ office ​  ​ of ​  ​ information ​  ​ and ​  ​ regulatory ​  ​ affairs. ​  ​ they, ​  ​ in ​  ​ turn, ​  ​ draw ​  ​ on ​  ​ psychological ​  ​ studies ​  ​ of ​  ​ how ​  ​ people    make ​  ​ decisions ​  ​ under ​  ​ conditions ​  ​ of ​  ​ uncertainty ​  ​ and ​  ​ avoid ​  ​ errors ​  ​ due ​  ​ to ​  ​ heuristics—like ​  ​ an ​  ​ earnings ​  ​ goal—and ​  ​ biases. ​  ​ these    were ​  ​ first ​  ​ identified ​  ​ by ​  ​ the ​  ​ influential ​  ​ psychologists ​  ​ amos ​  ​ tversky ​  ​ and ​  ​ daniel ​  ​ kahneman. ​  ​ v.:richard ​  ​ h. ​  ​ thaler ​  ​ and ​  ​ cass ​  ​ r.    sunstein, ​  ​​ nudge: ​  ​ improving ​  ​ decisions ​  ​ about ​  ​ health, ​  ​ wealth, ​  ​ and ​  ​ happiness ​ ​  ​ (new ​  ​ york: ​  ​ penguin ​  ​ books, ​  ​ 2009); ​  ​ amos ​  ​ tversky    and ​  ​ daniel ​  ​ kahneman, ​  ​ “judgment ​  ​ under ​  ​ uncertainty: ​  ​ heuristics ​  ​ and ​  ​ biases,” ​  ​​ science ​ ​  ​ 185, ​  ​ no. ​  ​ 4157 ​  ​ (september ​  ​ 27, ​  ​ 1974):    1124–31, ​  ​ doi:10.1126/science.185.4157.1124.    25  noam ​  ​ scheiber, ​  ​ “how ​  ​ uber ​  ​ uses ​  ​ psychological ​  ​ tricks ​  ​ to ​  ​ push ​  ​ its ​  ​ drivers’ ​  ​ buttons,” ​  ​​ the ​  ​ new ​  ​ york ​  ​ times ​ , ​  ​ april ​  ​ 2, ​  ​ 2017,    https://www.nytimes.com/interactive/2017/04/02/technology/uber-drivers-psychological-tricks.html?_r=0 ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 11    workers ​  ​ and ​  ​ the ​  ​ public ​  ​ will ​  ​ never ​  ​ know ​  ​ about.     this ​  ​ case ​  ​ illustrates ​  ​ how ​  ​ ai ​  ​ management ​  ​ might ​  ​ differ ​  ​ from ​  ​ past ​  ​ forms ​  ​ of ​  ​ incentive-based    control. ​  ​ as ​  ​ companies ​  ​ gather ​  ​ more ​  ​ data ​  ​ on ​  ​ their ​  ​ workers, ​  ​ they ​  ​ no ​  ​ longer ​  ​ need ​  ​ to ​  ​ rely ​  ​ on    generalized ​  ​ psychological ​  ​ theories ​  ​ or ​  ​ human-to-human ​  ​ assessments ​  ​ of ​  ​ merit. ​  ​ they ​  ​ can    instead ​  ​ exploit ​  ​ information ​  ​ asymmetries ​  ​ to ​  ​ identify ​  ​ behavioral ​  ​ patterns ​  ​ at ​  ​ the ​ ​  ​ individual    level ​  ​​ and ​  ​ nudge ​  ​ people ​  ​ toward ​  ​ the ​  ​ most ​  ​ profitable ​  ​ activities ​  ​ for ​  ​ the ​  ​ platform ​  ​ owners, ​  ​ even    when ​  ​ these ​  ​ operate ​  ​ against ​  ​ the ​  ​ best ​  ​ interests ​  ​ of ​  ​ workers ​  ​ themselves. ​  ​ by ​  ​ selectively    exploiting ​  ​ workers’ ​  ​ behavior, ​  ​ often ​  ​ without ​  ​ workers’ ​  ​ consent ​  ​ or ​  ​ even ​  ​ knowledge, ​  ​ these    technologies ​  ​ have ​  ​ the ​  ​ potential ​  ​ to ​  ​ make ​  ​ workers ​  ​ complicit ​  ​ in ​  ​ their ​  ​ own ​  ​ exploitation. ​  ​ to    address ​  ​ these ​  ​ emerging ​  ​ imbalances ​  ​ of ​  ​ workplace ​  ​ power, ​  ​ it ​  ​ will ​  ​ likely ​  ​ be ​  ​ necessary ​  ​ for    unions, ​  ​ labor ​  ​ rights ​  ​ advocates ​  ​ and ​  ​ individual ​  ​ workers ​  ​ to ​  ​ participate ​  ​ in ​  ​ the ​  ​ design ​  ​ of ​  ​ worker    platforms. ​  ​ it ​  ​ will ​  ​ also ​  ​ likely ​  ​ be ​  ​ necessary ​  ​ to ​  ​ give ​  ​ workers ​  ​ a ​  ​ democratic ​  ​ voice ​  ​ in ​  ​ shaping ​  ​ both    whether ​  ​ and ​  ​ how ​  ​ they ​  ​ are ​  ​ monitored ​  ​ and ​  ​ how ​  ​ machine ​  ​ learning ​  ​ techniques ​  ​ will ​  ​ be ​  ​ used ​  ​ to    process ​  ​ such ​  ​ data. ​  ​ this ​  ​ is ​  ​ a ​  ​ rich ​  ​ area ​  ​ of ​  ​ research ​  ​ and ​  ​ design ​  ​ for ​  ​ the ​  ​ technical ​  ​ architects ​  ​ of    ai ​  ​ management ​  ​ systems, ​  ​ labor ​  ​ organizers ​  ​ and ​  ​ advocates ​  ​ to ​  ​ explore.     ai ​  ​ management ​  ​ systems ​  ​ also ​  ​ provide ​  ​ new ​  ​ and ​  ​ invasive ​  ​ methods ​  ​ for ​  ​ evaluating ​  ​ employees    and ​  ​ making ​  ​ retention ​  ​ decisions. ​  ​ for ​  ​ example, ​  ​ the ​  ​ employee ​  ​ monitoring ​  ​ firm ​  ​ veriato    captures ​  ​ information ​  ​ from ​  ​ nearly ​  ​ any ​  ​ task ​  ​ a ​  ​ worker ​  ​ performs ​  ​ on ​  ​ a ​  ​ computer, ​  ​ from    browsing ​  ​ history ​  ​ to ​  ​ email ​  ​ and ​  ​ chat, ​  ​ even ​  ​ taking ​  ​ periodic ​  ​ screenshots ​  ​ of ​  ​ workers’ ​  ​ monitor    displays. ​  ​ the ​  ​ firm’s ​  ​ software ​  ​ aggregates ​  ​ this ​  ​ information, ​  ​ then ​  ​ uses ​  ​ machine ​  ​ learning ​  ​ to    detect ​  ​ anomalous ​  ​ behaviors. ​  ​ the ​  ​ program ​  ​ can ​  ​ then ​  ​ send ​  ​ warning ​  ​ messages ​  ​ to ​  ​ employees    who ​  ​ deviate ​  ​ from ​  ​ the ​  ​ norm. ​  ​ what ​  ​ the ​  ​ consequences ​  ​ of ​  ​ such ​  ​ deviance ​  ​ are ​  ​ for ​  ​ workers ​  ​ is  26  up ​  ​ to ​  ​ the ​  ​ employer. ​  ​ and ​  ​ this ​  ​ isn’t ​  ​ all. ​  ​ veriato’s ​  ​ software ​  ​ also ​  ​ offers ​  ​ features ​  ​ to ​  ​ score ​  ​ email    and ​  ​ chats ​  ​ for ​  ​ sentiment ​  ​ using ​  ​ natural ​  ​ language ​  ​ processing. ​  ​ language ​  ​ that ​  ​ their ​  ​ program    determines ​  ​ to ​  ​ be ​  ​ “negative” ​  ​ is ​  ​ interpreted ​  ​ by ​  ​ the ​  ​ company ​  ​ as ​  ​ an ​  ​ indication ​  ​ of ​  ​ a    productivity ​  ​ risk, ​  ​ or ​  ​ of ​  ​ an ​  ​ employee ​  ​ who ​  ​ is ​  ​ getting ​  ​ ready ​  ​ to ​  ​ leave ​  ​ the ​  ​ company. ​  ​ similarly,    another ​  ​ company, ​  ​ workday, ​  ​ assigns ​  ​ employees ​  ​ individualized ​  ​ risk ​  ​ score ​  ​ based ​  ​ on ​  ​ 60    factors. ​  ​ many ​  ​ employees ​  ​ who ​  ​ use ​  ​ a ​  ​ work-issued ​  ​ computer ​  ​ or ​  ​ mobile ​  ​ are ​  ​ already ​  ​ subject  27  to ​  ​ this ​  ​ type ​  ​ of ​  ​ monitoring ​  ​ and ​  ​ software-driven ​  ​ ranking ​  ​ and ​  ​ assessment. ​  ​ additionally, ​  ​ many    of ​  ​ them ​  ​ likely ​  ​ have ​  ​ no ​  ​ idea ​  ​ that ​  ​ their ​  ​ value ​  ​ as ​  ​ an ​  ​ employee ​  ​ is ​  ​ being ​  ​ determined ​  ​ in ​  ​ part ​  ​ by    software ​  ​ systems ​  ​ scoring ​  ​ everything ​  ​ from ​  ​ the ​  ​ emotional ​  ​ content ​  ​ of ​  ​ their ​  ​ emails ​  ​ to ​  ​ their    frequency ​  ​ of ​  ​ accepting ​  ​ meeting ​  ​ requests.     beyond ​  ​ employee ​  ​ surveillance, ​  ​ the ​  ​ combination ​  ​ of ​  ​ customer ​  ​ surveillance ​  ​ and ​  ​ ai ​  ​ has ​  ​ the    potential ​  ​ to ​  ​ turn ​  ​ previously ​  ​ stable ​  ​ employment ​  ​ in ​  ​ sectors ​  ​ like ​  ​ food ​  ​ service ​  ​ and ​  ​ retail ​  ​ into ​  ​ a    form ​  ​ of ​  ​ gig ​  ​ work. ​  ​ so-called ​  ​ scheduling ​  ​ software ​  ​ has ​  ​ allowed ​  ​ retailers ​  ​ to ​  ​ switch ​  ​ from    standard ​  ​ shifts ​  ​ to ​  ​ a ​  ​ more ​  ​ “on ​  ​ call” ​  ​ model, ​  ​ based ​  ​ on ​  ​ algorithmic ​  ​ predictions ​  ​ about ​  ​ whether    customers ​  ​ will ​  ​ be ​  ​ in ​  ​ a ​  ​ store ​  ​ at ​  ​ a ​  ​ given ​  ​ time. ​  ​ while ​  ​ the ​  ​ use ​  ​ of ​  ​ such ​  ​ software ​  ​ can ​  ​ cut ​  ​ an    employer’s ​  ​ costs ​  ​ by ​  ​ reducing ​  ​ staff ​  ​ during ​  ​ off-peak ​  ​ customer ​  ​ hours, ​  ​ as ​  ​ solon ​  ​ barocas ​  ​ and    26  ted ​  ​ greenwald, ​  ​ “how ​  ​ ai ​  ​ is ​  ​ transforming ​  ​ the ​  ​ workplace,” ​  ​ wall ​  ​ street ​  ​ journal, ​  ​ march ​  ​ 10, ​  ​ 2017, ​  ​ sec. ​  ​ business,    https://www.wsj.com/articles/how-ai-is-transforming-the-workplace-1489371060 ​ .    27  ibid.  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 12    karen ​  ​ levy ​  ​ have ​  ​ observed, ​  ​ it ​  ​ is ​  ​ “highly ​  ​ destabilizing” ​  ​ for ​  ​ workers ​  ​ who ​  ​ never ​  ​ know ​  ​ ahead ​  ​ of    time ​  ​ whether ​  ​ or ​  ​ not ​  ​ they ​  ​ will ​  ​ be ​  ​ called ​  ​ in ​  ​ for ​  ​ work. ​  ​ the ​  ​ use ​  ​ of ​  ​ predictive ​  ​ scheduling  28  software, ​  ​ whether ​  ​ by ​  ​ gig ​  ​ employers ​  ​ like ​  ​ uber ​  ​ or ​  ​ more ​  ​ traditional ​  ​ employers, ​  ​ collapses    work-life ​  ​ boundaries. ​  ​ it ​  ​ also ​  ​ puts ​  ​ workers ​  ​ at ​  ​ risk ​  ​ of ​  ​ over- ​  ​ or ​  ​ underwork, ​  ​ gives ​  ​ workers ​  ​ little    to ​  ​ no ​  ​ control ​  ​ over ​  ​ shift ​  ​ times, ​  ​ and ​  ​ provides ​  ​ them ​  ​ with ​  ​ little ​  ​ ability ​  ​ to ​  ​ predict ​  ​ income ​  ​ flows    or ​  ​ to ​  ​ plan ​  ​ ahead ​  ​ for ​  ​ things ​  ​ like ​  ​ child ​  ​ care ​  ​ or ​  ​ a ​  ​ second ​  ​ job. ​  ​ recognizing ​  ​ the ​  ​ negative ​  ​ impacts    that ​  ​ such ​  ​ precarious ​  ​ schedules ​  ​ can ​  ​ have ​  ​ on ​  ​ workers, ​  ​ the ​  ​ oregon ​  ​ state ​  ​ senate ​  ​ and ​  ​ house    recently ​  ​ passed ​  ​ a ​  ​ bill ​  ​ mandating ​  ​ that ​  ​ large ​  ​ employers ​  ​ in ​  ​ retail, ​  ​ manufacturing ​  ​ and    hospitality ​  ​ provide ​  ​ workers ​  ​ a ​  ​ written ​  ​ estimate ​  ​ of ​  ​ their ​  ​ schedule ​  ​ at ​  ​ least ​  ​ 7 ​  ​ days ​  ​ before ​  ​ the    start ​  ​ of ​  ​ the ​  ​ work ​  ​ week. ​  ​ barring ​  ​ a ​  ​ veto ​  ​ from ​  ​ the ​  ​ state’s ​  ​ governor, ​  ​ oregon ​  ​ will ​  ​ join ​  ​ new  29  york, ​  ​ san ​  ​ francisco ​  ​ and ​  ​ seattle, ​  ​ who ​  ​ have ​  ​ also ​  ​ passed ​  ​ laws ​  ​ mandating ​  ​ predictable    scheduling.        the ​  ​ increasing ​  ​ role ​  ​ of ​  ​ ai ​  ​ and ​  ​ automation ​  ​ within ​  ​ various ​  ​ labor ​  ​ sectors ​  ​ has ​  ​ the ​  ​ potential ​  ​ to    revise ​  ​ our ​  ​ understanding ​  ​ of ​  ​ labor ​  ​ and ​  ​ our ​  ​ expectations ​  ​ of ​  ​ goods ​  ​ and ​  ​ services. ​  ​ as    consumers ​  ​ grow ​  ​ accustomed ​  ​ to ​  ​ dealing ​  ​ with ​  ​ automated ​  ​ systems, ​  ​ there ​  ​ is ​  ​ a ​  ​ potential ​  ​ to    ignore ​  ​ or ​  ​ devalue ​  ​ the ​  ​ human ​  ​ labor ​  ​ that ​  ​ remains ​  ​ essential ​  ​ in ​  ​ many ​  ​ instances. ​  ​ the ​  ​​ ai ​  ​ now    2016 ​  ​ labor ​  ​ primer ​ ​  ​ emphasized ​  ​ that ​  ​ ai ​  ​ often ​  ​ demands ​  ​ “human ​  ​ caretakers” — ​  ​ these ​  ​ vary,  30  from ​  ​ workers ​  ​ who ​  ​ maintain ​  ​ and ​  ​ repair ​  ​ data ​  ​ centers ​  ​ to ​  ​ moderators ​  ​ who ​  ​ check ​  ​ the ​  ​ results ​  ​ of    even ​  ​ the ​  ​ most ​  ​ sophisticated ​  ​ computer ​  ​ vision ​  ​ algorithms. ​  ​ since ​  ​ the ​  ​​ ai ​  ​ now ​  ​ 2016 ​  ​ labor  31  primer ​ , ​  ​ facebook ​  ​ has ​  ​ announced ​  ​ the ​  ​ hiring ​  ​ of ​  ​ 3,000 ​  ​ workers ​  ​ to ​  ​ monitor ​  ​ its ​  ​ live ​  ​ video    streaming ​  ​ services ​  ​ for ​  ​ violence, ​  ​ exploitation ​  ​ and ​  ​ hate ​  ​ speech. ​  ​ this ​  ​ is ​  ​ both ​  ​ an  32  acknowledgement ​  ​ that ​  ​ ai ​  ​ systems ​  ​ don’t ​  ​ always ​  ​ do ​  ​ the ​  ​ work ​  ​ as ​  ​ intended, ​  ​ and ​  ​ an ​  ​ example    of ​  ​ how ​  ​ essential ​  ​ human ​  ​ work ​  ​ happening ​  ​ behind ​  ​ the ​  ​ scenes ​  ​ of ​  ​ complex ​  ​ systems ​  ​ is ​  ​ often    invisible. ​  ​ not ​  ​ surprisingly, ​  ​ this ​  ​ work ​  ​ tends ​  ​ to ​  ​ be ​  ​ outsourced ​  ​ to ​  ​ countries ​  ​ where ​  ​ wages ​  ​ are    very ​  ​ low. ​  ​ how ​  ​ will ​  ​ such ​  ​ maintenance ​  ​ and ​  ​ repair ​  ​ work ​  ​ be ​  ​ valued ​  ​ by ​  ​ consumers ​  ​ who ​  ​ have    been ​  ​ led ​  ​ to ​  ​ believe ​  ​ that ​  ​ such ​  ​ services ​  ​ are ​  ​ entirely ​  ​ automated? ​  ​ how ​  ​ will ​  ​ companies ​  ​ that    promote ​  ​ themselves ​  ​ as ​  ​ fully ​  ​ automated ​  ​ “ai ​  ​ magic” ​  ​ treat ​  ​ and ​  ​ recognize ​  ​ workers ​  ​ within    these ​  ​ systems? ​  ​ additionally, ​  ​ how ​  ​ will ​  ​ this ​  ​ lack ​  ​ of ​  ​ visibility ​  ​ impact ​  ​ workers’ ​  ​ ability ​  ​ to    organize ​  ​ and ​  ​ shape ​  ​ their ​  ​ own ​  ​ working ​  ​ conditions?     managers ​  ​ too, ​  ​ will ​  ​ need ​  ​ to ​  ​ rethink ​  ​ how ​  ​ they ​  ​ formulate ​  ​ goals ​  ​ and ​  ​ use ​  ​ data, ​  ​ while    acknowledging ​  ​ the ​  ​ limits ​  ​ and ​  ​ risks ​  ​ of ​  ​ automated ​  ​ systems. ​  ​ michael ​  ​ luca, ​  ​ jon ​  ​ kleinberg, ​  ​ and    sendhil ​  ​ mullainathan ​  ​ argue ​  ​ that ​  ​ these ​  ​ systems ​  ​ can ​  ​ miss ​  ​ contextual ​  ​ details ​  ​ and ​  ​ may ​  ​ not    28  solon ​  ​ barocas ​  ​ and ​  ​ karen ​  ​ levy, ​  ​ “what ​  ​ customer ​  ​ data ​  ​ collection ​  ​ could ​  ​ mean ​  ​ for ​  ​ workers,” ​  ​​ harvard ​  ​ business ​  ​ review ​ , ​  ​ august    31, ​  ​ 2016, ​  ​​ https://hbr.org/2016/08/the-unintended-consequence-of-customer-data-collection ​ .    29 ​  ​​  ​​  ​​ hillary ​  ​ borrud, ​  ​ “oregon ​  ​ on ​  ​ way ​  ​ to ​  ​ become ​  ​ first ​  ​ state ​  ​ to ​  ​ guarantee ​  ​ predictable ​  ​ work ​  ​ schedules,” ​  ​ oregonian, ​  ​ june ​  ​ 29, ​  ​ 2017,    sec. ​  ​ oregon ​  ​ live, ​  ​​ http://www.oregonlive.com/politics/index.ssf/2017/06/oregon_on_way_to_become_first.html    30  “ai’s ​  ​ human ​  ​ caretakers” ​  ​ in ​  ​ the ​  ​​ 2016 ​ ​  ​​ ai ​  ​ now ​ ​  ​​ labor ​  ​ and ​  ​ automation ​  ​ prime ​ r, ​  ​ “labor ​  ​ and ​  ​ ai,”    https://ainowinstitute.org/ai_now_2016_primers.pdf ​ .    31  sarah ​  ​ t. ​  ​ roberts, ​  ​ “commercial ​  ​ content ​  ​ moderation: ​  ​ digital ​  ​ laborers’ ​  ​ dirty ​  ​ work,” ​  ​ in ​  ​​ the ​  ​ intersectional ​  ​ internet: ​  ​ race, ​  ​ sex,    class ​  ​ and ​  ​ culture ​  ​ online ​ , ​  ​ ed. ​  ​ safiya ​  ​ umoja ​  ​ noble ​  ​ and ​  ​ brendesha ​  ​ m. ​  ​ tynes ​  ​ (new ​  ​ york: ​  ​ peter ​  ​ lang, ​  ​ 2016), ​  ​ 147–60.    32  kathleen ​  ​ chaykowski, ​  ​ “facebook ​  ​ is ​  ​ hiring ​  ​ 3,000 ​  ​ moderators ​  ​ in ​  ​ push ​  ​ to ​  ​ curb ​  ​ violent ​  ​ videos,” ​  ​​ forbes ​ , ​  ​ accessed ​  ​ may ​  ​ 10,    2017, ​ http://www.forbes.com/sites/kathleenchaykowski/2017/05/03/facebook-is-hiring-3000-moderators-in-push-to-curb violent-videos/ ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 13    provide ​  ​ clear ​  ​ reasoning ​  ​ for ​  ​ decisions. ​  ​ they ​  ​ advise ​  ​ managers ​  ​ to ​  ​ ask ​  ​ employees ​  ​ and    stakeholders ​  ​ to ​  ​ articulate ​  ​ concerns ​  ​ with ​  ​ such ​  ​ systems; ​  ​ more ​  ​ democratic ​  ​ input ​  ​ can ​  ​ often    improve ​  ​ performance. ​  ​ similarly ​  ​ they ​  ​ recommend ​  ​ that ​  ​ diverse ​  ​ data-inputs ​  ​ be ​  ​ used ​  ​ in    pursuit ​  ​ of ​  ​ long-term ​  ​ goals ​  ​ and ​  ​ values, ​  ​ instead ​  ​ of ​  ​ focusing ​  ​ too ​  ​ narrowly ​  ​ on ​  ​ low-hanging    fruit, ​  ​ which ​  ​ can ​  ​ often ​  ​ produce ​  ​ unintended ​  ​ consequences, ​  ​ like ​  ​ clickbait ​  ​ in ​  ​ search ​  ​ of ​  ​ social    media ​  ​ engagement.   33  inequality ​  ​ and ​  ​ redistribution    what ​  ​ happens ​  ​ to ​  ​ workers ​  ​ after ​  ​ their ​  ​ jobs ​  ​ have ​  ​ been ​  ​ automated? ​  ​ the ​  ​ potential ​  ​ for ​  ​ ai    systems ​  ​ to ​  ​ exacerbate ​  ​ inequality ​  ​ has ​  ​ been ​  ​ widely ​  ​ acknowledged. ​  ​ to ​  ​ address ​  ​ what ​  ​ to ​  ​ do    about ​  ​ it, ​  ​ some ​  ​ are ​  ​ turning ​  ​ to ​  ​ models ​  ​ of ​  ​ resource ​  ​ redistribution, ​  ​ and ​  ​ to ​  ​ the ​  ​ idea ​  ​ of ​  ​ a    universal ​  ​ basic ​  ​ income ​  ​ (ubi). ​  ​ the ​  ​ past ​  ​ year ​  ​ has ​  ​ seen ​  ​ a ​  ​ number ​  ​ of ​  ​ high-profile ​  ​ experiments    in ​  ​ redistributive ​  ​ social ​  ​ welfare, ​  ​ based ​  ​ on ​  ​ assumptions ​  ​ that ​  ​ ai ​  ​ and ​  ​ automation ​  ​ will ​  ​ require    resource ​  ​ distribution ​  ​ not ​  ​ explicitly ​  ​ tied ​  ​ to ​  ​ the ​  ​ sale ​  ​ of ​  ​ individual ​  ​ labor. ​  ​ some ​  ​ of ​  ​ the ​  ​ most    visible ​  ​ efforts ​  ​ have ​  ​ come ​  ​ from ​  ​ governments ​  ​ and ​  ​ private ​  ​ actors ​  ​ running ​  ​ small ​  ​ trials ​  ​ where    people ​  ​ receive ​  ​ direct ​  ​ cash ​  ​ transfers ​  ​ in ​  ​ the ​  ​ form ​  ​ of ​  ​ a ​  ​ basic ​  ​ income ​  ​ stipend. ​  ​ it ​  ​ bears ​  ​ noting    that ​  ​ payments ​  ​ made ​  ​ as ​  ​ a ​  ​ part ​  ​ of ​  ​ these ​  ​ experiments ​  ​ cannot ​  ​ be ​  ​ considered ​  ​ “universal”    insofar ​  ​ as ​  ​ they ​  ​ are ​  ​ provided ​  ​ to ​  ​ a ​  ​ limited ​  ​ number ​  ​ of ​  ​ people. ​  ​ thus, ​  ​ while ​  ​ these ​  ​ experiments    can ​  ​ gather ​  ​ informative ​  ​ data ​  ​ that ​  ​ tells ​  ​ us ​  ​ about ​  ​ individual ​  ​ reactions ​  ​ to ​  ​ the ​  ​ receipt ​  ​ of ​  ​ such    funds, ​  ​ they ​  ​ cannot ​  ​ account ​  ​ for ​  ​ the ​  ​ society-wide ​  ​ impact ​  ​ of ​  ​ a ​  ​ universal ​  ​ payment. ​  ​ for    example, ​  ​ in ​  ​ april ​  ​ of ​  ​ 2017, ​  ​ the ​  ​ government ​  ​ of ​  ​ ontario ​  ​ began ​  ​ a ​  ​ ubi ​  ​ pilot ​  ​ research ​  ​ program    with ​  ​ 4,000 ​  ​ participants ​  ​ that ​  ​ will ​  ​ provide ​  ​ up ​  ​ to ​  ​ c$16,989 ​  ​ per ​  ​ year ​  ​ for ​  ​ a ​  ​ single ​  ​ person ​  ​ and    c$24,027 ​  ​ per ​  ​ year ​  ​ for ​  ​ a ​  ​ couple, ​  ​ less ​  ​ 50 ​  ​ percent ​  ​ of ​  ​ any ​  ​ earned ​  ​ income. ​  ​ y ​  ​ combinator, ​  ​ a  34  silicon ​  ​ valley-based ​  ​ startup ​  ​ incubator, ​  ​ began ​  ​ a ​  ​ one ​  ​ year ​  ​ ubi ​  ​ pilot ​  ​ study ​  ​ in ​  ​ oakland ​  ​ in ​  ​ which    one ​  ​ hundred ​  ​ families ​  ​ will ​  ​ receive ​  ​ $1,000 ​  ​ to ​  ​ $2,000 ​  ​ per ​  ​ month ​  ​ over ​  ​ the ​  ​ course ​  ​ of ​  ​ a ​  ​ year. ​  ​ y  35  combinator ​  ​ president ​  ​ (and ​  ​ openai ​  ​ co-chairman) ​  ​ sam ​  ​ altman ​  ​ explicitly ​  ​ references ​  ​ job    displacement ​  ​ due ​  ​ to ​  ​ technology ​  ​ as ​  ​ a ​  ​ motivating ​  ​ factor ​  ​ for ​  ​ ubi ​  ​ research. ​ ​  ​ while ​  ​ ubi  36  remains ​  ​ a ​  ​ politically ​  ​ contentious ​  ​ idea ​  ​ with ​  ​ significant ​  ​ variations ​  ​ in ​  ​ approach ​  ​ and    implementation, ​  ​ it ​  ​ is ​  ​ currently ​  ​ one ​  ​ of ​  ​ the ​  ​ most ​  ​ commonly ​  ​ proposed ​  ​ policy ​  ​ responses ​  ​ to    ai-driven ​  ​ job ​  ​ losses, ​  ​ and ​  ​ as ​  ​ such ​  ​ deserves ​  ​ close ​  ​ assessment.    bias ​  ​ and ​  ​ inclusion    the ​  ​ word ​  ​ “bias” ​  ​ has ​  ​ multiple ​  ​ meanings ​  ​ that ​  ​ intersect ​  ​ with ​  ​ ai ​  ​ applications ​  ​ in ​  ​ ways ​  ​ that ​  ​ can    overlap ​  ​ and ​  ​ occasionally ​  ​ contradict ​  ​ each ​  ​ other. ​  ​ this ​  ​ can ​  ​ add ​  ​ unnecessary ​  ​ confusion ​  ​ to    what ​  ​ is ​  ​ a ​  ​ critically ​  ​ needed ​  ​ domain ​  ​ of ​  ​ research. ​  ​ in ​  ​ statistics—used ​  ​ in ​  ​ many ​  ​ machine ​  ​ learning    33 ​  ​​  ​​  ​​ michael ​  ​ luca, ​  ​ jon ​  ​ kleinberg, ​  ​ and ​  ​ sendhil ​  ​ mullainathan, ​  ​ “algorithms ​  ​ need ​  ​ manage rs, ​  ​ too,” ​  ​​ harvard ​  ​ business ​  ​ review ​ ,    january ​  ​ 1, ​  ​ 2016, ​  ​ https://hbr.org/2016/01/algorithms-need-managers-too.    34  ministry ​  ​ of ​  ​ community ​  ​ and ​  ​ social ​  ​ services, ​  ​ “ontario’s ​  ​ basic ​  ​ income ​  ​ pilot,” ​  ​​ news.ontario.ca ​ , ​  ​ april ​  ​ 24, ​  ​ 2017,    https://news.ontario.ca/mcss/en/2017/04/ontarios-basic-income-pilot.html ​ .    35  michael ​  ​ j. ​  ​ coren, ​  ​ “y ​  ​ combinator ​  ​ is ​  ​ running ​  ​ a ​  ​ basic ​  ​ income ​  ​ experiment ​  ​ with ​  ​ 100 ​  ​ oakland ​  ​ families,” ​  ​​ quartz ​ , ​  ​ june ​  ​ 1, ​  ​ 2017,    https://qz.com/696377/y-combinator-is-running-a-basic-income-experiment-with-100-oakland-families/ ​ .    36  sam ​  ​ altman, ​  ​ “moving ​  ​ forward ​  ​ on ​  ​ basic ​  ​ income,” ​  ​​ y ​  ​ combinator ​ , ​  ​ may ​  ​ 31, ​  ​ 2016,    https://blog.ycombinator.com/moving-forward-on-basic-income/ ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 14    applications—“bias” ​  ​ has ​  ​ a ​  ​ specific ​  ​ meaning ​  ​ that ​  ​ differs ​  ​ from ​  ​ the ​  ​ popular ​  ​ and ​  ​ social    scientific ​  ​ definitions. ​  ​ for ​  ​ example, ​  ​ the ​  ​ idea ​  ​ of ​  ​ “selection ​  ​ bias” ​  ​ refers ​  ​ to ​  ​ errors ​  ​ in ​  ​ estimation    that ​  ​ result ​  ​ when ​  ​ some ​  ​ members ​  ​ of ​  ​ a ​  ​ population ​  ​ are ​  ​ more ​  ​ likely ​  ​ to ​  ​ be ​  ​ sampled ​  ​ than ​  ​ others.    so ​  ​ when ​  ​ a ​  ​ machine ​  ​ learning ​  ​ program ​  ​ trained ​  ​ to ​  ​ recognize, ​  ​ say, ​  ​ faces ​  ​ of ​  ​ a ​  ​ particular ​  ​ racial    group ​  ​ is ​  ​ applied ​  ​ to ​  ​ larger ​  ​ or ​  ​ more ​  ​ diverse ​  ​ populations, ​  ​ it ​  ​ may ​  ​ produce ​  ​ biased ​  ​ results ​  ​ in ​  ​ the    sense ​  ​ of ​  ​ having ​  ​ a ​  ​ lower ​  ​ measure ​  ​ of ​  ​ accuracy.    the ​  ​ word ​  ​ “bias” ​  ​ also ​  ​ has ​  ​ normative ​  ​ meanings ​  ​ in ​  ​ both ​  ​ colloquial ​  ​ and ​  ​ legal ​  ​ language, ​  ​ where    it ​  ​ refers ​  ​ to ​  ​ judgement ​  ​ based ​  ​ on ​  ​ preconceived ​  ​ notions ​  ​ or ​  ​ prejudices, ​  ​ as ​  ​ opposed ​  ​ to ​  ​​  ​ the    impartial ​  ​ evaluation ​  ​ of ​  ​ facts. ​  ​ impartiality ​  ​ is ​  ​ a ​  ​ core ​  ​ value ​  ​ of ​  ​ many ​  ​ legal ​  ​ systems ​  ​ and ​  ​ governs    many ​  ​ legal ​  ​ processes, ​  ​ from ​  ​ juror ​  ​ selection ​  ​ to ​  ​ the ​  ​ limitations ​  ​ placed ​  ​ on ​  ​ judges. ​  ​ for ​  ​ example,    in ​  ​ the ​  ​ united ​  ​ states ​  ​ the ​  ​ sixth ​  ​ amendment ​  ​ to ​  ​ the ​  ​ constitution ​  ​ mandates ​  ​ a ​  ​ right ​  ​ to ​  ​ an    impartial ​  ​ jury ​  ​ and ​  ​ the ​  ​ fourteenth ​  ​ mandates ​  ​ equal ​  ​ protection ​  ​ under ​  ​ the ​  ​ law. ​  ​ this ​  ​ sense ​  ​ of    the ​  ​ word ​  ​ bias ​  ​ is ​  ​ closely ​  ​ linked ​  ​ to ​  ​ normative ​  ​ and ​  ​ ethical ​  ​ perspectives ​  ​ on ​  ​ fairness, ​  ​ and ​  ​ the    idea ​  ​ that ​  ​ different ​  ​ groups ​  ​ should ​  ​ be ​  ​ treated ​  ​ equally.     when ​  ​ examining ​  ​ technical ​  ​ systems, ​  ​ there ​  ​ can ​  ​ be ​  ​ a ​  ​ temptation ​  ​ to, ​  ​ or ​  ​ vested ​  ​ interest ​  ​ in,    limiting ​  ​ discussion ​  ​ of ​  ​ bias ​  ​ to ​  ​ the ​  ​ first ​  ​ more ​  ​ ‘neutral’ ​  ​ statistical ​  ​ sense ​  ​ of ​  ​ the ​  ​ term. ​  ​ however,    in ​  ​ practice ​  ​ there ​  ​ is ​  ​ rarely ​  ​ a ​  ​ clear ​  ​ demarcation ​  ​ between ​  ​ the ​  ​ statistical ​  ​ and ​  ​ the ​  ​ normative    definitions: ​  ​ biased ​  ​ models ​  ​ or ​  ​ learning ​  ​ algorithms, ​  ​ as ​  ​ defined ​  ​ statistically, ​  ​ can ​  ​ lead ​  ​ to    unequal ​  ​ and ​  ​ unfair ​  ​ treatments ​  ​ and ​  ​ outcomes ​  ​ for ​  ​ different ​  ​ social ​  ​ or ​  ​ racial ​  ​ groups.     the ​  ​ danger ​  ​ of ​  ​ bias ​  ​ increases ​  ​ when ​  ​ these ​  ​ systems ​  ​ are ​  ​ applied, ​  ​ often ​  ​ in ​  ​ non-transparent    ways, ​  ​ to ​  ​ critical ​  ​ institutions ​  ​ like ​  ​ criminal ​  ​ justice ​  ​ and ​  ​ healthcare. ​  ​ the ​  ​ social ​  ​ sciences ​  ​ and    critical ​  ​ humanities ​  ​ have ​  ​ decades ​  ​ of ​  ​ research ​  ​ on ​  ​ bias ​  ​ within ​  ​ social ​  ​ systems ​  ​ that ​  ​ have ​  ​ much    to ​  ​ offer ​  ​ the ​  ​ current ​  ​ debate ​  ​ on ​  ​ bias ​  ​ in ​  ​ ai ​  ​ and ​  ​ algorithmic ​  ​ systems. ​  ​ since ​  ​​ ai ​  ​ now ​ ​  ​ is ​  ​ deeply  37  interested ​  ​ in ​  ​ the ​  ​ social ​  ​ and ​  ​ political ​  ​ implications ​  ​ of ​  ​ ai, ​  ​ this ​  ​ report ​  ​ will ​  ​ use ​  ​ the ​  ​ word ​  ​ “bias”    in ​  ​ its ​  ​ broader, ​  ​ normative ​  ​ sense ​  ​ in ​  ​ the ​  ​ following ​  ​ section, ​  ​ while ​  ​ acknowledging ​  ​ its ​  ​ close    relationship ​  ​ with ​  ​ statistical ​  ​ usages.    while ​  ​ the ​  ​ potential ​  ​ impact ​  ​ of ​  ​ such ​  ​ biases ​  ​ are ​  ​ extremely ​  ​ worrying, ​  ​ solutions ​  ​ are    complicated. ​  ​ this ​  ​ is ​  ​ in ​  ​ part ​  ​ because ​  ​ biased ​  ​ ai ​  ​ can ​  ​ result ​  ​ from ​  ​ a ​  ​ number ​  ​ of ​  ​ factors, ​  ​ alone ​  ​ or    in ​  ​ combination, ​  ​ such ​  ​ as ​  ​ who ​  ​ develops ​  ​ systems, ​  ​ what ​  ​ goals ​  ​ system ​  ​ developers ​  ​ have ​  ​ in ​  ​ mind    during ​  ​ development, ​  ​ what ​  ​ training ​  ​ data ​  ​ they ​  ​ use, ​  ​ and ​  ​ whether ​  ​ the ​  ​ the ​  ​ systems ​  ​ work ​  ​ well    for ​  ​ different ​  ​ parts ​  ​ of ​  ​ the ​  ​ population. ​  ​ this ​  ​ section ​  ​ addresses ​  ​ the ​  ​ latest ​  ​ research ​  ​ on ​  ​ bias ​  ​ in  38  ai ​  ​ and ​  ​ discusses ​  ​ some ​  ​ of ​  ​ the ​  ​ emerging ​  ​ strategies ​  ​ being ​  ​ used ​  ​ to ​  ​ address ​  ​ it.    where ​  ​ bias ​  ​ comes ​  ​ from      ai ​  ​ systems ​  ​ are ​  ​ taught ​  ​ what ​  ​ they ​  ​ “know” ​  ​ from ​  ​ training ​  ​ data. ​  ​ training ​  ​ data ​  ​ can ​  ​ be    37 ​  ​​  ​​  ​​ barocas, ​  ​ crawford, ​  ​ shapiro ​  ​ and ​  ​ wallach, ​  ​ “the ​  ​ problem ​  ​ with ​  ​ bias: ​  ​ allocative ​  ​ versus ​  ​ representational ​  ​ harms ​  ​ in ​  ​ machine    learning,” ​  ​ sigcis ​  ​ conference, ​  ​ october ​  ​ 2017.     38  solon ​  ​ barocas ​  ​ and ​  ​ andrew ​  ​ d. ​  ​ selbst, ​  ​ “big ​  ​ data’s ​  ​ disparate ​  ​ impact,” ​  ​ california ​  ​ law ​  ​ review ​  ​ 104, ​  ​ no. ​  ​ 3 ​  ​ (june ​  ​ 1, ​  ​ 2016): ​  ​ 671,    doi:10.15779/z38bg31; ​  ​ sarah ​  ​ bird, ​  ​ solon ​  ​ barocas, ​  ​ kate ​  ​ crawford, ​  ​ fernando ​  ​ diaz ​  ​ and ​  ​ hanna ​  ​ wallach, ​  ​ \"exploring ​  ​ or    exploiting? ​  ​ social ​  ​ and ​  ​ ethical ​  ​ implications ​  ​ of ​  ​ autonomous ​  ​ experimentation ​  ​ in ​  ​ ai,\" ​  ​ (2016).  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 15    incomplete, ​  ​ biased ​  ​ or ​  ​ otherwise ​  ​ skewed, often ​  ​ drawing ​  ​ on ​  ​ limited ​  ​ and  39 40  non-representative ​  ​ samples ​  ​ that ​  ​ are ​  ​ poorly ​  ​ defined ​  ​ before ​  ​ use. ​  ​ such ​  ​ problems ​  ​ with  41  training ​  ​ data ​  ​ may ​  ​ not ​  ​ be ​  ​ obvious, ​  ​ as ​  ​ datasets ​  ​ may ​  ​ be ​  ​ constructed ​  ​ in ​  ​ non-transparent ​  ​ ways.  ​  ​ additionally, ​  ​ given ​  ​ that ​  ​ humans ​  ​ must ​  ​ label ​  ​ much ​  ​ of ​  ​ the ​  ​ training ​  ​ data ​  ​ by ​  ​ hand, ​  ​ human  42  biases ​  ​ and ​  ​ cultural ​  ​ assumptions ​  ​ are ​  ​ transmitted ​  ​ by ​  ​ classification ​  ​ choices. ​  ​ exclusion ​  ​ of  43  certain ​  ​ data ​  ​ can, ​  ​ in ​  ​ turn, ​  ​ mean ​  ​ exclusion ​  ​ of ​  ​ sub-populations ​  ​ from ​  ​ what ​  ​ ai ​  ​ is ​  ​ able ​  ​ to ​  ​ “see”    and ​  ​ “know.” ​  ​ while ​  ​ pernicious, ​  ​ these ​  ​ biases ​  ​ are ​  ​ difficult ​  ​ to ​  ​ find ​  ​ and ​  ​ understand, ​  ​ especially  44  when ​  ​ systems ​  ​ are ​  ​ proprietary, ​  ​ treated ​  ​ as ​  ​ black ​  ​ boxes ​  ​ or ​  ​ taken ​  ​ at ​  ​ face ​  ​ value. ​  ​ computer  45  scientists ​  ​ have ​  ​ noted ​  ​ that ​  ​ the ​  ​ complexity ​  ​ of ​  ​ machine ​  ​ learning ​  ​ systems ​  ​ not ​  ​ only ​  ​ must ​  ​ face    difficulties ​  ​ in ​  ​ interpreting ​  ​ opaque, ​  ​ unsupervised ​  ​ models, ​  ​ but ​  ​ may ​  ​ also ​  ​ take ​  ​ on ​  ​ “technical    debt” ​  ​ that ​  ​ makes ​  ​ maintenance ​  ​ and ​  ​ improvement ​  ​ costly—leading ​  ​ to ​  ​ situations ​  ​ where ​  ​ bias    may ​  ​ be ​  ​ difficult ​  ​ to ​  ​ identify ​  ​​ and ​  ​​ mitigate.  46  non-representative ​  ​ collection ​  ​ of ​  ​ data ​  ​ can ​  ​ also ​  ​ produce ​  ​ bias. ​  ​ data ​  ​ is ​  ​ expensive, ​  ​ and ​  ​ data ​  ​ at    scale ​  ​ is ​  ​ hard ​  ​ to ​  ​ come ​  ​ by. ​  ​ thus, ​  ​ those ​  ​ who ​  ​ want ​  ​ to ​  ​ train ​  ​ an ​  ​ ai ​  ​ system ​  ​ are ​  ​ drawn ​  ​ to ​  ​ the ​  ​ use    of ​  ​ easily ​  ​ available ​  ​ data, ​  ​ often ​  ​ crowd-sourced, ​  ​ scraped, ​  ​ or ​  ​ otherwise ​  ​ gathered ​  ​ from  47  existing ​  ​ user-facing ​  ​ apps ​  ​ and ​  ​ properties. ​  ​ this ​  ​ type ​  ​ of ​  ​ data ​  ​ can ​  ​ easily ​  ​ privilege    socioeconomically ​  ​ advantaged ​  ​ populations, ​  ​ those ​  ​ with ​  ​ greater ​  ​ access ​  ​ to ​  ​ connected ​  ​ devices    and ​  ​ online ​  ​ services. ​  ​ these ​  ​ same ​  ​ types ​  ​ of ​  ​ bias ​  ​ can ​  ​ also ​  ​ exist ​  ​ when ​  ​ data ​  ​ is ​  ​ collected ​  ​ from    particular ​  ​ groups ​  ​ and ​  ​ not ​  ​ others. ​  ​​ a ​  ​ recent ​  ​ example ​  ​ comes ​  ​ from ​  ​ an ​  ​ experiment ​  ​ by ​  ​ openai  48  in ​  ​ which ​  ​ a ​  ​ year’s ​  ​ worth ​  ​ of ​  ​ messages ​  ​ from ​  ​ the ​  ​ discussion ​  ​ forum ​  ​ reddit ​  ​ were ​  ​ used ​  ​ as ​  ​ data ​  ​ to    train ​  ​ an ​  ​ ai ​  ​ model ​  ​ to ​  ​ “speak.” ​  ​ reddit ​  ​ is ​  ​ itself ​  ​ a ​  ​ skewed ​  ​ sub-population ​  ​ of ​  ​ internet ​  ​ users,  49  and ​  ​ this ​  ​ experiment ​  ​ can ​  ​ give ​  ​ us ​  ​ a ​  ​ sense ​  ​ of ​  ​ the ​  ​ types ​  ​ of ​  ​ bias ​  ​ that ​  ​ can ​  ​ occur ​  ​ when ​  ​ a ​  ​ small,    39  david ​  ​ j. ​  ​ beymer, ​  ​ karen ​  ​ w. ​  ​ brannon, ​  ​ ting ​  ​ chen, ​  ​ moritz ​  ​ aw ​  ​ hardt, ​  ​ ritwik ​  ​ k. ​  ​ kumar ​  ​ and ​  ​ tanveer ​  ​ f. ​  ​ syeda-mahmoo, ​  ​ \"machine    learning ​  ​ with ​  ​ incomplete ​  ​ data ​  ​ sets,\" ​  ​ u.s. ​  ​ patent ​  ​ 9,349,105, ​  ​ issued ​  ​ may ​  ​ 24, ​  ​ 2016.    40  lisa ​  ​ gitelman, ​  ​​ raw ​  ​ data ​  ​ is ​  ​ an ​  ​ oxymoron, ​ ​  ​ (mit ​  ​ press: ​  ​ 2013).    41  ishan ​  ​ misra, ​  ​ c. ​  ​ lawrence ​  ​ zitnick, ​  ​ margaret ​  ​ mitchell ​  ​ and ​  ​ ross ​  ​ girshick, ​  ​ \"seeing ​  ​ through ​  ​ the ​  ​ human ​  ​ reporting ​  ​ bias: ​  ​ visual    classifiers ​  ​ from ​  ​ noisy ​  ​ human-centric ​  ​ labels,\" ​  ​ (in ​  ​ proceedings ​  ​ of ​  ​ the ​  ​ ieee ​  ​ conference ​  ​ on ​  ​ computer ​  ​ vision ​  ​ and ​  ​ pattern    recognition, ​  ​ pp. ​  ​ 2930-2939, ​  ​ 2016).    42  josh ​  ​ attenberg, ​  ​ prem ​  ​ melville, ​  ​ foster ​  ​ provost ​  ​ and ​  ​ maytal ​  ​ saar-tsechansky, ​  ​ \"selective ​  ​ data ​  ​ acquisition ​  ​ for ​  ​ machine    learning,\" ​  ​ in ​  ​​ cost-sensitive ​  ​ machine ​  ​ learning ​ . ​  ​ (crc ​  ​ press: ​  ​ 2011), ​  ​ pp. ​  ​ 101-155; ​  ​ christian ​  ​ beyer, ​  ​ georg ​  ​ krempl ​  ​ and ​  ​ vincent    lemaire, ​  ​ \"how ​  ​ to ​  ​ select ​  ​ information ​  ​ that ​  ​ matters: ​  ​ a ​  ​ comparative ​  ​ study ​  ​ on ​  ​ active ​  ​ learning ​  ​ strategies ​  ​ for ​  ​ classification,\" ​  ​ in    proceedings ​  ​ of ​  ​ the ​  ​ 15th ​  ​ international ​  ​ conference ​  ​ on ​  ​ knowledge ​  ​ technologies ​  ​ and ​  ​ data-driven ​  ​ business ​ , ​  ​ p. ​  ​ 2. ​  ​ acm, ​  ​ 2015.    43  moritz ​  ​ hardt, ​  ​ nimrod ​  ​ megiddo, ​  ​ christos ​  ​ papadimitriou ​  ​ and ​  ​ mary ​  ​ wootters, ​  ​ \"strategic ​  ​ classification.\" ​  ​ (in ​  ​ proceedings ​  ​ of ​  ​ the    2016 ​  ​ acm ​  ​ conference ​  ​ on ​  ​ innovations ​  ​ in ​  ​ theoretical ​  ​ computer ​  ​ science, ​  ​ pp. ​  ​ 111-122, ​  ​ 2016).    44  matthew ​  ​ zook, ​  ​ solon ​  ​ barocas, ​  ​ kate ​  ​ crawford, ​  ​ emily ​  ​ keller, ​  ​ seeta ​  ​ peña ​  ​ gangadharan, ​  ​ alyssa ​  ​ goodman, ​  ​ rachelle ​  ​ hollander,    barbara ​  ​ a. ​  ​ koenig, ​  ​ jacob ​  ​ metcalf, ​  ​ arvind ​  ​ narayanan, ​  ​ alondra ​  ​ nelson ​  ​ and ​  ​ frank ​  ​ pasquale, ​  ​ \"ten ​  ​ simple ​  ​ rules ​  ​ for ​  ​ responsible    big ​  ​ data ​  ​ research,\" ​  ​ plos ​  ​ computational ​  ​ biology ​  ​ 13, ​  ​ no. ​  ​ 3 ​  ​ (2017): ​  ​ e1005399.    45  frank ​  ​ pasquale, ​  ​​ the ​  ​ black ​  ​ box ​  ​ society: ​  ​ the ​  ​ secret ​  ​ algorithms ​  ​ that ​  ​ control ​  ​ money ​  ​ and ​  ​ information ​ , ​  ​ (harvard ​  ​ university ​  ​ press,    2015).    46 ​  ​​  ​​  ​​  ​ d. ​  ​ sculley ​  ​ et ​  ​ al., ​  ​ “machine ​  ​ learning: ​  ​ the ​  ​ high ​  ​ interest ​  ​ credit ​  ​ card ​  ​ of ​  ​ technical ​  ​ debt,” ​  ​ se4ml: ​  ​ software ​  ​ engineering ​  ​ for    machine ​  ​ learning ​  ​ (nips ​  ​ 2014 ​  ​ workshop), ​  ​ 2014, ​  ​ https://research.google.com/pubs/pub43146.html.    47  amanda ​  ​ levendowski, ​  ​ “how ​  ​ copyright ​  ​ law ​  ​ creates ​  ​ biased ​  ​ artificial ​  ​ intelligence,”    http://www.werobot2017.com/wp-content/uploads/2017/03/levendowski-how-copyright-law-creates-biased-artificial-in  telligence-abstract-and-introduction-1.pdf ​ .    48  josh ​  ​ terrell, ​  ​ andrew ​  ​ kofink, ​  ​ justin ​  ​ middleton, ​  ​ clarissa ​  ​ rainear, ​  ​ emerson ​  ​ murphy-hill, ​  ​ chris ​  ​ parnin ​  ​ and ​  ​ jon ​  ​ stallings, ​  ​​ gender    differences ​  ​ and ​  ​ bias ​  ​ in ​  ​ open ​  ​ source: ​  ​ pull ​  ​ request ​  ​ acceptance ​  ​ of ​  ​ women ​  ​ versus ​  ​ men ​ , ​  ​ no. ​  ​ e1733v2. ​  ​ peerj ​  ​ preprints, ​  ​ 2016.    49  ananya ​  ​ bhattacharya, ​  ​ “elon ​  ​ musk’s ​  ​ openai ​  ​ is ​  ​ using ​  ​ reddit ​  ​ to ​  ​ teach ​  ​ ai ​  ​ to ​  ​ speak ​  ​ like ​  ​ humans,” ​  ​​ quartz ​ , ​  ​ october ​  ​ 12, ​  ​ 2016,    https://qz.com/806321/open-ai-reddit-human-conversation ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 16    nonrepresentative ​  ​ group ​  ​ is ​  ​ used ​  ​ as ​  ​ a ​  ​ stand-in ​  ​ for ​  ​ the ​  ​ whole.     problems ​  ​ may ​  ​ also ​  ​ result ​  ​ from ​  ​ the ​  ​ disconnect ​  ​ between ​  ​ the ​  ​ context ​  ​ in ​  ​ which ​  ​ an ​  ​ ai ​  ​ system ​  ​ is    used ​  ​ and ​  ​ the ​  ​ assumptions ​  ​ built ​  ​ into ​  ​ the ​  ​ ai ​  ​ system ​  ​ when ​  ​ it ​  ​ was ​  ​ designed. ​  ​ a ​  ​ group ​  ​ of    researchers ​  ​ recently ​  ​ assessed ​  ​ how ​  ​ ai-based ​  ​ mapping ​  ​ apps ​  ​ often ​  ​ provide ​  ​ indirect ​  ​ routes ​  ​ to    some ​  ​ users ​  ​ as ​  ​ a ​  ​ way ​  ​ to ​  ​ accomplish ​  ​ traffic ​  ​ load-balancing. ​  ​ the ​  ​ system ​  ​ will ​  ​ not ​  ​ be ​  ​ able ​  ​ to ​  ​ tell    when ​  ​ the ​  ​ person ​  ​ asking ​  ​ for ​  ​ directions ​  ​ is ​  ​ driving ​  ​ to ​  ​ the ​  ​ hospital ​  ​ in ​  ​ an ​  ​ emergency. ​  ​ such    decontextualized ​  ​ assumptions ​  ​ can ​  ​ put ​  ​ non-consenting ​  ​ and ​  ​ unaware ​  ​ populations ​  ​ at ​  ​ risk    while ​  ​ providing ​  ​ little ​  ​ opportunity ​  ​ for ​  ​ direct ​  ​ input.   50  while ​  ​ widely ​  ​ acknowledged ​  ​ as ​  ​ a ​  ​ problem, ​  ​ bias ​  ​ within ​  ​ and ​  ​ beyond ​  ​ ai ​  ​ is ​  ​ difficult ​  ​ to ​  ​ measure.    unintended ​  ​ consequences ​  ​ and ​  ​ inequalities ​  ​ are ​  ​ by ​  ​ nature ​  ​ collective, ​  ​ relative ​  ​ and  51  contextual, ​  ​ making ​  ​ measurement ​  ​ and ​  ​ baseline ​  ​ comparisons ​  ​ difficult. ​  ​ information ​  ​ biases  52  in ​  ​ particular ​  ​ are ​  ​ difficult ​  ​ to ​  ​ measure, ​  ​ given ​  ​ the ​  ​ many ​  ​ possible ​  ​ reference ​  ​ points ​  ​ in ​  ​ context:    content, ​  ​ users, ​  ​ ranking ​  ​ and ​  ​ access. ​  ​ there ​  ​ is ​  ​ potential ​  ​ for ​  ​ both ​  ​ over- ​  ​ and ​  ​ under-counting  53  biases ​  ​ in ​  ​ measurement ​  ​ of ​  ​ distributions ​  ​ given ​  ​ the ​  ​ limits ​  ​ on ​  ​ observable ​  ​ circumstances ​  ​ for    individuals, ​  ​ problems ​  ​ with ​  ​ population ​  ​ gaps ​  ​ and ​  ​ possible ​  ​ measurement ​  ​ errors.  54  given ​  ​ the ​  ​ difficulty ​  ​ (and ​  ​ sometimes ​  ​ even ​  ​ technical ​  ​ impossibility) ​  ​ of ​  ​ understanding ​  ​ exactly    how ​  ​ ai ​  ​ systems ​  ​ have ​  ​ reached ​  ​ a ​  ​ given ​  ​ decision, ​  ​ bias ​  ​ is ​  ​ often ​  ​ only ​  ​ revealed ​  ​ by  55  demonstrating ​  ​ an ​  ​ inequality ​  ​ in ​  ​ outcomes, ​  ​ post-hoc. ​  ​ examples ​  ​ of ​  ​ this ​  ​ are ​  ​ familiar ​  ​ from    recent ​  ​ news ​  ​ stories. ​  ​ julia ​  ​ angwin’s ​  ​ propublica ​  ​ piece ​  ​ on ​  ​ northpointe’s ​  ​ racially-biased    compas ​  ​ system, ​  ​ used ​  ​ to ​  ​ make ​  ​ sentencing ​  ​ decisions ​  ​ in ​  ​ courts ​  ​ across ​  ​ the ​  ​ united ​  ​ states, ​  ​ is    an ​  ​ exemplar ​  ​ of ​  ​ the ​  ​ genre. ​  ​ similarly, ​  ​ bloomberg ​  ​ found ​  ​ that ​  ​ amazon’s ​  ​ same-day ​  ​ delivery  56  service ​  ​ was ​  ​ bypassing ​  ​ zip ​  ​ codes ​  ​ that ​  ​ are ​  ​ predominantly ​  ​ black. ​  ​ this ​  ​ decision ​  ​ may ​  ​ have ​  ​ been    made ​  ​ for ​  ​ many ​  ​ reasons, ​  ​ but ​  ​ its ​  ​ result ​  ​ was ​  ​ racial ​  ​ bias.  57  the ​  ​ ai ​  ​ field ​  ​ is ​  ​ not ​  ​ diverse    bias ​  ​ can ​  ​ also ​  ​ emerge ​  ​ in ​  ​ ai ​  ​ systems ​  ​ because ​  ​ of ​  ​ the ​  ​ very ​  ​ narrow ​  ​ subset ​  ​ of ​  ​ the ​  ​ population    that ​  ​ design ​  ​ them. ​  ​ ai ​  ​ developers ​  ​ are ​  ​ mostly ​  ​ male, ​  ​ generally ​  ​ highly ​  ​ paid, ​  ​ and ​  ​ similarly    50  sarah ​  ​ bird, ​  ​ solon ​  ​ barocas, ​  ​ kate ​  ​ crawford, ​  ​ fernando ​  ​ diaz ​  ​ and ​  ​ hanna ​  ​ wallach, ​  ​ \"exploring ​  ​ or ​  ​ exploiting? ​  ​ social ​  ​ and ​  ​ ethical    implications ​  ​ of ​  ​ autonomous ​  ​ experimentation ​  ​ in ​  ​ ai,\" ​  ​​ workshop ​  ​ on ​  ​ fairness, ​  ​ accountability, ​  ​ and ​  ​ transparency ​  ​ in ​  ​ machine    learning ​ , ​  ​​ https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2846909 ​ ​  ​ (2016).    51  marco ​  ​ j ​  ​ haenssgen ​  ​ and ​  ​ proochista ​  ​ ariana, ​  ​ \"the ​  ​ social ​  ​ implications ​  ​ of ​  ​ technology ​  ​ diffusion: ​  ​ uncovering ​  ​ the ​  ​ unintended    consequences ​  ​ of ​  ​ people’s ​  ​ health-related ​  ​ mobile ​  ​ phone ​  ​ use ​  ​ in ​  ​ rural ​  ​ india ​  ​ and ​  ​ china,\" ​  ​​ world ​  ​ development ​ ​  ​ 94 ​  ​ (2017):    286-304.    52  frank ​  ​ cowell, ​  ​​ measuring ​  ​ inequality ​ , ​  ​ (oxford ​  ​ university ​  ​ press, ​  ​ 2011).    53  evaggelia ​  ​ pitoura, ​  ​ panayiotis ​  ​ tsaparas, ​  ​ giorgos ​  ​ flouris, ​  ​ irini ​  ​ fundulaki, ​  ​ panagiotis ​  ​ papadakos, ​  ​ serge ​  ​ abiteboul ​  ​ and ​  ​ gerhard    weikum, ​  ​ \"on ​  ​ measuring ​  ​ bias ​  ​ in ​  ​ online ​  ​ information,\" ​  ​​ arxiv ​  ​ preprint ​  ​ arxiv:1704.05730 ​ ​  ​ (2017).    54  ashton ​  ​ anderson, ​  ​ jon ​  ​ kleinberg ​  ​ and ​  ​ sendhil ​  ​ mullainathan, ​  ​ \"assessing ​  ​ human ​  ​ error ​  ​ against ​  ​ a ​  ​ benchmark ​  ​ of ​  ​ perfection,\"    arxiv ​  ​ preprint ​  ​ arxiv:1606.04956 ​  ​ (2016).    55  jenna ​  ​ burrell, ​  ​ \"how ​  ​ the ​  ​ machine ​  ​ ‘thinks’: ​  ​ understanding ​  ​ opacity ​  ​ in ​  ​ machine ​  ​ learning ​  ​ algorithms,\" ​  ​​ big ​  ​ data ​  ​ & ​  ​ society ​ ​  ​ 3, ​  ​ no.    1 ​  ​ (2016): ​  ​ doi: ​  ​​ https://doi.org/10.1177/2053951715622512 ​ .    56  angwin, ​  ​ larson ​  ​ and ​  ​ kirchner, ​  ​ “machine ​  ​ bias: ​  ​ there’s ​  ​ software ​  ​ used ​  ​ across ​  ​ the ​  ​ country ​  ​ to ​  ​ predict ​  ​ future ​  ​ criminals. ​  ​ and ​  ​ it’s    biased ​  ​ against ​  ​ blacks,” ​  ​​ propublica ​ , ​  ​ may ​  ​ 23, ​  ​ 2016    https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing    57  david ​  ​ ingold ​  ​ and ​  ​ spencer ​  ​ soper, ​  ​ “amazon ​  ​ doesn’t ​  ​ consider ​  ​ the ​  ​ race ​  ​ of ​  ​ its ​  ​ customers. ​  ​ should ​  ​ it?,” ​ ​  ​​ bloomberg, ​  ​ april ​  ​ 21,    2016, ​  ​​ https://www.bloomberg.com/graphics/2016-amazon-same-day/ ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 17    technically ​  ​ educated. ​  ​ their ​  ​ interests, ​  ​ needs, ​  ​ and ​  ​ life ​  ​ experiences ​  ​ will ​  ​ necessarily ​  ​ be    reflected ​  ​ in ​  ​ the ​  ​ ai ​  ​ they ​  ​ create. ​  ​ bias, ​  ​ whether ​  ​ conscious ​  ​ or ​  ​ unconscious, ​  ​ reflects ​  ​ problems  58  of ​  ​ inclusion ​  ​ and ​  ​ representation. ​  ​ the ​  ​ lack ​  ​ of ​  ​ women ​  ​ and ​  ​ minorities ​  ​ in ​  ​ tech ​  ​ fields, ​  ​ and    artificial ​  ​ intelligence ​  ​ in ​  ​ particular, ​  ​ is ​  ​ well ​  ​ known. ​  ​ but ​  ​ this ​  ​ was ​  ​ not ​  ​ always ​  ​ the ​  ​ case. ​  ​ early  59  programming ​  ​ and ​  ​ data ​  ​ entry ​  ​ work ​  ​ was ​  ​ characterized ​  ​ as ​  ​ secretarial, ​  ​ and ​  ​ was    female-dominated. ​  ​ these ​  ​ women ​  ​ were ​  ​ themselves ​  ​ called ​  ​ “computers,” ​  ​ and ​  ​ they ​  ​ were    often ​  ​ undercompensated ​  ​ and ​  ​ rarely ​  ​ credited. ​ ​  ​ ​  ​ all ​  ​ the ​  ​ while, ​  ​ they ​  ​ were ​  ​ responsible ​  ​ for  60  things ​  ​ like ​  ​ maintaining ​  ​ sophisticated ​  ​ systems ​  ​ that ​  ​ targeted ​  ​ bomb ​  ​ strikes ​  ​ in ​  ​ world ​  ​ war ​  ​ ii  61  and ​  ​ tabulating ​  ​ decades ​  ​ of ​  ​ census ​  ​ data.   62  the ​  ​ history ​  ​ of ​  ​ ai ​  ​ reflects ​  ​ this ​  ​ pattern ​  ​ of ​  ​ gender ​  ​ exclusion. ​  ​ the ​  ​ 1956 ​  ​ dartmouth ​  ​ summer    research ​  ​ project ​  ​ on ​  ​ artificial ​  ​ intelligence, ​  ​ which ​  ​ initiated ​  ​ the ​  ​ concept ​  ​ of ​  ​ artificial    intelligence, ​  ​ was ​  ​ exclusively ​  ​ attended ​  ​ by ​  ​ men. ​  ​ pioneering ​  ​ work ​  ​ in ​  ​ natural ​  ​ language  63  processing ​  ​ and ​  ​ computational ​  ​ linguistics, ​  ​ key ​  ​ to ​  ​ contemporary ​  ​ ai ​  ​ systems, ​  ​ has ​  ​ been    credited ​  ​ to ​  ​ male ​  ​ colleagues ​  ​ and ​  ​ students ​  ​ rather ​  ​ than ​  ​ to ​  ​ margaret ​  ​ masterman, ​  ​ who    founded ​  ​ the ​  ​ cambridge ​  ​ language ​  ​ research ​  ​ unit ​  ​ and ​  ​ was ​  ​ one ​  ​ of ​  ​ the ​  ​ leaders ​  ​ in ​  ​ the ​  ​ field.  64  intentional ​  ​ exclusion ​  ​ and ​  ​ unintentional ​  ​ “like-me” ​  ​ bias ​  ​ is ​  ​ responsible ​  ​ for ​  ​ a ​  ​ continued ​  ​ lack ​  ​ of    demographic ​  ​ representation ​  ​ within ​  ​ the ​  ​ ai ​  ​ field ​  ​ and ​  ​ within ​  ​ the ​  ​ tech ​  ​ industry ​  ​ for ​  ​ women,    hispanics, ​  ​ and ​  ​ african ​  ​ americans.  65  gender ​  ​ and ​  ​ racial ​  ​ disparities ​  ​ among ​  ​ developer ​  ​ cohorts ​  ​ in ​  ​ tech ​  ​ companies ​  ​ are ​  ​ even ​  ​ more    skewed ​  ​ than ​  ​ the ​  ​ demographics ​  ​ of ​  ​ students ​  ​ or ​  ​ academics. ​  ​ in ​  ​ the ​  ​ united ​  ​ states, ​  ​ women    make ​  ​ up ​  ​ about ​  ​ 18 ​  ​ percent ​  ​ of ​  ​ computer ​  ​ science ​  ​ (cs) ​  ​ graduates, ​  ​ yet ​  ​ only ​  ​ 11 ​  ​ percent ​  ​ of    computer ​  ​ engineers ​  ​ are ​  ​ female. ​  ​ african ​  ​ americans ​  ​ and ​  ​ hispanics ​  ​ represent ​  ​ only ​  ​ 11 ​  ​ percent    of ​  ​ total ​  ​ technology ​  ​ sector ​  ​ employees ​  ​ although ​  ​ they ​  ​ comprise ​  ​ 27 ​  ​ percent ​  ​ of ​  ​ the ​  ​ overall    population. ​  ​ representation ​  ​ in ​  ​ the ​  ​ u.s. ​  ​ context ​  ​ has ​  ​ wide ​  ​ reaching ​  ​ implications, ​  ​ given ​  ​ that  66  33 ​  ​ percent ​  ​ of ​  ​ knowledge ​  ​ and ​  ​ technology ​  ​ intensive ​  ​ (kti) ​  ​ jobs ​  ​ worldwide ​  ​ are ​  ​ u.s. ​  ​ based ​  ​ and    those ​  ​ firms ​  ​ contribute ​  ​ 29 ​  ​ percent ​  ​ of ​  ​ global ​  ​ gdp, ​  ​ of ​  ​ which ​  ​ 39 ​  ​ percent ​  ​ are ​  ​ u.s. ​  ​ based. 67  efforts ​  ​ to ​  ​ address ​  ​ gender ​  ​ biases ​  ​ in ​  ​ google ​  ​ ad ​  ​ settings, ​  ​ revealed ​  ​ in ​  ​ 2015, ​  ​ have ​  ​ failed ​  ​ to  68  58  cathy ​  ​ o’neil, ​  ​​ weapons ​  ​ of ​  ​ math ​  ​ destruction: ​  ​ how ​  ​ big ​  ​ data ​  ​ increases ​  ​ inequality ​  ​ and ​  ​ threatens ​  ​ democracy ​ , ​  ​ (new ​  ​ york: ​  ​ crown    publishing ​  ​ group, ​  ​ 2016).    59 ​  ​​  ​​  ​​ kate ​  ​ crawford, ​  ​ “artificial ​  ​ intelligence’s ​  ​ white ​  ​ guy ​  ​ problem,” ​  ​​ the ​  ​ new ​  ​ york ​  ​ times ​ , ​  ​ june ​  ​ 25, ​  ​ 2016.    60  ellen ​  ​ van ​  ​ oost, ​  ​ \"making ​  ​ the ​  ​ computer ​  ​ masculine,\" ​  ​ in ​  ​​ women, ​  ​ work ​  ​ and ​  ​ computerization ​  ​​ (2000), ​  ​ pp. ​  ​ 9-16.    61  nathan ​  ​ ensmenger, ​  ​ \"making ​  ​ programming ​  ​ masculine,\" ​  ​ in ​  ​​ gender ​  ​ codes: ​  ​ why ​  ​ women ​  ​ are ​  ​ leaving ​  ​ computing ​ ​  ​ (2010): ​  ​ 115-42.    62  margaret ​  ​ ann ​  ​ boden, ​  ​​ mind ​  ​ as ​  ​ machine: ​  ​ a ​  ​ history ​  ​ of ​  ​ cognitive ​  ​ science ​ , ​  ​ (clarendon ​  ​ press, ​  ​ 2006).    63  ronald ​  ​ kline, ​  ​ \"cybernetics, ​  ​ automata ​  ​ studies, ​  ​ and ​  ​ the ​  ​ dartmouth ​  ​ conference ​  ​ on ​  ​ artificial ​  ​ intelligence,\" ​  ​​ ieee ​  ​ annals ​  ​ of ​  ​ the    history ​  ​ of ​  ​ computing ​ ​  ​ 33, ​  ​ no. ​  ​ 4 ​  ​ (2011): ​  ​ 5-16.    64  margaret ​  ​ masterman, ​  ​ \"1 ​  ​ personal ​  ​ background,\" ​  ​​ early ​  ​ years ​  ​ in ​  ​ machine ​  ​ translation: ​  ​ memoirs ​  ​ and ​  ​ biographies ​  ​ of ​  ​ pioneers    97 ​  ​ (2000): ​  ​ 279; ​  ​ william ​  ​ williams ​  ​ and ​  ​ frank ​  ​ knowles, ​  ​ \"margaret ​  ​ masterman: ​  ​ in ​  ​ memoriam,\" ​  ​​ computers ​  ​ and ​  ​ translation ​ ​  ​ 2,    no. ​  ​ 4 ​  ​ (1987): ​  ​ 197-203.    65  google, ​  ​ inc. ​  ​ and ​  ​ gallup, ​  ​ inc., ​  ​ “diversity ​  ​ gaps ​  ​ in ​  ​ computer ​  ​ science: ​  ​ exploring ​  ​ the ​  ​ underrepresentation ​  ​ of ​  ​ girls, ​  ​ blacks, ​  ​ and    hispanics,” ​  ​ retrieved ​  ​ from ​  ​ http://goo.gl/pg34ah. ​  ​ additional ​  ​ reports ​  ​ from ​  ​ google’s ​  ​ computer ​  ​ science ​  ​ education ​  ​ research    are ​  ​ available ​  ​ at ​  ​​ https://edu.google.com/resources/computerscience/research ​ .    66  national ​  ​ science ​  ​ foundation, ​  ​ “science ​  ​ and ​  ​ engineering ​  ​ indicators,” ​  ​ 2016, ​  ​ chapter ​  ​ 2,    https://nsf.gov/statistics/2016/nsb20161/#/report/chapter-2 ​ .    67  national ​  ​ science ​  ​ foundation, ​  ​ “science ​  ​ and ​  ​ engineering ​  ​ indicators,” ​  ​ 2016, ​  ​ chapter ​  ​ 6,    https://nsf.gov/statistics/2016/nsb20161/#/report/chapter-6 ​ .    68  amit ​  ​ datta, ​  ​ michael ​  ​ carl ​  ​ tschantz ​  ​ and ​  ​ anupam ​  ​ datta, ​  ​ \"automated ​  ​ experiments ​  ​ on ​  ​ ad ​  ​ privacy ​  ​ settings,\" ​  ​​ proceedings ​  ​ on  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 18    stop ​  ​ inequality ​  ​ in ​  ​ presentation ​  ​ of ​  ​ stem ​  ​ job ​  ​ ads, ​  ​ even ​  ​ when ​  ​ language ​  ​ in ​  ​ ads ​  ​ are ​  ​ controlled    for ​  ​ gender-neutral ​  ​ language.   69  ai ​  ​ is ​  ​ not ​  ​ impartial ​  ​ or ​  ​ neutral. ​  ​ technologies ​  ​ are ​  ​ as ​  ​ much ​  ​ products ​  ​ of ​  ​ the ​  ​ context ​  ​ in ​  ​ which    they ​  ​ are ​  ​ created ​  ​ as ​  ​ they ​  ​ are ​  ​ potential ​  ​ agents ​  ​ for ​  ​ change. ​  ​ machine ​  ​ predictions ​  ​ and  70  performance ​  ​ are ​  ​ constrained ​  ​ by ​  ​ human ​  ​ decisions ​  ​ and ​  ​ values, ​  ​ and ​  ​ those ​  ​ who ​  ​ design,  71  develop, ​  ​ and ​  ​ maintain ​  ​ ai ​  ​ systems ​  ​ will ​  ​ shape ​  ​ such ​  ​ systems ​  ​ within ​  ​ their ​  ​ own ​  ​ understanding    of ​  ​ the ​  ​ world. ​  ​​ many ​  ​ of ​  ​ the ​  ​ biases ​  ​ embedded ​  ​ in ​  ​ ai ​  ​ systems ​  ​ are ​  ​ products ​  ​ of ​  ​ a ​  ​ complex  72  history ​  ​ with ​  ​ respect ​  ​ to ​  ​ diversity ​  ​ and ​  ​ equality.    recent ​  ​ developments ​  ​ in ​  ​ bias ​  ​ research    in ​  ​ the ​  ​ year ​  ​ since ​  ​ the ​  ​​ ai ​  ​ now ​  ​ 2016 ​  ​ symposium, ​  ​​ there ​ ​  ​​ has ​  ​ been ​  ​ a ​  ​ bumper ​  ​ crop ​  ​ of ​  ​ new    research ​  ​ on ​  ​ bias ​  ​ in ​  ​ machine ​  ​ learning. ​  ​ one ​  ​ promising ​  ​ development ​  ​ is ​  ​ that ​  ​ many ​  ​ of ​  ​ these    studies ​  ​ have ​  ​ reflexively ​  ​ used ​  ​ ai ​  ​ techniques ​  ​ to ​  ​ understand ​  ​ the ​  ​ the ​  ​ ways ​  ​ by ​  ​ which ​  ​ ai ​  ​ systems    introduce ​  ​ or ​  ​ perpetuate ​  ​ unequal ​  ​ treatment.     new ​  ​ research ​  ​ on ​  ​ word ​  ​ embeddings ​  ​ has ​  ​ shown ​  ​ the ​  ​ ways ​  ​ in ​  ​ which ​  ​ language, ​  ​ as ​  ​ it ​  ​ is ​  ​ used    within ​  ​ our ​  ​ complex ​  ​ and ​  ​ often ​  ​ biased ​  ​ social ​  ​ contexts, ​  ​ reflects ​  ​ bias. ​  ​​  ​​ word ​  ​ embeddings ​  ​ are  73  set ​  ​ of ​  ​ natural ​  ​ language ​  ​ processing ​  ​ techniques ​  ​ that ​  ​ map ​  ​ the ​  ​ semantic ​  ​ relationship ​  ​ between    words, ​  ​ creating ​  ​ a ​  ​ model ​  ​ that ​  ​ predicts ​  ​ which ​  ​ words ​  ​ are ​  ​ likely ​  ​ to ​  ​ be ​  ​ associated ​  ​ with ​  ​ which.    researchers ​  ​ looking ​  ​ at ​  ​ word ​  ​ embeddings ​  ​ showed ​  ​ that ​  ​ predictable ​  ​ gendered ​  ​ associations    between ​  ​ words, ​  ​ such ​  ​ as ​  ​ “female” ​  ​ and ​  ​ “queen” ​  ​ are ​  ​ reflected ​  ​ in ​  ​ the ​  ​ models, ​  ​ as ​  ​ are    stereotypes, ​  ​ such ​  ​ as ​  ​ “female” ​  ​ and ​  ​ “receptionist,” ​  ​ while ​  ​ “man” ​  ​ and ​  ​ typically ​  ​ masculine    names ​  ​ are ​  ​ associated ​  ​ with ​  ​ programming, ​  ​ engineering ​  ​ and ​  ​ other ​  ​ stem ​  ​ professions.   74  such ​  ​ biases ​  ​ have ​  ​ daily, ​  ​ real-world ​  ​ impacts. ​  ​ recent ​  ​ analysis ​  ​ of ​  ​ search ​  ​ results ​  ​ and    advertisements ​  ​ similarly ​  ​ reveals ​  ​ persistent ​  ​ gendered, ​  ​ racial ​  ​ and ​  ​ cultural ​  ​ biases.  75  privacy ​  ​ enhancing ​  ​ technologies ​ ​  ​ 2015, ​  ​ no. ​  ​ 1 ​  ​ (2015): ​  ​ 92-112.    69  anja ​  ​ lambrecht ​  ​ and ​  ​ catherine ​  ​ e. ​  ​ tucker, ​  ​ “algorithmic ​  ​ bias? ​  ​ an ​  ​ empirical ​  ​ study ​  ​ into ​  ​ apparent ​  ​ gender-based ​  ​ discrimination    in ​  ​ the ​  ​ display ​  ​ of ​  ​ stem ​  ​ career ​  ​ ads,” ​  ​ october ​  ​ 13, ​  ​ 2016. ​  ​ available ​  ​ at ​  ​ ssrn: ​  ​​ https://ssrn.com/abstract=2852260 ​ ​  ​ or    http://dx.doi.org/10.2139/ssrn.2852260 ​ .    70  zdenek ​  ​ smutny, ​  ​ \"social ​  ​ informatics ​  ​ as ​  ​ a ​  ​ concept: ​  ​ widening ​  ​ the ​  ​ discourse,\" ​  ​​ journal ​  ​ of ​  ​ information ​  ​ science ​ ​  ​ 42, ​  ​ no. ​  ​ 5 ​  ​ (2016):    681-710.    71  kenneth ​  ​ a. ​  ​ bamberger ​  ​ and ​  ​ deirdre ​  ​ mulligan, ​  ​ \"public ​  ​ values, ​  ​ private ​  ​ infrastructure ​  ​ and ​  ​ the ​  ​ internet ​  ​ of ​  ​ things: ​  ​ the ​  ​ case ​  ​ of    automobile,\" ​  ​​ journal ​  ​ of ​  ​ law ​  ​ & ​  ​ economic ​  ​ regulation ​ ​  ​ 9 ​  ​ (2016): ​  ​ 7-44; ​  ​ jon ​  ​ kleinberg, ​  ​ himabindu ​  ​ lakkaraju, ​  ​ jure ​  ​ leskovec, ​  ​ jens    ludwig ​  ​ and ​  ​ sendhil ​  ​ mullainathan, ​  ​ “human ​  ​ decisions ​  ​ and ​  ​ machine ​  ​ predictions,” ​  ​ no. ​  ​ w23180. ​  ​ national ​  ​ bureau ​  ​ of ​  ​ economic    research, ​  ​ 2017.    72  brent ​  ​ daniel ​  ​ mittelstadt, ​  ​ patrick ​  ​ allo, ​  ​ mariarosaria ​  ​ taddeo, ​  ​ sandra ​  ​ wachter ​  ​ and ​  ​ luciano ​  ​ floridi, ​  ​ \"the ​  ​ ethics ​  ​ of ​  ​ algorithms:    mapping ​  ​ the ​  ​ debate,\" ​  ​​ big ​  ​ data ​  ​ & ​  ​ society ​ ​  ​ 3, ​  ​ no. ​  ​ 2 ​  ​ (2016): ​  ​ 2053951716679679.    73  aylin ​  ​ caliskan, ​  ​ joanna ​  ​ j. ​  ​ bryson ​  ​ and ​  ​ arvind ​  ​ narayanan, ​  ​ \"semantics ​  ​ derived ​  ​ automatically ​  ​ from ​  ​ language ​  ​ corpora ​  ​ contain    human-like ​  ​ biases,\" ​  ​​ science ​ ​  ​ 356, ​  ​ no. ​  ​ 6334 ​  ​ (2017): ​  ​ 183-186; ​  ​ anthony ​  ​ g. ​  ​ greenwald, ​  ​ \"an ​  ​ ai ​  ​ stereotype ​  ​ catcher,\" ​  ​​ science ​ ​  ​ 356,    no. ​  ​ 6334 ​  ​ (2017): ​  ​ 133-134.    74  tolga ​  ​ bolukbasi, ​  ​ kai-wei ​  ​ chang, ​  ​ james ​  ​ zou, ​  ​ venkatesh ​  ​ saligrama ​  ​ and ​  ​ adam ​  ​ kalai, ​  ​ \"quantifying ​  ​ and ​  ​ reducing ​  ​ stereotypes ​  ​ in    word ​  ​ embeddings,\" ​  ​​ arxiv ​  ​ preprint ​  ​ arxiv:1606.06121 ​ ​  ​ (2016); ​  ​ tolga ​  ​ bolukbasi, ​  ​ kai-wei ​  ​ chang, ​  ​ james ​  ​ y. ​  ​ zou, ​  ​ venkatesh    saligrama ​  ​ and ​  ​ adam ​  ​ t. ​  ​ kalai, ​  ​ \"man ​  ​ is ​  ​ to ​  ​ computer ​  ​ programmer ​  ​ as ​  ​ woman ​  ​ is ​  ​ to ​  ​ homemaker? ​  ​ debiasing ​  ​ word ​  ​ embeddings,\"    in ​  ​​ advances ​  ​ in ​  ​ neural ​  ​ information ​  ​ processing ​  ​ systems ​ , ​  ​ pp. ​  ​ 4349-4357, ​  ​ 2016.    75  datta, ​  ​ tschantz, ​  ​ and ​  ​ datta, ​  ​ 2015; ​  ​ tarleton ​  ​ gillespie, ​  ​ “algorithmically ​  ​ recognizable: ​  ​ santorum’s ​  ​ google ​  ​ problem, ​  ​ and    google’s ​  ​ santorum ​  ​ problem,” ​  ​​ information, ​  ​ communication ​  ​ & ​  ​ society ​ ​  ​ 20, ​  ​ no. ​  ​ 1 ​  ​ (2017): ​  ​ 63-80; ​  ​ safiya ​  ​ umoja ​  ​ noble,    “algorithms ​  ​ of ​  ​ oppression: ​  ​ how ​  ​ search ​  ​ engines ​  ​ enforce ​  ​ racism,” ​  ​ (nyu ​  ​ press, ​  ​ forthcoming ​  ​ 2018).  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 19    new ​  ​ work ​  ​ has ​  ​ also ​  ​ highlighted ​  ​ the ​  ​ way ​  ​ in ​  ​ which ​  ​ ai ​  ​ poses ​  ​ risks ​  ​ of ​  ​ significant ​  ​ bias-driven    impacts ​  ​ in ​  ​ the ​  ​ educational ​  ​ context, ​  ​ where ​  ​ k-12 ​  ​ educators ​  ​ subject ​  ​ children ​  ​ to ​  ​ treatment,    discipline ​  ​ and ​  ​ tracking ​  ​ decisions ​  ​ based ​  ​ on ​  ​ ai-determined ​  ​ characterizations ​  ​ of ​  ​ their ​  ​ abilities    and ​  ​ behaviors. ​  ​ analysis ​  ​ of ​  ​ large ​  ​ data ​  ​ sets ​  ​ reflecting ​  ​ stem ​  ​ education ​  ​ in ​  ​ k-12 ​  ​ classrooms  76  reveals ​  ​ racial ​  ​ disparities ​  ​ in ​  ​ disciplinary ​  ​ actions ​  ​ and ​  ​ recommendations ​  ​ for ​  ​ advanced    coursework. ​  ​ these ​  ​ data, ​  ​ along ​  ​ with ​  ​ the ​  ​ biases ​  ​ they ​  ​ reflect, ​  ​ are ​  ​ very ​  ​ likely ​  ​ to ​  ​ be ​  ​ used ​  ​ to    train ​  ​ these ​  ​ educational ​  ​ ai ​  ​ systems, ​  ​ which ​  ​ would ​  ​ then ​  ​​ reproduce ​  ​ and ​  ​ further ​  ​ normalize    these ​  ​ biases.     in ​  ​ a ​  ​ study ​  ​ that ​  ​ examined ​  ​ the ​  ​ potential ​  ​ for ​  ​ bias, ​  ​ the ​  ​ human ​  ​ rights ​  ​ data ​  ​ analysis ​  ​ group    demonstrated ​  ​ how ​  ​ commonly ​  ​ used ​  ​ predictive ​  ​ policing ​  ​ system ​  ​ predpol, ​  ​ were ​  ​ it ​  ​ used ​  ​ in    oakland, ​  ​ ca, ​  ​ would ​  ​ reinforce ​  ​ racially-biased ​  ​ police ​  ​ practices ​  ​ by ​  ​ recommending ​  ​ increased    police ​  ​ deployment ​  ​ in ​  ​ neighborhoods ​  ​ of ​  ​ color. ​  ​ decades ​  ​ of ​  ​ policing ​  ​ research ​  ​ has ​  ​ shown  77  that ​  ​ foot ​  ​ patrols ​  ​ and ​  ​ community ​  ​ rapport ​  ​ decrease ​  ​ policing ​  ​ biases, ​  ​ while ​  ​ studies ​  ​ of ​  ​ “driving    while ​  ​ black” ​  ​ and ​  ​ “hot ​  ​ spots” ​  ​ illustrate ​  ​ biases ​  ​ in ​  ​ routine ​  ​ strategies. ​  ​ new ​  ​ technologies  78  appear ​  ​ to ​  ​ prevent ​  ​ the ​  ​ former ​  ​ and ​  ​ amplify ​  ​ the ​  ​ latter, ​  ​ reproducing ​  ​ the ​  ​ most ​  ​ extreme ​  ​ racial    stereotyping.   79  legal ​  ​ scholarship ​  ​ has ​  ​ also ​  ​ explored ​  ​ the ​  ​ applications ​  ​ of ​  ​ machine ​  ​ testimony ​  ​ at ​  ​ criminal ​  ​ trials,  ​  ​ among ​  ​ many ​  ​ possible ​  ​ instances ​  ​ identified ​  ​ in ​  ​ which ​  ​ these ​  ​ skewed ​  ​ systems ​  ​ and ​  ​ biased  80  data ​  ​ could ​  ​​  ​ negatively ​  ​ impact ​  ​ human ​  ​ lives ​  ​ due ​  ​ to ​  ​ reproducing ​  ​ stereotypes, ​  ​ with ​  ​ the ​  ​ added    challenge ​  ​ that ​  ​ the ​  ​ systems ​  ​ are ​  ​ poorly ​  ​ understood ​  ​ and ​  ​ proprietary.    when ​  ​ bias ​  ​ is ​  ​ embedded ​  ​ in ​  ​ ai ​  ​ health ​  ​ applications, ​  ​ it ​  ​ can ​  ​ have ​  ​ an ​  ​ incredibly ​  ​ high ​  ​ cost.    worryingly, ​  ​ data ​  ​ sets ​  ​ used ​  ​ to ​  ​ train ​  ​ health-related ​  ​ ai ​  ​ often ​  ​ rely ​  ​ on ​  ​ clinical ​  ​ trial ​  ​ data, ​  ​ which    are ​  ​ historically ​  ​ skewed ​  ​ toward ​  ​ white ​  ​ men, ​  ​ even ​  ​ when ​  ​ the ​  ​ health ​  ​ conditions ​  ​ studied    primarily ​  ​ affect ​  ​ people ​  ​ of ​  ​ color ​  ​ or ​  ​ women. ​  ​ even ​  ​ without ​  ​ ai ​  ​ amplifying ​  ​ such ​  ​ biases, ​  ​ african  81  americans ​  ​ with ​  ​ sickle ​  ​ cell ​  ​ anemia ​  ​ are ​  ​ overdiagnosed ​  ​ and ​  ​ unnecessarily ​  ​ treated ​  ​ for ​  ​ diabetes    based ​  ​ on ​  ​ insights ​  ​ from ​  ​ studies ​  ​ that ​  ​ excluded ​  ​ them. ​  ​ the ​  ​ prevalence ​  ​ of ​  ​ biases ​  ​ when  82  combined ​  ​ with ​  ​ opacity ​  ​ and ​  ​ inscrutability ​  ​ leads ​  ​ to ​  ​ a ​  ​ lack ​  ​ of ​  ​ trust ​  ​ in ​  ​ ai ​  ​ currently ​  ​ being    76  benjamin ​  ​ herold, ​  ​ “algorithmic ​  ​ bias ​  ​ as ​  ​ a ​  ​ rising ​  ​ concern ​  ​ for ​  ​ ed-tech ​  ​ field, ​  ​ rand ​  ​ researchers ​  ​ say,” ​  ​ education ​  ​ week, ​  ​ april ​  ​ 11,    2017, ​  ​​ http://blogs.edweek.org/edweek/digitaleducation/2017/04/algorithmic_bias_edtech_rand.html ​ .    77  kristian ​  ​ lum ​  ​ and ​  ​ william ​  ​ isaac, ​  ​ \"to ​  ​ predict ​  ​ and ​  ​ serve?,\" ​  ​​ significance ​ ​  ​ 13, ​  ​ no. ​  ​ 5 ​  ​ (2016): ​  ​ 14-19.    78  prashan ​  ​ ranasinghe, ​  ​ \"rethinking ​  ​ the ​  ​ place ​  ​ of ​  ​ crime ​  ​ in ​  ​ police ​  ​ patrol: ​  ​ a ​  ​ re-reading ​  ​ of ​  ​ classic ​  ​ police ​  ​ ethnographies,\" ​  ​​ british    journal ​  ​ of ​  ​ criminology ​ ​  ​ (2016): ​  ​ azw028; ​  ​ patricia ​  ​ warren, ​  ​ donald ​  ​ tomaskovic-devey, ​  ​ william ​  ​ smith, ​  ​ matthew ​  ​ zingraff ​  ​ and    marcinda ​  ​ mason, ​  ​ \"driving ​  ​ while ​  ​ black: ​  ​ bias ​  ​ processes ​  ​ and ​  ​ racial ​  ​ disparity ​  ​ in ​  ​ police ​  ​ stops,\" ​  ​​ criminology ​ ​  ​ 44, ​  ​ no. ​  ​ 3 ​  ​ (2006):    709-738; ​  ​ david ​  ​ weisburd, ​  ​ \"does ​  ​ hot ​  ​ spots ​  ​ policing ​  ​ inevitably ​  ​ lead ​  ​ to ​  ​ unfair ​  ​ and ​  ​ abusive ​  ​ police ​  ​ practices, ​  ​ or ​  ​ can ​  ​ we    maximize ​  ​ both ​  ​ fairness ​  ​ and ​  ​ effectiveness ​  ​ in ​  ​ the ​  ​ new ​  ​ proactive ​  ​ policing,\" ​  ​​ university ​  ​ of ​  ​ chicago ​  ​ legal ​  ​ forum ​ ​  ​ (2016):    661-689.    79  andrew ​  ​ guthrie ​  ​ ferguson, ​  ​​ the ​  ​ rise ​  ​ of ​  ​ big ​  ​ data ​  ​ policing: ​  ​ surveillance, ​  ​ race, ​  ​ and ​  ​ the ​  ​ future ​  ​ of ​  ​ law ​  ​ enforcement ​ , ​  ​ (nyu ​  ​ press,    forthcoming ​  ​ 2017).    80  andrea ​  ​ roth, ​  ​ “machine ​  ​ testimony,” ​  ​ yale ​  ​ law ​  ​ journal, ​  ​ forthcoming ​  ​ 2017.    81  anita ​  ​ kurt, ​  ​ lauren ​  ​ semler, ​  ​ jeanne ​  ​ l. ​  ​ jacoby, ​  ​ melanie ​  ​ b. ​  ​ johnson, ​  ​ beth ​  ​ a. ​  ​ careyva, ​  ​ brian ​  ​ stello, ​  ​ timothy ​  ​ friel, ​  ​ mark ​  ​ c.    knouse, ​  ​ hope ​  ​ kincaid ​  ​ and ​  ​ john ​  ​ c. ​  ​ smulian, ​  ​ \"racial ​  ​ differences ​  ​ among ​  ​ factors ​  ​ associated ​  ​ with ​  ​ participation ​  ​ in ​  ​ clinical    research ​  ​ trials,\" ​  ​​ journal ​  ​ of ​  ​ racial ​  ​ and ​  ​ ethnic ​  ​ health ​  ​ disparities ​ ​  ​ (2016): ​  ​ 1-10.    82  mary ​  ​ e ​  ​ lacy, ​  ​ gregory ​  ​ a. ​  ​ wellenius, ​  ​ anne ​  ​ e. ​  ​ sumner, ​  ​ adolfo ​  ​ correa, ​  ​ mercedes ​  ​ r. ​  ​ carnethon, ​  ​ robert ​  ​ i. ​  ​ liem, ​  ​ james ​  ​ g.    wilson, ​  ​ david ​  ​ b. ​  ​ saks, ​  ​ david ​  ​ r. ​  ​ jacobs ​  ​ jr., ​  ​ april ​  ​ carson, ​  ​ xi ​  ​ luo, ​  ​ annie ​  ​ gjelsvik, ​  ​ alexander ​  ​ p. ​  ​ reiner, ​  ​ rhaki ​  ​ naik, ​  ​ simin ​  ​ liu,    solomon ​  ​ k. ​  ​ musani, ​  ​ charles ​  ​ b. ​  ​ eaton ​  ​ and ​  ​ wen-chih ​  ​ wu, ​  ​ \"association ​  ​ of ​  ​ sickle ​  ​ cell ​  ​ trait ​  ​ with ​  ​ hemoglobin ​  ​ a1c ​  ​ in ​  ​ african    americans,\" ​  ​​ jama ​ ​  ​ 317, ​  ​ no. ​  ​ 5 ​  ​ (2017): ​  ​ 507-515.  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 20    developed ​  ​ for ​  ​ neuroscience ​  ​ and ​  ​ mental ​  ​ health ​  ​ applications. ​  ​ the ​  ​ prospect ​  ​ of ​  ​ misdiagnosis  83  or ​  ​ improper ​  ​ treatment ​  ​ leading ​  ​ to ​  ​ patient ​  ​ death ​  ​ motivates ​  ​ some ​  ​ to ​  ​ avoid ​  ​ ai ​  ​ systems    entirely ​  ​ in ​  ​ the ​  ​ health ​  ​ context.   84  emerging ​  ​ strategies ​  ​ to ​  ​ address ​  ​ bias    there ​  ​ is ​  ​ an ​  ​ urgent ​  ​ need ​  ​ to ​  ​ expand ​  ​ cultural, ​  ​ disciplinary ​  ​ and ​  ​ ethnic ​  ​ diversity ​  ​ within ​  ​ the ​  ​ ai    field ​  ​ in ​  ​ order ​  ​ to ​  ​ diminish ​  ​ groupthink, ​  ​ mitigate ​  ​ bias ​  ​ and ​  ​ broaden ​  ​ intellectual ​  ​ frames ​  ​ of    reference ​  ​ beyond ​  ​ the ​  ​ purely ​  ​ technical. ​  ​ while ​  ​ some ​  ​ have ​  ​ suggested ​  ​ that ​  ​ ai ​  ​ systems ​  ​ can ​  ​ be    used ​  ​ to ​  ​ address ​  ​ diversity ​  ​ problems ​  ​ at ​  ​ companies, ​  ​ if ​  ​ ai ​  ​ development ​  ​ is ​  ​ not ​  ​ inclusive, ​  ​ the  85  success ​  ​ of ​  ​ such ​  ​ a ​  ​ bootstrapped ​  ​ approach ​  ​ is ​  ​ doubtful. ​  ​ there ​  ​ have ​  ​ been ​  ​ positive    developments ​  ​ prompting ​  ​ inclusion ​  ​ within ​  ​ the ​  ​ ai ​  ​ community, ​  ​ such ​  ​ as ​  ​ fei-fei ​  ​ li’s ​  ​ sailors    summer ​  ​ camp, ​  ​ a ​  ​ program ​  ​ that ​  ​ helps ​  ​ high ​  ​ school ​  ​ girls ​  ​ acquire ​  ​ comfort ​  ​ and ​  ​ experience ​  ​ with    ai. ​  ​ similarly, ​  ​ the ​  ​ association ​  ​ of ​  ​ computing ​  ​ machinery ​  ​ (acm) ​  ​ increasingly ​  ​ recognizes ​  ​ the  86  need ​  ​ to ​  ​ address ​  ​ algorithmic ​  ​ bias ​  ​ and ​  ​ emphasize ​  ​ diversity. ​  ​ various ​  ​ conferences ​  ​ have ​  ​ also  87  sought ​  ​ to ​  ​ explore ​  ​ accountability ​  ​ and ​  ​ transparency ​  ​ issues ​  ​ surrounding ​  ​ ai ​  ​ and ​  ​ algorithmic    systems ​  ​ as ​  ​ a ​  ​ way ​  ​ to ​  ​ better ​  ​ understand ​  ​ and ​  ​ evaluate ​  ​ biases. ​  ​ among ​  ​ conferences, ​  ​ the  88  fairness, ​  ​ accountability, ​  ​ and ​  ​ transparency ​  ​ in ​  ​ machine ​  ​ learning ​  ​ (fat/ml ​  ​ and ​  ​ now ​  ​ fat* ​  ​ )    conferences ​  ​ are ​  ​ notable ​  ​ for ​  ​ a ​  ​ focus ​  ​ on ​  ​ technical ​  ​ research ​  ​ and ​  ​ experimentation ​  ​ dedicated    to ​  ​ making ​  ​ ai ​  ​ more ​  ​ inclusive, ​  ​ legible ​  ​ and ​  ​ representative.  89  while ​  ​ steps ​  ​ are ​  ​ being ​  ​ made ​  ​ to ​  ​ understand ​  ​ and ​  ​ combat ​  ​ bias ​  ​ in ​  ​ some ​  ​ sectors, ​  ​ bias ​  ​ can ​  ​ also    be ​  ​ profitable. ​  ​ insurance ​  ​ and ​  ​ financial ​  ​ lending ​  ​ have ​  ​ long ​  ​ discriminated ​  ​ for ​  ​ their ​  ​ financial    advantage, ​  ​ choosing ​  ​ to ​  ​ serve ​  ​ the ​  ​ least ​  ​ risky ​  ​ and, ​  ​ sometimes, ​  ​ leaving ​  ​ the ​  ​ most ​  ​ vulnerable    behind. ​  ​ ai ​  ​ systems ​  ​ are ​  ​ now ​  ​ being ​  ​ used ​  ​ to ​  ​ make ​  ​ credit ​  ​ and ​  ​ lending ​  ​ decisions. ​  ​ when  90  underwriting ​  ​ decisions ​  ​ are ​  ​ made ​  ​ by ​  ​ ai ​  ​ systems ​  ​ trained ​  ​ on ​  ​ data ​  ​ that ​  ​ reflects ​  ​ past ​  ​ biased    practices ​  ​ and ​  ​ calibrated ​  ​ to ​  ​ detect ​  ​ nuanced ​  ​ signals ​  ​ of ​  ​ “risk,” ​  ​ creditors ​  ​ will ​  ​ be ​  ​ able ​  ​ to ​  ​ make    more ​  ​ profitable ​  ​ loans ​  ​ while ​  ​ leaving ​  ​ those ​  ​ in ​  ​ precarious ​  ​ situations ​  ​ behind. ​  ​ due ​  ​ to    misaligned ​  ​ interests ​  ​ and ​  ​ the ​  ​ information ​  ​ asymmetry ​  ​ that ​  ​ ai ​  ​ exacerbates ​  ​ in ​  ​ these    industries, ​  ​ new ​  ​ incentives ​  ​ for ​  ​ fairness ​  ​ and ​  ​ new ​  ​ methods ​  ​ for ​  ​ validating ​  ​ fair ​  ​ practices ​  ​ need    83  andreas ​  ​ holzinger, ​  ​ \"interactive ​  ​ machine ​  ​ learning ​  ​ for ​  ​ health ​  ​ informatics: ​  ​ when ​  ​ do ​  ​ we ​  ​ need ​  ​ the ​  ​ human-in-the-loop?,\" ​  ​​ brain    informatics ​ ​  ​ 3, ​  ​ no. ​  ​ 2 ​  ​ (2016): ​  ​ 119-131.    84  rich ​  ​ caruana, ​  ​ \"intelligible ​  ​ machine ​  ​ learning ​  ​ for ​  ​ critical ​  ​ applications ​  ​ such ​  ​ as ​  ​ health ​  ​ care,\" ​  ​​ 2017 ​  ​ aaas ​  ​ annual ​  ​ meeting    (february ​  ​ 16-20, ​  ​ 2017) ​ , ​  ​ aaas, ​  ​ 2017.    85  ji-a ​  ​ min, ​  ​ “ten ​  ​ ways ​  ​ hr ​  ​ tech ​  ​ leaders ​  ​ can ​  ​ make ​  ​ the ​  ​ most ​  ​ of ​  ​ artificial ​  ​ intelligence,” ​  ​ personnel ​  ​ today, ​  ​ april ​  ​ 26, ​  ​ 2017,    http://www.personneltoday.com/hr/ten-ways-hr-tech-leaders-can-make-artificial-intelligence/ ​ .    86  marie ​  ​ e ​  ​ vachovsky, ​  ​ grace ​  ​ wu, ​  ​ sorathan ​  ​ chaturapruek, ​  ​ olga ​  ​ russakovsky, ​  ​ richard ​  ​ sommer ​  ​ and ​  ​ li ​  ​ fei-fei, ​  ​ \"toward ​  ​ more    gender ​  ​ diversity ​  ​ in ​  ​ cs ​  ​ through ​  ​ an ​  ​ artificial ​  ​ intelligence ​  ​ summer ​  ​ program ​  ​ for ​  ​ high ​  ​ school ​  ​ girls,\" ​  ​ (in ​  ​​ proceedings ​  ​ of ​  ​ the ​  ​ 47th    acm ​  ​ technical ​  ​ symposium ​  ​ on ​  ​ computing ​  ​ science ​  ​ education ​ , ​  ​ pp. ​  ​ 303-308), ​  ​ acm, ​  ​ 2016.    87  kieth ​  ​ kirkpatrick, ​  ​ \"battling ​  ​ algorithmic ​  ​ bias: ​  ​ how ​  ​ do ​  ​ we ​  ​ ensure ​  ​ algorithms ​  ​ treat ​  ​ us ​  ​ fairly?,\" ​  ​​ communications ​  ​ of ​  ​ the ​  ​ acm ​ ​  ​ 59,    no. ​  ​ 10 ​  ​ (2016): ​  ​ 16-17.    88  algorithms ​  ​ and ​  ​ explanations, ​  ​ information ​  ​ law ​  ​ insitute, ​  ​ new ​  ​ york ​  ​ university, ​  ​ april ​  ​ 27-28, ​  ​ 2017,    http://www.law.nyu.edu/centers/ili/events/algorithms-and-explanations ​ .    89  3rd ​ ​  ​​ workshop ​  ​ on ​  ​ fairness, ​  ​ accountability, ​  ​ and ​  ​ transparency ​  ​ in ​  ​ machine ​  ​ learning, ​  ​ new ​  ​ york, ​  ​ november ​  ​ 18, ​  ​ 2016,    http://www.fatml.org/ ​ .    90  jm ​  ​ schumacher, ​  ​ “linear ​  ​ versus ​  ​ nonlinear ​  ​ allocation ​  ​ rules ​  ​ in ​  ​ risk ​  ​ sharing ​  ​ under ​  ​ financial ​  ​ fairness,” ​  ​ (march ​  ​ 2, ​  ​ 2017),    http://dx.doi.org/10.2139/ssrn.2892760 ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 21    to ​  ​ be ​  ​ developed.  91  part ​  ​ of ​  ​ the ​  ​ fundamental ​  ​ difficulty ​  ​ in ​  ​ defining, ​  ​ understanding ​  ​ and ​  ​ measuring ​  ​ bias ​  ​ stems    from ​  ​ the ​  ​ contentious ​  ​ and ​  ​ conceptually ​  ​ difficult ​  ​ task ​  ​ of ​  ​ defining ​  ​ fairness. ​  ​ tradeoffs ​  ​ are    inherent ​  ​ in ​  ​ the ​  ​ adoption ​  ​ of ​  ​ particular ​  ​ fairness ​  ​ definitions, ​  ​ possibly ​  ​ perpetuating ​  ​ particular    biases ​  ​ in ​  ​ the ​  ​ service ​  ​ of ​  ​ addressing ​  ​ others. ​  ​ recent ​  ​ efforts ​  ​ have ​  ​ sought ​  ​ to ​  ​ implement  92  fairness ​  ​ by ​  ​ mathematically ​  ​ specifying ​  ​ social ​  ​ norms ​  ​ and ​  ​ values, ​  ​ then ​  ​ using ​  ​ those    specifications ​  ​ as ​  ​ constraints ​  ​ when ​  ​ training ​  ​ ai ​  ​ systems. ​  ​ while ​  ​ these ​  ​ are ​  ​ hopeful  93  developments, ​  ​ none ​  ​ of ​  ​ these ​  ​ methods ​  ​ cleanly ​  ​ solve ​  ​ the ​  ​ problem ​  ​ of ​  ​ bias. ​  ​ understanding ​  ​ ai    not ​  ​ as ​  ​ a ​  ​ purely ​  ​ technical ​  ​ implementation, ​  ​ but ​  ​ as ​  ​ a ​  ​ contextually-specific ​  ​ combination ​  ​ of    norms, ​  ​ technical ​  ​ systems ​  ​ and ​  ​ strategic ​  ​ interests ​  ​ is ​  ​ an ​  ​ important ​  ​ step ​  ​ toward ​  ​ addressing    bias ​  ​ in ​  ​ ai. ​  ​ there ​  ​ continues ​  ​ to ​  ​ be ​  ​ a ​  ​ deep ​  ​ need ​  ​ for ​  ​ interdisciplinary, ​  ​ socially ​  ​ aware ​  ​ work  94  that ​  ​ integrates ​  ​ the ​  ​ long ​  ​ history ​  ​ of ​  ​ bias ​  ​ research ​  ​ from ​  ​ the ​  ​ social ​  ​ sciences ​  ​ and ​  ​ humanities    into ​  ​ the ​  ​ field ​  ​ of ​  ​ ai ​  ​ research.    rights ​  ​ and ​  ​ liberties    in ​  ​ the ​  ​ period ​  ​ since ​  ​ the ​  ​​ ai ​  ​ now ​  ​ 2016 ​  ​ symposium ​ , ​  ​ the ​  ​ global ​  ​ political ​  ​ landscape ​  ​ has ​  ​ shifted    considerably. ​  ​ the ​  ​ election ​  ​ of ​  ​ donald ​  ​ trump ​  ​ is ​  ​ part ​  ​ of ​  ​ a ​  ​ larger ​  ​ wave ​  ​ of ​  ​ populist ​  ​ political    movements ​  ​ across ​  ​ the ​  ​ globe, ​  ​ and ​  ​ shares ​  ​ with ​  ​ these ​  ​ a ​  ​ number ​  ​ of ​  ​ hallmark ​  ​ traits. ​  ​ in    governing, ​  ​ populists ​  ​ seek ​  ​ to ​  ​ delegitimize ​  ​ political ​  ​ opposition—from ​  ​ opposition ​  ​ parties ​  ​ to    institutions ​  ​ like ​  ​ the ​  ​ media ​  ​ and ​  ​ the ​  ​ judiciary—and ​  ​ to ​  ​ crack ​  ​ down ​  ​ on ​  ​ perceived ​  ​ threats ​  ​ to    the ​  ​ imagined ​  ​ homogeneous ​  ​ people ​  ​ they ​  ​ claim ​  ​ to ​  ​ represent. ​  ​ while ​  ​ regional ​  ​ instantiations  95  vary, ​  ​ they ​  ​ share ​  ​ an ​  ​ opposition ​  ​ to ​  ​ existing ​  ​ political ​  ​ elites ​  ​ and ​  ​ a ​  ​ nationalist, ​  ​ anti-pluralist    approach ​  ​ that ​  ​ claims ​  ​ a ​  ​ moral ​  ​ imperative ​  ​ to ​  ​ represent ​  ​ a ​  ​ silent ​  ​ majority.     the ​  ​ election ​  ​ of ​  ​ emmanuel ​  ​ macron ​  ​ in ​  ​ france ​  ​ and ​  ​ the ​  ​ gains ​  ​ by ​  ​ labour ​  ​ in ​  ​ the ​  ​ uk ​  ​ may ​  ​ indicate    a ​  ​ coming ​  ​ backlash ​  ​ to ​  ​ the ​  ​ global ​  ​ populist ​  ​ wave, ​  ​ but ​  ​ given ​  ​ the ​  ​ strong ​  ​ showing ​  ​ from    germany’s ​  ​ far-right ​  ​ alternative ​  ​ für ​  ​ deutschland ​  ​ party ​  ​ in ​  ​ their ​  ​ 2017 ​  ​ elections, ​  ​ this ​  ​ is ​  ​ by ​  ​ no    means ​  ​ certain.     it ​  ​ remains ​  ​ necessary ​  ​ to ​  ​ ask ​  ​ how ​  ​ ai ​  ​ systems ​  ​ are ​  ​ likely ​  ​ to ​  ​ be ​  ​ deployed ​  ​ in ​  ​ governing, ​  ​ and ​  ​ how    91  hamid ​  ​ r. ​  ​ ekbia ​  ​ and ​  ​ bonnie ​  ​ a. ​  ​ nardi, ​  ​​ heteromation, ​  ​ and ​  ​ other ​  ​ stories ​  ​ of ​  ​ computing ​  ​ and ​  ​ capitalism ​ , ​  ​ (mit ​  ​ press, ​  ​ 2017);    sampath ​  ​ kannan, ​  ​ michael ​  ​ kearns, ​  ​ jamie ​  ​ morgenstern, ​  ​ mallesh ​  ​ pai, ​  ​ aaron ​  ​ roth, ​  ​ rakesh ​  ​ vohra ​  ​ and ​  ​ z. ​  ​ steven ​  ​ wu, ​  ​ \"fairness    incentives ​  ​ for ​  ​ myopic ​  ​ agents,\" ​  ​ arxiv ​  ​ preprint ​  ​ arxiv:1705.02321 ​  ​ (2017); ​  ​ julia ​  ​ lane, ​  ​ \"perspective: ​  ​ fix ​  ​ the ​  ​ incentives,\" ​  ​​ nature    537, ​  ​ no. ​  ​ 7618 ​  ​ (2016): ​  ​ s20-s20.    92  jon ​  ​ kleinberg, ​  ​ sendhil ​  ​ mullainathan ​  ​ and ​  ​ manish ​  ​ raghavan, ​  ​ \"inherent ​  ​ trade-offs ​  ​ in ​  ​ the ​  ​ fair ​  ​ determination ​  ​ of ​  ​ risk ​  ​ scores,\"    arxiv ​  ​ preprint ​  ​ arxiv:1609.05807 ​  ​ (2016).    93  yiling ​  ​ chen, ​  ​ arpita ​  ​ ghosh, ​  ​ michael ​  ​ kearns, ​  ​ tim ​  ​ roughgarden ​  ​ and ​  ​ jennifer ​  ​ wortman ​  ​ vaughan, ​  ​ \"mathematical ​  ​ foundations    for ​  ​ social ​  ​ computing,\" ​  ​​ communications ​  ​ of ​  ​ the ​  ​ acm ​ ​  ​ 59, ​  ​ no. ​  ​ 12 ​  ​ (2016): ​  ​ 102-108; ​  ​ shahin ​  ​ jabbari, ​  ​ matthew ​  ​ joseph, ​  ​ michael    kearns, ​  ​ jamie ​  ​ morgenstern ​  ​ and ​  ​ aaron ​  ​ roth, ​  ​ \"fair ​  ​ learning ​  ​ in ​  ​ markovian ​  ​ environments,\" ​  ​ arxiv ​  ​ preprint ​  ​ arxiv:1611.03071    (2016); ​  ​ matthew ​  ​ joseph, ​  ​ michael ​  ​ kearns, ​  ​ jamie ​  ​ morgenstern, ​  ​ seth ​  ​ neel ​  ​ and ​  ​ aaron ​  ​ roth, ​  ​ \"rawlsian ​  ​ fairness ​  ​ for ​  ​ machine    learning,\" ​  ​ arxiv ​  ​ preprint ​  ​ arxiv:1610.09559 ​  ​ (2016).    94  mike ​  ​ ananny, ​  ​ \"toward ​  ​ an ​  ​ ethics ​  ​ of ​  ​ algorithms: ​  ​ convening, ​  ​ observation, ​  ​ probability, ​  ​ and ​  ​ timeliness,\" ​  ​​ science, ​  ​ technology, ​  ​ &    human ​  ​ values ​ ​  ​ 41, ​  ​ no. ​  ​ 1 ​  ​ (2016): ​  ​ 93-117.    95  jan-werner ​  ​ müller, ​  ​​ what ​  ​ is ​  ​ populism? ​ ​  ​ (philadelphia: ​  ​ university ​  ​ of ​  ​ pennsylvania ​  ​ press, ​  ​ 2016).  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 22    they ​  ​ might ​  ​ be ​  ​ used ​  ​ within ​  ​ populist ​  ​ and ​  ​ authoritarian ​  ​ contexts. ​  ​ what ​  ​ effects ​  ​ will ​  ​ these  96  systems ​  ​ have ​  ​ on ​  ​ vulnerable ​  ​ individuals ​  ​ and ​  ​ minorities? ​  ​ how ​  ​ will ​  ​ ai ​  ​ systems ​  ​ be ​  ​ used ​  ​ by ​  ​ law    enforcement ​  ​ or ​  ​ national ​  ​ security ​  ​ agencies? ​  ​ how ​  ​ will ​  ​ ai’s ​  ​ use ​  ​ in ​  ​ the ​  ​ criminal ​  ​ justice ​  ​ system    affect ​  ​ our ​  ​ understanding ​  ​ of ​  ​ due ​  ​ process ​  ​ and ​  ​ the ​  ​ principle ​  ​ of ​  ​ equal ​  ​ justice ​  ​ under ​  ​ the ​  ​ law?    how ​  ​ might ​  ​ complex ​  ​ ai ​  ​ systems ​  ​ centralize ​  ​ authority ​  ​ and ​  ​ power? ​  ​ this ​  ​ section ​  ​ examines    these ​  ​ questions, ​  ​ describes ​  ​ applications ​  ​ of ​  ​ ai ​  ​ that ​  ​ pose ​  ​ challenges ​  ​ to ​  ​ rights ​  ​ and ​  ​ liberties,    and ​  ​ touches ​  ​ on ​  ​ the ​  ​ technical ​  ​ and ​  ​ normative ​  ​ frameworks ​  ​ we ​  ​ might ​  ​ construct ​  ​ to ​  ​ ensure ​  ​ ai    can ​  ​ be ​  ​ a ​  ​ force ​  ​ for ​  ​ good ​  ​ in ​  ​ the ​  ​ face ​  ​ of ​  ​ our ​  ​ contemporary ​  ​ political ​  ​ realities.     population ​  ​ registries ​  ​ and ​  ​ computing ​  ​ power    in ​  ​ political ​  ​ contexts ​  ​ where ​  ​ minorities ​  ​ or ​  ​ opposition ​  ​ points ​  ​ of ​  ​ view ​  ​ are ​  ​ seen ​  ​ as ​  ​ threats ​  ​ to ​  ​ an    imagined ​  ​ homogeneous ​  ​ “people,” ​  ​ information ​  ​ technology ​  ​ has ​  ​ been ​  ​ used ​  ​ to ​  ​ monitor ​  ​ and    control ​  ​ these ​  ​ segments ​  ​ of ​  ​ a ​  ​ population. ​  ​ such ​  ​ techno-political ​  ​ projects ​  ​ often ​  ​ build ​  ​ on ​  ​ older    colonial ​  ​ histories ​  ​ of ​  ​ censuses ​  ​ and ​  ​ population ​  ​ registries, ​  ​ as ​  ​ well ​  ​ as ​  ​ racialized ​  ​ modes ​  ​ of  97  surveillance ​  ​ and ​  ​ control ​  ​ rooted ​  ​ in ​  ​ the ​  ​ atlantic ​  ​ slave ​  ​ trade ​  ​ and ​  ​ the ​  ​ plantation ​  ​ system. ​  ​ in    dark ​  ​ matters ​ , ​  ​ simone ​  ​ browne ​  ​ connects ​  ​ this ​  ​ deep ​  ​ history ​  ​ of ​  ​ surveillance ​  ​ to ​  ​ contemporary    biometric ​  ​ techniques ​  ​ of ​  ​ governing ​  ​ black ​  ​ bodies.  98  the ​  ​ book ​  ​ of ​  ​ life ​  ​ registry ​  ​ project ​  ​ in ​  ​ apartheid ​  ​ south ​  ​ africa ​  ​ is ​  ​ a ​  ​ useful ​  ​ modern ​  ​ example. ​  ​ in    that ​  ​ project, ​  ​ which ​  ​ ran ​  ​ from ​  ​ 1967 ​  ​ to ​  ​ 1983, ​  ​ ibm ​  ​ assisted ​  ​ south ​  ​ africa ​  ​ in ​  ​ classifying ​  ​ its    population ​  ​ by ​  ​ racial ​  ​ descent. ​  ​ this ​  ​ system ​  ​ was ​  ​ used ​  ​ to ​  ​ move ​  ​ all ​  ​ so-called ​  ​ ‘non-white    citizens’ ​  ​ from ​  ​ their ​  ​ homes ​  ​ into ​  ​ segregated ​  ​ neighborhoods. ​  ​ the ​  ​ book ​  ​ of ​  ​ life ​  ​ was ​  ​ plagued  99  by ​  ​ technical ​  ​ and ​  ​ operational ​  ​ problems ​  ​ and ​  ​ eventually ​  ​ abandoned. ​  ​ however, ​  ​ as ​  ​ paul    edwards ​  ​ and ​  ​ gabrielle ​  ​ hecht ​  ​ note, ​  ​ “technopolitical ​  ​ projects ​  ​ do ​  ​ not ​  ​ need ​  ​ to ​  ​ fully ​  ​ achieve    their ​  ​ technical ​  ​ goals ​  ​ in ​  ​ order ​  ​ to ​  ​ ‘work’ ​  ​ politically … ​  ​ the ​  ​ registries ​  ​ ‘worked’ ​  ​ to ​  ​ establish    racialized ​  ​ personal ​  ​ identities ​  ​ as ​  ​ elements ​  ​ of ​  ​ governance.” ​  ​ as ​  ​ kate ​  ​ crawford ​  ​ has ​  ​ recently  100  argued, ​  ​ registries ​  ​ like ​  ​ the ​  ​ book ​  ​ of ​  ​ life ​  ​ were ​  ​ reinforcing ​  ​ a ​  ​ way ​  ​ of ​  ​ thinking ​  ​ that ​  ​ was ​  ​ itself    autocratic.   101  more ​  ​ recent ​  ​ computerized ​  ​ registries ​  ​ like ​  ​ the ​  ​ national ​  ​ security ​  ​ entry-exit ​  ​ registration    system ​  ​ (nseers) ​  ​ proliferated ​  ​ in ​  ​ the ​  ​ united ​  ​ states ​  ​ and ​  ​ among ​  ​ its ​  ​ allies ​  ​ following ​  ​ the ​  ​ attacks    of ​  ​ september ​  ​ 11, ​  ​ 2001. ​  ​ nseers ​  ​ centralized ​  ​ documentation ​  ​ for ​  ​ non-citizens ​  ​ in ​  ​ the ​  ​ united    states ​  ​ who ​  ​ hailed ​  ​ from ​  ​ a ​  ​ list ​  ​ of ​  ​ 25 ​  ​ predominantly ​  ​ muslim ​  ​ countries ​  ​ that ​  ​ the ​  ​ bush    administration ​  ​ deemed ​  ​ dangerous. ​  ​ as ​  ​ with ​  ​ the ​  ​ book ​  ​ of ​  ​ life, ​  ​ nseers’ ​  ​ effectiveness ​  ​ in ​  ​ its    96  see ​  ​ kate ​  ​ crawford, ​  ​ “dark ​  ​ days: ​  ​ ai ​  ​ and ​  ​ the ​  ​ rise ​  ​ of ​  ​ fascism,” ​  ​ sxsw ​  ​ featured ​  ​ talk, ​  ​ march ​  ​ 15 ​  ​ 2017,    https://www.youtube.com/watch?v=dlr4o1aejvi ​ .    97  for ​  ​ just ​  ​ one ​  ​ example ​  ​ in ​  ​ colonial ​  ​ india, ​  ​ v.: ​  ​ radhika ​  ​ singha, ​  ​ “settle, ​  ​ mobilize, ​  ​ verify: ​  ​ identification ​  ​ practices ​  ​ in ​  ​ colonial ​  ​ india,”    studies ​  ​ in ​  ​ history ​ ​  ​ 16, ​  ​ no. ​  ​ 2 ​  ​ (august ​  ​ 1, ​  ​ 2000): ​  ​ 151–98, ​  ​ doi:10.1177/025764300001600201.    98  simone ​  ​ browne, ​  ​​ dark ​  ​ matters: ​  ​ on ​  ​ the ​  ​ surveillance ​  ​ of ​  ​ blackness, ​ ​  ​ (durham: ​  ​ duke ​  ​ university ​  ​ press, ​  ​ 2015).    99  national ​  ​ action/research ​  ​ on ​  ​ the ​  ​ military-industrial ​  ​ complex ​  ​ and ​  ​ american ​  ​ friends ​  ​ service ​  ​ committee, ​  ​​ automating    apartheid: ​  ​ u.s. ​  ​ computer ​  ​ exports ​  ​ to ​  ​ south ​  ​ africa ​  ​ and ​  ​ the ​  ​ arms ​  ​ embargo ​ , ​  ​ (philadelphia: ​  ​ narmic/america ​  ​ friends ​  ​ service    committee, ​  ​ 1982), ​  ​ 19.    100  paul ​  ​ n. ​  ​ edwards ​  ​ and ​  ​ gabrielle ​  ​ hecht, ​  ​ \"history ​  ​ and ​  ​ the ​  ​ technopolitics ​  ​ of ​  ​ identity: ​  ​ the ​  ​ case ​  ​ of ​  ​ apartheid ​  ​ south ​  ​ africa,\"    journal ​  ​ of ​  ​ southern ​  ​ african ​  ​ studies ​ ​  ​ 36, ​  ​ no. ​  ​ 3 ​  ​ (2010): ​  ​ 619-639.    101 ​  ​​  ​​ kate ​  ​ crawford, ​  ​ “dark ​  ​ days: ​  ​ ai ​  ​ and ​  ​ the ​  ​ rise ​  ​ of ​  ​ fascism,” ​  ​ sxsw ​  ​ featured ​  ​ talk, ​  ​ march ​  ​ 15 ​  ​ 2017,    https://www.youtube.com/watch?v=dlr4o1aejvi ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 23    stated ​  ​ goal ​  ​ of ​  ​ stopping ​  ​ domestic ​  ​ terrorism ​  ​ was ​  ​ questionable, ​  ​ and ​  ​ it ​  ​ was ​  ​ dismantled ​  ​ in ​  ​ the    final ​  ​ days ​  ​ of ​  ​ the ​  ​ obama ​  ​ administration ​  ​ (although ​  ​ the ​  ​ data ​  ​ collected ​  ​ by ​  ​ the ​  ​ program ​  ​ still    exists). ​  ​ consistent ​  ​ with ​  ​ edwards’ ​  ​ and ​  ​ hecht’s ​  ​ analysis, ​  ​ nseers ​  ​ set ​  ​ into ​  ​ motion ​  ​ state  102  projects ​  ​ of ​  ​ muslim ​  ​ surveillance ​  ​ and ​  ​ deportation.   103  the ​  ​ history ​  ​ and ​  ​ political ​  ​ efficacy ​  ​ of ​  ​ registries ​  ​ exposes ​  ​ the ​  ​ urgent ​  ​ need ​  ​ for ​  ​ lines ​  ​ of ​  ​ research    that ​  ​ can ​  ​ examine ​  ​ the ​  ​ way ​  ​ citizen ​  ​ registries ​  ​ work ​  ​ currently, ​  ​ enhanced ​  ​ by ​  ​ data ​  ​ mining ​  ​ and ​  ​ ai    techniques, ​  ​ and ​  ​ how ​  ​ they ​  ​ may ​  ​ work ​  ​ in ​  ​ the ​  ​ future. ​  ​ contemporary ​  ​ ai ​  ​ systems ​  ​ intensify  104  these ​  ​ longer-standing ​  ​ practices ​  ​ of ​  ​ surveillance ​  ​ and ​  ​ control. ​  ​ such ​  ​ systems ​  ​ require ​  ​ the    collection ​  ​ of ​  ​ massive ​  ​ amounts ​  ​ of ​  ​ data, ​  ​ which ​  ​ is ​  ​ now ​  ​ possible ​  ​ at ​  ​ large ​  ​ scale ​  ​ via ​  ​ the ​  ​ internet    and ​  ​ connected ​  ​ devices. ​  ​ when ​  ​ these ​  ​ practices ​  ​ are ​  ​ carried ​  ​ out ​  ​ by ​  ​ private ​  ​ enterprise ​  ​ in    addition ​  ​ to ​  ​ states, ​  ​ as ​  ​ we ​  ​ will ​  ​ discuss ​  ​ in ​  ​ the ​  ​ next ​  ​ section, ​  ​ they ​  ​ introduce ​  ​ new ​  ​ forms ​  ​ of ​  ​ value    extraction ​  ​ and ​  ​ population ​  ​ control ​  ​ unregulated ​  ​ and ​  ​ often ​  ​ unacknowledged ​  ​ by ​  ​ current ​  ​ legal    frameworks.  105  ​  ​ corporate ​  ​ and ​  ​ government ​  ​ entanglements    it ​  ​ remains ​  ​ critically ​  ​ important ​  ​ to ​  ​ understand ​  ​ the ​  ​ history ​  ​ of ​  ​ ai ​  ​ and ​  ​ its ​  ​ shifting ​  ​ relationship ​  ​ to    the ​  ​ state. ​  ​ in ​  ​ the ​  ​ mid-twentieth ​  ​ century, ​  ​ advanced ​  ​ computing ​  ​ projects ​  ​ tended ​  ​ to ​  ​ be ​  ​ closely    associated ​  ​ with ​  ​ the ​  ​ state, ​  ​ and ​  ​ especially ​  ​ the ​  ​ military ​  ​ agencies ​  ​ who ​  ​ funded ​  ​ their    fundamental ​  ​ research ​  ​ and ​  ​ development. ​  ​ although ​  ​ ai ​  ​ emerged ​  ​ from ​  ​ this ​  ​ context, ​  ​ its  106  present ​  ​ is ​  ​ characterized ​  ​ by ​  ​ a ​  ​ more ​  ​ collaborative ​  ​ approach ​  ​ between ​  ​ state ​  ​ agencies ​  ​ and    private ​  ​ corporations ​  ​ engaged ​  ​ in ​  ​ ai ​  ​ research ​  ​ and ​  ​ development. ​  ​ as ​  ​ gary ​  ​ marchant ​  ​ and    wendell ​  ​ wallach ​  ​ argue, ​  ​ governance ​  ​ has ​  ​ expanded ​  ​ far ​  ​ beyond ​  ​ both ​  ​ governmental    institutions ​  ​ and ​  ​ legal ​  ​ codes ​  ​ to ​  ​ include ​  ​ a ​  ​ wide ​  ​ range ​  ​ of ​  ​ industry ​  ​ standards ​  ​ and ​  ​ practices ​  ​ that    will ​  ​ shape ​  ​ how ​  ​ ai ​  ​ systems ​  ​ are ​  ​ implemented.  107  palantir—co-founded ​  ​ by ​  ​ trump ​  ​ supporter ​  ​ and ​  ​ advisor ​  ​ peter ​  ​ thiel ​  ​ with ​  ​ seed ​  ​ money ​  ​ from    the ​  ​ cia’s ​  ​ venture ​  ​ capital ​  ​ fund ​  ​ in-q-tel—typifies ​  ​ this ​  ​ dynamic. ​  ​ gotham, ​  ​ palantir’s  108  national ​  ​ security ​  ​ and ​  ​ government ​  ​ software, ​  ​ allows ​  ​ analysts ​  ​ to ​  ​ easily ​  ​ combine, ​  ​ query ​  ​ and    visualize ​  ​ structured ​  ​ and ​  ​ unstructured ​  ​ data ​  ​ at ​  ​ large ​  ​ scales. ​  ​ ai ​  ​ can ​  ​ now ​  ​ be ​  ​ used ​  ​ in ​  ​ palantir  109  products ​  ​ for ​  ​ activities ​  ​ such ​  ​ as ​  ​ lead ​  ​ generation, ​  ​ including ​  ​ a ​  ​ bank’s ​  ​ ability ​  ​ to ​  ​ identify    102  j. ​  ​ david ​  ​ goodman ​  ​ and ​  ​ ron ​  ​ nixon, ​  ​ “obama ​  ​ to ​  ​ dismantle ​  ​ visitor ​  ​ registry ​  ​ before ​  ​ trump ​  ​ can ​  ​ revive ​  ​ it,” ​  ​​ the ​  ​ new ​  ​ york ​  ​ times ​ ,    december ​  ​ 22, ​  ​ 2016,    https://www.nytimes.com/2016/12/22/nyregion/obama-to-dismantle-visitor-registry-before-trump-can-revive-it.html ​ .     103  “raza ​  ​ v. ​  ​ city ​  ​ of ​  ​ new ​  ​ york ​  ​ - ​  ​ legal ​  ​ challenge ​  ​ to ​  ​ nypd ​  ​ muslim ​  ​ surveillance ​  ​ program,” ​  ​ american ​  ​ civil ​  ​ liberties ​  ​ union, ​  ​ march ​  ​ 6,    2017, ​  ​​ https://www.aclu.org/cases/raza-v-city-new-york-legal-challenge-nypd-muslim-surveillance-program ​ .     104  kate ​  ​ crawford, ​  ​ “letter ​  ​ to ​  ​ silicon ​  ​ valley,” ​  ​​ harper’s ​  ​ magazin ​ e, ​  ​ feb ​  ​ 9, ​  ​ 2017,    https://harpers.org/archive/2017/02/trump-a-resisters-guide/11/ ​ .     105 ​  ​​  ​​  ​ julie ​  ​ e. ​  ​ cohen, ​  ​ “the ​  ​ biopolitical ​  ​ public ​  ​ domain: ​  ​ the ​  ​ legal ​  ​ construction ​  ​ of ​  ​ the ​  ​ surveillance ​  ​ economy,” ​  ​ philosophy ​  ​ &    technology, ​  ​ march ​  ​ 28, ​  ​ 2017, ​  ​ 1–21, ​  ​ doi:10.1007/s13347-017-0258-2.    106  paul ​  ​ n. ​  ​ edwards, ​  ​ the ​  ​ closed ​  ​ world: ​  ​​ computers ​  ​ and ​  ​ the ​  ​ politics ​  ​ of ​  ​ discourse ​  ​ in ​  ​ cold ​  ​ war ​  ​ america ​ ​  ​ (cambridge, ​  ​ ma: ​  ​ the ​  ​ mit    press, ​  ​ 1996).    107 ​  ​​  ​​ gary ​  ​ e. ​  ​ marchant ​  ​ and ​  ​ wendell ​  ​ wallach, ​  ​ “coordinating ​  ​ technology ​  ​ governance ​  ​ | ​  ​ issues ​  ​ in ​  ​ science ​  ​ and ​  ​ technology,” ​  ​ issues    in ​  ​ science ​  ​ and ​  ​ technology ​  ​ xxxi, ​  ​ no. ​  ​ 4 ​  ​ (summer ​  ​ 2015), ​  ​ http://issues.org/31-4/coordinating-technology-governance/.    108  sam ​  ​ biddle, ​  ​ “how ​  ​ peter ​  ​ thiel’s ​  ​ palantir ​  ​ helped ​  ​ the ​  ​ nsa ​  ​ spy ​  ​ on ​  ​ the ​  ​ whole ​  ​ world,” ​  ​​ the ​  ​ intercept ​ , ​  ​ february ​  ​ 22, ​  ​ 2017,    https://theintercept.com/2017/02/22/how-peter-thiels-palantir-helped-the-nsa-spy-on-the-whole-world/ ​ .    109  ibid.  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 24    anomalous ​  ​ credit ​  ​ card ​  ​ activity ​  ​ for ​  ​ fraud ​  ​ protection. ​  ​ more ​  ​ advanced ​  ​ capabilities ​  ​ are    available ​  ​ to ​  ​ national ​  ​ security ​  ​ clients ​  ​ as ​  ​ well. ​  ​ how ​  ​ rights ​  ​ and ​  ​ liberties ​  ​ need ​  ​ to ​  ​ be ​  ​ understood    and ​  ​ reconfigured ​  ​ in ​  ​ the ​  ​ face ​  ​ of ​  ​ opaque ​  ​ public-private ​  ​ ai ​  ​ systems ​  ​ is ​  ​ still ​  ​ an ​  ​ open ​  ​ question.    immigration ​  ​ and ​  ​ law ​  ​ enforcement ​  ​ are ​  ​ critical ​  ​ within ​  ​ this ​  ​ debate. ​  ​ in ​  ​ the ​  ​ united ​  ​ states,    immigration ​  ​ and ​  ​ customs ​  ​ enforcement ​  ​ (ice) ​  ​ is ​  ​ expanding ​  ​ its ​  ​ technological ​  ​ reach ​  ​ through    tools ​  ​ like ​  ​ investigative ​  ​ case ​  ​ management ​  ​ (icm), ​  ​ a ​  ​ platform ​  ​ that ​  ​ allows ​  ​ agents ​  ​ to ​  ​ access ​  ​ a    wide ​  ​ variety ​  ​ of ​  ​ previously ​  ​ separate ​  ​ databases, ​  ​ including ​  ​ information ​  ​ on ​  ​ a ​  ​ suspect’s    “schooling, ​  ​ family ​  ​ relationships, ​  ​ employment ​  ​ information, ​  ​ phone ​  ​ records, ​  ​ immigration    history, ​  ​ foreign ​  ​ exchange ​  ​ program ​  ​ status, ​  ​ personal ​  ​ connections, ​  ​ biometric ​  ​ traits, ​  ​ criminal    records ​  ​ and ​  ​ home ​  ​ and ​  ​ work ​  ​ addresses.” ​  ​ this ​  ​ is ​  ​ another ​  ​ palantir ​  ​ system, ​  ​ first ​  ​ procured ​  ​ by  110  the ​  ​ obama ​  ​ administration ​  ​ in ​  ​ 2014 ​  ​ and ​  ​ scheduled ​  ​ to ​  ​ become ​  ​ operational ​  ​ late ​  ​ in ​  ​ 2017.     other ​  ​ law ​  ​ enforcement ​  ​ agencies ​  ​ are ​ ​  ​​ currently ​  ​ integrating ​  ​ ai ​  ​ and ​  ​ related ​  ​ algorithmic    decision-support ​  ​ systems ​  ​ from ​  ​ the ​  ​ private ​  ​ sector ​  ​ into ​  ​ their ​  ​ existing ​  ​ arsenals. ​  ​ axon    (formerly ​  ​ taser ​  ​ international) ​  ​ is ​  ​ a ​  ​ publicly ​  ​ traded ​  ​ maker ​  ​ of ​  ​ law ​  ​ enforcement ​  ​ products,    including ​  ​ their ​  ​ famous ​  ​ electroshock ​  ​ weapon. ​  ​ the ​  ​ company ​  ​ has ​  ​ now ​  ​ shifted ​  ​ toward ​  ​ body    camera ​  ​ technologies, ​  ​ recently ​  ​ offering ​  ​ them ​  ​ for ​  ​ free ​  ​ to ​  ​ any ​  ​ police ​  ​ department ​  ​ in ​  ​ the ​  ​ u.s.  111  in ​  ​ 2017, ​  ​ axon ​  ​ started ​  ​ an ​  ​ ai ​  ​ division ​  ​ following ​  ​ their ​  ​ acquisition ​  ​ of ​  ​ two ​  ​ machine ​  ​ vision    companies. ​  ​ among ​  ​ their ​  ​ goals ​  ​ is ​  ​ to ​  ​ more ​  ​ efficiently ​  ​ analyze ​  ​ the ​  ​ over ​  ​ 5.2 ​  ​ petabytes ​  ​ of ​  ​ data    that ​  ​ they ​  ​ have ​  ​ already ​  ​ acquired ​  ​ from ​  ​ their ​  ​ existing ​  ​ camera ​  ​ systems. ​  ​ video ​  ​ expands ​  ​ axon’s    existing ​  ​ digital ​  ​ evidence ​  ​ management ​  ​ system, ​  ​ signaling ​  ​ a ​  ​ larger ​  ​ shift ​  ​ beyond ​  ​ machine    learning ​  ​ and ​  ​ natural ​  ​ language ​  ​ processing ​  ​ of ​  ​ textual ​  ​ sources. ​  ​ axon ​  ​ ceo ​  ​ rick ​  ​ smith ​  ​ has  112  argued ​  ​ that ​  ​ the ​  ​ vast ​  ​ scale ​  ​ of ​  ​ existing ​  ​ law ​  ​ enforcement ​  ​ data ​  ​ could ​  ​ help ​  ​ drive ​  ​ research ​  ​ in    machine ​  ​ vision ​  ​ as ​  ​ a ​  ​ whole: ​  ​ “we’ve ​  ​ got ​  ​ all ​  ​ of ​  ​ this ​  ​ law ​  ​ enforcement ​  ​ information ​  ​ with ​  ​ these    videos, ​  ​ which ​  ​ is ​  ​ one ​  ​ of ​  ​ the ​  ​ richest ​  ​ treasure ​  ​ troves ​  ​ you ​  ​ could ​  ​ imagine ​  ​ for ​  ​ machine ​  ​ learning.”  ​  ​ there ​  ​ are ​  ​ real ​  ​ concerns ​  ​ about ​  ​ the ​  ​ forms ​  ​ of ​  ​ bias ​  ​ embedded ​  ​ in ​  ​ these ​  ​ data ​  ​ sets, ​  ​ and ​  ​ how  113  they ​  ​ would ​  ​ subsequently ​  ​ function ​  ​ as ​  ​ training ​  ​ data ​  ​ for ​  ​ an ​  ​ ai ​  ​ system.    there ​  ​ are ​  ​ some ​  ​ who ​  ​ argue ​  ​ in ​  ​ favor ​  ​ of ​  ​ body ​  ​ camera ​  ​ and ​  ​ machine ​  ​ vision ​  ​ systems ​  ​ for    supporting ​  ​ civil ​  ​ liberties, ​  ​ including ​  ​ enhanced ​  ​ law ​  ​ enforcement ​  ​ transparency ​  ​ and    accountability. ​  ​ axon ​  ​ promises ​  ​ that ​  ​ its ​  ​ ai ​  ​ techniques ​  ​ will ​  ​ reduce ​  ​ the ​  ​ time ​  ​ officers  114  currently ​  ​ spend ​  ​ on ​  ​ report-writing ​  ​ and ​  ​ data ​  ​ entry. ​  ​ however, ​  ​ axon’s ​  ​ new ​  ​ focus ​  ​ on  115  110  spencer ​  ​ woodma, ​  ​ “palantir ​  ​ provides ​  ​ the ​  ​ engine ​  ​ for ​  ​ donald ​  ​ trump’s ​  ​ deportation ​  ​ machine,” ​  ​​ the ​  ​ intercept ​ , ​  ​ march ​  ​ 2, ​  ​ 2017,    https://theintercept.com/2017/03/02/palantir-provides-the-engine-for-donald-trumps-deportation-machine/ ​ .     111  laurel ​  ​ wamsley, ​  ​ “taser ​  ​ changes ​  ​ its ​  ​ name ​  ​ to ​  ​ axon ​  ​ and ​  ​ offers ​  ​ free ​  ​ body ​  ​ cameras ​  ​ for ​  ​ police,” ​  ​​ npr ​ , ​  ​ arpil ​  ​ 7, ​  ​ 2017,    http://www.npr.org/sections/thetwo-way/2017/04/07/522878573/we-re-more-than-stun-guns-says-taser-as-it-changes-co  mpany-name ​ .    112  “taser ​  ​ makes ​  ​ two ​  ​ acquisitions ​  ​ to ​  ​ create ​  ​ ‘axon ​  ​ ai,’” ​  ​​ police ​  ​ magazine ​ , ​  ​ february ​  ​ 9, ​  ​ 2017,    http://www.policemag.com/channel/technology/news/2017/02/09/taser-makes-two-acquisitions-to-create-axon-ai.aspx ​ .     113  doug ​  ​ wyllie, ​  ​ “what ​  ​ taser’s ​  ​ acquisition ​  ​ of ​  ​ 2 ​  ​ ai ​  ​ companies ​  ​ means ​  ​ for ​  ​ the ​  ​ future ​  ​ of ​  ​ policing,” ​  ​​ policeone ​ , ​  ​ february ​  ​ 10, ​  ​ 2017,    https://www.policeone.com/police-products/less-lethal/taser/articles/289203006-what-tasers-acquisition-of-2-ai-comp  anies-means-for-the-future-of-policing/ ​ .     114  jay ​  ​ stanley, ​  ​ “police ​  ​ body-mounted ​  ​ cameras: ​  ​ with ​  ​ right ​  ​ policies ​  ​ in ​  ​ place, ​  ​ a ​  ​ win ​  ​ for ​  ​ all,” ​  ​ (aclu, ​  ​ 2013),    http://www.urbanaillinois.us/sites/default/files/attachments/police-body-mounted-cameras-stanley.pdf ​ .    115  alex ​  ​ pasternack, ​  ​ “police ​  ​ body ​  ​ cameras ​  ​ will ​  ​ do ​  ​ more ​  ​ than ​  ​ just ​  ​ record ​  ​ you,” ​  ​​ fast ​  ​ company ​ , ​  ​​ march ​  ​ 3, ​  ​ 2017,    https://www.fastcompany.com/3061935/police-body-cameras-livestreaming-face-recognition-and-ai ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 25    predictive ​  ​ methods ​  ​ of ​  ​ policing—inspired ​  ​ by ​  ​ wal-mart’s ​  ​ and ​  ​ google’s ​  ​ embrace ​  ​ of ​  ​ deep    learning ​  ​ to ​  ​ increase ​  ​ sales—raises ​  ​ new ​  ​ civil ​  ​ liberties ​  ​ concerns. ​  ​ instead ​  ​ of ​  ​ purchasing    patterns, ​  ​ these ​  ​ systems ​  ​ will ​  ​ be ​  ​ looking ​  ​ for ​  ​ much ​  ​ more ​  ​ vague, ​  ​ context-dependent ​  ​ targets,    like ​  ​ “suspicious ​  ​ activity.” ​  ​ behind ​  ​ appearances ​  ​ of ​  ​ technical ​  ​ neutrality, ​  ​ these ​  ​ systems ​  ​ rely ​  ​ on    deeply ​  ​ subjective ​  ​ assumptions ​  ​ about ​  ​ what ​  ​ constitutes ​  ​ suspicious ​  ​ behavior ​  ​ or ​  ​ who ​  ​ counts    as ​  ​ a ​  ​ suspicious ​  ​ person.  116  unsurprisingly, ​  ​ machine ​  ​ vision ​  ​ techniques ​  ​ may ​  ​ reproduce ​  ​ and ​  ​ present ​  ​ as ​  ​ objective ​  ​ existing    forms ​  ​ of ​  ​ racial ​  ​ bias. ​  ​ researchers ​  ​ affiliated ​  ​ with ​  ​ google’s ​  ​ machine ​  ​ intelligence ​  ​ group ​  ​ and  117  columbia ​  ​ university ​  ​ make ​  ​ a ​  ​ compelling ​  ​ comparison ​  ​ between ​  ​ machine ​  ​ learning ​  ​ systems    designed ​  ​ to ​  ​ predict ​  ​ criminality ​  ​ from ​  ​ facial ​  ​ photos ​  ​ and ​  ​ discredited ​  ​ theories ​  ​ of    physiognomy—both ​  ​ of ​  ​ which ​  ​ problematically ​  ​ claim ​  ​ to ​  ​ be ​  ​ able ​  ​ to ​  ​ predict ​  ​ character ​  ​ or    behavioral ​  ​ traits ​  ​ simply ​  ​ by ​  ​ examining ​  ​ physical ​  ​ features. ​  ​ more ​  ​ generally, ​  ​ cathy ​  ​ o’neil  118  identifies ​  ​ the ​  ​ potential ​  ​ for ​  ​ advanced ​  ​ ai ​  ​ systems ​  ​ in ​  ​ law ​  ​ enforcement ​  ​ to ​  ​ create ​  ​ a ​  ​ “pernicious    feedback ​  ​ loop”—if ​  ​ these ​  ​ systems ​  ​ are ​  ​ built ​  ​ on ​  ​ top ​  ​ of ​  ​ racially-biased ​  ​ policing ​  ​ practices, ​  ​ then    their ​  ​ training ​  ​ data ​  ​ will ​  ​ reflect ​  ​ these ​  ​ existing ​  ​ biases, ​  ​ and ​  ​ integrate ​  ​ such ​  ​ bias ​  ​ into ​  ​ the ​  ​ logic ​  ​ of    decision ​  ​ making ​  ​ and ​  ​ prediction.   119  ethical ​  ​ questions ​  ​ of ​  ​ bias ​  ​ and ​  ​ accountability ​  ​ will ​  ​ become ​  ​ even ​  ​ more ​  ​ urgent ​  ​ in ​  ​ the ​  ​ context ​  ​ of    rights ​  ​ and ​  ​ liberties ​  ​ as ​  ​ ai ​  ​ systems ​  ​ capable ​  ​ of ​  ​ violent ​  ​ force ​  ​ against ​  ​ humans ​  ​ are ​  ​ developed    and ​  ​ deployed ​  ​ in ​  ​ law ​  ​ enforcement ​  ​ and ​  ​ military ​  ​ contexts. ​  ​ robotic ​  ​ police ​  ​ officers, ​  ​ for    example, ​  ​ recently ​  ​ debuted ​  ​ in ​  ​ dubai. ​  ​ if ​  ​ these ​  ​ were ​  ​ to ​  ​ carry ​  ​ weapons, ​  ​ new ​  ​ questions  120  would ​  ​ arise ​  ​ about ​  ​ how ​  ​ to ​  ​ determine ​  ​ when ​  ​ the ​  ​ use ​  ​ of ​  ​ force ​  ​ is ​  ​ appropriate. ​  ​ drawing ​  ​ on    analysis ​  ​ of ​  ​ the ​  ​ black ​  ​ lives ​  ​ matter ​  ​ movement, ​  ​ peter ​  ​ asaro ​  ​ has ​  ​ pointed ​  ​ to ​  ​ difficult ​  ​ ethical    issues ​  ​ involving ​  ​ how ​  ​ lethal ​  ​ autonomous ​  ​ weapons ​  ​ systems ​  ​ (laws) ​  ​ will ​  ​ detect ​  ​ threats ​  ​ or    gestures ​  ​ of ​  ​ cooperation, ​  ​ especially ​  ​ involving ​  ​ vulnerable ​  ​ populations. ​  ​ he ​  ​ concludes ​  ​ that ​  ​ ai    and ​  ​ robotics ​  ​ researchers ​  ​ should ​  ​ adopt ​  ​ ethical ​  ​ and ​  ​ legal ​  ​ standards ​  ​ that ​  ​ maintain ​  ​ human    control ​  ​ and ​  ​ accountability ​  ​ over ​  ​ these ​  ​ systems.   121  similar ​  ​ questions ​  ​ apply ​  ​ in ​  ​ the ​  ​ military ​  ​ use ​  ​ of ​  ​ laws. ​  ​ heather ​  ​ roff ​  ​ argues ​  ​ that ​  ​ fully    autonomous ​  ​ systems ​  ​ would ​  ​ violate ​  ​ current ​  ​ legal ​  ​ definitions ​  ​ of ​  ​ war ​  ​ that ​  ​ require ​  ​ human    judgment ​  ​ in ​  ​ the ​  ​ proportionate ​  ​ use ​  ​ of ​  ​ force, ​  ​ and ​  ​ guard ​  ​ against ​  ​ targeting ​  ​ of ​  ​ civilians.    furthermore, ​  ​ she ​  ​ argues ​  ​ that ​  ​ ai ​  ​ learning ​  ​ systems ​  ​ may ​  ​ make ​  ​ it ​  ​ difficult ​  ​ for ​  ​ commanders ​  ​ to    even ​  ​ know ​  ​ how ​  ​ their ​  ​ weapons ​  ​ will ​  ​ respond ​  ​ in ​  ​ battle ​  ​ situations. ​  ​ given ​  ​ these ​  ​ legal, ​  ​ ethical  122  116  ava ​  ​ kofman, ​  ​ “taser ​  ​ will ​  ​ use ​  ​ police ​  ​ body ​  ​ camera ​  ​ videos ​  ​ ‘to ​  ​ anticipate ​  ​ criminal ​  ​ activity,’” ​  ​​ the ​  ​ intercept ​ , ​  ​ april ​  ​ 30, ​  ​ 2017,    https://theintercept.com/2017/04/30/taser-will-use-police-body-camera-videos-to-anticipate-criminal-activity/ ​ .     117  clare ​  ​ garvie ​  ​ and ​  ​ jonathan ​  ​ frankle, ​  ​ “facial-recognition ​  ​ software ​  ​ might ​  ​ have ​  ​ a ​  ​ racial ​  ​ bias ​  ​ problem,” ​  ​​ the ​  ​ atlantic ​ , ​  ​ april ​  ​ 7,    2016,    https://www.theatlantic.com/technology/archive/2016/04/the-underlying-bias-of-facial-recognition-systems/476991/ ​ .    118  blaise ​  ​ agüera ​  ​ y ​  ​ arcas, ​  ​ margaret ​  ​ mitchell ​  ​ and ​  ​ alexander ​  ​ todorov, ​  ​ “physiognomy’s ​  ​ new ​  ​ clothes,” ​  ​​ medium ​ , ​  ​ may ​  ​ 7, ​  ​ 2017,    https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a ​ .     119  kofman, ​  ​ “taser ​  ​ will ​  ​ use ​  ​ police ​  ​ body ​  ​ camera ​  ​ videos ​  ​ ‘to ​  ​ anticipate ​  ​ criminal ​  ​ activity.’”    120  rory ​  ​ cellan-jones, ​  ​ “robot ​  ​ police ​  ​ officer ​  ​ goes ​  ​ on ​  ​ duty ​  ​ in ​  ​ dubai,” ​  ​​ bbc ​  ​ news ​ , ​  ​ may ​  ​ 24, ​  ​ 2017, ​  ​ sec. ​  ​ technology,    http://www.bbc.com/news/technology-40026940 ​ .     121  peter ​  ​ asaro, ​  ​ “‘hands ​  ​ up, ​  ​ don’t ​  ​ shoot!’ ​  ​ hri ​  ​ and ​  ​ the ​  ​ automation ​  ​ of ​  ​ police ​  ​ use ​  ​ of ​  ​ force,” ​  ​​ journal ​  ​ of ​  ​ human-robot ​  ​ interaction    5, ​  ​ no. ​  ​ 3 ​  ​ (december ​  ​ 14, ​  ​ 2016): ​  ​ 55–69.    122  heather ​  ​ m. ​  ​ roff, ​  ​ “meaningful ​  ​ human ​  ​ control ​  ​ or ​  ​ appropriate ​  ​ human ​  ​ judgment? ​  ​ the ​  ​ necessary ​  ​ limits ​  ​ on ​  ​ autonomous  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 26    and ​  ​ design ​  ​ concerns, ​  ​ both ​  ​ researchers ​  ​ call ​  ​ for ​  ​ strict ​  ​ limitations ​  ​ on ​  ​ the ​  ​ use ​  ​ of ​  ​ ai ​  ​ in ​  ​ weapons    systems.     while ​  ​ predictive ​  ​ policing ​  ​ and ​  ​ the ​  ​ use ​  ​ of ​  ​ force ​  ​ have ​  ​ always ​  ​ been ​  ​ important ​  ​ issues, ​  ​ they ​  ​ take    on ​  ​ new ​  ​ salience ​  ​ in ​  ​ populist ​  ​ or ​  ​ authoritarian ​  ​ contexts. ​  ​ as ​  ​ ai ​  ​ systems ​  ​ promise ​  ​ new ​  ​ forms ​  ​ of    technical ​  ​ efficiency ​  ​ in ​  ​ the ​  ​ service ​  ​ of ​  ​ safety, ​  ​ we ​  ​ may ​  ​ need ​  ​ to ​  ​ confront ​  ​ a ​  ​ fundamental    tension ​  ​ between ​  ​ technological ​  ​ efficiency ​  ​ and ​  ​ a ​  ​ commitment ​  ​ to ​  ​ ideals ​  ​ of ​  ​ justice.     ai ​  ​ and ​  ​ the ​  ​ legal ​  ​ system    the ​  ​ legal ​  ​ system ​  ​ is ​  ​ the ​  ​ institution ​  ​ tasked ​  ​ with ​  ​ defending ​  ​ civil ​  ​ rights ​  ​ and ​  ​ liberties. ​  ​ thus,    there ​  ​ are ​  ​ two ​  ​ separate ​  ​ questions ​  ​ to ​  ​ consider ​  ​ regarding ​  ​ ai ​  ​ and ​  ​ the ​  ​ legal ​  ​ system: ​  ​ 1) ​  ​ can ​  ​ the    legal ​  ​ system ​  ​ serve ​  ​ the ​  ​ rights-protection ​  ​ functions ​  ​ it ​  ​ is ​  ​ expected ​  ​ to ​  ​ when ​  ​ an ​  ​ ai ​  ​ system    produces ​  ​ an ​  ​ unfair ​  ​ result? ​  ​ and, ​  ​ 2) ​  ​ how ​  ​ and ​  ​ where ​  ​ (if ​  ​ at ​  ​ all) ​  ​ should ​  ​ the ​  ​ legal ​  ​ system    incorporate ​  ​ ai?    scholars ​  ​ like ​  ​ kate ​  ​ crawford ​  ​ and ​  ​ jason ​  ​ schultz ​  ​ have ​  ​ identified ​  ​ a ​  ​ series ​  ​ of ​  ​ conflicts ​  ​ between    ai ​  ​ techniques ​  ​ and ​  ​ constitutional ​  ​ due ​  ​ process ​  ​ requirements, ​  ​ such ​  ​ as ​  ​ how ​  ​ ai ​  ​ techniques  123  affect ​  ​ procedural ​  ​ considerations ​  ​ and ​  ​ equal ​  ​ justice ​  ​ under ​  ​ the ​  ​ law. ​  ​ the ​  ​ proliferation ​  ​ of    predictive ​  ​ systems ​  ​ demands ​  ​ new ​  ​ regulatory ​  ​ techniques ​  ​ to ​  ​ protect ​  ​ legal ​  ​ rights. ​  ​ danielle    citron ​  ​ and ​  ​ frank ​  ​ pasquale ​  ​ argue ​  ​ that ​  ​ safeguards ​  ​ to ​  ​ rights ​  ​ should ​  ​ be ​  ​ introduced ​  ​ at ​  ​ all ​  ​ stages    of ​  ​ the ​  ​ implementation ​  ​ of ​  ​ an ​  ​ ai ​  ​ system, ​  ​ from ​  ​ safeguarding ​  ​ privacy ​  ​ rights ​  ​ in ​  ​ data ​  ​ collection    to ​  ​ public ​  ​ audits ​  ​ of ​  ​ scoring ​  ​ systems ​  ​ that ​  ​ critically ​  ​ affect ​  ​ the ​  ​ public ​  ​ in ​  ​ areas ​  ​ like ​  ​ employment    and ​  ​ healthcare.   124  in ​  ​ a ​  ​ similar ​  ​ vein, ​  ​ andrew ​  ​ selbst ​  ​ has ​  ​ argued ​  ​ that ​  ​ an ​  ​ impact ​  ​ assessment ​  ​ requirement ​  ​ can    force ​  ​ those ​  ​ building ​  ​ and ​  ​ buying ​  ​ ai ​  ​ systems ​  ​ to ​  ​ make ​  ​ explicit ​  ​ the ​  ​ normative ​  ​ choices ​  ​ they ​  ​ are    making ​  ​ before ​  ​ implementing ​  ​ them. ​  ​ and ​  ​ as ​  ​ lilian ​  ​ edwards ​  ​ and ​  ​ michael ​  ​ veale ​  ​ have  125 126  pointed ​  ​ out, ​  ​ the ​  ​ new ​  ​ eu ​  ​ general ​  ​ data ​  ​ protection ​  ​ regulation ​  ​ (gdpr) ​  ​ includes ​  ​ a    requirement ​  ​ for ​  ​ data ​  ​ protection ​  ​ impact ​  ​ assessments, ​  ​ the ​  ​ import ​  ​ of ​  ​ which ​  ​ is ​  ​ unclear ​  ​ as ​  ​ yet.    there ​  ​ is ​  ​ also ​  ​ a ​  ​ rapidly ​  ​ emerging ​  ​ scholarly ​  ​ debate ​  ​ about ​  ​ the ​  ​ value ​  ​ of ​  ​ requiring ​  ​ an    explanation ​  ​ or ​  ​ interpretation ​  ​ of ​  ​ ai ​  ​ and ​  ​ machine ​  ​ learning ​  ​ systems ​  ​ as ​  ​ a ​  ​ regulatory ​  ​ technique    to ​  ​ ensure ​  ​ individual ​  ​ rights, ​  ​ how ​  ​ to ​  ​ operationalize ​  ​ such ​  ​ a ​  ​ requirement, ​  ​ whether ​  ​ such ​  ​ a  127 128  weapons” ​  ​ (geneva: ​  ​ review ​  ​ conference ​  ​ of ​  ​ the ​  ​ convention ​  ​ on ​  ​ certain ​  ​ conventional ​  ​ weapons, ​  ​ december ​  ​ 2016),    https://globalsecurity.asu.edu/sites/default/files/files/control-or-judgment-understanding-the-scope.pdf ​ .    123  kate ​  ​ crawford ​  ​ and ​  ​ jason ​  ​ schultz, ​  ​ “big ​  ​ data ​  ​ and ​  ​ due ​  ​ process: ​  ​ toward ​  ​ a ​  ​ framework ​  ​ to ​  ​ redress ​  ​ predictive ​  ​ privacy ​  ​ harms,”    boston ​  ​ college ​  ​ law ​  ​ review ​ ​  ​ 55, ​  ​ no. ​  ​ 1 ​  ​ (january ​  ​ 29, ​  ​ 2014): ​  ​ 93.    124  danielle ​  ​ keats ​  ​ citron ​  ​ and ​  ​ frank ​  ​ a. ​  ​ pasquale, ​  ​ “the ​  ​ scored ​  ​ society: ​  ​ due ​  ​ process ​  ​ for ​  ​ automated ​  ​ predictions,” ​  ​​ washington ​  ​ law    review ​ ​  ​ 89, ​  ​ no. ​  ​ 1 ​  ​ (2014): ​  ​ 1–33.    125  andrew ​  ​ d. ​  ​ selbst, ​  ​ “disparate ​  ​ impact ​  ​ in ​  ​ big ​  ​ data ​  ​ policing”, ​  ​ georgia ​  ​ law ​  ​ review, ​  ​ forthcoming ​  ​ 2017. ​  ​ ssrn ​  ​ preprint:    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2819182 ​ .    126  lilian ​  ​ edwards ​  ​ and ​  ​ michael ​  ​ veale, ​  ​ “slave ​  ​ to ​  ​ the ​  ​ algorithm? ​  ​ why ​  ​ a ​  ​ ‘right ​  ​ to ​  ​ explanation’ ​  ​ is ​  ​ probably ​  ​ not ​  ​ the ​  ​ remedy ​  ​ you    are ​  ​ looking ​  ​ for”, ​  ​ ssrn ​  ​ preprint, ​  ​​ https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2972855 ​ .    127  ibid; ​  ​ kiel ​  ​ brennan-marquez, ​  ​ “’plausible ​  ​ cause’: ​  ​ explanatory ​  ​ standards ​  ​ in ​  ​ the ​  ​ age ​  ​ of ​  ​ powerful ​  ​ machines, ​  ​​ vanderbilt ​  ​ law    review ​ ​  ​ (2017) ​  ​ vol. ​  ​ 70, ​  ​ p. ​  ​ 1249; ​  ​ andrew ​  ​ d. ​  ​ selbst, ​  ​ “a ​  ​ mild ​  ​ defense ​  ​ of ​  ​ our ​  ​ new ​  ​ machine ​  ​ overlords,” ​  ​​ vanderbilt ​  ​ law ​  ​ review    en ​  ​ banc ​ ​  ​ (2017) ​  ​ vol. ​  ​ 70, ​  ​ p. ​  ​ 87; ​  ​ katherine ​  ​ jo ​  ​ strandburg, ​  ​ “decisionmaking, ​  ​ machine ​  ​ learning ​  ​ and ​  ​ the ​  ​ value ​  ​ of ​  ​ explanation,”    (the ​  ​ human ​  ​ use ​  ​ of ​  ​ machine ​  ​ learning: ​  ​ an ​  ​ interdisciplinary ​  ​ workshop, ​  ​ 16 ​  ​ december ​  ​ 2016),    http://www.dsi.unive.it/huml2016/assets/slides/talk%202.pdf ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 27    requirement ​  ​ presently ​  ​ exists ​  ​ under ​  ​ the ​  ​ gdpr ​  ​ and ​  ​ more ​  ​ generally ​  ​ how ​  ​ competing  129  interpretations ​  ​ or ​  ​ explanations ​  ​ might ​  ​ be ​  ​ technically ​  ​ formulated ​  ​ and ​  ​ understood ​  ​ by    different ​  ​ stakeholders.   130  the ​  ​ criminal ​  ​ justice ​  ​ system’s ​  ​ implementation ​  ​ of ​  ​ risk ​  ​ assessment ​  ​ algorithms ​  ​ provides ​  ​ an    example ​  ​ of ​  ​ the ​  ​ legal ​  ​ system’s ​  ​ use ​  ​ of ​  ​ ai ​  ​ and ​  ​ its ​  ​ attendant ​  ​ risks. ​  ​ proponents ​  ​ of ​  ​ risk-based  131  sentencing ​  ​ argue ​  ​ that ​  ​ evidence-based ​  ​ machine ​  ​ learning ​  ​ techniques ​  ​ can ​  ​ be ​  ​ used ​  ​ in ​  ​ concert    with ​  ​ the ​  ​ expertise ​  ​ of ​  ​ judges ​  ​ to ​  ​ improve ​  ​ the ​  ​ accuracy ​  ​ of ​  ​ prior ​  ​ statistical ​  ​ and ​  ​ actuarial    methods ​  ​ for ​  ​ risk ​  ​ forecasting, ​  ​ such ​  ​ as ​  ​ regression ​  ​ analysis. ​  ​ along ​  ​ these ​  ​ lines, ​  ​ a ​  ​ recent  132  study ​  ​ by ​  ​ computer ​  ​ scientist ​  ​ jon ​  ​ kleinberg, ​  ​ sendhil ​  ​ mullainathan, ​  ​ and ​  ​ their ​  ​ co-authors    showed ​  ​ that ​  ​ a ​  ​ predictive ​  ​ machine ​  ​ learning ​  ​ algorithm ​  ​ could ​  ​ be ​  ​ used ​  ​ by ​  ​ judges ​  ​ to ​  ​ reduce ​  ​ the    number ​  ​ of ​  ​ defendants ​  ​ held ​  ​ in ​  ​ jail ​  ​ as ​  ​ they ​  ​ await ​  ​ trial ​  ​ by ​  ​ making ​  ​ more ​  ​ accurate ​  ​ predictions    of ​  ​ future ​  ​ crimes.   133  while ​  ​ algorithmic ​  ​ decision-making ​  ​ tools ​  ​ show ​  ​ promise, ​  ​ many ​  ​ of ​  ​ these ​  ​ researchers ​  ​ caution    against ​  ​ misleading ​  ​ performance ​  ​ measures ​  ​ for ​  ​ emerging ​  ​ ai-assisted ​  ​ legal ​  ​ techniques. ​  ​ for  134  example, ​  ​ the ​  ​ value ​  ​ of ​  ​ recidivism ​  ​ as ​  ​ a ​  ​ means ​  ​ to ​  ​ evaluate ​  ​ the ​  ​ correctness ​  ​ of ​  ​ an    algorithmically-assigned ​  ​ risk ​  ​ score ​  ​ is ​  ​ questionable ​  ​ because ​  ​ judges ​  ​ make ​  ​ decisions ​  ​ about    risk ​  ​ in ​  ​ sentencing, ​  ​ which, ​  ​ in ​  ​ turn, ​  ​ influences ​  ​ recidivism ​  ​ – ​  ​ or, ​  ​ those ​  ​ assessed ​  ​ as ​  ​ “low ​  ​ risk”    and ​  ​ subsequently ​  ​ released ​  ​ are ​  ​ the ​  ​ only ​  ​ ones ​  ​ who ​  ​ will ​  ​ have ​  ​ an ​  ​ opportunity ​  ​ to ​  ​ re-offend,    making ​  ​ it ​  ​ difficult ​  ​ to ​  ​ measure ​  ​ the ​  ​ accuracy ​  ​ of ​  ​ such ​  ​ scoring. ​  ​ meanwhile, ​  ​ rebecca ​  ​ wexler ​  ​ has    documented ​  ​ the ​  ​ disturbing ​  ​ trend ​  ​ of ​  ​ trade ​  ​ secret ​  ​ doctrine ​  ​ being ​  ​ expressly ​  ​ adopted ​  ​ in ​  ​ courts    to ​  ​ prevent ​  ​ criminal ​  ​ defendants ​  ​ from ​  ​ asserting ​  ​ their ​  ​ rights ​  ​ at ​  ​ trial.  135  sandra ​  ​ mayson ​  ​ has ​  ​ recently ​  ​ written ​  ​ on ​  ​ risk ​  ​ assessment ​  ​ in ​  ​ the ​  ​ bail ​  ​ reform ​  ​ movement.    well-intentioned ​  ​ proponents ​  ​ of ​  ​ bail ​  ​ reform ​  ​ argue ​  ​ that ​  ​ risk ​  ​ assessment ​  ​ can ​  ​ be ​  ​ used ​  ​ to    spare ​  ​ poor, ​  ​ low-risk ​  ​ defendants ​  ​ from ​  ​ onerous ​  ​ bail ​  ​ requirements ​  ​ or ​  ​ pretrial ​  ​ incarceration.    such ​  ​ arguments ​  ​ tend ​  ​ to ​  ​ miss ​  ​ the ​  ​ potential ​  ​ of ​  ​ risk ​  ​ assessment ​  ​ to ​  ​ “legitimize ​  ​ and ​  ​ entrench”    problematic ​  ​ reliance ​  ​ on ​  ​ statistical ​  ​ correlation, ​  ​ and ​  ​ to ​  ​ “[lend ​  ​ such ​  ​ assessments] ​  ​ the ​  ​ aura ​  ​ of    scientific ​  ​ reliability.” ​  ​ mayson ​  ​ argues ​  ​ that ​  ​ we ​  ​ also ​  ​ need ​  ​ to ​  ​ ask ​  ​ deeper ​  ​ questions ​  ​ about  136  128  andrew ​  ​ d. ​  ​ selbst ​  ​ and ​  ​ solon ​  ​ barocas, ​  ​ “regulating ​  ​ inscrutable ​  ​ systems,” ​  ​ in ​  ​ progress.    129  bryce ​  ​ goodman ​  ​ and ​  ​ seth ​  ​ flaxman, ​  ​ “european ​  ​ union ​  ​ regulations ​  ​ on ​  ​ algorithmic ​  ​ decision-making ​  ​ and ​  ​ a ​  ​ ‘right ​  ​ to    explanation,’” ​  ​ icml ​  ​ workshop ​  ​ on ​  ​ human ​  ​ interpretability ​  ​ in ​  ​ machine ​  ​ learning, ​  ​​ arxiv ​  ​ preprint ​ : ​  ​ arxiv:1606.08813 ​  ​ (v3) ​  ​ (2016);    forthcoming, ​  ​​ ai ​  ​ magazine ​  ​​ (2017); ​  ​ sandra ​  ​ wachter, ​  ​ brent ​  ​ mittelstadt ​  ​ and ​  ​ luciano ​  ​ floridi, ​  ​ “why ​  ​ a ​  ​ right ​  ​ to ​  ​ explanation ​  ​ of    automated ​  ​ decision-making ​  ​ does ​  ​ not ​  ​ exist ​  ​ in ​  ​ the ​  ​ general ​  ​ data ​  ​ protection ​  ​ regulation,” ​  ​​ international ​  ​ data ​  ​ protection ​  ​ law    (2017), ​  ​​ https://doi.org/10.1093/idpl/ipx005 ​ .    130 ​  ​​ ​  ​ zachary ​  ​ c. ​  ​ lipton, ​  ​ “the ​  ​ mythos ​  ​ of ​  ​ model ​  ​ interpretability,” ​  ​ arxiv ​  ​ preprint ​  ​ [cs, ​  ​ stat] , ​  ​ june ​  ​ 10, ​  ​ 2016,    http://arxiv.org/abs/1606.03490.    131  richard ​  ​ berk ​  ​ and ​  ​ jordan ​  ​ hyatt, ​  ​ “machine ​  ​ learning ​  ​ forecasts ​  ​ of ​  ​ risk ​  ​ to ​  ​ inform ​  ​ sentencing ​  ​ decisions,” ​  ​​ federal ​  ​ sentencing    reporter ​ ​  ​ 27, ​  ​ no. ​  ​ 4 ​  ​ (april ​  ​ 1, ​  ​ 2015): ​  ​ 222–28, ​  ​ doi:10.1525/fsr.2015.27.4.222.     132  berk ​  ​ and ​  ​ hyatt, ​  ​ “machine ​  ​ learning ​  ​ forecasts ​  ​ of ​  ​ risk ​  ​ to ​  ​ inform ​  ​ sentencing ​  ​ decisions,” ​  ​ 222.     133  jon ​  ​ kleinberg ​  ​ et ​  ​ al., ​  ​ “human ​  ​ decisions ​  ​ and ​  ​ machine ​  ​ predictions,” ​  ​ working ​  ​ paper ​  ​ (national ​  ​ bureau ​  ​ of ​  ​ economic ​  ​ research,    february ​  ​ 2017), ​  ​ doi:10.3386/w23180. ​  ​​ http://nber.org/papers/w23180 ​ .     134  jon ​  ​ kleinberg, ​  ​ jens ​  ​ ludwig ​  ​ and ​  ​ sendhil ​  ​ mullainathan, ​  ​ “a ​  ​ guide ​  ​ to ​  ​ solving ​  ​ social ​  ​ problems ​  ​ with ​  ​ machine ​  ​ learning,” ​  ​​ harvard    business ​  ​ review ​ , ​  ​ december ​  ​ 8, ​  ​ 2016, ​  ​​ https://hbr.org/2016/12/a-guide-to-solving-social-problems-with-machine-learning ​ .    135  rebecca ​  ​ wexler, ​  ​ “life, ​  ​ liberty, ​  ​ and ​  ​ trade ​  ​ secrets: ​  ​ intellectual ​  ​ property ​  ​ in ​  ​ the ​  ​ criminal ​  ​ justice ​  ​ system,” ​  ​ ssrn ​  ​ preprint:    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2920883 ​ .    136  sandra ​  ​ g. ​  ​ mayson, ​  ​​  ​ “bail ​  ​ reform ​  ​ and ​  ​ restraint ​  ​ for ​  ​ dangerousness: ​  ​ are ​  ​ defendants ​  ​ a ​  ​ special ​  ​ case?”, ​ ssrn ​  ​ scholarly ​  ​ paper  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 28    how ​  ​ pretrial ​  ​ restraints ​  ​ are ​  ​ justified ​  ​ in ​  ​ the ​  ​ first ​  ​ place. ​  ​ in ​  ​ other ​  ​ words, ​  ​ policymakers ​  ​ who    hope ​  ​ to ​  ​ employ ​  ​ risk ​  ​ assessment ​  ​ in ​  ​ bail ​  ​ reform ​  ​ and ​  ​ pretrial ​  ​ forms ​  ​ of ​  ​ detention ​  ​ need ​  ​ to    publicly ​  ​ specify ​  ​ what ​  ​ types ​  ​ of ​  ​ risks ​  ​ can ​  ​ justify ​  ​ these ​  ​ such ​  ​ restraints ​  ​ on ​  ​ liberty, ​  ​ as    defendants ​  ​ receiving ​  ​ these ​  ​ scores ​  ​ have ​  ​ not ​  ​ been ​  ​ convicted ​  ​ of ​  ​ anything ​  ​ and ​  ​ these ​  ​ restraints    are ​  ​ not ​  ​ imposed ​  ​ on ​  ​ dangerous ​  ​ individuals ​  ​ in ​  ​ the ​  ​ rest ​  ​ of ​  ​ society.    separately, ​  ​ criminologist ​  ​ richard ​  ​ berk ​  ​ and ​  ​ his ​  ​ colleagues ​  ​ argue ​  ​ that ​  ​ there ​  ​ are ​  ​ intractable    tradeoffs ​  ​ between ​  ​ accuracy ​  ​ and ​  ​ fairness—the ​  ​ occurrence ​  ​ of ​  ​ false ​  ​ positives ​  ​ and    negatives—in ​  ​ populations ​  ​ where ​  ​ base ​  ​ rates ​  ​ (the ​  ​ percentage ​  ​ of ​  ​ a ​  ​ given ​  ​ population ​  ​ that ​  ​ fall    into ​  ​ a ​  ​ specific ​  ​ category) ​  ​ vary ​  ​ between ​  ​ different ​  ​ social ​  ​ groups. ​  ​ difficult ​  ​ decisions ​  ​ need ​  ​ to  137  be ​  ​ made ​  ​ about ​  ​ how ​  ​ we ​  ​ value ​  ​ fairness ​  ​ and ​  ​ accuracy ​  ​ in ​  ​ risk ​  ​ assessment. ​  ​ it ​  ​ is ​  ​ not ​  ​ merely ​  ​ a    technical ​  ​ problem, ​  ​ but ​  ​ one ​  ​ that ​  ​ involves ​  ​ important ​  ​ value ​  ​ judgments ​  ​ about ​  ​ how ​  ​ society    should ​  ​ work. ​  ​ left ​  ​ unchecked, ​  ​ the ​  ​ legal ​  ​ system ​  ​ is ​  ​ thus ​  ​ as ​  ​ susceptible ​  ​ to ​  ​ perpetuating    ai-driven ​  ​ harm ​  ​ as ​  ​ any ​  ​ other ​  ​ institution.       finally, ​  ​ machine ​  ​ learning ​  ​ and ​  ​ data ​  ​ analysis ​  ​ techniques ​  ​ are ​  ​ also ​  ​ being ​  ​ used ​  ​ to ​  ​ identify ​  ​ and    explain ​  ​ the ​  ​ abuses ​  ​ of ​  ​ rights. ​  ​ working ​  ​ with ​  ​ human ​  ​ rights ​  ​ advocates ​  ​ in ​  ​ mexico, ​  ​ the ​  ​ human    rights ​  ​ data ​  ​ analysis ​  ​ group ​  ​ created ​  ​ a ​  ​ machine ​  ​ learning ​  ​ model ​  ​ that ​  ​ can ​  ​ help ​  ​ guide ​  ​ the    search ​  ​ for ​  ​ mass ​  ​ graves.  138  ai ​  ​ and ​  ​ privacy       ai ​  ​ challenges ​  ​ current ​  ​ understandings ​  ​ of ​  ​ privacy ​  ​ and ​  ​ strains ​  ​ the ​  ​ laws ​  ​ and ​  ​ regulations ​  ​ we    have ​  ​ in ​  ​ place ​  ​ to ​  ​ protect ​  ​ personal ​  ​ information. ​  ​ established ​  ​ approaches ​  ​ to ​  ​ privacy ​  ​ have    become ​  ​ less ​  ​ and ​  ​ less ​  ​ effective ​  ​ because ​  ​ they ​  ​ are ​  ​ focused ​  ​ on ​  ​ previous ​  ​ metaphors ​  ​ of    computing, ​  ​ ones ​  ​ where ​  ​ adversaries ​  ​ were ​  ​ primarily ​  ​ human. ​  ​ ai ​  ​ systems’ ​  ​ intelligence, ​  ​ as    such, ​  ​ depends ​  ​ on ​  ​ ingesting ​  ​ as ​  ​ much ​  ​ training ​  ​ data ​  ​ as ​  ​ possible. ​  ​ this ​  ​ primary ​  ​ objective ​  ​ is    adverse ​  ​ to ​  ​ the ​  ​ goals ​  ​ of ​  ​ privacy. ​  ​ ai ​  ​ thus ​  ​ poses ​  ​ significant ​  ​ challenges ​  ​ to ​  ​ traditional ​  ​ efforts ​  ​ to    minimize ​  ​ data ​  ​ collection ​  ​ and ​  ​ to ​  ​ reform ​  ​ government ​  ​ and ​  ​ industry ​  ​ surveillance ​  ​ practices.        of ​  ​ course, ​  ​ privacy ​  ​ as ​  ​ a ​  ​ “right” ​  ​ has ​  ​ always ​  ​ been ​  ​ unevenly ​  ​ distributed. ​  ​ rights-based    discourses ​  ​ are ​  ​ regularly ​  ​ critiqued ​  ​ as ​  ​ being ​  ​ disproportionately ​  ​ beneficial ​  ​ to ​  ​ the ​  ​ privileged    while ​  ​ leaving ​  ​ many ​  ​ vulnerable ​  ​ populations ​  ​ partially ​  ​ or ​  ​ entirely ​  ​ exposed. ​  ​ yet ​  ​ what ​  ​ is    different ​  ​ with ​  ​ ai ​  ​ and ​  ​ privacy ​  ​ is ​  ​ that ​  ​ while ​  ​ individualistic ​  ​ and ​  ​ rights-based    conceptualizations ​  ​ of ​  ​ privacy ​  ​ remain ​  ​ important ​  ​ to ​  ​ some ​  ​ of ​  ​ the ​  ​ systems ​  ​ at ​  ​ work ​  ​ today,    computational ​  ​ systems ​  ​ are ​  ​ now ​  ​ operating ​  ​ outside ​  ​ of ​  ​ the ​  ​ data ​  ​ collection ​  ​ metaphors ​  ​ that    privacy ​  ​ law ​  ​ is ​  ​ built ​  ​ on. ​  ​ we ​  ​ are ​  ​ in ​  ​ new ​  ​ terrain, ​  ​ and ​  ​ one ​  ​ that ​  ​ 20th ​  ​ century ​  ​ models ​  ​ of ​  ​ privacy    are ​  ​ not ​  ​ designed ​  ​ to ​  ​ contend ​  ​ with.        for ​  ​ example, ​  ​ privacy ​  ​ discourse ​  ​ has ​  ​ not ​  ​ sufficiently ​  ​ accounted ​  ​ for ​  ​ the ​  ​ growing ​  ​ power    asymmetries ​  ​ between ​  ​ the ​  ​ institutions ​  ​ that ​  ​ accumulate ​  ​ data ​  ​ and ​  ​ the ​  ​ people ​  ​ who ​  ​ generate    (rochester, ​  ​ ny: ​  ​ social ​  ​ science ​  ​ research ​  ​ network, ​  ​ august ​  ​ 15, ​  ​ 2016), ​  ​​ https://papers.ssrn.com/abstract=2826600 ​ , ​  ​ 2.     137  richard ​  ​ berk, ​  ​ hoda ​  ​ heidari, ​  ​ shahin ​  ​ jabbari, ​  ​ michael ​  ​ kearns ​  ​ and ​  ​ aaron ​  ​ roth, ​  ​ “fairness ​  ​ in ​  ​ criminal ​  ​ justice ​  ​ risk ​  ​ assessments:    the ​  ​ state ​  ​ of ​  ​ the ​  ​ art,” ​  ​ arxiv:1703.09207, ​  ​ march ​  ​ 27, ​  ​ 2017.    138 ​  ​​  ​​  ​ j. ​  ​ m. ​  ​ porup, ​  ​ “hunting ​  ​ for ​  ​ mexico’s ​  ​ mass ​  ​ graves ​  ​ with ​  ​ machine ​  ​ learning,” ​  ​ ars ​  ​ technica ​  ​ uk, ​  ​ april ​  ​ 17, ​  ​ 2017,    https://arstechnica.co.uk/information-technology/2017/04/hunting-for-mexicos-mass-graves-with-machine-learning/.  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 29    that ​  ​ data, ​  ​ even ​  ​ as ​  ​ they ​  ​ are ​  ​ approaching ​  ​ threshold ​  ​ levels ​  ​ which ​  ​ may ​  ​ make ​  ​ these    asymmetries ​  ​ very ​  ​ hard ​  ​ to ​  ​ reverse. ​  ​ models ​  ​ of ​  ​ privacy ​  ​ based ​  ​ on ​  ​ data ​  ​ as ​  ​ a ​  ​ tradable ​  ​ good ​  ​ fail    to ​  ​ contend ​  ​ with ​  ​ this ​  ​ power ​  ​ difference. ​  ​ people ​  ​ cannot ​  ​ trade ​  ​ effectively ​  ​ with ​  ​ systems ​  ​ they    do ​  ​ not ​  ​ understand, ​  ​ particularly ​  ​ when ​  ​ the ​  ​ system ​  ​ understands ​  ​ them ​  ​ all ​  ​ too ​  ​ well ​  ​ and ​  ​ knows    how ​  ​ to ​  ​ manipulate ​  ​ their ​  ​ preferences. ​  ​ additionally, ​  ​ adaptive ​  ​ algorithms ​  ​ are ​  ​ changing    constantly, ​  ​ such ​  ​ that ​  ​ even ​  ​ the ​  ​ designers ​  ​ who ​  ​ created ​  ​ them ​  ​ cannot ​  ​ fully ​  ​ explain ​  ​ the ​  ​ results    they ​  ​ generate. ​  ​ in ​  ​ this ​  ​ new ​  ​ model ​  ​ of ​  ​ computational ​  ​ privacy ​  ​ adversaries, ​  ​ both ​  ​ power ​  ​ and    knowledge ​  ​ gaps ​  ​ will ​  ​ continue ​  ​ to ​  ​ widen. ​  ​ we ​  ​ must ​  ​ ask ​  ​ how ​  ​ ‘notice ​  ​ and ​  ​ consent’ ​  ​ is ​  ​ possible    or ​  ​ what ​  ​ it ​  ​ would ​  ​ mean ​  ​ to ​  ​ have ​  ​ ‘access ​  ​ to ​  ​ your ​  ​ data’ ​  ​ or ​  ​ to ​  ​ ‘control ​  ​ your ​  ​ data’ ​  ​ when ​  ​ so ​  ​ much    is ​  ​ unknown ​  ​ or ​  ​ in ​  ​ flux.       there ​  ​ has ​  ​ also ​  ​ been ​  ​ a ​  ​ shift ​  ​ in ​  ​ the ​  ​ quality ​  ​ of ​  ​ the ​  ​ data ​  ​ used ​  ​ for ​  ​ ai. ​  ​ in ​  ​ order ​  ​ to ​  ​ help ​  ​ develop    sophisticated ​  ​ diagnostic ​  ​ models, ​  ​ designers ​  ​ often ​  ​ seek ​  ​ to ​  ​ use ​  ​ inputs ​  ​ that ​  ​ are ​  ​ extremely    sensitive ​  ​ in ​  ​ nature. ​  ​ for ​  ​ example, ​  ​ in ​  ​ the ​  ​ case ​  ​ of ​  ​ deepmind's ​  ​ partnership ​  ​ with ​  ​ the ​  ​ uk’s    national ​  ​ health ​  ​ service, ​  ​ the ​  ​ company ​  ​ acquired ​  ​ large ​  ​ amounts ​  ​ of ​  ​ very ​  ​ sensitive ​  ​ public    health ​  ​ data. ​  ​ even ​  ​ though ​  ​ this ​  ​ data ​  ​ may ​  ​ have ​  ​ been ​  ​ required ​  ​ for ​  ​ some ​  ​ of ​  ​ the ​  ​ project’s ​  ​ goals,    the ​  ​ resulting ​  ​ backlash ​  ​ and ​  ​ government ​  ​ censure ​  ​ illustrate ​  ​ the ​  ​ emerging ​  ​ tensions ​  ​ related  139  to ​  ​ the ​  ​ ai ​  ​ industry’s ​  ​ use ​  ​ of ​  ​ such ​  ​ data ​  ​ and ​  ​ the ​  ​ current ​  ​ limits ​  ​ of ​  ​ democratic ​  ​ processes ​  ​ to    address ​  ​ questions ​  ​ of ​  ​ agency, ​  ​ accountability ​  ​ and ​  ​ oversight ​  ​ for ​  ​ these ​  ​ endeavors.       the ​  ​ expansion ​  ​ of ​  ​ ai ​  ​ into ​  ​ diverse ​  ​ realms ​  ​ like ​  ​ urban ​  ​ planning ​  ​ also ​  ​ raises ​  ​ privacy ​  ​ concerns    over ​  ​ the ​  ​ deployment ​  ​ of ​  ​ iot ​  ​ devices ​  ​ and ​  ​ sensors, ​  ​ arrayed ​  ​ throughout ​  ​ our ​  ​ daily ​  ​ lives,    tracking ​  ​ human ​  ​ movements, ​  ​ preferences ​  ​ and ​  ​ environments. ​  ​ these ​  ​ devices ​  ​ and ​  ​ sensors  140  collect ​  ​ the ​  ​ data ​  ​ ai ​  ​ requires ​  ​ to ​  ​ function ​  ​ in ​  ​ these ​  ​ realms. ​  ​ not ​  ​ only ​  ​ does ​  ​ this ​  ​ expansion    significantly ​  ​ increase ​  ​ the ​  ​ amount ​  ​ and ​  ​ type ​  ​ of ​  ​ data ​  ​ being ​  ​ gathered ​  ​ on ​  ​ individuals, ​  ​ it ​  ​ also    raises ​  ​ significant ​  ​ questions ​  ​ around ​  ​ security ​  ​ and ​  ​ accuracy ​  ​ as ​  ​ iot ​  ​ devices ​  ​ are ​  ​ notoriously    insecure, ​  ​ and ​  ​ often ​  ​ difficult ​  ​ to ​  ​ update ​  ​ and ​  ​ maintain.   141     ai’s ​  ​ capacity ​  ​ for ​  ​ prediction ​  ​ and ​  ​ inference ​  ​ also ​  ​ adds ​  ​ to ​  ​ the ​  ​ set ​  ​ of ​  ​ privacy ​  ​ concerns. ​  ​ much ​  ​ of    the ​  ​ value ​  ​ that ​  ​ ai ​  ​ offers ​  ​ is ​  ​ the ​  ​ ability ​  ​ to ​  ​ predict ​  ​ or ​  ​ “imagine” ​  ​ information ​  ​ about ​  ​ individuals    and ​  ​ groups ​  ​ that ​  ​ is ​  ​ otherwise ​  ​ difficult ​  ​ to ​  ​ collect, ​  ​ compute ​  ​ or ​  ​ distribute. ​  ​ as ​  ​ more ​  ​ ai ​  ​ systems    are ​  ​ deployed ​  ​ and ​  ​ focus ​  ​ on ​  ​ ever-more ​  ​ granular ​  ​ levels ​  ​ of ​  ​ detail, ​  ​ such ​  ​ “predictive ​  ​ privacy    harms” ​  ​ will ​  ​ become ​  ​ greater ​  ​ concerns, ​  ​ especially ​  ​ if ​  ​ there ​  ​ are ​  ​ few ​  ​ or ​  ​ no ​  ​ due ​  ​ process    constraints ​  ​ on ​  ​ how ​  ​ such ​  ​ information ​  ​ impacts ​  ​ vulnerable ​  ​ individuals. ​  ​ part ​  ​ of ​  ​ the ​  ​ promise  142  of ​  ​ predictive ​  ​ techniques ​  ​ is ​  ​ to ​  ​ make ​  ​ accurate, ​  ​ often ​  ​ intimate ​  ​ deductions ​  ​ based ​  ​ on ​  ​ a    seemingly-unrelated ​  ​ pieces ​  ​ of ​  ​ data ​  ​ or ​  ​ information, ​  ​ such ​  ​ as ​  ​ detecting ​  ​ substance ​  ​ abusers    from ​  ​ facebook ​  ​ posts , ​  ​ or ​  ​ identifying ​  ​ gang ​  ​ members ​  ​ based ​  ​ on ​  ​ twitter ​  ​ data. ​  ​ significant  143 144  139 ​  ​​  ​​ information ​  ​ commissioner’s ​  ​ office, ​  ​ “royal ​  ​ free ​  ​ - ​  ​ google ​  ​ deepmind ​  ​ trial ​  ​ failed ​  ​ to ​  ​ comply ​  ​ with ​  ​ data ​  ​ protection ​  ​ law,” ​  ​ july ​  ​ 3    (2017)    https://ico.org.uk/about-the-ico/news-and-events/news-and-blogs/2017/07/royal-free-google-deepmind-trial-failed-to-co  mply-with-data-protection-law/     140 ​  ​​  ​​ see, ​  ​ for ​  ​ example, ​  ​ nvida’s ​  ​ deep ​  ​ learning ​  ​ for ​  ​ smart ​  ​ cities:    https://www.nvidia.com/en-us/deep-learning-ai/industries/ai-cities/    141 ​  ​​  ​​ there ​  ​ have ​  ​ been ​  ​ several ​  ​ serious ​  ​ attacks ​  ​ on ​  ​ network ​  ​ infrastructure ​  ​ and ​  ​ logistics ​  ​ from ​  ​ hacked ​  ​ iot ​  ​ networked ​  ​ devices ​  ​ in ​  ​ the    last ​  ​ 12 ​  ​ months. ​  ​ for ​  ​ example: ​  ​ andrew ​  ​ peterson, ​  ​ “internet ​  ​ of ​  ​ things ​  ​ compounded ​  ​ friday’s ​  ​ hack ​  ​ of ​  ​ major ​  ​ web ​  ​ sites,”    washington ​  ​ post, ​ ​  ​ october ​  ​ 21 ​  ​ (2016)     142 ​  ​​  ​​ kate ​  ​ crawford ​  ​ and ​  ​ jason ​  ​ schultz, ​  ​ “big ​  ​ data ​  ​ and ​  ​ due ​  ​ process: ​  ​ toward ​  ​ a ​  ​ framework ​  ​ to ​  ​ redress ​  ​ predictive ​  ​ privacy ​  ​ harms,”    boston ​  ​ college ​  ​ law ​  ​ review ​ ​  ​ 55, ​  ​ no. ​  ​ 1 ​  ​ (january ​  ​ 29, ​  ​ 2014): ​  ​ 93.    143 ​  ​​  ​ tao ​  ​ ding, ​  ​ warren ​  ​ bickel ​  ​ and ​  ​ shimei ​  ​ pan, ​  ​ “social ​  ​ media-based ​  ​ substance ​  ​ use ​  ​ prediction,” ​  ​ may ​  ​ 16 ​  ​ (2017)  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 30    shifts ​  ​ are ​  ​ needed ​  ​ in ​  ​ the ​  ​ legal ​  ​ and ​  ​ regulatory ​  ​ approaches ​  ​ to ​  ​ privacy ​  ​ if ​  ​ they ​  ​ are ​  ​ to ​  ​ keep ​  ​ pace    with ​  ​ the ​  ​ emerging ​  ​ capacities ​  ​ of ​  ​ ai ​  ​ systems.    ethics ​  ​ and ​  ​ governance    so ​  ​ far, ​  ​ this ​  ​ report ​  ​ has ​  ​ addressed ​  ​ issues ​  ​ of ​  ​ power, ​  ​ markets, ​  ​ bias, ​  ​ fairness ​  ​ and ​  ​ rights ​  ​ and    liberties ​  ​ – ​  ​ all ​  ​ subjects ​  ​ closely ​  ​ tied ​  ​ to ​  ​ ethics. ​  ​ this ​  ​ section ​  ​ presents ​  ​ a ​  ​ distinct ​  ​ discussion ​  ​ of    ethics ​  ​ in ​  ​ the ​  ​ uses, ​  ​ deployment ​  ​ and ​  ​ creation ​  ​ of ​  ​ ai.   145  ethical ​  ​ questions ​  ​ surrounding ​  ​ ai ​  ​ systems ​  ​ are ​  ​ wide-ranging, ​  ​ spanning ​  ​ creation, ​  ​ uses ​  ​ and    outcomes. ​  ​ there ​  ​ are ​  ​ important ​  ​ questions ​  ​ about ​  ​ which ​  ​ set ​  ​ of ​  ​ values ​  ​ and ​  ​ interests ​  ​ are    reflected ​  ​ in ​  ​ ai, ​  ​ as ​  ​ well ​  ​ as ​  ​ how ​  ​ machines ​  ​ can ​  ​ recognize ​  ​ values ​  ​ and ​  ​ ethical ​  ​ paradigms. ​  ​ an    important ​  ​ distinction ​  ​ in ​  ​ this ​  ​ area ​  ​ is ​  ​ between ​  ​ what ​  ​ is ​  ​ called ​  ​ ‘machine ​  ​ ethics’ ​  ​ and ​  ​ the ​  ​ wider    domain ​  ​ of ​  ​ the ​  ​ ethics ​  ​ of ​  ​ ai. ​  ​ machine ​  ​ ethics ​  ​ is ​  ​ more ​  ​ narrowly ​  ​ and ​  ​ explicitly ​  ​ concerned ​  ​ with    the ​  ​ ethics ​  ​ of ​  ​ artificially ​  ​ intelligent ​  ​ beings ​  ​ and ​  ​ systems; ​  ​ isaac ​  ​ asimov’s ​  ​ laws ​  ​ of ​  ​ robotics ​  ​ are    one ​  ​ example ​  ​ that ​  ​ captured ​  ​ the ​  ​ popular ​  ​ imagination. ​  ​ ai ​  ​ ethics ​  ​ concerns ​  ​ wider ​  ​ social    concerns ​  ​ about ​  ​ the ​  ​ effects ​  ​ of ​  ​ ai ​  ​ systems ​  ​ and ​  ​ the ​  ​ choices ​  ​ made ​  ​ by ​  ​ their ​  ​ designers ​  ​ and    users. ​  ​ here, ​  ​ we ​  ​ are ​  ​ mostly ​  ​ concerned ​  ​ with ​  ​ the ​  ​ latter ​  ​ approach.     ai ​  ​ is ​  ​ certainly ​  ​ not ​  ​ unique ​  ​ among ​  ​ emerging ​  ​ technologies ​  ​ in ​  ​ creating ​  ​ ethical ​  ​ quandaries, ​  ​ and,    similar ​  ​ to ​  ​ other ​  ​ computational ​  ​ technologies, ​  ​ ai ​  ​ ethics ​  ​ have ​  ​ roots ​  ​ in ​  ​ the ​  ​ complex ​  ​ history ​  ​ of    military ​  ​ influence ​  ​ on ​  ​ computing ​  ​ development ​  ​ and ​  ​ the ​  ​ more ​  ​ recent ​  ​ commercialization ​  ​ and    corporate ​  ​ dominance ​  ​ of ​  ​ networked ​  ​ technologies. ​  ​​  ​ yet ​  ​ ethical ​  ​ questions ​  ​ in ​  ​ ai ​  ​ research ​  ​ and    development ​  ​ present ​  ​ unique ​  ​ challenges ​  ​ in ​  ​ that ​  ​ they ​  ​ ask ​  ​ us ​  ​ to ​  ​ consider ​  ​ whether, ​  ​ when ​  ​ and    how ​  ​ machines ​  ​ should ​  ​ to ​  ​ make ​  ​ decisions ​  ​ about ​  ​ human ​  ​ lives ​  ​ - ​  ​ and ​  ​ whose ​  ​ values ​  ​ should    guide ​  ​ those ​  ​ decisions.    ethical ​  ​ concerns ​  ​ in ​  ​ ai    articulating ​  ​ ethical ​  ​ values ​  ​ for ​  ​ ai ​  ​ systems ​  ​ has ​  ​ never ​  ​ been ​  ​ simple. ​  ​ in ​  ​ the ​  ​ 1960s, ​  ​ ai ​  ​ pioneer    joseph ​  ​ weizenbaum ​  ​ created ​  ​ the ​  ​ early ​  ​ chatbot ​  ​ system ​  ​ eliza ​  ​ as ​  ​ a ​  ​ technical ​  ​ demonstration    of ​  ​ a ​  ​ system ​  ​ capable ​  ​ of ​  ​ maintaining ​  ​ an ​  ​ interrogative ​  ​ “conversation” ​  ​ with ​  ​ a ​  ​ human    counterpart. ​  ​ rudimentary ​  ​ as ​  ​ it ​  ​ was ​  ​ by ​  ​ today’s ​  ​ standards, ​  ​ some ​  ​ psychologists ​  ​ adopted ​  ​ it ​  ​ as    a ​  ​ tool ​  ​ for ​  ​ treatment, ​  ​ much ​  ​ to ​  ​ the ​  ​ creator’s ​  ​ concern ​  ​ and ​  ​ dismay. ​  ​ in ​  ​ response, ​  ​ weizenbaum    raised ​  ​ ethical ​  ​ concerns ​  ​ around ​  ​ our ​  ​ reflexive ​  ​ reliance ​  ​ and ​  ​ trust ​  ​ in ​  ​ automated ​  ​ systems ​  ​ that    may ​  ​ appear ​  ​ to ​  ​ be ​  ​ objective ​  ​ and ​  ​ “intelligent,” ​  ​ but ​  ​ are ​  ​ ultimately ​  ​ simplistic ​  ​ and ​  ​ prone ​  ​ to    error.   146  https://arxiv.org/abs/1705.05633     144 ​  ​​  ​​ see ​  ​ lakshika ​  ​ balasuriya ​  ​ et ​  ​ al., ​  ​ “finding ​  ​ street ​  ​ gang ​  ​ members ​  ​ on ​  ​ twitter,” ​  ​​  ​ 2016 ​  ​ ieee/acm ​  ​ international ​  ​ conference ​  ​ on    advances ​  ​ in ​  ​ social ​  ​ networks ​  ​ analysis ​  ​ and ​  ​ mining ​  ​ (asonam ​  ​ 2016)    http://knoesis.wright.edu/sites/default/files/asonam2016_gang_member_identification_lakshika.pdf     145  vincent ​  ​ conitzer, ​  ​ walter ​  ​ sinnott-armstrong, ​  ​ jana ​  ​ schaich ​  ​ borg, ​  ​ yuan ​  ​ deng ​  ​ and ​  ​ max ​  ​ kramer, ​  ​ \"moral ​  ​ decision ​  ​ making    frameworks ​  ​ for ​  ​ artificial ​  ​ intelligence,\" ​  ​ (association ​  ​ for ​  ​ the ​  ​ advancement ​  ​ of ​  ​ artificial ​  ​ intelligence, ​  ​ 2017).    146  hans ​  ​ pruijt, ​  ​ \"social ​  ​ interaction ​  ​ with ​  ​ computers: ​  ​ an ​  ​ interpretation ​  ​ of ​  ​ weizenbaum's ​  ​ eliza ​  ​ and ​  ​ her ​  ​ heritage,\" ​  ​​ social ​  ​ science    computer ​  ​ review ​ ​  ​ 24, ​  ​ no. ​  ​ 4 ​  ​ (2006): ​  ​ 516-523; ​  ​ joseph ​  ​ weizenbaum, ​  ​​ computer ​  ​ power ​  ​ and ​  ​ human ​  ​ reason: ​  ​ from ​  ​ judgement ​  ​ to    calculation, ​ ​  ​ harmondsworth, ​  ​ uk: ​  ​ penguin, ​  ​ 1984.  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 31    currently ​  ​ there ​  ​ are ​  ​ heated ​  ​ debates ​  ​ about ​  ​ whether ​  ​ ai ​  ​ systems ​  ​ should ​  ​ be ​  ​ used ​  ​ in ​  ​ sensitive    or ​  ​ high-stakes ​  ​ contexts, ​  ​ who ​  ​ gets ​  ​ to ​  ​ make ​  ​ these ​  ​ important ​  ​ decisions ​  ​ , ​  ​ and ​  ​ what ​  ​ the ​  ​ proper    degree ​  ​ of ​  ​ human ​  ​ involvement ​  ​ should ​  ​ be ​  ​ in ​  ​ various ​  ​ types ​  ​ of ​  ​ decision-making. ​  ​ these ​  ​ are  147  ethical ​  ​ questions ​  ​ with ​  ​ a ​  ​ longstanding ​  ​ history. ​  ​ in ​  ​ examining ​  ​ these ​  ​ questions, ​  ​ we ​  ​ must ​  ​ also    look ​  ​ at ​  ​ the ​  ​ power ​  ​ dynamics ​  ​ of ​  ​ current ​  ​ ai ​  ​ development ​  ​ and ​  ​ deployment ​  ​ – ​  ​ and ​  ​ the ​  ​ way ​  ​ in    which ​  ​ decision-making, ​  ​ both ​  ​ by ​  ​ ai ​  ​ systems ​  ​ and ​  ​ the ​  ​ people ​  ​ who ​  ​ build ​  ​ them, ​  ​ is ​  ​ often    obscured ​  ​ from ​  ​ public ​  ​ view ​  ​ and ​  ​ accountability ​  ​ practices.     just ​  ​ in ​  ​ the ​  ​ last ​  ​ year, ​  ​ we’ve ​  ​ learned ​  ​ how ​  ​ facebook ​  ​ mines ​  ​ user ​  ​ data ​  ​ to ​  ​ reveal ​  ​ teenagers’    emotional ​  ​ state ​  ​ for ​  ​ advertisers, ​  ​ specifically ​  ​ targeting ​  ​ depressed ​  ​ teens. ​  ​ cambridge  148  analytica, ​  ​ a ​  ​ controversial ​  ​ data ​  ​ analytics ​  ​ firm ​  ​ that ​  ​ claims ​  ​ to ​  ​ be ​  ​ able ​  ​ to ​  ​ shift ​  ​ election ​  ​ results    through ​  ​ micro-targeting, ​  ​ has ​  ​ been ​  ​ reported ​  ​ to ​  ​ have ​  ​ expansive ​  ​​ individual ​ ​  ​ profiles ​  ​ on ​  ​ 220    million ​  ​ adult ​  ​ americans, ​  ​ and ​  ​ fake ​  ​ news ​  ​ has ​  ​ been ​  ​ instrumented ​  ​ to ​  ​ gain ​  ​ traction ​  ​ within  149  algorithmically ​  ​ filtered ​  ​ news ​  ​ feeds ​  ​ and ​  ​ search ​  ​ rankings ​  ​ in ​  ​ order ​  ​ to ​  ​ influence ​  ​ elections.  150  there ​  ​ are ​  ​ now ​  ​ multiple ​  ​ approaches ​  ​ for ​  ​ using ​  ​ machine ​  ​ learning ​  ​ techniques ​  ​ to ​  ​ synthesize    audio- ​  ​ and ​  ​ video-realistic ​  ​ representations ​  ​ of ​  ​ public ​  ​ figures ​  ​ and ​  ​ news ​  ​ events. ​  ​ each ​  ​ of  151  these ​  ​ examples ​  ​ shows ​  ​ how ​  ​ the ​  ​ interests ​  ​ of ​  ​ those ​  ​ deploying ​  ​ advanced ​  ​ data ​  ​ systems ​  ​ can    overshadow ​  ​ the ​  ​ public ​  ​ interest, ​  ​ acting ​  ​ in ​  ​ ways ​  ​ contrary ​  ​ to ​  ​ individual ​  ​ autonomy ​  ​ and    collective ​  ​ welfare, ​  ​ often ​  ​ without ​  ​ this ​  ​ being ​  ​ visible ​  ​ at ​  ​ all ​  ​ to ​  ​ those ​  ​ affected.  152  ai ​  ​ reflects ​  ​ its ​  ​ origins      the ​  ​ u.s. ​  ​ military ​  ​ has ​  ​ been ​  ​ one ​  ​ of ​  ​ the ​  ​ single ​  ​ most ​  ​ influential ​  ​ institutions ​  ​ in ​  ​ shaping ​  ​ modern    ai, ​  ​ with ​  ​ darpa’s ​  ​ funding ​  ​ of ​  ​ ai ​  ​ being ​  ​ among ​  ​ the ​  ​ most ​  ​ visible. ​  ​ indeed, ​  ​ ai ​  ​ has ​  ​ historically  153  been ​  ​ shaped ​  ​ largely ​  ​ by ​  ​ military ​  ​ goals, ​  ​ with ​  ​ its ​  ​ capabilities ​  ​ and ​  ​ incentives ​  ​ defined ​  ​ by ​  ​ military    objectives ​  ​ and ​  ​ desires. ​  ​ ai ​  ​ development ​  ​ continues ​  ​ to ​  ​ be ​  ​ supported ​  ​ by ​  ​ darpa ​  ​ and ​  ​ other  154  national ​  ​ defense ​  ​ agencies, ​  ​ particularly ​  ​ in ​  ​ the ​  ​ area ​  ​ of ​  ​ lethal ​  ​ autonomous ​  ​ weapons ​  ​ systems,    as ​  ​ discussed ​  ​ above.    however, ​  ​ current ​  ​ research ​  ​ into ​  ​ ai ​  ​ technology ​  ​ is ​  ​ highly ​  ​ industry-driven, ​  ​ with ​  ​ proprietary    systems ​  ​ supplementing ​  ​ military-funded ​  ​ classified ​  ​ systems ​  ​ and ​  ​ ai ​  ​ research ​  ​ increasingly    147  sue ​  ​ newell ​  ​ and ​  ​ marco ​  ​ marabelli, ​  ​ \"strategic ​  ​ opportunities ​  ​ (and ​  ​ challenges) ​  ​ of ​  ​ algorithmic ​  ​ decision-making: ​  ​ a ​  ​ call ​  ​ for ​  ​ action    on ​  ​ the ​  ​ long-term ​  ​ societal ​  ​ effects ​  ​ of ​  ​ ‘datification’,\" ​  ​​ the ​  ​ journal ​  ​ of ​  ​ strategic ​  ​ information ​  ​ systems ​ ​  ​ 24, ​  ​ no. ​  ​ 1 ​  ​ (2015): ​  ​ 3-14.    148  darren ​  ​ davidson, ​  ​ “facebook ​  ​ targets ​  ​ ‘insecure’ ​  ​ young ​  ​ people,” ​  ​ the ​  ​ australian, ​  ​ may ​  ​ 1, ​  ​ 2017.    149  hannes ​  ​ grassegger ​  ​ and ​  ​ mikael ​  ​ krogerus, ​  ​ \"thedata ​  ​ that ​  ​ turned ​  ​ the ​  ​ world ​  ​ upside ​  ​ down,\" ​  ​​ motherboard ​ , ​  ​ 2017,    https://publicpolicy.stanford.edu/news/data-turned-world-upside-down ​ .    150 ​  ​ chengcheng ​  ​ shao, ​  ​ giovanni ​  ​ luca ​  ​ ciampaglia, ​  ​ alessandro ​  ​ flammini ​  ​ and ​  ​ filippo ​  ​ menczer, ​  ​ \"hoaxy: ​  ​ a ​  ​ platform ​  ​ for ​  ​ tracking    online ​  ​ misinformation,\" ​  ​ in ​  ​​ proceedings ​  ​ of ​  ​ the ​  ​ 25th ​  ​ international ​  ​ conference ​  ​ companion ​  ​ on ​  ​ world ​  ​ wide ​  ​ web ​ , ​  ​ pp. ​  ​ 745-750.    international ​  ​ world ​  ​ wide ​  ​ web ​  ​ conferences ​  ​ steering ​  ​ committee, ​  ​ 2016.     151 ​  ​​  ​​ simon ​  ​ adler, ​  ​ “breaking ​  ​ news,” ​  ​ radiolab, ​  ​ july ​  ​ 27, ​  ​ 2017, ​  ​​ http://www.radiolab.org/story/breaking-news/ ​ .     152  kate ​  ​ crawford ​  ​ and ​  ​ meredith ​  ​ whittaker, ​  ​ “artificial ​  ​ intelligence ​  ​ is ​  ​ hard ​  ​ to ​  ​ see,” ​  ​​ medium ​ , ​  ​ september ​  ​ 11, ​  ​ 2016, ​  ​ :    https://medium.com/@katecrawford/artificial-intelligence-is-hard-to-see-a71e74f386db ​ .    153  sidney ​  ​ g ​  ​ reed, ​  ​ richard ​  ​ h. ​  ​ van ​  ​ atta ​  ​ and ​  ​ seymour ​  ​ j. ​  ​ dietman, ​  ​ “darpa ​  ​ technical ​  ​ accomplishments: ​  ​ an ​  ​ historical ​  ​ review ​  ​ of    selected ​  ​ darpa ​  ​ projects,” ​  ​ defense ​  ​ advanced ​  ​ research ​  ​ projects ​  ​ agency, ​  ​ vol. ​  ​ 1, ​  ​ 1990.    sidney ​  ​ g ​  ​ reed, ​  ​ richard ​  ​ h. ​  ​ van ​  ​ atta ​  ​ and ​  ​ seymour ​  ​ j. ​  ​ dietman, ​  ​ “darpa ​  ​ technical ​  ​ accomplishments: ​  ​ an ​  ​ historical ​  ​ review ​  ​ of    selected ​  ​ darpa ​  ​ projects,” ​  ​ defense ​  ​ advanced ​  ​ research ​  ​ projects ​  ​ agency, ​  ​ vol. ​  ​ 2, ​  ​ 1991.    154  alex ​  ​ roland ​  ​ and ​  ​ philip ​  ​ shiman, ​  ​​ strategic ​  ​ computing: ​  ​ darpa ​  ​ and ​  ​ the ​  ​ quest ​  ​ for ​  ​ machine ​  ​ intelligence, ​  ​ 1983-1993 ​ , ​  ​ (mit ​  ​ press,    2002).  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 32    taking ​  ​ place ​  ​ in ​  ​ closed-door ​  ​ industry ​  ​ settings, ​  ​ often ​  ​ without ​  ​ peer ​  ​ review ​  ​ or ​  ​ oversight.    accordingly, ​  ​ user ​  ​ consent, ​  ​ privacy ​  ​ and ​  ​ transparency ​  ​ are ​  ​ often ​  ​ overlooked ​  ​ in ​  ​ favor ​  ​ of    frictionless ​  ​ functionality ​  ​ that ​  ​ supports ​  ​ profit-driven ​  ​ business ​  ​ models ​  ​ based ​  ​ on ​  ​ aggregated    data ​  ​ profiles. ​  ​ while ​  ​ there ​  ​ are ​  ​ those ​  ​ advocating ​  ​ for ​  ​ clearer ​  ​ laws ​  ​ and ​  ​ policies, ​  ​ the  155  ambiguous ​  ​ space ​  ​ in ​  ​ which ​  ​ information ​  ​ rights ​  ​ are ​  ​ governed ​  ​ does ​  ​ not ​  ​ clearly ​  ​ regulate ​  ​ in    favor ​  ​ of ​  ​ individual ​  ​ control ​  ​ over ​  ​ personal ​  ​ technologies ​  ​ or ​  ​ online ​  ​ services.  156  the ​  ​ make ​  ​ up ​  ​ of ​  ​ ai ​  ​ researchers ​  ​ – ​  ​ what ​  ​ is ​  ​ and ​  ​ is ​  ​ not ​  ​ considered ​  ​ “ai ​  ​ research” ​  ​ – ​  ​ also ​  ​ has ​  ​ a    history ​  ​ which ​  ​ influences ​  ​ the ​  ​ current ​  ​ state ​  ​ of ​  ​ ai ​  ​ and ​  ​ its ​  ​ ethical ​  ​ parameters. ​  ​ beginning ​  ​ with    the ​  ​ dartmouth ​  ​ conference ​  ​ in ​  ​ 1956, ​  ​ ai ​  ​ researchers ​  ​ established ​  ​ a ​  ​ male-dominated,    narrowly-defined ​  ​ community. ​  ​ the ​  ​ boundaries ​  ​ of ​  ​ participation ​  ​ in ​  ​ the ​  ​ ai ​  ​ community ​  ​ were    relatively ​  ​ closed, ​  ​ and ​  ​ privileged ​  ​ mathematics, ​  ​ computer ​  ​ science ​  ​ and ​  ​ engineering ​  ​ over    perspectives ​  ​ that ​  ​ would ​  ​ provide ​  ​ for ​  ​ a ​  ​ more ​  ​ rigorous ​  ​ discussion ​  ​ of ​  ​ ai’s ​  ​ ethical ​  ​ implications.  ​  ​ producing ​  ​ technologies ​  ​ that ​  ​ work ​  ​ within ​  ​ complex ​  ​ social ​  ​ realities ​  ​ and ​  ​ existing ​  ​ systems  157  requires ​  ​ understanding ​  ​ social, ​  ​ legal ​  ​ and ​  ​ ethical ​  ​ contexts, ​  ​ which ​  ​ can ​  ​ only ​  ​ be ​  ​ done ​  ​ by    incorporating ​  ​ diverse ​  ​ perspectives ​  ​ and ​  ​ disciplinary ​  ​ expertise.     ethical ​  ​ codes    while ​  ​ decades ​  ​ of ​  ​ ai ​  ​ research ​  ​ have ​  ​ cited ​  ​ asimov’s ​  ​ three ​  ​ laws ​  ​ of ​  ​ robotics, ​  ​ and ​  ​ some  158  applied ​  ​ ai ​  ​ systems ​  ​ have ​  ​ been ​  ​ designed ​  ​ to ​  ​ comply ​  ​ with ​  ​ biomedical ​  ​ ethics, ​  ​ the ​  ​ tools ​  ​ that  159  have ​  ​ been ​  ​ available ​  ​ to ​  ​ developers ​  ​ to ​  ​ contend ​  ​ with ​  ​ social ​  ​ and ​  ​ ethical ​  ​ questions ​  ​ have ​  ​ been    relatively ​  ​ limited. ​  ​ ethical ​  ​ codes ​  ​ are ​  ​ gradually ​  ​ being ​  ​ developed ​  ​ in ​  ​ the ​  ​ ai ​  ​ research ​  ​ space, ​  ​ as    we ​  ​ discuss ​  ​ below, ​  ​ but ​  ​ they ​  ​ are ​  ​ necessarily ​  ​ incomplete: ​  ​ they ​  ​ will ​  ​ always ​  ​ need ​  ​ to ​  ​ evolve ​  ​ in    ways ​  ​ that ​  ​ are ​  ​ sensitive ​  ​ to ​  ​ the ​  ​ rapidly ​  ​ changing ​  ​ contexts ​  ​ and ​  ​ conditions ​  ​ in ​  ​ which ​  ​ ai ​  ​ systems    are ​  ​ deployed. ​  ​ these ​  ​ codes ​  ​ constitute ​  ​ one ​  ​ form ​  ​ of ​  ​ soft ​  ​ governance, ​  ​ where ​  ​ industry    standards ​  ​ and ​  ​ technical ​  ​ practices ​  ​ serve ​  ​ as ​  ​ alternatives ​  ​ to ​  ​ more ​  ​ traditional ​  ​ “hard” ​  ​ forms ​  ​ of    government ​  ​ regulation ​  ​ and ​  ​ legal ​  ​ oversight ​  ​ of ​  ​ ai. ​  ​​  ​ as ​  ​ ai ​  ​ systems ​  ​ are ​  ​ woven ​  ​ through ​  ​ a    growing ​  ​ number ​  ​ of ​  ​ domains, ​  ​ the ​  ​ needs ​  ​ for ​  ​ such ​  ​ a ​  ​ contextually-anchored ​  ​ approach ​  ​ to    ethics ​  ​ and ​  ​ governance ​  ​ only ​  ​ grows.   160  two ​  ​ related ​  ​ problems ​  ​ have ​  ​ emerged: ​  ​ there ​  ​ is ​  ​ no ​  ​ tracking ​  ​ of ​  ​ adherence ​  ​ to ​  ​ ethical    guidelines ​  ​ or ​  ​ soft ​  ​ governance ​  ​ standards ​  ​ in ​  ​ the ​  ​ ai ​  ​ industry, ​  ​ and ​  ​ we ​  ​ have ​  ​ not ​  ​ developed    ways ​  ​ to ​  ​ link ​  ​ the ​  ​ adherence ​  ​ to ​  ​ ethical ​  ​ guidelines ​  ​ to ​  ​ the ​  ​ ultimate ​  ​ impact ​  ​ of ​  ​ an ​  ​ ai ​  ​ systems ​  ​ in    155  elaine ​  ​ sedenberg ​  ​ and ​  ​ ann ​  ​ lauren ​  ​ hoffmann, ​  ​ “recovering ​  ​ the ​  ​ history ​  ​ of ​  ​ informed ​  ​ consent ​  ​ for ​  ​ data ​  ​ science ​  ​ and ​  ​ internet    industry ​  ​ research ​  ​ ethics” ​  ​ (september ​  ​ 12, ​  ​ 2016). ​  ​ available ​  ​ at ​  ​ ssrn: ​  ​​ https://ssrn.com/abstract=2837585 ​ .    156  united ​  ​ states ​  ​ (2016) ​  ​ executive ​  ​ office ​  ​ of ​  ​ the ​  ​ president ​  ​ and ​  ​ jason ​  ​ furman, ​  ​ john ​  ​ p. ​  ​ holdren, ​  ​ cecilia ​  ​ muñoz, ​  ​ megan ​  ​ smith ​  ​ and    jeffery ​  ​ zients, ​  ​ “artificial ​  ​ intelligence, ​  ​ automation, ​  ​ and ​  ​ the ​  ​ economy,” ​  ​ technical ​  ​ report, ​  ​ national ​  ​ science ​  ​ and ​  ​ technology    council, ​  ​ washington ​  ​ d.c. ​  ​ 20502, ​  ​ october ​  ​ 2016,    https://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/documents/artificial-intelligence-automation-economy.  pdf ​ .    157  nils ​  ​ j ​  ​ nilsson, ​  ​​ the ​  ​ quest ​  ​ for ​  ​ artificial ​  ​ intelligence ​ , ​  ​ (cambridge ​  ​ university ​  ​ press, ​  ​ 2009).    158  susan ​  ​ leigh ​  ​ anderson, ​  ​ \"asimov’s ​  ​ “three ​  ​ laws ​  ​ of ​  ​ robotics” ​  ​ and ​  ​ machine ​  ​ metaethics,\" ​  ​​ ai ​  ​ & ​  ​ society ​ ​  ​ 22, ​  ​ no. ​  ​ 4 ​  ​ (2008): ​  ​ 477-493.    159  raymond ​  ​ heatherly, ​  ​ \"privacy ​  ​ and ​  ​ security ​  ​ within ​  ​ biobanking: ​  ​ the ​  ​ role ​  ​ of ​  ​ information ​  ​ technology,\" ​  ​​ the ​  ​ journal ​  ​ of ​  ​ law,    medicine ​  ​ & ​  ​ ethics ​ ​  ​ 44, ​  ​ no. ​  ​ 1 ​  ​ (2016): ​  ​ 156-160.    160  robert ​  ​ rosenberger, ​  ​ \"phenomenological ​  ​ approaches ​  ​ to ​  ​ technological ​  ​ ethics,\" ​  ​​ the ​  ​ ethics ​  ​ of ​  ​ technology: ​  ​ methods ​  ​ and    approaches ​ ​  ​ (2017): ​  ​ 67.  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 33    the ​  ​ world.     examples ​  ​ of ​  ​ intertwined ​  ​ practice ​  ​ and ​  ​ ethics ​  ​ can ​  ​ be ​  ​ found ​  ​ in ​  ​ the ​  ​ biomedical ​  ​ uses ​  ​ of ​  ​ ai.    bioethics ​  ​ already ​  ​ offers ​  ​ a ​  ​ series ​  ​ of ​  ​ standards, ​  ​ values ​  ​ and ​  ​ procedures, ​  ​ along ​  ​ with  161  enforcement ​  ​ and ​  ​ accountability ​  ​ mechanisms. ​  ​ but ​  ​ how ​  ​ these ​  ​ should ​  ​ apply ​  ​ to ​  ​ medical ​  ​ ai    systems ​  ​ is ​  ​ often ​  ​ unclear, ​  ​ and ​  ​ researchers ​  ​ have ​  ​ been ​  ​ tracking ​  ​ the ​  ​ disparities. ​  ​ this ​  ​ is ​  ​ also  162  true ​  ​ of ​  ​ privacy ​  ​ requirements, ​  ​ which, ​  ​ given ​  ​ modern ​  ​ ai’s ​  ​ capability ​  ​ to ​  ​ make ​  ​ very ​  ​ personal    inferences ​  ​ given ​  ​ only ​  ​ limited ​  ​ data, ​  ​ are ​  ​ increasingly ​  ​ insufficient. ​  ​ where ​  ​ ethical ​  ​ standards  163  aimed ​  ​ at ​  ​ protecting ​  ​ patient ​  ​ privacy ​  ​ have ​  ​ been ​  ​ proposed, ​  ​ some ​  ​ biomedical ​  ​ researchers    have ​  ​ rejected ​  ​ them, ​  ​ seeing ​  ​ them ​  ​ as ​  ​ an ​  ​ impediment ​  ​ to ​  ​ innovation.  164  a ​  ​ more ​  ​ intentional ​  ​ approach ​  ​ to ​  ​ ethics ​  ​ is ​  ​ needed, ​  ​ and ​  ​ some ​  ​ are ​  ​ working ​  ​ toward ​  ​ this.    teaching ​  ​ ethics ​  ​ to ​  ​ practitioners ​  ​ is ​  ​ one ​  ​ such ​  ​ example. ​  ​ the ​  ​ blue ​  ​ sky ​  ​ agenda ​  ​ for ​  ​ ai  165  education, ​  ​ a ​  ​ collection ​  ​ of ​  ​ ideas ​  ​ for ​  ​ ethics ​  ​ education ​  ​ in ​  ​ ai, ​  ​ seeks ​  ​ democratization ​  ​ of ​  ​ ai    education ​  ​ and ​  ​ emphasizes ​  ​ inclusiveness ​  ​ in ​  ​ development ​  ​ toward ​  ​ the ​  ​ goal ​  ​ of ​  ​ respecting ​  ​ the    values ​  ​ and ​  ​ rights ​  ​ of ​  ​ diverse ​  ​ populations. ​  ​ but ​  ​ education ​  ​ is ​  ​ not ​  ​ enough. ​  ​ opportunities  166  must ​  ​ open ​  ​ up ​  ​ for ​  ​ ethics ​  ​ to ​  ​ be ​  ​ integrated ​  ​ in ​  ​ early ​  ​ stage ​  ​ design, ​  ​ and ​  ​ incentives ​  ​ for ​  ​ designing    and ​  ​ implementing ​  ​ ai ​  ​ ethically ​  ​ must ​  ​ be ​  ​ built ​  ​ into ​  ​ the ​  ​ companies ​  ​ and ​  ​ institutions ​  ​ currently    driving ​  ​ development.     ethical ​  ​ values ​  ​ and ​  ​ norms ​  ​ around ​  ​ accountability, ​  ​ social ​  ​ and ​  ​ political ​  ​ responsibility,  167  inclusion ​  ​ and ​  ​ connectivity, ​  ​ legibility ​  ​ and ​  ​ security ​  ​ and ​  ​ privacy ​  ​ are ​  ​ embedded ​  ​ in ​  ​ every  168 169  system ​  ​ via ​  ​ their ​  ​ default ​  ​ settings, ​  ​ whether ​  ​ intentionally ​  ​ or ​  ​ not. ​  ​ often, ​  ​ these  170  invisibly-embedded ​  ​ values ​  ​ reflect ​  ​ the ​  ​ status ​  ​ quo, ​  ​ the ​  ​ context ​  ​ and ​  ​ interests ​  ​ of ​  ​ their    developers, ​  ​ and ​  ​ matters ​  ​ of ​  ​ convenience ​  ​ and ​  ​ profit. ​  ​ once ​  ​ set, ​  ​ these ​  ​ implicit ​  ​ values ​  ​ are ​  ​ hard    to ​  ​ change ​  ​ for ​  ​ a ​  ​ variety ​  ​ of ​  ​ reasons, ​  ​ even ​  ​ as ​  ​ they ​  ​ tend ​  ​ to ​  ​ shape ​  ​ the ​  ​ capabilities ​  ​ and ​  ​ roles  171  of ​  ​ systems ​  ​ within ​  ​ various ​  ​ lived ​  ​ contexts. ​  ​ ethical ​  ​ codes ​  ​ should ​  ​ work ​  ​ to ​  ​ ensure ​  ​ that ​  ​ these  172  161  stephen ​  ​ brotherton, ​  ​ audiey ​  ​ kao, ​  ​ and ​  ​ b. ​  ​ j. ​  ​ crigger, ​  ​ \"professing ​  ​ the ​  ​ values ​  ​ of ​  ​ medicine: ​  ​ the ​  ​ modernized ​  ​ ama ​  ​ code ​  ​ of    medical ​  ​ ethics,\" ​  ​​ jama ​ ​  ​ 316, ​  ​ no. ​  ​ 10 ​  ​ (2016): ​  ​ 1041-1042.    162 ​  ​​  ​​ jake ​  ​ metcalf ​  ​ and ​  ​ kate ​  ​ crawford, ​  ​ “ ​ where ​  ​ are ​  ​ human ​  ​ subjects ​  ​ in ​  ​ big ​  ​ data ​  ​ research? ​  ​ the ​  ​ emerging ​  ​ ethics ​  ​ divide,\" ​  ​​ big ​  ​ data ​  ​ &    society ​ ​  ​ 3.1 ​  ​ (2016): ​  ​​ http://journals.sagepub.com/doi/pdf/10.1177/2053951716650211     163  stephen ​  ​ j ​  ​ tipton, ​  ​ sara ​  ​ forkey ​  ​ and ​  ​ young ​  ​ b ​  ​ choi, ​  ​ \"toward ​  ​ proper ​  ​ authentication ​  ​ methods ​  ​ in ​  ​ electronic ​  ​ medical ​  ​ record    access ​  ​ compliant ​  ​ to ​  ​ hipaa ​  ​ and ​  ​ cia ​  ​ triangle,\" ​  ​​ journal ​  ​ of ​  ​ medical ​  ​ systems ​ ​  ​ 40, ​  ​ no. ​  ​ 4 ​  ​ (2016): ​  ​ 1-8.    164  wendy ​  ​ lipworth ​  ​ and ​  ​ renata ​  ​ axler, ​  ​ \"towards ​  ​ a ​  ​ bioethics ​  ​ of ​  ​ innovation,\" ​  ​​ journal ​  ​ of ​  ​ medical ​  ​ ethics ​ ​  ​ (2016): ​  ​ medethics-2015.    165  judy ​  ​ goldsmith ​  ​ and ​  ​ emanuelle ​  ​ burton, ​  ​ \"why ​  ​ teaching ​  ​ ethics ​  ​ to ​  ​ ai ​  ​ practitioners ​  ​ is ​  ​ important,\" ​  ​ (2017).    166  eric ​  ​ eaton, ​  ​ sven ​  ​ koenig, ​  ​ claudia ​  ​ schulz, ​  ​ francesco ​  ​ maurelli, ​  ​ john ​  ​ lee, ​  ​ joshua ​  ​ eckroth, ​  ​ mark ​  ​ crowley, ​  ​ richard ​  ​ g. ​  ​ freedman,    rogelio ​  ​ e. ​  ​ cardona-rivera, ​  ​ tiago ​  ​ machado ​  ​ and ​  ​ tom ​  ​ williams, ​  ​ \"blue ​  ​ sky ​  ​ ideas ​  ​ in ​  ​ artificial ​  ​ intelligence ​  ​ education ​  ​ from ​  ​ the    eaai ​  ​ 2017 ​  ​ new ​  ​ and ​  ​ future ​  ​ ai ​  ​ educator ​  ​ program,\" ​  ​​ arxiv ​  ​ preprint ​  ​ arxiv:1702.00137 ​ ​  ​ (2017).    167  wendell ​  ​ wallach, ​  ​​ a ​  ​ dangerous ​  ​ master: ​  ​ how ​  ​ to ​  ​ keep ​  ​ technology ​  ​ from ​  ​ slipping ​  ​ beyond ​  ​ our ​  ​ control ​ , ​  ​ (basic ​  ​ books, ​  ​ 2014).    168  anna ​  ​ lauren ​  ​ hoffmann, ​  ​ nicholas ​  ​ proferes ​  ​ and ​  ​ michael ​  ​ zimmer, ​  ​ \"“making ​  ​ the ​  ​ world ​  ​ more ​  ​ open ​  ​ and ​  ​ connected”: ​  ​ mark    zuckerberg ​  ​ and ​  ​ the ​  ​ discursive ​  ​ construction ​  ​ of ​  ​ facebook ​  ​ and ​  ​ its ​  ​ users.\" ​  ​​ new ​  ​ media ​  ​ & ​  ​ society ​ ​  ​ (2016): ​  ​ 1461444816660784.    169  john ​  ​ wilbanks, ​  ​ \"public ​  ​ domain, ​  ​ copyright ​  ​ licenses ​  ​ and ​  ​ the ​  ​ freedom ​  ​ to ​  ​ integrate ​  ​ science,\" ​  ​​ public ​  ​ communication ​  ​ of ​  ​ science    and ​  ​ technology ​  ​​ 25 ​  ​ (2016): ​  ​ 11.    170  ian ​  ​ kerr, ​  ​ \"the ​  ​ devil ​  ​ is ​  ​ in ​  ​ the ​  ​ defaults,\" ​  ​​ critical ​  ​ analysis ​  ​ of ​  ​ law ​ ​  ​ 4, ​  ​ no. ​  ​ 1 ​  ​ (2017).    171  dylan ​  ​ wesley ​  ​ mulvin, ​  ​ \"embedded ​  ​ dangers: ​  ​ the ​  ​ history ​  ​ of ​  ​ the ​  ​ year ​  ​ 2000 ​  ​ problem ​  ​ and ​  ​ the ​  ​ politics ​  ​ of ​  ​ technological ​  ​ repair,\"    a ​ oir ​  ​ selected ​  ​ papers ​  ​ of ​  ​ internet ​  ​ research ​ ​  ​ 6 ​  ​ (2017).    172  narendra ​  ​ kumar, ​  ​ nidhi ​  ​ kharkwal, ​  ​ rashi ​  ​ kohli ​  ​ and ​  ​ shakeeluddin ​  ​ choudhary, ​  ​ \"ethical ​  ​ aspects ​  ​ and ​  ​ future ​  ​ of ​  ​ artificial    intelligence,\" ​  ​ (in ​  ​​ innovation ​  ​ and ​  ​ challenges ​  ​ in ​  ​ cyber ​  ​ security ​  ​ (iciccs-inbush), ​  ​ 2016 ​  ​ international ​  ​ conference ​  ​ on ​ , ​  ​ pp.    111-114), ​  ​ ieee, ​  ​ 2016.  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 34    values ​  ​ are ​  ​ expressly ​  ​ designed ​  ​ into ​  ​ ai ​  ​ systems ​  ​ through ​  ​ processes ​  ​ of ​  ​ open ​  ​ and    well-documented ​  ​ decision-making ​  ​ that ​  ​ center ​  ​ the ​  ​ populations ​  ​ who ​  ​ will ​  ​ be ​  ​ most ​  ​ affected.     while ​  ​ nascent, ​  ​ efforts ​  ​ to ​  ​ address ​  ​ these ​  ​ concerns ​  ​ have ​  ​ emerged ​  ​ in ​  ​ recent ​  ​ years. ​  ​ a ​  ​ series ​  ​ of    white ​  ​ house ​  ​ reports ​  ​ under ​  ​ president ​  ​ obama ​  ​ examined ​  ​ tensions ​  ​ between ​  ​ social ​  ​ interests    and ​  ​ ethical ​  ​ values ​  ​ on ​  ​ one ​  ​ hand, ​  ​ and ​  ​ business ​  ​ and ​  ​ industry ​  ​ objectives ​  ​ on ​  ​ the ​  ​ other.  173  recent ​  ​ soft ​  ​ governance ​  ​ efforts ​  ​ from ​  ​ ieee, ​  ​ the ​  ​ future ​  ​ of ​  ​ life ​  ​ institute, ​ ​  ​ the ​  ​ acm ​  ​ and  174 175 176  the ​  ​ oxford ​  ​ internet ​  ​ institute ​  ​ have ​  ​ produced ​  ​ principles ​  ​ and ​  ​ codes ​  ​ of ​  ​ ethics ​  ​ for ​  ​ ai.  177  perspectives ​  ​ from ​  ​ diverse ​  ​ industry ​  ​ and ​  ​ intellectual ​  ​ leaders ​  ​ are ​  ​ often ​  ​ reflected ​  ​ in ​  ​ these    documents. ​  ​ while ​  ​ these ​  ​ are ​  ​ positive ​  ​ steps, ​  ​ they ​  ​ have ​  ​ real ​  ​ limitations. ​  ​ key ​  ​ among ​  ​ these ​  ​ is    that ​  ​ they ​  ​ share ​  ​ an ​  ​ assumption ​  ​ that ​  ​ industry ​  ​ will ​  ​ voluntarily ​  ​ begin ​  ​ to ​  ​ adopt ​  ​ their    approaches. ​  ​ they ​  ​ rarely ​  ​ mention ​  ​ the ​  ​ power ​  ​ asymmetries ​  ​ that ​  ​ complicate ​  ​ and ​  ​ underlie    terms ​  ​ like ​  ​ “social ​  ​ good,” ​  ​ and ​  ​ the ​  ​ means ​  ​ by ​  ​ which ​  ​ such ​  ​ a ​  ​ term ​  ​ would ​  ​ be ​  ​ defined ​  ​ and    measured. ​  ​ the ​  ​ codes ​  ​ are ​  ​ necessarily ​  ​ limited ​  ​ in ​  ​ what ​  ​ they ​  ​ address, ​  ​ how ​  ​ much ​  ​ insider    information ​  ​ they ​  ​ have ​  ​ access ​  ​ to ​  ​ and ​  ​ what ​  ​ mechanisms ​  ​ would ​  ​ be ​  ​ used ​  ​ for ​  ​ monitoring ​  ​ and    enforcement. ​ ​  ​ while ​  ​ these ​  ​ efforts ​  ​ set ​  ​ moral ​  ​ precedents ​  ​ and ​  ​ start ​  ​ conversations, ​ ​  ​ they  178 179  provide ​  ​ little ​  ​ to ​  ​ help ​  ​​  ​ practitioners ​  ​ in ​  ​ navigating ​  ​ daily ​  ​ ethical ​  ​ problems ​  ​ in ​  ​ practice ​  ​ or ​  ​ in  180  diagnosing ​  ​ ethical ​  ​ harms, ​  ​ and ​  ​ do ​  ​ little ​  ​ to ​  ​ directly ​  ​ change ​  ​ ethics ​  ​ in ​  ​ the ​  ​ design ​  ​ and ​  ​ use ​  ​ of  181  ai.  182  challenges ​  ​ and ​  ​ concerns ​  ​ going ​  ​ forward    current ​  ​ framings ​  ​ of ​  ​ ai ​  ​ ethics ​  ​ are ​  ​ failing ​  ​ partly ​  ​ because ​  ​ they ​  ​ rely ​  ​ on ​  ​ individual ​  ​ responsibility,    placing ​  ​ the ​  ​ onus ​  ​ of ​  ​ appropriate ​  ​ information ​  ​ flow ​  ​ with ​  ​ users ​  ​ and ​  ​ concentrating    decision-making ​  ​ power ​  ​ in ​  ​ individual ​  ​ ai ​  ​ developers ​  ​ and ​  ​ designers. ​  ​ in ​  ​ order ​  ​ to ​  ​ achieve  183  ethical ​  ​ ai ​  ​ systems ​  ​ in ​  ​ which ​  ​ their ​  ​ wider ​  ​ implications ​  ​ are ​  ​ addressed, ​  ​ there ​  ​ must ​  ​ be    173  united ​  ​ states ​  ​ (2016) ​  ​ executive ​  ​ office ​  ​ of ​  ​ the ​  ​ president ​  ​ and ​  ​ m. ​  ​ holden, ​  ​ j.p. ​  ​ and ​  ​ smith. ​  ​ “preparing ​  ​ for ​  ​ the ​  ​ future ​  ​ of ​  ​ artificial    intelligence,” ​  ​ technical ​  ​ report, ​  ​ national ​  ​ science ​  ​ and ​  ​ technology ​  ​ council, ​  ​ washington ​  ​ d.c. ​  ​ 20502, ​  ​ october ​  ​ 2016,    https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/nstc/preparing_for_the_futu  re_of_ai.pdf ​ .    174  ieee, ​  ​ “ethically ​  ​ aligned ​  ​ design: ​  ​ a ​  ​ vision ​  ​ for ​  ​ prioritizing ​  ​ human ​  ​ wellbeing ​  ​ with ​  ​ artificial ​  ​ intelligence ​  ​ and ​  ​ autonomous    systems,” ​  ​ the ​  ​ ieee ​  ​ global ​  ​ initiative ​  ​ for ​  ​ ethical ​  ​ considerations ​  ​ in ​  ​ artificial ​  ​ intelligence ​  ​ and ​  ​ autonomous ​  ​ systems, ​  ​ december    13, ​  ​ 2016, ​  ​​ http://standards.ieee.org/develop/indconn/ec/ead_v1.pdf ​ .    175  “asilomar ​  ​ ai ​  ​ principles,” ​  ​ 2017, ​  ​ https://futureoflife.org/ai-principles/.    176  acm, ​  ​ “code ​  ​ 2018,” ​  ​ version ​  ​ 2, ​  ​ 2017, ​  ​ https://ethics.acm.org/2018-code-draft-2/.    177  mike ​  ​ wooldridge, ​  ​ peter ​  ​ millican, ​  ​ and ​  ​ paula ​  ​ boddington, ​  ​ “towards ​  ​ a ​  ​ code ​  ​ of ​  ​ ethics ​  ​ for ​  ​ artificial ​  ​ intelligence ​  ​ research,”    oxford, ​  ​ 2017 ​  ​​ https://www.cs.ox.ac.uk/efai/towards-a-code-of-ethics-for-artificial-intelligence/ ​ .    178  bo ​  ​ brinkman, ​  ​ catherine ​  ​ flick, ​  ​ don ​  ​ gotterbarn, ​  ​ keith ​  ​ miller, ​  ​ kate ​  ​ vazansky ​  ​ and ​  ​ marty ​  ​ j. ​  ​ wolf, ​  ​ \"listening ​  ​ to ​  ​ professional    voices: ​  ​ draft ​  ​ 2 ​  ​ of ​  ​ the ​  ​ acm ​  ​ code ​  ​ of ​  ​ ethics ​  ​ and ​  ​ professional ​  ​ conduct,\" ​  ​​ communications ​  ​ of ​  ​ the ​  ​ acm ​ ​  ​ 60, ​  ​ no. ​  ​ 5 ​  ​ (2017): ​  ​ 105-111.    179  eugene ​  ​ schlossberger, ​  ​ \"engineering ​  ​ codes ​  ​ of ​  ​ ethics ​  ​ and ​  ​ the ​  ​ duty ​  ​ to ​  ​ set ​  ​ a ​  ​ moral ​  ​ precedent,\" ​  ​​ science ​  ​ and ​  ​ engineering ​  ​ ethics    22, ​  ​ no. ​  ​ 5 ​  ​ (2016): ​  ​ 1333-1344.    180  stuart ​  ​ ferguson, ​  ​ clare ​  ​ thornley ​  ​ and ​  ​ forbes ​  ​ gibb, ​  ​ \"beyond ​  ​ codes ​  ​ of ​  ​ ethics: ​  ​ how ​  ​ library ​  ​ and ​  ​ information ​  ​ professionals    navigate ​  ​ ethical ​  ​ dilemmas ​  ​ in ​  ​ a ​  ​ complex ​  ​ and ​  ​ dynamic ​  ​ information ​  ​ environment,\" ​  ​​ international ​  ​ journal ​  ​ of ​  ​ information    management ​ ​  ​ 36, ​  ​ no. ​  ​ 4 ​  ​ (2016): ​  ​ 543-556.    181  christian ​  ​ sandvig, ​  ​ kevin ​  ​ hamilton, ​  ​ karrie ​  ​ karahalios ​  ​ and ​  ​ cedric ​  ​ langbort, ​  ​ \"automation, ​  ​ algorithms, ​  ​ and ​  ​ politics| ​  ​ when ​  ​ the    algorithm ​  ​ itself ​  ​ is ​  ​ a ​  ​ racist: ​  ​ diagnosing ​  ​ ethical ​  ​ harm ​  ​ in ​  ​ the ​  ​ basic ​  ​ components ​  ​ of ​  ​ software,\" ​  ​​ international ​  ​ journal ​  ​ of    communication ​ ​  ​ 10 ​  ​ (2016): ​  ​ 19.    182  mike ​  ​ ananny, ​  ​ \"toward ​  ​ an ​  ​ ethics ​  ​ of ​  ​ algorithms: ​  ​ convening, ​  ​ observation, ​  ​ probability, ​  ​ and ​  ​ timeliness,\" ​  ​​ science, ​  ​ technology, ​  ​ &    human ​  ​ values ​ ​  ​ 41, ​  ​ no. ​  ​ 1 ​  ​ (2016): ​  ​ 93-117.    183  florencia ​  ​ marotta-wurgler, ​  ​ \"self-regulation ​  ​ and ​  ​ competition ​  ​ in ​  ​ privacy ​  ​ policies.\" ​  ​​ the ​  ​ journal ​  ​ of ​  ​ legal ​  ​ studies ​ ​  ​ 45, ​  ​ no. ​  ​ s2    (2016): ​  ​ s13-s39.  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 35    institutional ​  ​ changes ​  ​ to ​  ​ hold ​  ​ power ​  ​ accountable. ​  ​ yet, ​  ​ there ​  ​ are ​  ​ obvious ​  ​ challenges ​  ​ to ​  ​ this  184  approach, ​  ​ such ​  ​ as ​  ​ disagreement ​  ​ about ​  ​ the ​  ​ risks ​  ​ of ​  ​ ai, ​  ​ the ​  ​ potential ​  ​ for ​  ​ greenwashing  185  ethical ​  ​ ai ​  ​ as ​  ​ a ​  ​ superficial ​  ​ marketing ​  ​ strategy ​  ​ rather ​  ​ than ​  ​ a ​  ​ substantive ​  ​ commitment, ​  ​ the  186  practical ​  ​ challenges ​  ​ of ​  ​ stopping ​  ​ unethical ​  ​ ai ​  ​ research ​  ​ designed ​  ​ to ​  ​ privilege ​  ​ the ​  ​ interests ​  ​ of    a ​  ​ few ​  ​ over ​  ​ the ​  ​ many ​  ​ and ​  ​ the ​  ​ current ​  ​ economic ​  ​ system ​  ​ within ​  ​ which ​  ​ the ​  ​ incentives  187  driving ​  ​ ai ​  ​ development ​  ​ are ​  ​ embedded. ​  ​ in ​  ​ addition, ​  ​ the ​  ​ effective ​  ​ invisibility ​  ​ of ​  ​ many ​  ​ of    these ​  ​ systems ​  ​ to ​  ​ the ​  ​ people ​  ​ on ​  ​ whom ​  ​ they ​  ​ act, ​  ​ the ​  ​ obscurity ​  ​ of ​  ​ their ​  ​ algorithmic    mechanisms, ​  ​ the ​  ​ ambiguity ​  ​ of ​  ​ their ​  ​ origins ​  ​ and ​  ​ their ​  ​ inescapable ​  ​ pervasiveness ​  ​ make    public ​  ​ discourse ​  ​ difficult ​  ​ and ​  ​ opting-out ​  ​ impossible. ​  ​ the ​  ​ responsibility ​  ​ to ​  ​ strive ​  ​ for ​  ​ better  188  outcomes ​  ​ thus ​  ​ falls ​  ​ squarely ​  ​ on ​  ​ creators ​  ​ and ​  ​ regulators, ​  ​ who ​  ​ are ​  ​ only ​  ​ beginning ​  ​ to    establish ​  ​ dialogue ​  ​ even ​  ​ as ​  ​ there ​  ​ are ​  ​ few ​  ​ incentives ​  ​ for ​  ​ change ​  ​ and ​  ​ significant ​  ​ tension  189  between ​  ​ ethics ​  ​ and ​  ​ “compliance.”  190  this ​  ​ brings ​  ​ us ​  ​ to ​  ​ the ​  ​ wider ​  ​ political ​  ​ landscape ​  ​ in ​  ​ which ​  ​ ai ​  ​ is ​  ​ being ​  ​ created ​  ​ in ​  ​ the ​  ​ u.s.: ​  ​ how    will ​  ​ the ​  ​ trump ​  ​ administration ​  ​ affect ​  ​ the ​  ​ use ​  ​ of ​  ​ these ​  ​ technologies? ​  ​ prior ​  ​ to ​  ​ the ​  ​ election,    over ​  ​ 100 ​  ​ technology ​  ​ sector ​  ​ leaders ​  ​ articulated ​  ​ their ​  ​ priorities: ​  ​ “freedom ​  ​ of ​  ​ expression,    openness ​  ​ to ​  ​ newcomers, ​  ​ equality ​  ​ of ​  ​ opportunity, ​  ​ public ​  ​ investments ​  ​ in ​  ​ research ​  ​ and    infrastructure ​  ​ and ​  ​ respect ​  ​ for ​  ​ the ​  ​ rule ​  ​ of ​  ​ law. ​  ​ we ​  ​ embrace ​  ​ an ​  ​ optimistic ​  ​ vision ​  ​ for ​  ​ a ​  ​ more    inclusive ​  ​ country ​  ​ where ​  ​ american ​  ​ innovation ​  ​ continues ​  ​ to ​  ​ fuel ​  ​ opportunity, ​  ​ prosperity ​  ​ and    leadership.” ​ ​  ​ president ​  ​ trump’s ​  ​ policies ​  ​ do ​  ​ not ​  ​ reflect ​  ​ these ​  ​ priorities. ​  ​ rather, ​  ​ there ​  ​ has  191  been ​  ​ significant ​  ​ defunding ​  ​ of ​  ​ research, ​  ​ an ​  ​ increase ​  ​ in ​  ​ deportations, ​  ​ and ​  ​ heightened    screening ​  ​ of ​  ​ personal ​  ​ communications ​  ​ and ​  ​ social ​  ​ media ​  ​ at ​  ​ national ​  ​ borders, ​  ​ among ​  ​ many    other ​  ​ concerning ​  ​ policy ​  ​ shifts. ​  ​ simply ​  ​ put, ​  ​ it ​  ​ does ​  ​ not ​  ​ appear ​  ​ that ​  ​ the ​  ​ current    administration ​  ​ can ​  ​ be ​  ​ counted ​  ​ on ​  ​ to ​  ​ support ​  ​ the ​  ​ creation ​  ​ and ​  ​ adoption ​  ​ of ​  ​ ethical    frameworks ​  ​ for ​  ​ ai.        184  ira ​  ​ s. ​  ​ rubinstein, ​  ​ “the ​  ​ future ​  ​ of ​  ​ self-regulation ​  ​ is ​  ​ co-regulation” ​  ​ (october ​  ​ 5, ​  ​ 2016). ​  ​ the ​  ​ cambridge ​  ​ handbook ​  ​ of ​  ​ consumer    privacy, ​  ​ from ​  ​ cambridge ​  ​ university ​  ​ press ​  ​ (forthcoming). ​  ​ available ​  ​ at ​  ​ ssrn: ​  ​​ https://ssrn.com/abstract=2848513 ​ .     185  vincent ​  ​ c ​  ​ müller, ​  ​ ed. ​  ​​ risks ​  ​ of ​  ​ artificial ​  ​ intelligence ​ . ​  ​ crc ​  ​ press, ​  ​ 2016.    186  michael ​  ​ stocker, ​  ​ \"decision-making: ​  ​ be ​  ​ wary ​  ​ of ​  ​ 'ethical' ​  ​ artificial ​  ​ intelligence,\" ​  ​​ nature ​ ​  ​ 540, ​  ​ no. ​  ​ 7634 ​  ​ (2016): ​  ​ 525-525.    187  federico ​  ​ pistono ​  ​ and ​  ​ roman ​  ​ v. ​  ​ yampolskiy, ​  ​ \"unethical ​  ​ research: ​  ​ how ​  ​ to ​  ​ create ​  ​ a ​  ​ malevolent ​  ​ artificial ​  ​ intelligence,\" ​  ​​ arxiv    preprint ​  ​ arxiv:1605.02817 ​ ​  ​ (2016).    188  jatin ​  ​ borana, ​  ​ \"applications ​  ​ of ​  ​ artificial ​  ​ intelligence ​  ​ & ​  ​ associated ​  ​ technologies.\" ​  ​​ proceeding ​  ​ of ​  ​ international ​  ​ conference ​  ​ on    emerging ​  ​ technologies ​  ​ in ​  ​ engineering, ​  ​ biomedical, ​  ​ management ​  ​ and ​  ​ science ​  ​ [etebms-2016] ​ ​  ​ 5 ​  ​ (2016): ​  ​ 64-67; ​  ​ ira ​  ​ s.    rubinstein, ​  ​ \"big ​  ​ data: ​  ​ the ​  ​ end ​  ​ of ​  ​ privacy ​  ​ or ​  ​ a ​  ​ new ​  ​ beginning?.\" ​  ​​ international ​  ​ data ​  ​ privacy ​  ​ law ​ ​  ​ (2013): ​  ​ ips036; ​  ​ fred ​  ​ turner.    \"can ​  ​ we ​  ​ write ​  ​ a ​  ​ cultural ​  ​ history ​  ​ of ​  ​ the ​  ​ internet? ​  ​ if ​  ​ so, ​  ​ how?.\" ​  ​ internet ​  ​ histories ​  ​ 1, ​  ​ no. ​  ​ 1-2 ​  ​ (2017): ​  ​ 39-46.    189  kate ​  ​ darling ​  ​ and ​  ​ ryan ​  ​ calo, ​  ​ \"introduction ​  ​ to ​  ​ journal ​  ​ of ​  ​ human-robot ​  ​ interaction ​  ​ special ​  ​ issue ​  ​ on ​  ​ law ​  ​ and ​  ​ policy,\" ​  ​ journal    of ​  ​ human-robot ​  ​ interaction ​  ​ 5, ​  ​ no. ​  ​ 3 ​  ​ (2016): ​  ​ 1-2.    190  kate ​  ​ crawford ​  ​ and ​  ​ ryan ​  ​ calo, ​  ​ \"there ​  ​ is ​  ​ a ​  ​ blind ​  ​ spot ​  ​ in ​  ​ ai ​  ​ research,\" ​  ​​ nature ​ ​  ​ 538 ​  ​ (2016): ​  ​ 311-313; ​  ​ gary ​  ​ e ​  ​ merchant ​  ​ and    wendell ​  ​ wallach, ​  ​​ emerging ​  ​ technologies: ​  ​ ethics, ​  ​ law ​  ​ and ​  ​ governance, ​ ​  ​ (ashgate ​  ​ publishing, ​  ​ 2017).    191  marvin ​  ​ ammori, ​  ​ adrian ​  ​ aoun, ​  ​ greg ​  ​ badros, ​  ​ clayton ​  ​ banks, ​  ​ phin ​  ​ barnes, ​  ​ niti ​  ​ bashambu, ​  ​ et ​  ​ al., ​  ​ “an ​  ​ open ​  ​ letter ​  ​ from    technology ​  ​ sector ​  ​ leaders ​  ​ on ​  ​ donald ​  ​ trump’s ​  ​ candidacy ​  ​ for ​  ​ president,” ​  ​ 2016,    https://shift.newco.co/an-open-letter-from-technology-sector-leaders-on-donald-trumps-candidacy-for-president-5bf734c1  59e4 ​ .  \n",
      "      ai ​  ​ now ​  ​ 2017 ​  ​ report 36       conclusion    ai ​  ​ systems ​  ​ are ​  ​ now ​  ​ being ​  ​ adopted ​  ​ across ​  ​ multiple ​  ​ sectors, ​  ​ and ​  ​ the ​  ​ social ​  ​ effects ​  ​ are    already ​  ​ being ​  ​ felt: ​  ​ so ​  ​ far, ​  ​ the ​  ​ benefits ​  ​ and ​  ​ risks ​  ​ are ​  ​ unevenly ​  ​ distributed. ​  ​ too ​  ​ often, ​  ​ those    effects ​  ​ simply ​  ​ happen, ​  ​ without ​  ​ public ​  ​ understanding ​  ​ or ​  ​ deliberation, ​  ​ led ​  ​ by ​  ​ technology    companies ​  ​ and ​  ​ governments ​  ​ that ​  ​ are ​  ​ yet ​  ​ to ​  ​ understand ​  ​ the ​  ​ broader ​  ​ implications ​  ​ of ​  ​ their    technologies ​  ​ once ​  ​ they ​  ​ are ​  ​ released ​  ​ into ​  ​ complex ​  ​ social ​  ​ systems. ​  ​ we ​  ​ urgently ​  ​ need    rigorous ​  ​ research ​  ​ that ​  ​ incorporates ​  ​ diverse ​  ​ disciplines ​  ​ and ​  ​ perspectives ​  ​ to ​  ​ help ​  ​ us ​  ​ measure    and ​  ​ understand ​  ​ the ​  ​ short ​  ​ and ​  ​ long-term ​  ​ effects ​  ​ of ​  ​ ai ​  ​ across ​  ​ our ​  ​ core ​  ​ social ​  ​ and ​  ​ economic    institutions.     fortunately, ​  ​ more ​  ​ researchers ​  ​ are ​  ​ turning ​  ​ to ​  ​ these ​  ​ tasks ​  ​ all ​  ​ the ​  ​ time. ​  ​ but ​  ​ research ​  ​ is ​  ​ just    the ​  ​ beginning. ​  ​ advocates, ​  ​ members ​  ​ of ​  ​ affected ​  ​ communities ​  ​ and ​  ​ those ​  ​ with ​  ​ practical    domain ​  ​ expertise ​  ​ should ​  ​​  ​ be ​  ​ included ​  ​ at ​  ​ the ​  ​ center ​  ​ of ​  ​ decision ​  ​ making ​  ​ around ​  ​ how ​  ​ ai ​  ​ is    deployed, ​  ​ assessed ​  ​ and ​  ​ governed. ​  ​ processes ​  ​ must ​  ​ be ​  ​ developed ​  ​ to ​  ​ accommodate ​  ​ and ​  ​ act    on ​  ​ these ​  ​ perspectives, ​  ​ which ​  ​ are ​  ​ traditionally ​  ​ far ​  ​ removed ​  ​ from ​  ​ engineering ​  ​ and ​  ​ product    development ​  ​ practices. ​  ​ there ​  ​ is ​  ​ a ​  ​ pressing ​  ​ need ​  ​ now ​  ​ to ​  ​ understand ​  ​ these ​  ​ technologies ​  ​ in    the ​  ​ context ​  ​ of ​  ​ existing ​  ​ social ​  ​ systems, ​  ​ to ​  ​ connect ​  ​ technological ​  ​ development ​  ​ to ​  ​ social ​  ​ and    political ​  ​ concerns, ​  ​ to ​  ​ develop ​  ​ ethical ​  ​ codes ​  ​ with ​  ​ force ​  ​ and ​  ​ accountability, ​  ​ to ​  ​ diversify ​  ​ the    field ​  ​ of ​  ​ ai ​  ​ and ​  ​ to ​  ​ integrate ​  ​ diverse ​  ​ social ​  ​ scientific ​  ​ and ​  ​ humanistic ​  ​ research ​  ​ practices ​  ​ into    the ​  ​ core ​  ​ of ​  ​ ai ​  ​ development. ​  ​ only ​  ​ then ​  ​ can ​  ​ the ​  ​ ai ​  ​ industry ​  ​ ensure ​  ​ that ​  ​ its ​  ​ decisions ​  ​ and    practices ​  ​ are ​  ​ sensitive ​  ​ to ​  ​ the ​  ​ complex ​  ​ social ​  ​ domains ​  ​ into ​  ​ which ​  ​ these ​  ​ technologies ​  ​ are    rapidly ​  ​ moving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:27<00:00, 27.31s/it]\n"
     ]
    }
   ],
   "source": [
    "metricis = {'Précision': 0,\n",
    "            'Rappel': 0,\n",
    "            'F-mesure': 0,\n",
    "            'Spécificité': 0,\n",
    "            'Taux de faux positifs': 0}\n",
    "documents_count = 0\n",
    "for file in tqdm(glob.glob(os.path.join(os.getenv('TXT_FOLDER2'), \"*.txt\"))[1:2]):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read().strip().lower()\n",
    "        if len(doc) > 10:\n",
    "            documents_count += 1\n",
    "            documents = sent_tokenize(doc)\n",
    "            bert = Bert(documents=documents)\n",
    "            bert.processing()\n",
    "            top_doc_indices_bert, resume_doc_bert = bert.get_resume(top_n=1)\n",
    "\n",
    "            preprocessing = Processing(\n",
    "                log_file=\"\", corpus_dir=\"processed\", documents=documents)\n",
    "            preprocessing.process_corpus()\n",
    "            top_doc_indices_keywords, resume_doc_keywords = preprocessing.get_resume_docs(\n",
    "                top_n=1, number_key_words=30)\n",
    "            P_bert, R_bert, F1_bert = bert.get_bert_score(\n",
    "                resume_doc_bert[0], doc)\n",
    "            P_keyword, R_keyword, F1_key_word = bert.get_bert_score(\n",
    "                resume_doc_keywords[0], doc)\n",
    "            print(f\"Précision des deux méthode vaut; Méthode de Keyword: {\n",
    "                  P_keyword}, Méthode Bert: {P_bert}\")\n",
    "            print(f\"Le Rappel des deux méthode vaut; Méthode Keyword {\n",
    "                  R_keyword}, La méthode Bert : {R_bert}\")\n",
    "            print(f\"La F-mesure des deux méthode vaut; Méthode Keyword: {\n",
    "                  F1_key_word}, Méthode Bert: {F1_bert}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \"\"\" metricis_calcul = calculate_metrics(top_indices, resume_indeces)\n",
    "            metricis['Précision'] += metricis_calcul['Précision']\n",
    "            metricis['Rappel'] += metricis_calcul['Rappel']\n",
    "            metricis['F-mesure'] += metricis_calcul['F-mesure']\n",
    "            metricis['Spécificité'] += metricis_calcul['Spécificité'] \"\"\"\n",
    "\n",
    "            \"\"\"  P_keyword, R_keyword, F1_key_word = bert.get_bert_score(\n",
    "                resume_doc_keywords[0], doc)\n",
    "            P_bert, R_bert, F1_bert = bert.get_score(resume_doc_bert[0], doc)\n",
    "            print(f\"Précision des deux méthode vaut; Méthode de Keyword: {\n",
    "                  P_keyword}, Méthode Bert: {P_bert}\")\n",
    "            print(f\"Le Rappel des deux méthode vaut; Méthode Keyword {\n",
    "                  R_keyword}, La méthode Bert : {R_bert}\")\n",
    "            print(f\"La F-mesure des deux méthode vaut; Méthode Keyword: {\n",
    "                  F1_key_word}, Méthode Bert: {F1_bert}\") \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" metricis['Précision'] /= documents_count\n",
    "metricis['Rappel'] /= documents_count\n",
    "metricis['F-mesure'] /= documents_count\n",
    "metricis['Spécificité'] /= documents_count\n",
    "\n",
    "print(metricis) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp-nlp-R8Mk3SIc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
