
 This  report  was  prepared  by  the  Montreal  AI  Ethics  Institute  (MAIEI)  —  an  international  non-profit   organization  democratizing  AI  ethics  literacy.  Learn  more  on  our  website  or  subscribe  to  our  weekly   newsletter  The AI Ethics Brief  .   This work is licensed open-access under a  Creative  Commons Attribution 4.0 International License  .   Primary contact for the report:  Abhishek Gupta (  abhishek@montrealethics.ai  )   Full team behind the report:   Special thanks to our contributor network:   Ramya  Srinivasan,  Jonas  Schuett,  Jimmy  Huang,  Robert  de  Neufville,  Natalie  Klym,  Andrea  Pedeferri,   Andrea  Owe,  Nga  Than,  Khoa  Lam,  Angshuman  Kaushik,  Avantika  Bhandari,  Sarah  P.  Grant,  Anne  Boily,   Philippe Dambly, Axel Beelen, Laird Gallaghar, Ravit Dotan, Sean McGregor, and Azfar Adib.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1   
 Table of Contents   Founder ’s Not e  9   1. Wha t we’re thinking b y MAIEI St aff and Community   12   Introduction b y Abhishek Gup ta, Founder and Principal R esear cher , Mon treal AI E thics    Institut e  12   From the F ounder ’s Desk   15   Patterns of pr actice will be fundamen tal to the success of AI g overnance   15   How to build an AI e thics t eam a t your or ganization?   16   Office Hour s  19   Exploring the under -explor ed ar eas in t eaching t ech e thics t oday  19   AI Applic ation Spotligh t  21   Jake Elw es: Cons tructing and Dec onstructing Gender with AI-Gener ated Art   21   Will an Artificial In telliche f be Cooking Y our Ne xt Meal a t a Michelin St ar Restaurant?  24   Permission t o be uncert ain  27   The T echnologis ts ar e Not in Con trol: Wha t the In terne t Experience Can T each us about    AI Ethics and R esponsibility   27   Fusing Art and Engineering f or a mor e Humane T ech Futur e  33   Sociology of AI E thics   42   Challeng es of AI De velopmen t in Vie tnam: Funding , Talen t and E thics   42   Other   46   Analy sis of the “ Artificial In telligence g overnance principles: t owards ethical and    trustworth y artificial in telligence in the Eur opean insur ance sect or”  46   The Pr oliferation of AI E thics Principles: Wha t’s Ne xt?  48   Represen tation and Imagina tion f or Pr eventing AI Harms   53   Evolution in Ag e-Verific ation Applic ations: Can AI Open Some Ne w Horiz ons?   55   Managing Human and R obots T ogether – Can Tha t Be a Leader ship Dilemma?   58   “Welcome t o AI”; a t alk giv en to the Mon treal In tegrity Ne twork  61   2. Analy sis of the AI E cosystem  66   Introduction b y Connor W right, Partner ships Manag er, Mon treal AI E thics Ins titut e  66   Go Deep: R esear ch Summaries   69   The V alues Enc oded in Machine Learning R esear ch  69   Comba ting An ti-Blackness in the AI Community   71   Achie ving a ‘ Good AI Socie ty’: Comparing the Aims and Pr ogress of the EU and the US   73   Responsible Use of T echnology: The IBM Case Study   76   U.S.-EU T rade and T echnology Council Inaugur al Join t Statemen t – A look in to wha t’s in   store for AI?   79   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2   
 Public Str ategies f or Artificial In telligence: Which V alue Driv ers?  82   Artificial In telligence: the global landsc ape of e thics guidelines   83   UK’s roadmap t o AI supr emacy: Is the ‘ AI W ar’ hea ting up?   86   Putting AI e thics t o work: ar e the t ools fit f or purpose?   89   UNE SCO’s Recommenda tion on the E thics of AI   92   Go Wide: Article Summaries (summariz ed b y Abhishek Gup ta)  97   Americ a's global leader ship in human-cen tered AI c an't c ome fr om indus try alone   97   Training self -driving c ars for $1 an hour   97   How Da ta Brokers Sell Access t o the Backbone of the In terne t  98   How ne w regula tion is driving the AI g overnance mark et  99   How open-sour ce softw are shapes AI policy   100   AI indus try, obsessed with speed, is loa the t o consider the ener gy cost in la test MLP erf   benchmark   101   Facebook R olls Out Ne ws Feed Chang e Tha t Block s Watchdogs fr om Ga thering Da ta  101   Ther e's a Multibillion-Dollar Mark et for Your Phone's Loc ation Da ta  102   Chinese AI g ets ethical guidelines f or the fir st time, aligning with Beijing ’s goal of r eining    in Big T ech  103   3. Priv acy  105   Introduction b y Idoia Salaz ar, Cof ounder , Observatory of the Social and E thical Impact of    Artificial In telligence   105   Go Wide: Article Summaries (summariz ed b y Abhishek Gup ta)  110   The Limits of Dif ferential Priv acy (and Its Misuse in Da ta Release and Machine Learning)    110   Should F amilies’ Sur veillance Camer as Be Allo wed in Nur sing Homes?   110   Huge da ta leak sha tters the lie tha t the innocen t need not f ear sur veillance   111   The Ine vitable W eaponiz ation of App Da ta Is Her e  112   Amaz on will pa y you $10 in cr edit f or your palm prin t biome trics   112   Apple W alks a Priv acy Tigh trope t o Spot Child Abuse in iCloud   113   Privacy in the Br ain: The E thics of Neur otechnology   114   This is the r eal s tory of the Af ghan biome tric da tabases abandoned t o the T aliban   115   The Do wnside t o Sur veilling Y our Neighbor s  116   Leak ed Documen ts Sho w Ho w Amaz on's As tro Robot T racks Everything Y ou Do   116   “The po wer to sur veil, c ontrol, and punish”: The dy stopian dang er of a manda tory   biome tric da tabase in Me xico  117   The P opular F amily Sa fety App Lif e360 Is Selling Pr ecise Loc ation Da ta on Its T ens of    Millions of User s  118   Singapor e’s tech-ut opia dr eam is turning in to a sur veillance s tate nigh tmar e  119   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   3   
 4. Bias   121   Introduction b y Abhishek Gup ta, Founder and Principal R esear cher , Mon treal AI E thics    Institut e  121   Go Deep: R esear ch Summaries   123   “A Proposal f or Iden tifying and Managing Bias in Artificial In telligence” . A dr aft from the    NIST  123   Co-Designing Checklis ts to Under stand Or ganizational Challeng es and Opportunities    around F airness in AI   125   Go Wide: Article Summaries (summariz ed b y Abhishek Gup ta)  130   How TikT ok’s hate speech de tection t ool se t off a deba te about r acial bias on the app   130   We tested AI in terview tools. Her e’s wha t we found.   130   How Humans Can F orce the Machines t o Pla y Fair  131   The Secr et Bias Hidden in Mortg age-Appr oval Alg orithms   132   Facebook Apologiz es Aft er A.I. Puts ‘Prima tes’ Label on Video of Black Men   133   Minority V oices ‘Filt ered’ Out of Google Na tural Languag e Processing Models   134   Facebook, Citing Socie tal Concerns, Plans t o Shut Do wn F acial R ecognition S ystem  135   Uncovering bias in sear ch and r ecommenda tions   135   5. Social Media and Pr oblema tic In forma tion  137   Introduction b y Abhishek Gup ta, Founder and Principal R esear cher , Mon treal AI E thics    Institut e  137   Go Wide: Article Summaries (summariz ed b y Abhishek Gup ta)  139   The W orld Needs Deep fake Experts t o Stem This Chaos   139   After Repea tedly Pr omising Not t o, Facebook K eeps Recommending P olitic al Gr oups to   Its User s  139   Targeted ads isola te and divide us e ven when the y’re not politic al – ne w resear ch  140   Facebook T ells Biden: ‘F acebook Is Not the R eason’ V accina tion Goal W as Missed   141   To Figh t Vaccine Lies, Authorities R ecruit an ‘In fluencer Arm y’  142   Let’s Keep the V accine Misin forma tion Pr oblem in P erspectiv e  142   Troll farms r eached 140 million Americ ans a mon th on F acebook be fore 2020 election,    internal r eport sho ws  143   The F acebook whis tleblo wer sa ys its alg orithms ar e dang erous. Her e’s wh y.  144   How W e Investigated Facebook’ s Mos t Popular Con tent  145   How social media c ompanies help Afric an governmen ts abuse “ disin forma tion la ws” to   target critics   146   The Me taverse Is Mark Zuck erber g’s Mobile Do-Ov er  147   How Facebook and Google fund global misin forma tion  147   China’ s queer in terne t is being er ased   148   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   4   
 6. AI Design and Go vernance   150   Introduction b y Michael Klenk, Assis tant Professor , Philosoph y, Delft Univ ersity of    Technology   150   Go Deep: R esear ch Summaries   153   Experts Doub t Ethical AI Design Will Be Br oadly Adop ted as the Norm Within the Ne xt   Decade  153   The Logic of Str ategic Asse ts: Fr om Oil t o AI  156   Corpor ate Go vernance of Artificial In telligence in the Public In terest  158   AI Certific ation: Adv ancing E thical Pr actice b y Reducing In forma tion As ymme tries   160   Collectiv e Action on Artificial In telligence: A Primer and R eview  161   AI Ethics Ma turity Model   164   Mapping v alue sensitiv e design on to AI f or social g ood principles   166   Embedding V alues in Artificial In telligence (AI) S ystems  169   Mor al consider ation of nonhumans in the e thics of artificial in telligence   171   Governance of artificial in telligence   174   Avoiding an Oppr essiv e Futur e of Machine Learning: A Design Theor y for Emancipa tory   Assis tants  177   Against Interpr etability: a Critic al Ex amina tion  181   Transpar ency as design publicity: e xplaining and jus tifying inscrut able alg orithms   183   Ethics-based auditing of aut oma ted decision-making s ystems: in tervention poin ts and    policy implic ations   185   Trustworthiness of Artificial In telligence   188   Getting fr om Commitmen t to Con tent in AI and Da ta Ethics: Jus tice and Explainability   190   Founda tions f or the futur e: ins titution building f or the purpose of artificial in telligence    governance   194   Go Wide: Article Summaries (summariz ed b y Abhishek Gup ta)  198   Five Recommenda tions F or Cr eating Mor e Ethical AI   198   Police Ar e Telling ShotSpot ter to Alt er Evidence Fr om Gunshot -Detecting AI   198   Optimizing P eople Y ou Ma y Know (P YMK) f or equity in ne twork cr eation  199   AI da tasets ar e prone t o mismanag emen t, study finds   200   Apple sa ys collision in child-abuse hashing s ystem is not a c oncern   201   Dele ting une thical da ta sets isn’t g ood enough   201   Six Essen tial Elemen ts Of A R esponsible AI Model   202   Why you should hir e a chie f AI e thics of ficer   203   Thinking Thr ough the E thics of Ne w Tech … Before Ther e’s a Pr oblem   204   7. La ws and R egula tions   205   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   5   
 Introduction b y Abhishek Gup ta, Founder and Principal R esear cher , Mon treal AI E thics    Institut e  205   Go Deep: R esear ch Summaries   207   The Eur opean Commission’ s Artificial In telligence Act (St anford HAI P olicy Brie f)  207   Algorithmic acc ountability f or the public sect or  209   Summoning a Ne w Artificial In telligence P atent Model: In the Ag e of P andemic   211   NATO Artificial In telligence Str ategy  214   Go Wide: Article Summaries (summariz ed b y Abhishek Gup ta)  218   Judg e Thr ows Out 2 An titrus t Cases Ag ainst Facebook   218   To regula te AI, tr y pla ying in a sandbo x  218   Wha t Is Congr ess’s Plan t o Crack Do wn on Big T ech?   219   Analy zing the Leg al Implic ations of GitHub Copilot   220   Should Do xing Be Illeg al?  221   We need c oncr ete protections fr om artificial in telligence thr eatening human righ ts  221   Americ ans Need a Bill of Righ ts for an AI-P owered W orld  222   Applying arms-c ontrol frame works to aut onomous w eapons   223   Europe w ants to champion human righ ts. So wh y doesn’t it police biased AI in    recruiting?   224   Thousands of Geof ence W arrants Appear t o Be Missing fr om a Calif ornia DO J   Transpar ency Da tabase   224   8. Trends   226   Introduction b y Abhishek Gup ta, Founder and Principal R esear cher , Mon treal AI E thics    Institut e  226   Go Deep: R esear ch Summaries   228   Machines as t eamma tes: A r esear ch ag enda on AI in t eam c ollabor ation  228   Digit al transforma tion and the r enewal of social theor y: Unpacking the ne w fraudulen t   myths and misplaced me taphor s  229   AI Ethics: En ter the Dr agon!  231   Balancing Da ta Utility and Con fiden tiality in the 2020 US Census   235   Go Wide: Article Summaries (summariz ed b y Abhishek Gup ta)  239   DeepMind A GI paper adds ur gency t o ethical AI   239   The E thics of a Deep fake An thon y Bour dain V oice  239   Founda tion models risk e xacerba ting ML ’s ethical challeng es  240   Now Tha t Machines Can Learn, Can The y Unlearn?   241   Even e xperts ar e too quick t o rely on AI e xplana tions, s tudy finds   241   Stopping Deep fake Voices   242   The Thir d Revolution in W arfare  243   Everyone will be able t o clone their v oice in the futur e  244   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   6   
 The pandemic is t esting the limits of f ace r ecognition   244   Three pr edictions f or the futur e of r esponsible t echnology   245   A tin y tw eak t o Zoma to’s alg orithm led t o los t deliv ery rider s, stolen bik es and missed    wages  246   Small Da ta Are Also Crucial f or Machine Learning   247   How Big T ech Is Pit ching Digit al Elder Car e to Families   248   How Alibaba tr acks China’ s deliv ery driv ers  248   Curr ent AI Pr actices Could Be Enabling a Ne w Gener ation of Cop yrigh t Trolls  249   The Futur e of Digit al Assis tants is Queer   250   Robots W on’t Close the W arehouse W orker Gap An ytime Soon   251   9. Outside the bo xes  252   Introduction b y Kathy Baxter, Principal Ar chitect, E thical AI Pr actice, Sales force  252   From our e vents  256   Top 10 T akeaways from our Con versation with Sales force about Con versational AI   256   Top 5 t akeaways from our c onversation with I2AI on AI in dif ferent national c ontexts  259   Our T op-5 t akeaways from our mee tup “Pr otecting the E cosystem: AI, Da ta and    Algorithms”   261   Go Deep: R esear ch Summaries   264   Building Bridg es: Gener ative Artw orks to Explor e AI E thics   264   Brave: wha t it means t o be an AI E thicis t  266   You cannot ha ve AI e thics without e thics   268   Implic ations of the use of artificial in telligence in public g overnance: A s ystema tic   literature review and a r esear ch ag enda   270   Animism, Rinri, Moderniz ation; the Base of Japanese R obotics   273   Who is a fraid of black bo x alg orithms? On the epis temologic al and e thical basis of trus t   in medic al AI   275   Anthropomorphic in teractions with a r obot and r obot -like agent  278   Ubun tu’s Implic ations f or Philosophic al Ethics   280   Risk and T rust Perceptions of the Public of Artificial In telligence Applic ations   282   The E thics of Sus tainability f or Artificial In telligence   284   Go Wide: Article Summaries (summariz ed b y Abhishek Gup ta)  288   The hack er who spen t a y ear r eclaiming his f ace fr om Clear view AI   288   An Artificial In telligence Helped W rite This Pla y. It Ma y Con tain Racism   289   The St ealth y iPhone Hack s Tha t Apple Still Can't St op  289   The limit ations of AI sa fety tools  290   AI fake-face g ener ators can be r ewound t o reveal the r eal f aces the y trained on   291   Wha t Apple’ s Ne w Repair Pr ogram Means f or You (And Y our iPhone)   292   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   7   
 Support Our W ork  294   *Not e:  The  original   sour ces  are  linked  under   the  title   of  each   piece.   The  work  in  the  following   pages   combines   summaries   of  the  original   material    supplemen ted  with   insigh ts  from  MAIEI   resear ch  staff,   unless other wise indic ated.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   8   
 Founder ’s Not e   Welcome   to  2022!   (if  you’re  reading   this  report   at  the  time   of  the  release,   or  hello   to  you  in  the   futur e!)  2021   was  a  year  that  showed  us  a  lot  of  things.   We  came   together   as  socie ty  to  fight   one  of  the  toughes t  battles  of  our  time:   the  COVID-19   pandemic   that  reshaped   our  world  and   socie ty.  Oper ation  Warp  Speed   helped   us  get  vaccines   and  distribution   around   the  world   worked  in  overdrive  to  bring   elevated  safety  against  this  pandemic.   Yet,  so  much   more  needs   to   be  done   if  we’re  to  beat  this  back   and  go  back   to  the  world  as  it  was  before.  At  the  same   time,    technologic al  progress  didn’t   pause.   In  mos t  cases,   it  acceler ated.  This  warrants  continued    examina tion  to  ensur e  that  this  technology   doesn’t   insert   itself   into  our  socie ty  and  lives  in  a   way tha t is disc ordant with our v alues.    Now  in  its  sixth  cycle,  this  edition   of  the  State  of  AI  Ethics   Report   comes   to  you  with   a  wide    array  of  topics   and  contributions   from  leading   lights  in  the  field.   For  the  first  time,   we  have  a   Spanish   text  contribution   in  the  report   in  our  endea vor  to  produce   multilingual   content  for  the   community   to  consume.   We’ve  added   a  new  chap ter  on  Trends   that  highligh ts  subtle  and  not  so   subtle  chang es  taking   place   in  the  AI  ethics   landsc ape.   This  one  is  a  mus t-read  for  anyone  who   is   planning   on  bring   AI  ethics   meaningfully   into  their   organizations,   or  pursuing   resear ch  and   looking f or ideas on which ar eas t o mak e an impact in.    As  always,  we’ve  got  our  Wha t  we’re  thinking   section   that  brings   to  you  original   contributions    and  essa ys  diving   into  areas  like  How  to  build   an  AI  ethics   team   at  your  organization?   to  other    subjects   like  Cons tructing   and  Deconstructing   Gender   with   AI-gener ated  art  .  We  cover  other    ideas   in  this  chap ter  detailing   developmen ts  around   the  world  such   as  chang es  in  talen t,   funding ,  and  ethics   as  AI  developmen t  picks  up  in  Vietnam.   In-dep th  interviews  with   indus try   experts   to  under stand  wha t  it  takes  to  bring   AI  ethics   effectiv ely  into  an  organization’ s  practices    are  supplemen ted  by  interviews  with   educ ators  who   are  working   hard  to  bring   AI  ethics    educ ation  into  classr ooms   around   the  world.   They  yield   insigh ts  for  anyone  interested  in  either    building   training   programs  at  a  corpor ation  or  those   who   have  studen ts  coming   to  them   with    ques tions   about   AI  and  want  to  have  structur ed  courses  to  guide   them   on  this  journe y  of   building   Responsible   AI  systems.   A  few  other   pieces   mak e  sure  though   that  the  discussions    don’t   just  focus   on  principles   but  also  practic al  advice   such   as  The  Proliferation  of  AI  Ethics    Principles: Wha t’s Ne xt?  that analy zes wha t the g aps  are be tween principles and pr actice t oday.   Another   new  addition   to  the  report   is  the  Analy sis  of  the  AI  Ecosystem  chap ter  that  has  the   goal  of  taking   a  meta-level  appr oach   to  under standing   the  dynamics   at  play  in  the  field,    including   pieces   like  The  Values   Encoded   in  Machine   Learning   Resear ch  and  Putting  AI  ethics    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   9   
 to  work:  are  the  tools   fit  for  purpose?   We  also  cover  various   AI  regula tions   that  are  in   developmen t  around   the  world,   looking   at  those   coming   out  of  the  EU,  US,  NATO,  UK,  and   UNE SCO.   Privacy  ,  Bias   ,  and  Social   Media   and  Problema tic  Informa tion  also  have  a  presence   as  chap ters   in  this  report.   They  continue   to  remain   signific ant  areas  within   Responsible   AI  and  cover  a  lot  of   ground   (though   it  is  impossible   to  be  exhaus tive  even  in  a  ~300   page  report).   We’ve  worked   hard  to  curate  those   pieces   that  go  beyond  wha t  is  mos t  often  covered  and  pieces   that  we   though t  deser ved  a  bit  more  attention  for  their   particular   lens  on  the  issues   in  each   of  these    domains.    Another   new  addition   to  this  report   which   builds   on  our  push   towards  moving  from  principles    to  practice   is  the  chap ter  on  AI  Design   and  Governance   which   has  the  goal  of  dissecting   the   entire  ecosystem  around   AI  and  the  AI  lifecycle  itself   to  gain  a  very  deep   under standing   of  the   choices   and  decisions   that  lead   to  some   of  the  ethical  issues   that  arise   in  AI.  It  constitut es  about    one-six th  of  the  report   and  is  definitely  some thing   that  I  would   encourage  you  to  read  in  its   entirety to gain some ne w per spectiv es on ho w we can actualiz e Responsible AI.    Given  all  the  regula tions   coming   out,  the  Laws  and  Regula tions   chap ter  provides   a  dedic ated   space   to  discuss   the  chang es  that  are  taking   place   in  this  landsc ape  and  will  certainly   provide    you  with   mark ers  on  wha t  to  watch  out  for  in  2022   and  beyond  as  lawmak ers  and  governmen ts   around   the  world  scramble   into  action   to  regula te  the  relen tless   march  of  AI  developmen t  and   deplo ymen t.   And  finally ,  as  always  we  have  our  much-enjo yed  Outside   the  Boxes  chap ter  that  captures   eclectic   developmen ts  in  the  field,   things   that  migh t  evolve  into  their   own  subfields   as  the  years   roll  by.  From  covering   things   like  Ubun tu  ethics   to  animism   and  Rinri,   you  get  the  chance   to   zoom out and see the unbelie vable w ays tha t AI is impacting and tr ansforming our socie ty.   I  hope   that  you  will  enjo y  this  edition   of  the  report   as  much   as  we’ve  enjo yed  putting  it   together.  We  encourage  you  to  shar e  it  with   colleagues   and  friends,   and  those   who   are   interested  in  getting  an  in-dep th  and  broad  under standing   of  the  field.   My  recommenda tion  for   those   who   are  wondering   on  how  to  work  through   almos t  ~300   pages  of  this  report   is  this:   grab   a  beverage  of  choice,   and  flip  over  to  the  Table   of  Contents  and  click   through   on  a  title  that   catches   your  eye  and  go  on  from  there.  If  you’re  already   familiar   with   the  domain   of  AI  ethics,   I   recommend   starting   with   the  Trends   chap ter  and  for  those   who   are  new  and  looking   to  get   started in the domain, I enc ourage you to begin with   “Welcome t o AI”   .   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 0   
 If  this  is  your  first  time   reading   our  reports,   please   don’t   hesit ate  in  reaching   out  to  us  to  let  us   know  how  we’re  doing!   If  you’ve  been   with   us  on  this  fantastic  journe y  before,  we’d  be   deligh ted  to  learn   more  about   wha t  brings   you  back   to  the  report,   wha t  are  your  favorite  parts    and  how  we  can  impr ove.  Thank   you  for  the  trust  you  place   in  us  to  bring   you  the  latest  in   resear ch and r eporting in the domain of AI e thics.    For  now,  please   enjo y  the  pages  ahead,   and  I  will  see  you  again  at  the  end  of  the  report   in  the   Closing R emark s  !   Abhishek Gupta (  @atg_abhishek  )   Founder, Director, & Principal Researcher   Montreal AI Ethics Institute   Abhishek  Gupta  is  the  Founder,  Director,  and  Principal  Researcher  at  the   Montreal  AI  Ethics  Institute.  He  is  a  Machine  Learning  Engineer  at  Microsoft,   where  he  serves  on  the  CSE  Responsible  AI  Board.  He  also  serves  as  the  Chair   of the Standards Working Group at the Green Software Foundation.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 1   
 1. Wha t we’re thinking   by MAIEI Staff and Community   Introduction  by  Abhishek   Gup ta,  Founder   and  Principal   Resear cher ,  Mon treal  AI  Ethics    Institut e   This  section   is  geared  towards  showcasing   ideas   that  our  team   and  close   network  of   collabor ators  has  on  the  field   of  AI  ethics   with   a  focus   on  things   that  remain   currently   under explor ed and une xamined.    If  you  are  excited  about   wha t  it  takes  to  oper ationaliz e  AI  ethics   in  practice   and  how  to   effectiv ely  govern  the  developmen t  of  AI  systems,   then   you  will  find  a  lot  to  take  away  from  the   first  piece   on  “  Patterns   of  Practice   will  be  fundamen tal  to  the  success   of  AI  governance.   ”  But,   such   an  effort  does   requir e  robus t  supporting   infrastructur e,  and  it  all  starts  with   people.   The   next  article   on  “  How  to  build   an  AI  ethics   team   at  your  organization  ”  gives  insigh ts  into  a  few   actions   like  getting  leader ship  buy-in  and  empo wering   people   to  mak e  necessar y  chang es  along    with   aligning   these   principles   with   organizational   values   offer  some   concr ete  advice   on  a  way   forward.   A  recurring   ques tion  that  we  often  get  asked  at  the  institut e  is  how  to  increase   literacy  in  AI   ethics,   which   is  an  apt  ques tion  given  that  our  mission   is  to  “  Democr atize  AI  Ethics   Literacy  .”   The  next  article   from  Marianna   Ganapini,   our  Faculty   Director,  dives  into  a  conversation  with    Chris   McLean   from  Avanade   who   shar es  some   under explor ed  areas  in  tech  ethics   today  and   wha t we can do t o be tter cover them in curricula and else wher e.   We  also  get  a  chance   to  talk  about   the  role  that  AI-gener ated  art  can  play  in  constructing   and   deconstructing   gender .  MAIEI’ s  close   collabor ator  Jimm y  Huang   dives  into  a  conversation  with    artis t  Jake  Elwes  to  under stand  how  queerness   and  latent  spaces   come   together   and  highligh t   some   of  the  work  that  Elwes  has  done   in  this  space   to  cross-pollina te  ideas   between  the  worlds    of machine learning and art.    Our  residen t  foodie   went  on  out  on  a  deep   explor ation  of  wha t  it  migh t  be  like  to  have  a   Michelin-s tar  quality   meal   made   by  an  AI  system.  Masa   Sweidan,   our  Business   Developmen t   Manag er,  talks  about   wha t  role  AI  can  play  in  the  kitchen,   as  a  tool  for  discovery  and  inspir ation   when it c omes t o picking fla vors and mor e.   Another   close   collabor ator  of  the  institut e  Natalie  Klym   dives  into  wide-r anging ,  in-dep th   conversations   with   veterans  of  the  technology   and  interne t  domains   to  pick  their   brains  on   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 2   
 ideas   such   as  wha t  we  migh t  learn   from  the  interne t  experience   and  its  journe y  to  wha t  it  is   today  to  guide   our  though ts  in  addr essing   the  ethics   challeng es  in  the  domain   of  AI.  She  speak s   to  David  Clark,   a  senior   resear ch  scien tist  at  the  esteemed   MIT  CSAIL  to  bring   us  nuggets  that   I’m  sure  you  will  rumina te  on  long   after  perusing   these   pages.  In  another   conversation,   this  time    with   Domhnaill   Hernon   who   leads   the  Cognitiv e  Human   Experience   at  EY,  we  learn   about   how   fusing art and engineering c ould lead us t o a mor e humane t ech futur e.   A  lot  of  conversations   in  the  domain   of  AI  ethics   are  still  quite  Western-cen tric  but  there  is  a  lot   more  out  there  and  happening   in  other   parts   of  the  world.   Two  of  my  frequen t  collabor ators,   Nga  Than   and  Khoa   Lam,   come   together   to  shar e  with   us  insigh ts  into  the  state  of  funding ,   talen t,  and  ethics   in  the  AI  ecosystem  in  Vietnam.   A  mus t-read  piece   for  those   who   want  to   break  out  of  the  more  frequen tly  covered  areas  and  gain  an  outside   perspectiv e  on  how  other    countries   are  engaging   with   AI.  This  is  followed  by  a  piece   by  Philippe   Dambly   and  Axel  Beelen    who e xplain the impacts of the fir st EU r egula tion tha t is g eared towards the insur ance indus try.   An  exclusiv e  piece   by  my  collabor ator  Ravit  Dotan  dives  into  the  ever-growing   landsc ape  of  AI   ethics   principles   and  wha t  organizations   should   do  in  the  face  of  so  much   informa tion.   It  offers   advice   on  how  organizations   can  better  navigate  this  space   and  highligh ts  some   limit ations   in   the  sear ch  for  finding   unifying  principles   across  the  globe.   In  “  Represen tation  and  Imagina tion   for  Preventing  AI  Harms   ”,  Sean   McGr egor  details  the  work  he  has  done   for  Partner ship  on  AI  on   the  AI  Inciden t  Database   that  seek s  to  provide   a  public   reposit ory  of  ethical  issues   that  have   been   documen ted  in  the  real-w orld  use  of  AI  systems.   The  aspir ation  is  that  having  a  centralized   place   from  which   we  can  draw  lessons   will  help   to  reduce   those   harms   in  the  futur e  as  people    can learn fr om prior mis takes.   Azfar  Adib   then   dives  into  a  piece   exploring   how  the  indus try  for  age-verific ation  is  being    transformed   by  the  use  of  AI  and  wha t  that  means   for  us  in  the  futur e  as  we  go  out  to  our   favorite  club s  and  bars  (whene ver  they  open   up!)  In  a  follow-up  piece,   Adib   explor es  wha t  it   migh t  mean   if  we  had  robot   co-workers  and  if  we  migh t  need   new  policies   for  their   governance    or w e would apply the same ones tha t are applied t o human w orkers.   Even  as  I  wrote  the  text  above  and  read  it  again,  it  was  shocking   to  see  that  the  diversity  of   areas  being   covered  has  grown  signific antly  over  the  past  few  years  in  exploring   the  impacts    that  AI  is  going   to  have  on  socie ty.  This  is  a  testamen t  to  how  versatile  AI  is  as  a  piece   of   technology but also a w arning sign tha t it will in filtrate all parts of our liv es.   We  are  the  ones   who   need   to  play  the  role  of  a  sentinel,   carefully   guar ding   all  that  we  hold    sacred  so  that  we  are  the  ones   who   shape   AI  systems   to  help   us  build   a  better  world  rather    than   letting  the  invisible   hand   of  AI  shape   ours  without   our  consen t  .  As  they  say,  the  power   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 3   
 lies  in  our  hands!   I  hope   you  enjo y  this  chap ter  and  it  gives  you  a  lot  to  think   about   the  AI   systems tha t surr ound y ou all ar ound.    Abhishek Gupta (  @atg_abhishek  )   Founder, Director, & Principal Researcher   Montreal AI Ethics Institute   Abhishek  Gupta  is  the  Founder,  Director,  and  Principal  Researcher  at  the   Montreal  AI  Ethics  Institute.  He  is  a  Machine  Learning  Engineer  at  Microsoft,   where  he  serves  on  the  CSE  Responsible  AI  Board.  He  also  serves  as  the  Chair   of the Standards Working Group at the Green Software Foundation.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 4   
 From the Founder’s Desk   Patterns of pr actice will be fundamen tal to the success of AI g overnance    [Original article b y Abhishek Gup ta]   AI  governance   has  certainly   gained   steam   in  2020   with   a  lot  of  calls  to  action   that  have   leveraged  expertise   in  both   the  legal  and  technic al  fields   to  propose   frame works  to  govern  both    the  developmen t  and  deplo ymen t  of  AI  systems.   Ther e  are  a  lot  of  commonalities   in  these    initia tives,  with   mos t  of  them   focusing   on  areas  of  transpar ency ,  accountability ,  bias,   privacy,   non-discrimina tion,   and  other   gener ally  agreed  upon   values   from  the  over  100  sets  of  principles    in AI e thics, with mos t having a t leas t some c omponen t focused on AI g overnance.    Ther e  has  been   a  noticeable   movemen t  from  2019   when   AI  governance   was  a  topic   of   discussion   wher e  people   talked  about   abstract  ideas   and  2020   saw  much   more  of  a  push   to   actually   put  those   ideas   into  practice.   Yet,  as  much   as  we  saw  movemen t,  there  were  still  some    short comings   that  hinder ed  the  deplo ymen t  of  these   governance   mechanisms.   In  particular ,   2020   was  a  year  wher e  we  saw  hasty  roll-outs   of  these   systems  in  tracking   face  mask    compliance   ,  grading   studen ts  ,  handing   out  unemplo ymen t  bene fits  ,  and  more.  So,  wha t  could  1 2 3  we ha ve done be tter?   As  I  have  detailed   in  my  work  titled   Green   Lighting  ML:  Confiden tiality ,  Integrity ,  and  Availability    of  Machine   Learning   Systems  in  Deplo ymen t  that  I  presen ted  with   my  co-author   Erick   Galinkin  4  at  several  conferences   in  2020   including   ICML ,  wha t  we  have  seen   is  that  there  is  little  focus   on   the  practic al  manif estation  of  these   ideas.   Specific ally,  there  is  a  missing   focus   on  the  needs   and   patterns   of  practice   of  designer s  and  developer s  on  the  ground   who   will  have  at  least  partial    responsibility   in  oper ationalizing   these   ideas.   This  is  not  to  say  that  governmen t  manda tes  and   manag emen t  of  the  organization  is  not  going   to  play  an  import ant  role  in  how  AI  systems  are   governed.   Quit e  the  contrary.  It  is  in  fact  essen tial  that  we  consider   the  measur es  I  am  going   to   recommend   as  a  supplemen t  to  the  other s,  especially   as  they  will  help   to  bolster  the  efficacy  of   any other or ganizational-sc ale mechanisms tha t are applied in AI g overnance.    4  Gupta, A., & Galinkin, E. (2020). Green Lighting ML: Confidentiality, Integrity, and Availability of Machine   Learning Systems in Deployment. arXiv preprint arXiv:2007.04693.  3   https://www.usnews.com/news/best-states/articles/2020-02-14/ai-algorithms-intended-to-detect-welfare-fr   aud-often-punish-the-poor-instead  2  https://www.wired.co.uk/article/gcse-results-alevels-algorithm-explained  1   https://www.theverge.com/2020/5/7/21250357/france-masks-public-transport-mandatory-ai-surveillance-c   amera-software   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 5   
 From  a  practitioner ’s  perspectiv e,  there  are  numer ous  challeng es  that  one  faces   when   they   encounter  abstract  principles   coupled   with   business   pressur es  and  deadlines   to  deliv er  products    and  services   on  time   and  with   high   quality .  It  is  at  these   poin ts  that  there  is  a  breakdown  in  the   actual   oper ationaliz ation  of  the  AI  governance   mechanisms   which   needs   to  be  fixed.  From  my   experience,   the  first  method   that  help s  to  mitig ate  this  issue   is  to  strive  to  incorpor ate  pieces   of   the  governance   requir emen ts  within   existing  workflo ws  of  designer s  and  developer s  rather   than    jumping   to  create  net  new  mechanisms.   The  bene fit  of  doing   so  is  that  there  is  lower  friction   in   the  accep tance   of  these   new  requir emen ts  and  they  are  also  quick er  to  deplo y  and  then   gather    evidence   to  see  if  they  are  effectiv e  or  not.  Armed   with   this  evidence,   one  can  mak e  a  stronger   case f or their inc orpor ation a t a wider le vel.   Second,   and  perhap s  the  mos t  import ant  aspect   of  creating  AI  governance   solutions   is  to   include   the  practitioner s  in  the  process   of  developing   these   mechanisms.   The  requir emen t   there  is  two-fold:  one,   you  are  able   to  surface  the  exact  places   wher e  the  AI  governance    solutions   migh t  fail  when   they  are  asked  to  be  implemen ted  in  practice   based   on  the  experience    of  the  practitioner s  and  two,  you  also  build   trust  with   those   practitioner s  so  that  they  are  not   only   aware  of  wha t  will  be  asked  of  them   but  given  that  they  are  activ e  contribut ors,  they  will   have a s trong sense of o wner ship and desir e to see this succeed.    Thus,   keeping   in  mind   these   patterns   of  practice   will  be  crucial   if  we  are  to  actually   move   forward  in  putting  AI  governance   to  work  rather   than   spend   another   precious   few  mon ths  and   years  deba ting  on  the  abstract  ideas.   The  time   for  action   is  now  and  it  starts  by  paying  attention   to ho w these s ystems ar e actually designed and de veloped in pr actice.    How to build an AI e thics t eam a t your or ganization?    [Original article b y Abhishek Gup ta]   So  you're  working   on  AI  systems  and  are  interested  in  Responsible   AI?  Have  you  run  into   challeng es  in  making   this  a  reality?   Man y  articles   men tion  a  transition   from  principles   to   practice   but  end  up  falling   flat  when   you  try  to  implemen t  them   in  practice.   So  wha t's  missing?    Here are some ideas tha t I think will help y ou take the fir st step in making it a r eality .   Get leader ship buy -in   Yes,  this  is  import ant!  Why?  Well,  different  units   within   your  organization  have  different   incen tives  and  goals  that  they  are  working   towards.  Achie ving  Responsible   AI  in  practice    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 6   
 requir es  coordina tion  across  different  units.   The  leader ship  team   can  help   provide   a  unifying   manda te to bring t ogether dif ferent units t o achie ve this g oal.   Mor e  so,  they  can  act  as  a  central  poin t  of  dissemina tion  of  the  "why"  behind   pursuing    Responsible   AI  at  your  organization.   They  have  the  authority   to  create  policy   and  drive  chang e   en  masse   that  can  mak e  on-the-gr ound   work  easier   and  more  effectiv e.  Especially   in  cases    wher e  you  face  reluct ance   from  colleagues,   a  clear   messag e  from  leader ship  provides   a  North    Star for everyone.    Finally ,  leader ship  plays  an  essen tial  role  in  providing   you  with   necessar y  resour ces  and   "air-cover"  to  experimen t  with   tools  and  techniques   as  we  (the  resear ch  and  practitioner    community)   figur e  out  practic al  solutions   to  some   very  comple x  challeng es  in  the  field   of  AI   ethics.    Setup f eedback mechanisms    As  a  complemen tary  poin t  to  the  above  recommenda tion,   we  should   also  mak e  it  easy  for   on-the-gr ound   practitioner s  to  provide   feedback   on  the  tools,   techniques,   and  processes   that   work  well  and  those   that  don't.   This  is  critic al  when   you  have  a  large  organization  with   man y   teams   working   on  very  different  product   and  service  offerings.   The  guidelines   and  manda tes   coming   top-do wn  can  suffer  from  a  lack  of  context  and  nuance,   which   only   gets  clear er  closer   to   the place of oper ation.    Effectiv e  feedback   mechanisms   have  two  qualities:   they  are  easy  to  file  and  have  transpar ency    on  which   of  the  pieces   of  feedback   have  been   acted  upon.   Man y  places   fail  on  the  second   aspect,   without   which   the  entire  exercise  of  feedback   solicit ation  becomes   fruitless.   This  also   disincen tivizes  emplo yees  from  sharing   feedback   in  the  first  place   and  mak es  them   lose  trust  in   the  process.   Sharing   results   of  the  pieces   of  feedback   that  have  been   acted  upon   (which   will   often  be  visible   through   chang es  in  the  tools,   techniques,   and  processes)   and,   more   import antly,  those   that  haven't  been   acted  upon   along   with   a  reason   on  why  they  have  not   been act ed upon will e voke higher le vels of trus t from the emplo yees in the or ganization.    Empo wer people t o mak e decisions    Often,  those   closes t  to  the  problems   and  building   solutions   to  addr ess  those   problems   have   highly   contextual  insigh ts.  We  can  leverage  these   insigh ts  by  empo wering   those   people   to  mak e   decisions.   This  empo wermen t  is  import ant  because   it  ensur es  that  people   feel  a  greater  sense    of  owner ship  in  the  solutions   that  they  are  building.   They  become   more  capable   of  solving    problems tha t really ma tter to their user s and cus tomer s.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 7   
 A  hierarchical  organization  can  help   to  align   different  teams   together   towards  a  common   vision.    Still,   when   complemen ted  with   the  bottom-up   appr oach   of  gener ating  solutions   and   empo wering   the  staff  to  act  on  those   solutions,   we  increase   the  likelihood   of  achie ving  our   Responsible AI g oals.    A  practic al  appr oach   to  building   up  this  empo wermen t  is  to  start  alloc ating  small   amoun ts  of   direct  responsibility   on  amending   product   and  service  offerings   and  increasing   the  scope  of  that   responsibility   over  time   as  people   demons trate  aptitude   and  skill  for  it.  Mor e  so,  proactiv ely   supplemen ting  this  on-the-job   experience   with   training   programs  that  promot e  decision-making    regarding R esponsible AI will mak e this appr oach success ful.   Align with or ganization v alues    One  of  the  core  places   of  dissonance   occur s  when   AI  ethics   is  framed   in  a  context  aligned   to  the   organization's   mission   and  values.   Drawing   a  clear   connection   between  them   help s  boos t   uptake  and  leverage  other   evalua tive  instrumen ts  (like  performance   reviews)  and  policies   within    the or ganization.    It  also  help s  percolate  the  idea   of  responsible   AI  as  a  key  function   of  every  person's   job  role  in   the  organization  that  help s  with   organic  integration  of  these   responsibilities   in  the  existing  job   roles  and  making   it  easy  to  create  new  job  roles  in  the  organization  that  are  tasked  with    implemen ting AI e thics within the or ganization.    Mak e RAI the norm r ather than the e xception   Just  as  Micr osoft   has  invested  years  of  effort  in  tooling   and  processes   to  mak e  accessibility   a   first-class   citizen  in  their   products   and  services,   making   Responsible   AI  the  norm   rather   than   the   exception should be our North St ar.   If,  through   investmen ts,  we  can  mak e  the  implemen tation  of  these   ideas   a  default  action   and   easy  action,   then   not  only   will  we  get  higher   traction,   but  it  will  also  disincen tivize  developer s   from  doing   anything   other   than   the  "righ t"  thing.   Yes,  the  last  part  is  a  little  bit  aspir ational,   but   it isn't unr ealis tic!   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 8   
 Office Hours   Exploring the under -explor ed ar eas in t eaching t ech e thics t oday   [Original article b y Marianna Ganapini]    Chris   McClean   shar es  his  experience   as  the  global   lead   for  digit al  ethics   at  Avanade,   and  we  are   excited  to  learn   more  about   how  it  trains  tech  and  business   professionals   to  recogniz e  the  mos t   pressing   ethical  challeng es.  And  as  always,  please   get  in  touch   if  you  want  to  shar e  your   opinions and insigh ts on this f ast-developing field.    Wha t  is  your  backgr ound?   Wha t  courses  do  (or  did)  you  teach   connect ed  to  Tech  Ethics,   and   who’ s your audience (e. g., under grads, pr ofessionals)?    I  am  the  Global   Lead   for  Digit al  Ethics   at  Avanade,   a  40,000-emplo yee  technology   consulting    and  advisor y  firm.   A  substantial  part  of  my  role  includes   training   our  tech  and  business    emplo yees  worldwide   about   how  best  to  recogniz e  and  addr ess  ethical  issues   that  arise   in  the   technology   we  design,   develop,   deplo y,  and  oper ate.  I  also  offer  Digit al  Ethics   training ,   assessmen ts,  and  program  design   for  our  clien ts  (technology   and  business   executiv es)  as  part  of   a broad advisor y practice.    Wha t  kind   of  content  do  you  teach?   Wha t  topics   do  you  cover?  Wha t  types   of  readings   do  you   usually assign?    I  teach   gener al  concep ts  and  trends   in  Digit al  Ethics,   which   covers  a  wide   range  of  ways   technology   impacts   individuals   (such   as  privacy,  accessibility ,  financial   health   and  opportunity ,   men tal  well-being ,  personal   dignity ,  and  legal  status),   socie ty  (such   as  health   care,  educ ation,    the  econom y,  criminal   justice,   and  law  enforcemen t),  and  the  environmen t  (such   as  ener gy  use,   material   use,  waste,  pollution,   and  impact   on  biodiv ersity).   I  also  cover  a  wide   range  of  ethical   controls,  such   as  values   alignmen t,  ethical  testing,  security ,  resilience,   monit oring ,  oversight,   recourse,  and  accountability .  I  usually   distill  academic   resear ch  for  my  audience   given  the   amoun t  of  time   such   reading   migh t  take,  and  I  rely  heavily  on  real-w orld  cases   of  ethics   done    well or done poorly .   Wha t  are  some   teaching   techniques   you  have  emplo yed  that  have  worked  particularly   well?   For Tech E thics, wha t kind of appr oach t o teaching do y ou recommend?    I  found   it’s  especially   help ful  to  run  audiences   through   scenario   analy sis,  especially   if  we  can  use   real  case  examples.   I’ve  also  run  workshop s  that  include   a  detailed   assessmen t  of  a  technic al   product or pr oject using our Digit al Ethics Assessmen t Frame work.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 9   
 In  your  opinion,   wha t  are  some   of  the  things   missing   in  the  way  Tech  Ethics   is  currently   taugh t?  For  instance,   are  ther e  topics   that  are  not  covered  enough   (or  at  all)?   Wha t  could   be   done t o impr ove this field?    It’s  hard  to  say,  as  I  don’t   have  much   visibility   into  all  the  different  ways  people   are  teaching    these   topics.   However,  given  wha t  we’re  seeing   in  the  indus try,  it  seems   like  we’re  spending   a   good  deal   of  time   on  data  ethics/priv acy  and  responsible   AI  (which   are  critic ally  import ant)  but   not  enough   time   on  the  men tal  health,   personal   dignity ,  and  environmen tal  impacts   of   technology .  I  also  don’t   see  enough   emphasis   on  how  to  incorpor ate  ethical  practices   into   various pr ofessional disciplines, lik e design, engineering , mark eting, or audit.    How  do  you  see  the  Tech  Ethics   Curriculum   landsc ape  evolve  in  the  next  5  years?  Wha t  are   the chang es you see happening?    I’m  encouraged  to  see  how  much   more  often  Tech  Ethics   is  taugh t  as  part  of  gener al  comput er   science   and  data  science   curricula.   I’m  hope ful  that  this  trend  will  carry  into  business   curricula    as  well,  just  as  we’ve  seen   topics   like  sustainability   and  corpor ate  responsibility   become   more   popular .  Ideally ,  I  think   our  ethics-r elated  educ ation  needs   to  include   perspectiv es  from   economics,   sociology ,  and  even  mark eting  to  show  that  taking   ethics   seriously   can  positiv ely   impact business and social perf ormance.    Is ther e an ything else y ou’d lik e to add?    We  should   look   carefully   at  the  value   of  having  stand-alone   ethics   training   versus  embedding    ethics   consider ation  into  other   aspects   of  training.   As  a  dispar ate  subject,   it’s  very  easy  to   compartmen talize  ethics   as  some thing   that’s  done   occasionally ,  possibly   by  other   people.   But  if   it’s  incorpor ated  as  a  standar d  elemen t  of  other   courses,  it’s  easier   to  see  that  considering   and   addr essing e thics is e veryone’ s job thr oughout the en tire tech lif ecycle.   Bio of in terviewee:   As  the  global   lead   for  digit al  ethics   at  Avanade,   Chris   McClean   is  responsible   for  driving   the   compan y’s  digit al  ethics   fluency   and  internal   chang e  and  advising   clien ts  on  their   digit al  ethics    journe y.  Prior   to  Avanade,   Chris   spen t  12  years  at  Forrester  Resear ch,  leading   the  compan y’s   analy sis  and  advisor y  for  risk  manag emen t,  compliance,   corpor ate  values,   and  ethics.   Chris    earned   his  MS  in  Business   Ethics   and  Compliance   in  2010   and  BS  in  Business   with   a  Mark eting   emphasis in 2001.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 0   
 AI Application Spotlight   Jake Elw es: Cons tructing and Dec onstructing Gender with AI-Gener ated Art    [Original article b y Jimm y Huang]    “The  idea   behind   latent  space   is  that  there’s  this  continuous   space   between  the  classes.   You   have  these   multi-dimensional   vectors  which   relate  everything   it  [the  artificial   intelligence]   has   learned   about,   say,  a  female   face  as  well  as  everything   it  has  learned   about   a  male   face,  and   there’s  this  continuous   space   in  between.   It  doesn’t   actually   have  those   gender ed  binaries    anymor e  –  it’s  a  continua tion,   and  with   unsuper vised   learning   it  doesn’t   even  have  the   gender ed labels … ”   In  the  burgeoning   field   of  artificial   intelligence   (AI)  ethics,   resear cher s  at  the  Mon treal  AI  Ethics    Institut e  have  been   analy zing  how  AI  applic ations   frequen tly  learn   discrimina tory  beha viour    from  being   fed  biased   training   datasets.  This  could   be,  for  example,   from  a  lack  of  inclusion   in   the  training   set  resulting   in  an  applic ation’ s  inability   to  detect  the  faces   of  minorities   [1]  to  an   overinclusion   within   other   sets  for  the  express  purpose   of  surveilling   certain  minority   groups.   [2]   Ther e  are  also  statistically  signific ant,  yet,  barely  perceptible   biases   we  can  only   uncover   through   careful  resear ch  such   as  when   using   historical  US  mortg age  data  to  predict    creditw orthiness.   Using   standar d  logis tic  regression   and  Random   Forest  models,   Fuster  et  al.’s   CEPR   discussion   paper   concludes:   “minority   groups  appear   to  lose,   in  terms   of  the  distribution    of  predict ed  default  propensities,   and  in  our  counterfactual   evalua tion,   in  terms   of  equilibrium    rates … ” [3]    All  this  is  to  say  that  without   guidance   from  ethics,   by  the  very  nature  of  training   sets  requiring    bias  to  perform,   discrimina tory  beha viour   will  not  only   persist  but  increase   in  ubiquity    enhanced with modern, f ar-reaching t echnology .   Enter  London-based   artis t,  Jake  Elwes.  Carr ying  a  strikingly   warm  presence,   Elwes  takes  a  seat   across fr om me a t Dit to Cof fee in Shor editch for the in terview.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 1   
 For  the  past  half-dec ade,   Elwes  has  been   using   various   machine   learning   techniques   to  gener ate   media   art  that  compels   us  to  consider   our  place   in  socie ty  giving   a  unique   perspectiv e  in  viewing    human iden tity thr ough the lens of modern t echnology .   Elwes’  latest  venture,  The  Zizi  Project,   is  an  ongoing   series   of  works  applying   diverse  drag  and   gender   fluid   iden tities   as  training   sets  for  positiv e  AI  outcomes.   The  project   started  with   the   “Zizi   –  Queering   the  Dataset”  installation  in  2019   wher e  an  AI  program  attemp ts  to  constantly   gener ate,  shift,   and  regener ate  non-binar y  faces   in  a  work  that  celebr ates  difference   and   ambiguity .  In  2020,   Elwes  produced   the  “Zizi   &  Me”   installation,   a  double   act  between  London    drag  queen   Me  [4]  and  a  deep   fake  (AI)  as  well  as  “The  Zizi  Show”  [5],  a  deep   fake  drag  cabar et   featuring a number of acts.    Elwes  has  taken  a  look   at  all  emer ging   Gener ative  Adversarial   Networks  (GAN)   techniques   and   wonder s  ‘how  can  we  use  this  as  a  performance   tool? ’.  “The  Zizi  Project ”  explor es  the  effects  of   technology   on  gender   iden tity  through   performance   and  in  the  process   of  creating  the  show,  a   variety  of  ethical  topics   are  brough t  to  light  within   the  confines   of  a  safe  environmen t.  Elwes   leans   forward  over  the  table   between  us  and,   with   passion,   explains   the  discourse  within   both    the  drag  and  transg ender   communities   around   data  consen t,  namely ,  how  an  individual’ s  imag e   may be used as w ell as wha t the use w ould be f or.   On  the  data  consen t  side,   Elwes  ensur es  that  the  performer s  who   contribut ed  visual   content  to   “The  Zizi  Show”  and  “Zizi   &  Me”   will  have  control  over  their   imag e.  They  can  retract  their    likeness   from  training   sets  and  have  deriv ed  performances   from  their   likeness   taken  down.   However,  a  much   more  interesting  concern   is  brough t  forth   by  whe ther   the  inclusion   of  queer    iden tities   in  training   datasets  have  inher ent  issues.   Some   may  posit   that  since   we  live  in  a   technology -driv en  world,   real  harm   could   arise   due  to,  for  example,   doct ors  not  having  the   proper   data  poin ts  to  adequa tely  come   up  with   treatmen ts  for  transg ender   physiologies.   On  the   other   side,   Elwes  explains,   “ther e  is  a  real  pride   to  being   queer   and  having  this  otherness”   and,    historically  speaking ,  marginaliz ed  communities   are  right  to  be  wary  of  how  changing    technologic al  and  socie tal  landsc apes   affects  them.   Given  this  pride   in  othernness,   some    member s  of  the  queer   community   feel  hesit ant  to  be  included   in  training   datasets  or  in  having   their iden tities assimila ted to an e xtent.   Elwes  aims   to  honour   underr epresen ted  and  historically  marginaliz ed  non-binar y  groups  while    also  creating  a  uniquely   charming   cabar et  show.  In  creating  the  show,  from  a  technologic al   standpoin t, Elw es w as lar gely inspir ed b y the idea behind la tent space.    “There’ s a queerness t o lat ent spac e”   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 2   
 In  simple   terms,   latent  space   is  this  hidden   world,   opaque   to  human   intuition,   of  compr essed    data wher e similar f eatures ar e mapped closer t ogether.   Data  is  only   useful  insof ar  as  there  is  bias  in  the  set.  Without   bias,   data  would   either   be   comple tely  random   or,  on  the  other   side,   uniform  and  therefore  largely  useless.   Machine    learning   applic ations   find  similarities   in  features  by  first  compr essing   data  into  latent  space,   a   vector  space   represen ted  mathema tically,  and  then   in  grouping   similar   data  poin ts  closer    together acc ording t o meaningful f eatures.   Ther e’s  ambiguity   and  nigh-in finite  spectrums   of  data  groupings   that  may  be  applied   in  a  variety   of  contexts  hidden   within   latent  space.   In  this  way,  the  vague,   unlabelable   inner -mechanism   of   how  deep   learning   works  have  profound   parallels   to  the  gender   fluidity   of  non-binar y  iden tities.    It  is  especially   fascina ting  how  a  non-binar y  group  of  iden tities   is,  in  turn,   grouped   within   latent   space on a pr eviously unde fined spectrum.    Elwes’  works  aim,   in  part,   to  deconstruct   gender   and  then   reconstruct   the  features  in  an   ever-transit ory  state.  The  gender -fluid   appear ances   are  distilled   into  groupings   hidden   within    latent  space   and  then   constantly  reconstruct ed  becoming   an  evolving   spectrum   of  an  input   set   that is alr eady a spectrum of g ender iden tities.    In  a  world  wher e  we’re  constantly  inunda ted  with   articles   on  the  negative  effects  that  AI   applic ations   may  have  on  socie ty  at-large,  seeing   a  positiv e  outcome   for  a  historically   marginaliz ed  group,   if  only   for  cultur al  and  artis tic  insigh t,  is  a  breath  of  fresh  air.  Elwes  works  at   the  frontier  of  this  inno vative  space,   using   emer ging   gener ative  adversarial   networks  and  deep    fake techniques as the y’re disc overed to create though tful, e thical art.    References    [1]   https://ne ws.mit.edu/2018/ study -finds-g ender -skin-type-bias-artificial-in telligence-s ystems-021    2   [2] h ttps://w ww.nature.com/articles/d41586-020-03187-3    [3] h ttps://paper s.ssrn.c om/sol3/paper s.cfm?ab stract_id=3072038    [4] h ttps://w ww.instagram.c om/me thedr agqueen/?hl=en    [5] h ttps://zizi.ai/    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 3   
 Will  an  Artificial   Intelliche f  be  Cooking   Your  Next  Meal   at  a  Michelin   Star   Restaurant?   [Original article b y Masa S weidan]    The  use  of  AI  in  the  food  indus try  has  been   growing   over  the  past  few  years  with   applic ations    such   as  robotics,   kiosk s,  chatbots   and  recommenda tion  engines.   According   to  Mor dor   Intelligence,   AI  in  the  food  and  beverage  mark et  was  valued   at  $3.07   billion   in  2020   and  is   expect ed  to  reach   $29.94   billion   by  2026.   A  prime   example   of  this  progress  is  Miso   Robotics’    autonomous   robotic   kitchen   assis tant,  Flipp y,  which   was  introduced   in  2018   when   it  was  able   to   grill  150  burgers  per  hour .  Three  years  (and   a  pandemic   later)  and  it  can  now  cook  19  different   foods   including   burgers,  chick en  wings   and  onion   rings   while   keeping   track  of  cooking   times   and   temper atures.  Not  only   will  this  machine   reduce   labor   costs,  but  it  will  also  impr ove  the  quality    of food while pr oviding deep insigh t into oil usag e and pr oduct c ounts.   With   various   inno vative  solutions   like  Flipp y  popping   up  on  the  mark et,  the  bene fits  of  AI  in  the   restaurant  indus try  all  seem   to  stem  from  its  efficiency   and  precision.   When   working   properly ,   AI  can  increase   savings   and  impr ove  food  safety,  which   is  especially   import ant  as  we  navigate   this  COVID-19   era.  Although   these   aspects   are  vital  to  the  success   of  any  restaurant,  the  true    magic   happens   inside   the  kitchen   wher e  chefs  cook  delicious   meals   that  often  reflect   unique    social,   cultur al,  and  environmen tal  influences.   With   this  in  mind,   does   AI  have  a  place   in  the   kitchen   to  support   the  creative  process   of  professional   chefs  who   have  dedic ated  their   life  to   learning the t echniques and in tricacies of high-quality c ooking?    It  is  difficult   to  imagine   a  world  wher e  a  machine   prepar es  an  entire  meal   from  start  to  finish,    because   there  is  no  doub t  that  the  prepar ation  and  consump tion  of  a  delic ately  assembled   meal    is  inher ently  linked  to  unique   human   experiences.   How  will  a  machine   be  able   to  under stand   and,   more  import antly,  communic ate  a  story  through   a  particular   spice   that  may  remind   you  of   your  grandma’ s  specialty   dish  or  the  smell   of  a  dessert   that  transports   you  back   to  your  favorite   childhood memories?    At  the  tail  end  of  2020,   Sony  launched   their   Gastronom y  Flagship   Project   to  explor e  the   potential  of  new  technologies   like  AI  and  Robotics   through   interviews  with   chefs  and   professionals   in  the  indus try.  This  endea vor  consis ts  of  the  “resear ch  and  developmen t  of  an  AI   applic ation  for  new  recipe   creation,   a  robotics   solution   that  can  assis t  chefs  in  their   cooking    process,   and  a  community   co-creation  initia tive  that  will  serve  as  a  founda tion  to  these    activities. ”  Perhap s  the  mos t  interesting  aspect   of  this  project   is  the  focus   on  using   technology    to achie ve even gr eater cr eativity , rather than r eplacing the mor e repetitive tasks in the kit chen.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 4   
 Surprisingly ,  the  sentimen ts  of  chefs  towards  the  use  of  AI  in  the  world  of  gastronom y  seem   to   be  quite  optimis tic.  By  shifting   the  narrative  to  emphasiz e  the  fact  that  cooking   is  actually   both    art  and  science,   the  potential  of  technology   becomes   more  appar ent.  Jordi  Roca,  a  co-owner   of   3  Michelin   star  restaurants,  said  it  perfectly:   “AI  has  been   used   to  evolve  music al  compositions.    My  thinking   is  if  it  can  be  done   with   music,   it  can  be  done   with   flavors,  because   at  the  end  of   the da y it c onsis ts of harmonizing a sc ore or an ar oma tic chor d.”   This  is  the  key.  At  its  core,  the  process   of  cooking   and  baking   is  an  applied   science,   because   the   building   block s  of  all  food  are  large  biologic al  molecules   such   as  proteins,   carboh ydrates,  and   fats.  Put  simply ,  the  structur e  of  a  molecule   defines   how  it  functions   in  a  cell  and  how  a  food   may  taste  or  react   when   being   prepar ed.  However,  the  artis tic  elemen t  comes   into  play  with   the   creativity   and  emotion   that  is  involved  throughout   the  whole   process.   The  tricky   part  is   establishing   the  balance   between  using   AI  to  optimiz e  the  molecular   gastronom y  of  a  dish,   yet   leaving  room   for  the  human   chefs  to  express  their   imagina tion  during   the  prepar ation  of  that   meal. IBM spot ted this opportunity and decided t o de velop Che f Watson.    Through   comput ational   creativity ,  Chef  Watson   can  create  recipes   that  suggest  ingredien t   combina tions   and  styles   of  cooking   that  humans   would   never  have  consider ed,  due  to  its  ability    to  analy ze  large  data  sets.  After  being   fed  10,000   recipes   from  Bon  Appe tit’s  archives,  it  used    natural  languag e  processing   to  learn   the  underlying   logic   of  how  ingredien ts  were  combined.    This  is  particularly   useful  in  the  context  of  gastronom y  because   even  the  best  professional   chefs   can  only   reason   about   pairing   three  ingredien ts,  wher eas  Chef  Watson   can  examine   up  to  nine    ingredien t  combina tions.   The  power  of  this  machine   stems  from  its  ability   to  model   both   the   chemis try of the ingr edien ts and the human per ception of fla vor.   It  should   be  noted  that  this  particular   invention  has  not  receiv ed  much   media   attention  or   coverage  since   2015,   but  it  does   support   the  vision   that  the  futur e  of  gastronom y  can  integrate   technology   to  achie ve  emotion.   Josep   Roca,  co-owner   and  sommelier   of  El  Celler   de  Can  Roca,   which   was  ranked  the  best  restaurant  in  the  world  in  2015,   takes  it  a  step  further   and  belie ves   that  AI  can  be  used   to  create  more  personaliz ed  dining   experiences   for  each   individual.   By  using    inputs   about   a  customer ’s  origin   and  preferences,   they  could   be  provided   with   a  tailor ed  menu    that transports them back t o their f avorite memories.    These   novel  ideas   that  incorpor ate  AI  in  the  professional   kitchen   are  surely  exciting ,  but  they   also  beg  the  ques tion:   will  certain  cuisines   be  misr epresen ted  or  comple tely  left  behind?    Mor eover,  culinar y  traditions   and  recipes   are  typic ally  passed   down  from  gener ation  to   gener ation  and  are  not  necessarily   mean t  to  be  perfect.  Ther efore,  the  mos t  potential  seems   to   lie  in  developing   AI  for  the  discovery  and  inspir ation  of  flavors,  rather   than   the  actual   cooking   of   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 5   
 a  meal.   Although   it  is  unlik ely  that  an  “artificial   intelliche f”  will  be  preparing   your  order  at  a  fine   dining   restaurant  in  the  near   futur e,  the  developmen ts  in  this  space   poin t  towards  a  new  reality    wher e AI c an assis t with the r ecipe cr eation, but humans s till ha ve the final, artis tic touch.    Credits   to  my  dear   friend   and  colleague,   Connor   Wright,  for  coming   up  with   the  clever  term:    “Artificial In telliche f!”   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 6   
 Permission to be uncertain   The  Technologis ts  are  Not  in  Control:  Wha t  the  Interne t  Experience   Can   Teach us about AI E thics and R esponsibility    [Original article b y Na talie Klym]    Interview  with   David  Clark,   Senior   Resear ch  Scien tist,  MIT  Comput er  Science   &  Artificial    Intellig ence Lab    Artificial   intelligence   has  recen tly  emer ged  from  its  mos t  recen t  winter.  Man y  technic al   resear cher s  are  now  facing   a  moral  dilemma   as  they  watch  their   work  find  its  way  out  of  the  lab   and  into  our  lives  in  ways  they  had  not  intended   or  imagined   but  more  import antly,  in  ways  they   find objectionable.    The  atomic   bomb   is  a  classic   example   that  man y  commen tators  on  contempor ary  technologies    refer  to  when   discussing   ethics   and  responsibility .  But  a  more  recen t  and  relevant  example   that  I   would   like  to  draw  lessons   from  is  the  Interne t–a  founda tional   technology   that  has  reached    maturity and is fully embedded in socie ty.   My  focus   is  not  on  the  specific   social   issues   per  se,  e.g.,  net  neutr ality   or  univ ersal  access,    rather,  my  goal  is  to  provide   a  glimp se  into  some   of  the  dynamics   associa ted  with   the  Interne t’s   transition   from  lab  to  mark et  as  experienced   by  one  prominen t  member   of  the  resear ch   community ,  Dr.  David  Clark,   Senior   Resear ch  Scien tist  at  MIT’s  Comput er  Science   and  Artificial    Intelligence Lab (CS AIL).    Clark   has  been   involved  with   the  developmen t  of  the  Interne t  since   the  1970s.   He  served  as   Chie f  Protocol  Architect  and  chair ed  the  Interne t  Activities   Boar d  throughout   mos t  of  the  80s,   and  more  recen tly  worked  on  several  NSF-sponsor ed  projects   on  next  gener ation  Interne t   architectur e.  In  his  2019   book,   Designing   an  Interne t,  Clark   look s  at  how  multiple   technic al,   economic, and social r equir emen ts shaped and c ontinue t o shape the char acter of the In terne t.   In  discussing   his  lifelong   work,  Clark   mak es  an  arresting  statemen t:  “The  technologis ts  are  not  in   control  of  the  futur e  of  technology .”  In  this  interview,  I  explor e  the  signific ance   of  those   words   and ho w the y can in form t oday’s discussions on AI e thics and r esponsibility .   You  describe   your  observation  that  “the   technologis ts  are  not  in  control”  as  a  revelation  that   came   to  you  during   the  commer cializ ation  phase   of  the  Interne t.  Can  you  describe   this   momen t and wh y it w as revelatory?   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 7   
 The  goals  of  the  resear ch  community   in  the  1970s   and  1980s   were  purely  technic al.  In  the  70s,   we  were  just  trying  to  get  the  protocols  to  work.  In  the  1980s   the  challeng e  was  scale.  We  went   from  a  goal  of  connecting   about   100,000   institutional   comput ers  to  millions   of  personal    comput ers,  and  now  of  course,  we’re  looking   at  billions   of  connect ed  devices   including   cell   phones and sensor s of all kinds.    Commer cializ ation  of  the  Interne t  began  in  the  1990s.   During   this  period,   it  went  through   a   rapid   transition   from  being   an  infrastructur e  run  by  the  US  governmen t  to  a  service  provided   by   the priv ate sect or.   It  was  an  interesting  as  well  as  surprising   time   since   we  didn’t   know  wha t  a  commer cial  Interne t   would   look   like.  We  had  never  though t  about   it  that  way.  As  the  sour ce  of  investmen t  chang ed,   so  did  the  drivers  and  the  goals  of  the  resear ch,  which   were  now  being   led  by  indus try.  All  of  a   sudden, a ne w se t of f actors emer ged, things lik e profit-seeking , compe tition, e tc.   The  example   that  first  brough t  this  home   to  me  concerned   QoS,   or  “quality   of  service”   controls,   which   enable   the  prioritiz ation  of  pack ets.  Our  goal  in  designing   these   controls  was  to  mak e   time-sensitiv e  applic ations   like  real  time   voice  and  games   work  better,  and  the  controls  did  that.   We  initially   saw  that  as  a  technic al  enhancemen t.  However,  it’s  not  difficult   to  under stand  that   in  a  commer cial  context  pack et  prioritiz ation  has  everything   to  do  with   indus try  compe tition    and  therefore  mone y.  In  the  early   days  of  online   phone   and  video   services   it  was  difficult   for   provider s  of  Voice  over  IP  and  IPTV  to  compe te  with   the  telco  and  cablec o’s  proprie tary  services    because   the  quality   of  transmission   over  the  public   Interne t  was  still  relatively  poor   at  the  time    and  there  were  no  QoS  capabilities.   So,  if  you  were  an  ISP  offering   traditional   phone   and  TV   services,   why  would   you  build   capabilities   in  your  IP  network  that  would   offer  a  means   for  these    new  entrants  to  compe te  with   you?  As  one  ISP  executiv e  said  to  me,  “why  should   I  spend    mone y on QoS so tha t Bill Ga tes can mak e mone y selling In terne t telephon y?”   The  poin t  is  that  wha t  I  had  consider ed  pack et-routing   protocols  for  decades   were  in  effect   mone y-routing   protocols.  This  was  poin ted  out  to  me  by  an  economis t  who   said:   “the   Interne t  is   about   routing   mone y;  routing   pack ets  is  a  side  effect,  and  you  screwed  up  the  mone y-routing    protocols.”  In  my  defense,   I  replied,   “I  didn’t   design   any  mone y-routing   protocols,”  and  his   response w as, “tha t’s wha t I said. ” We were joking , but the poin t was real.   That  the  technic al  design   of  the  Interne t  has  implic ations   for  indus try  dynamics   is  obvious   to   economis ts  and  business   people,   and  it’s  obvious   to  me  now,  but  until  we,  as  technologis ts,   were  compelled   to  solve  indus try  problems,   none   of  this  was  obvious.   We  did  not  under stand  at   the  time   that  we  were  now  engineering   both   a  technology   and  an  indus try  structur e  that   determined who had ec onomic po wer.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 8   
 So, you’re saying tha t indus try, or the priv ate sect or, is driving the futur e?   Socie tal  concerns   have  become   more  import ant  in  the  last  decade.   We  now  have  a  fundamen tal   tussle   between  the  objectiv es  of  the  private  sector–c oncerned   with   things   like   commer cializ ation  and  profitability–and   those   of  the  public   sector.  I  worked  with   kc  claffy,  who    is  the  Director  of  the  Center  for  Applied   Interne t  Data  Analy sis  at  the  Univ ersity  of  Calif ornia,   on   resear ch  that  explor ed  the  socie tal  aspir ations   for  the  futur e  of  the  Interne t.  We  collect ed   statemen ts  from  a  variety  of  stakeholder s  including   governmen ts  and  public   interest  groups  and   catalog ed  them   into  a  list–things   like  reach,   ubiquity ,  trustworthiness.   These   social   aspir ations    can  be  in  direct  conflict  with   private  sector  goals.   And  it’s  this  tension   that  shapes   the  futur e  of   the In terne t.   You  chang ed  your  focus   to  the  social   implic ations   of  the  Interne t  and  policy   matters  in  the   2000s.   Would   it  be  accur ate  to  say  this  was  the  result   of  realizing   you  were  now  also   engineering a social s tructur e?   Yes  and  no.  Ther e’s  a  difference   between  the  Interne t  itself—the   pack age  carriag e  system—and    the  applic ations   that  run  on  top  of  it,  like  the  Web  or  Facebook,   for  example.   These   days,  when    people t alk about the In terne t, the y are oft en talking about the la tter.   So,  yes,  I  have  been   more  focused   on  the  social   implic ations   of  the  Interne t  in  the  last  2   decades,   but  in  terms   of  engineering   a  social   structur e,  this  stems  from  the  applic ation  space,   as   opposed t o the ne twork itself .   And  within   the  applic ation  space,   it’s  the  ad-driv en  business   model   that  I  mos t  have  issues   with.    This  model   creates  all  kinds   of  incen tives  that  have  negative  consequences   for  socie ty.   Facebook,   for  example,   is  designed   to  be  addictiv e.  They  want  to  keep  you  on  their   site  so  they   can  show  you  lots  of  ads,  so  they  manipula te  the  experience   to  mak e  it  “sticky ,”  and  influence    your per sonal beha vior with all kinds of t actics. It ’s an incr edibly dis torted space.    The  2020   documen tary  film,   The  Social   Dilemma,   and  Shoshana   Zuboff ’s  2019   book   The  Age   of  Surveillance   Capit alism   explor e  this  distortion.   And  a  few  years  ago,  in  2014,   Ethan    Zuck erman   (formerly   at  the  MIT  Media   Lab  and  now  at  U.  of  Western  Mass.)   wrote  a  public    apology   for  designing   the  pop-up   ad  back   in  1997,   declaring   advertising   in  gener al  as  “the    Interne t’s  original   sin.”  Do  you  feel  personally   responsible   for  the  things   you  think   are  bad   about t oday’s Interne t?   We  designed   the  Interne t  (the  pack et  carriag e  system)  for  gener ality;   that  was  its  strength.  I   don’t   think   there’s  any  way  I  could   have  built   an  Interne t  that  would   allow  for  gener ality   and  at   the  same   time   preclude   bad  beha vior  at  the  applic ation  layer.  Maybe  there  was  a  fork  in  the   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 9   
 road  wher e  someone   could   have  pushed   things   in  a  different  direction—but   not  at  the  pack et   level.   Wha t are your though ts on the mor al dilemmas f acing AI r esear cher s today?   I  would   say  AI  is  probably   more  like  the  pack et  carriag e  layer  in  that  it’s  a  basic   technology ,  with    applic ations   that  can  take  man y  forms.   And  it’s  difficult   if  not  impossible   to  preclude   bad   beha vior  or  only   allow  for  good  beha vior,  nor  is  it  easy  to  define  wha t  “good”   vs  “bad”   beha vior   is.  Stephen   Wolff,  who   ran  NSFNET   in  the  80s,  said  back   then   that  every  beha vior  we  see  in  the   real  world  is  going   to  manif est  in  cyber space   including   beha viors  that  we  find  unwelcome   and   offensiv e.   The  sociologis t  and  Interne t  historian,   Manuel   Castells,  has  said  that  the  Interne t  is  the  mirr or   of  socie ty.  It  is  neither   good  nor  bad,   nor  is  it  neutr al,  his  poin t  being   that  its  uses   are  socially    determined.    I  agree;  the  Interne t  evolved  to  defy  both   the  original   utopian   and  dystopian   visions.   The  more   gener al  ques tion  regarding   the  moral  responsibility   of  scien tists  has  been   deba ted  over  and   over  again.  My  view  is  that  technology   can  be  used   in  so  man y  different  ways,  and  rarely  do  we   under stand  the  futur e  implic ations   of  wha t  we  have  made,   even  if  we  have  a  clear   sense   of   intended uses. A cr eative per son c an and will c ome along and use it in w ays we ne ver imagined.    The  history  of  technology   is  full  of  stories   of  unin tended   consequences,   whe ther   good,   bad,   or   simply   frivolous.   GPS  is  an  interesting  case.  The  early   resear ch  paper s  stressed   milit ary   applic ations   of  GPS  because   the  resear cher s  wanted  the  milit ary  to  fund   its  developmen t.  Those    who   under stood  some   of  the  broader   socie tal  bene fits  were  afraid  that  if  they  put  too  much    emphasis   on  these,   the  milit ary  would   not  pay  for  it.  So,  GPS  emer ged  as  a  milit ary  technology .   But  today,  we  all  have  access   to  map s  and  directions,   and  my  childr en  have  not  had  the   experience   of  being   lost.  That  is  a  good  thing.   But  the  negative  consequences   include   things   like   neighborhood   traffic  congestion  that  results   from  traffic  apps  sending   cars  down  residen tial   streets,  and  global   tracking   of  everyone’ s  location.   These   outcomes   are  not  some thing   that   could   have  been   foreseen,   nor  is  it  at  the  level  of  some thing   like  autonomous   weapons,   even   though   it  can  lead   to  fatal  acciden ts.  But  these   are  negative  consequences   that  no  one  could    have predict ed a t the time.    I  think   we  should   teach   scien tists  to  think   through   the  social   consequences   of  wha t  they’re   doing ,  but  I  don’t   know  if  we  can  put  a  burden  on  resear cher s  that  says  they  have  some    oblig ation  or  responsibility   to  predict   all  the  consequences   and  then   try  to  embed   mechanisms    to prevent harm. I jus t don’t think the w orld w orks tha t way.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   3 0   
 The  cryptographer   Phillip   Rogaway  mak es  a  strong  case  for  comput er  scien tists  taking    responsibility f or their w ork, ar guing tha t their w ork is politic al in na ture.   I’m  very  sympa thetic  to  wha t  Rogaway  says,  but  it’s  one  thing   to  conclude   that  encr yption  is   going   to  shift   power  balances,   and  another   to  then   design   the  technology   to  preclude   bad   beha viors by its user s.   Ther e  should   definitely  be  a  sense   of  awareness   during   the  more  abstract  explor ation  phase,    but  it’s  not  until  you  get  closer   to  a  specific   applic ation  that  you  need   to  think   through   the   ethical  implic ations.   You’re  going   to  have  to  rectif y  problems   as  they  emer ge  in  each   context,  on   a case b y case basis.    Carly   Kind   from  the  Ada  Lovelace   Institut e  in  the  UK  refers  to  an  emer ging   “thir d  wave”  of   ethical  AI  that  addr esses   specific   use  cases   framed   as  social   problems   as  opposed   to   philosophic al  concep ts  (the   first  wave)  or  narr owly-defined   technic al  issues   focused   on   algorithmic   bias  (the   second  wave).  As  we  enter  this  third  wave,  it’s  clear   that  we  need   man y   voices   at  the  table,   but  under standing   each   other   and  integrating  multiple   perspectiv es  isn’t    always eas y or s traigh tforward. Ho w ha ve you addr essed this challeng e?   When   I  began  to  realiz e  that  the  Interne t  was  no  long er  a  purely  technic al  problem   I  stopped    running   a  purely  technic al  resear ch  group  at  MIT.  I  started  by  hiring   an  economis t  and  have  also   hired  politic al  scien tists  and  collabor ated  with   philosopher s  like  Helen   Nissanbaum.   The  last   project   you  and  I  worked  on  regarding   convergence   at  the  applic ation  layer  integrated  ideas    from  media   studies   and  other   social   sciences.   Taking   a  multidisciplinar y  or  interdisciplinar y   appr oach is k ey to under standing and shaping inno vation in a w ay tha t bene fits socie ty.   I  actually   first  started  thinking   in  multidisciplinar y  terms   much   earlier ,  in  the  late  1980s.   I  got   involved  with   the  Comput er  Science   and  Telecommunic ations   Boar d  at  the  National   Academies,    which   is  an  organization  chart ered  by  the  US  governmen t  to  advise   them   in  areas  wher e  a   multi-s takeholder   assessmen t  of  a  problem   is  needed.   I  chair ed  an  early   study   on  comput er   security   that  was  published   in  1991.   And  I  got  a  lot  out  of  it,  but  mos t  signific antly,  my   experience   with   the  National   Academies   taugh t  me  the  import ance   of  having  conversations    with   people   who   were  not  like  me—ec onomis ts,  social   scien tists,  artis ts,  lawyers,  regula tors.  I   chair ed  the  boar d  for  8  years,  learning   wha t  happens   when   you  get  people   with   very  different   poin ts  of  view  and  stakeholder   biases   together   to  produce   some thing   coher ent.  And  so,  as  I   moved  forward  with   my  technic al  resear ch,  I  carried   with   me  experiences   and  expertise   that   mos t pur e technologis ts did not ha ve.   This  third  wave  of  ethical  AI  is  intersecting   with   the  long-a waited  regula tion  of  big  tech.   Wha t   are your though ts on r egula tion?    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   3 1   
 In  terms   of  AI,  the  governmen t  has  awakened   to  social   implic ations   a  lot  earlier   in  the  lifecycle   of  the  technology   compar ed  to  the  Interne t.  With   the  Interne t,  it  took  about   15  years  after   commer cializ ation f or the g overnmen t to really w ake up, so some time ar ound 2010.    Ther e  is  always  tremendous   resistance   from  the  private  sector  around   regula tion  because   they   need   to  compe te,  which   can  mean   doing   things   that  harm   socie ty.  When   it  comes   to  matters  of   public   interest,  as  opposed   to  anti-compe titive  monopolis tic  practices,   if  one  actor  tries   to  be   “good, ”  they  will  lose.   But  if  you  impose   regula tion  on  everyone,   it  levels  the  playing  field.   The   financial   services   sector  is  an  example.   It  is  heavily  regula ted—not   to  addr ess  monopolis tic   practices,   but  for  public   interest  reasons.   It  adds   to  costs,  and  can  stifle  inno vation,   but  it  affects   all pla yers equally .   It  can  take  a  while   for  governmen ts  to  figur e  out  how  to  be  effectiv e.  They  may,  for  example,    impose   regula tion  on  the  wrong  players  for  the  wrong  reasons.   In  the  case  of  the  Interne t,   we’ve  seen   governmen ts  impose   regula tion  on  the  ISPs  regarding   objectionable   content,  like   child   pornogr aphy  or  terrorist  activities,   rather   than   the  applic ation  provider s.  The  rationale   is   that  it’s  easier   for  the  applic ation  provider s  to  escape  regula tion  by  relocating  oper ations   to   foreign   countries.   So,  the  ISPs  are  an  easier   target.  But  they  are  not  necessarily   the  right  target,   or an e ffectiv e one.    You  have  referred  to  an  “abstract”  explor ation  phase   of  resear ch  and  “basic”   vs  applied    technology   in  a  way  that  suggests  more  neutr al  phases   in  the  overall  resear ch  and  inno vation   process.   But  do  you  think   that  the  relationship   between  basic   and  applied   resear ch;  between   academic   and  indus try  resear ch;  and  the  path  from  discovery  to  invention  to  inno vation  in   gener al,  has  chang ed  over  the  years?  How  so?  Are  univ ersities   doing   less  basic,    curiosity -driv en resear ch as c ollabor ative inno vation incr eases?    Ther e  has  been   a  large  growth  in  comput er  science   (CS)  resear ch,  and  the  balance   has  certainly    shift ed  toward  more  applied   resear ch—closer   to  commer cializ ation.   Our  governmen t  is  pushing    investmen t  to  drive  inno vation,   mak e  our  country  more  compe titive,  and  so  on.  And  inno vation   is  not  the  only   driver  of  this  shift.   As  our  field   matures,  some   of  the  basic   ques tions   get   answered.  It  is  import ant  to  remember   that  in  the  late  1960s   and  early   1970s,   when   the  early    concep ts  behind   the  Interne t  were  emer ging ,  this  was  totally  a  venture  into  the  unkno wn.  But   there  are  still  folks  who   look   further   into  the  futur e.  I  am  actually   not  sure  I  buy  the  distinction    between  basic   and  applied   resear ch.  Those   may  not  be  the  right  divisions.   Some   resear ch  is   more  specula tive,  more  driven  by  curiosity—a   sense   of  explor ation.   If  you  are  looking   for  a   venture  today  that  is  going   into  the  great  unkno wn,  crypto-curr ency   comes   to  mind.   Specula te   on  good  and  bad  consequences   of  that  idea,   and  how  it  will  (or  may)  be  shaped   by  various    forces.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   3 2   
 Is ther e an ything y ou w ould ha ve done diff erently when designing the In terne t?   We  designed   the  Interne t  for  gener ality,  that  was  its  strength.  The  whole   idea   was  that  you   could   build   anything   on  top  of  that  platform.   Of  course,  I  hoped   that  smart   people   would   come    in  and  build   really   cool,  useful  things.   I  also  expect ed  that  people   would   come   in  and  build    frivolous   things,   which   is  fine,   and  I  always  knew  that  people   and  organizations   would    eventually do wick ed things on the In terne t.   I  love  the  writings   of  Terry  Pratchet.  He  writes  social   satire  cast  as  science   fiction.   His  view  of  the   world  is  that  life  is  about   performing   a  series   of  experimen ts  that  reveal  how  people   really   are.  I   see  the  Interne t  as  such   an  experimen t,  and  wha t  we’ve  discovered  is  that  much   of  the  world  is   evil, but I guess w e kne w tha t already .   Fusing Art and Engineering f or a mor e Humane T ech Futur e   [Original article b y Na talie Klym]    Interview  with   Domhnaill   Hernon,   Global   Lead   of  Cognitiv e  Human   Enterprise   at  EY  and   former Head of Experimen ts in Arts and T echnology (E. A.T.) at Nokia Bell Lab s   Marshall   McLuhan   belie ved  that  artis ts  were  the  best  probes   into  the  futur e  of  technology    because   they  lived  on  the  frontiers.  They  were  the  mos t  likely  to  take  technology   in  directions    beyond  the  intentions   of  the  scien tists  and  engineer s.  But  according   to  Domhnaill   Hernon,    artis ts  don’t   just  think   outside   the  box  in  terms   of  features  and  applic ations,   their   mos t   import ant  contribution   to  tech  inno vation  is  the  ability   to  create  a  much   needed   human-cen tric   vision of the futur e.   Domhnaill,   you  just  ended   a  5-year  term  leading   Bell  Labs’  Experimen ts  in  Arts   and   Technology   program,  one  of  the  handful   of  corpor ate  programs   in  the  U.S.  that  integrated  the   arts  with   R  &  D.  And  now  you  are  creating  a  new  initia tive  at  EY  (Erns t  &  Young)   called   the   Cognitiv e  Human   Enterprise.   Can  you  tell  us  more  about   the  work  you  do  with   artis ts  and  how   it br ough t you to EY?    I  was  asked  to  lead   a  new  initia tive  at  EY  to  show  the  potential  of  fusing   art/creativity   and   technology   to  create  the  mos t  cognitiv ely  diverse  organizations   possible.   The  role  builds   directly    on  wha t  I  had  achie ved  at  Bell  Labs’  Experimen ts  in  Arts  and  Technology   program  and  supports    EY’s commitmen t to wha t the y call Humans@Cen ter.   My  unique   appr oach   is  to  leverage  the  signific ant  differences   between  the  world  of   technology/business   and  art/creativity .  This  is  a  lofty   goal  but  I  truly   belie ve  that  the  futur e  of   human-cen tered inno vation lies a t the in tersection of art and t echnology .   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   3 3   
 Can y ou tell us mor e about the his tory of the E. A.T. program and ho w you ended up ther e?   Bell  Labs  had  been   bringing   engineering   science   and  the  arts  together   since   its  incep tion  in   1925,   all  the  way  up  until  about   the  early   1980s.   One  of  the  standout   momen ts  from  that  period    was  the  creation  of  a  global   not-for-profit  organization,   called   E.A.T.,  which   stands   for   Experimen ts  in  Art  and  Technology ,  in  the  1960s.   It  emer ged  out  of  a  series   of  art-perf ormance    events  called   9  Evenings:   Thea tre  and  Engineering   held   in  1966.   These   events  comprised    collabor ations   between  Bell  Labs’  engineer s  and  several  prominen t  artis ts  of  the  time   including    experimen tal  music   composer   John   Cage,  the  abstract  expressionis t  pain ter  Robert    Rauschenber g,  dancer   and  chor eogr apher   Yvonne   Rainer ,  and  man y  other s.  But  from  the  1980s    until about 2016 the fusion of art and engineering w as lar gely non-e xistent at Bell Lab s.   Wha t happened in the 80s? Wh y did the E. A.T. program end?    Ther e  were  major   chang es  in  U.S.  socioec onomic   policy   that  chang ed  how  indus try  in  gener al   was  being   regula ted  and  how  resear ch  was  being   funded,   and  a  lot  of  other   shifts   including    things   like  how  emplo yees  were  treated.  Ther e  was  pressur e  on  corpor ations   investing  in  wha t   man y  perceived  as  frivolous   artis tic  activity–and   R&D   in  gener al–to  reduce   funding   to  those    programs.    And then wha t happened in 2016?    I  moved  in  late  2015   from  Bell  Labs  in  Ireland   to  the  HQ  of  Bell  Labs  resear ch  in  New  Jersey.   Soon   after  I  arriv ed  it  was  the  50th   anniv ersary  of  E.A.T.’s  incarna tion.   Several  of  us  at  the   leader ship  level  got  invited  to  several  celebr atory  events  in  New  York  City  that  were  essen tially    engineer s  +  artis t  mee tups.  Through   those   events  we  learned   about   the  history  of  Bell  Labs’   work  with   artis ts  and  I  realiz ed  that  we,  as  an  institution,   had  forgotten  about   that  part  of  our   history.  From  that  poin t  onward,  we  learned   more  and  more  about   the  critic al  value   of  fusing    art and engineering , and the immensely signific ant role tha t Bell Lab s had pla yed.   And  at  the  same   time,   but  separ ately,  we  were  having  internal   conversations   about   wha t  was   missing   from  our  resear ch  strategy  and  wha t  we  wanted  from  new  talen t  and  our  organizational    cultur e.  So,  these   were  the  conversations   we  had  during   the  day,  and  then   in  the  evenings,   we   attended the artis t mee tups commemor ating the E. A.T. program.   Every  one  of  the  interactions   blew  my  mind.   I  realiz ed  that  the  artis ts  had  a  comple tely  different   perspectiv e  on  everything   —  from  the  intersection   of  technology   with   socie ty  to  life  in  gener al.   Mor e  specific ally,  the  role  that  humans   play  in  technology   developmen t  was  at  the  center  of   every  answer  they  provided   to  my  ques tions.   This  was  impr essiv e  because,   as  an  engineer ,  I  had   not  been   trained   that  way  and  I  could   not  belie ve  that  I  was  so  blind   to  this  perspectiv e.  It   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   3 4   
 struck   me  that  we  needed   an  organizational   cultur e  that  emphasiz ed  a  more  human- focused    appr oach t o inno vation.    So  I  decided   to  establish   a  new  initia tive,  based   on  the  original   E.A.T.  program  I  had  just  learned    about, but f ocused on modern da y needs.    So, how w as the ne w pr ogram diff erent from the original one?    The  original   program  was  a  not-for-profit  entity,  separ ate  from  Bell  Labs.  It  grew  organically  out   of  the  interper sonal   connections   between  the  artis ts  and  engineer s,  wher eas  the  new  initia tive   was  a  sanctioned,   funded,   internal   initia tive  that  evolved  to  become   its  own  resear ch  lab  within    Bell  Labs.  It  was  designed   more  purpose fully,  based   on  wha t  we  learned   from  the  original    program.   The  context  was  also  very  different.  Back   in  the  60s  and  70s  digit al  technology   was  very  new.   Artis ts  had  a  lot  of  creative  ideas   but  didn’t   have  the  technologic al  knowho w  to  manif est  them.    They  leveraged  Bell  Labs’  engineer s  and  scien tists  expertise   to  mak e  the  technology   requir ed  to   enable   their   creative  ideas.   That  is  not  the  case  today  as  man y  multimedia   artis ts  are   technologic ally gift ed.   Ther e’s  also  just  that  much   more  user -friendly   consumer   technology   today,  but  it  wasn’t   always   the  case.  Dan  Rich ter,  who   played  the  ape,   Moon watcher ,  in  the  opening   scene   of  Stanle y   Kubrick’ s  iconic   AI  film,   2001:   A  Space   Odyssey,  was  a  gues t  on  the  seminar   series   I  run  at  the   Univ ersity  of  Toronto’s  BMO   Lab,  which   focuses   on  the  relationship   between  AI  and  art.  He   talked  about   how  much   new  technology   had  to  be  created  to  enable   Kubrick’ s  vision.   That  was   back in the early 1960s.    Yes,  in  fact,  Artur   C.  Clark e  (the  author   of  the  novel)  spen t  a  lot  of  time   at  Bell  Labs.  Ther e  was  a   major   relationship   between  Bell  Labs  and  the  production   of  that  film.   They  developed   futuris tic   props  such   as  the  video   phone,   and  Max  Matthews,  who’ s  consider ed  the  godfather   of   comput er  music,   inspir ed  some   of  the  music   in  2001   such   as  HAL  singing   “Dais y  Bell”   towards   the end of the film.    How does the E. A.T. program bene fit the engineer s?   Today,  the  E.A.T.  artis ts  are  more  technologic ally  savvy  and  the  program  is  designed   to  be  more   mutually   bene ficial.   When   we  pair  artis ts  with   engineer s  and  scien tists,  the  artis ts,  as  before,   get  access   to  tech  they  wouldn’t   other wise,   but  in  the  new  program  they  infuse   their   human    focus   deep   into  the  R  &  D  community .  In  other   words,  the  artis ts  are  there  to  enligh ten  the   engineer s.  And  by  that  I  mean,   STEM   practitioner s  are  very  well  trained   in  the  scien tific   methods   but  that  blinds   us  to  other   ways  of  thinking   and  problem   solving.   We  wanted  the   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   3 5   
 artis ts  to  infuse   R  &  D  with   an  ethos   of  humanizing   technology .  We  wanted  the  engineer s  and   scien tists  to  always  have  in  their   minds   the  human   aspect   of  technology;   to  ques tion  how  this   technology   migh t  do  good  or  harm   to  socie ty,  and  how  they  migh t  design   out  the  ability   to  do   harm   in  the  earlies t  stages  of  a  resear ch  project.   We  also  wanted  to  expose   our  R&D   community    to ne w forms of cr eativity .   In  my  previous   interview  with   MIT’s  David  Clark,   one  of  the  early   Interne t  pioneer s,  he   emphasiz es  how  difficult   it  is  to  predict   the  outcomes   of  technology–t o  “design   out”  those    possibilities as y ou put it.    I’m  not  saying  it’s  easy.  It’s  very  difficult,   but  at  the  very  least,  technologis ts,  engineer s,   scien tists,  resear cher s  should   be  asking   those   ques tions,   they  should   be  aware  of  that  human    elemen t.  That  awareness   isn’t   part  of  how  engineer s  are  educ ated  or  expect ed  by  their    emplo yers  to  create  value   in  the  mark etplace.   Wher eas  artis ts  have  an  inher ent  way  of  keeping    the  human   in  mind,   first  and  foremos t.  I  wanted  to  integrate  their   way  of  thinking   into  our  R  &  D   community   in  a  deeply   purpose ful  way.  That  way,  we  could   drive  real  cultur al  chang e  around    this  founda tional   concep t  of  humanizing   technology .  I  see  this  type   of  holis tic  appr oach   as  the   driver of human-cen tered inno vation.    That  migh t  be  one  of  the  great  untapped   potentials  of  fusing   art  and  technology–the   ability   to   sense and cr eate a human-cen tric vision f or the futur e.   Your  poin t  about   creating  value   in  the  mark etplace   is  interesting  and  mak es  me  ques tion   whe ther   it’s  the  technology   and  technologis ts  that  need   to  be  “humaniz ed”  or  business   and   the e xecutiv es managing firms. In other w ords is t ech the pr oblem or is it the t ech indus try?   I  don’t   think   the  problems   in  socie ty  can  be  blamed   on  technology   directly .  Technology   is  just  a   tool  that  is  designed   and  used   by  humans   in  various   ways.  I  also  don’t   think   it’s  fair  to  say  it’s   purely  a  business   problem   either .  I  think   every  aspect   of  the  chain   needs   redefining   and  all   elemen ts  of  the  chain   need   to  work  together   in  tandem.   Much   of  my  work  is  about   getting  to   the  core  of  wher e  the  tensions   reside,   and  fundamen tally,  it’s  about   adding   the  human   elemen t   to  the  design   of  technology   and  a  human   elemen t  to  how  businesses   leverage  the  skills   of   engineer s  to  create  value   and  a  human   elemen t  to  how  businesses   push   technology   out  into  the   mark et.   We’ve  evolved  to  a  poin t  wher e  we  largely  rely  on  mark ets  and  we  develop   technology   to   survive  and  thriv e  as  humans.   A  core  part  of  the  human   condition   is  that  we’re  going   to  develop    systems  and  paradigms   and  tools  that  are  developed   by  humans   for  humans   and  they  will  have   an impact on socie ty.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   3 6   
 My  main   issue   today  when   I  look   across  the  chain   is  with   the  educ ation  and  training   aspect   of   science   and  engineering.   When   you’re  studying   or  working   in  technology ,  all  problems   are   technology   problems   and  all  solutions   are  technology   solutions.   It  was  really   an  eye  opening    experience   to  work  with   artis ts.  It  made   me  realiz e  the  trap  I  had  fallen   into  and  that  I  was  blind    to  the  other   lenses   through   which   you  can  view  the  world  and  solve  problems.   Ther e’s  a  lack  of   connection   to  the  humanities,   a  gap.  However,  making   that  connection   in  an  impactful   way  is   not  easy.  The  E.A.T.  program,  as  I  said  earlier ,  was  about   making   purpose ful  connections   and   really bringing the bes t of both w orlds t ogether.   You’re  reminding   me  of  my  experience   working   with   a  resear ch  group  at  the  MIT  Media   Lab   that  made   the  integration  of  art  as  one  of  its  goals.   But  it  was  a  vaguely   defined   objectiv e  and   the  project   leader s,  neither   of  whom   were  artis ts,  didn’t   know  exactly   wha t  it  mean t  or  how   to  do  it,  and  were  very  open   to  suggestions.   I  appr eciated  their   hones ty,  because   it  isn’t   easy,   as you sa y, to mak e the c onnection in an impactful w ay.   Ther e  is  a  real  lack  of  under standing   of  how  to  bring   these   different  ways  of  thinking   together.   It’s  very  hard  work.  I  still  see  a  lot  of  efforts  that  are  quite  superficial,   wha t  I  would   call  a  “check    the  box”  exercise.   And  I  see  a  lot  of  efforts  that  are  random–an   artis t  is  randomly   select ed  and   paired  with   a  randomly   select ed  engineer   and  they  are  put  together   randomly   in  some   common    space   for  a  short   period   of  time.   In  that  model   if  anything   good  was  to  come   out  of  the   interaction   it  would   be  just  fluke.  These   initia tives  need   to  be  though t  through   purpose fully   and   strategically and e xecut ed with pr ecision within the bounds of wha t you ha ve control over.   I  also  encountered  an  attitude   from  some   of  the  engineer s  I  worked  with   over  the  years  that   art,  or  any  of  the  social   sciences   for  that  matter,  was  someho w  inferior   or  insub stantial.  The   word “fluff ” was used on man y occ asions t o describe these disciplines.    Yes,  and  when   I  started  the  Bell  Labs  program  I  had  to  think   through   all  the  ways  in  which   the   program c ould be killed, giv en tha t kind of a ttitude.    But  even  when   there’s  a  lot  of  goodwill,   and  good  intention,   there’s  still  not  a  lot  of  good   execution.   I  found   there  were  two  main   appr oaches   to  art  and  tech  fusion.   One  was  extremely    transactional.   A  compan y  would   bring   in  an  artis t  for  a  couple   of  weeks  and  say,  here’s  our  new   product,   do  some thing   cool  with   it.  But  then   that  was  it.  The  impact   was  short   term  and   superficial, driv en primarily b y communic ations and br anding g oals.    Then   there  was  the  comple tely  ad  hoc  appr oach   wher e  someone   in  the  organization  would   say,   oh,  we  need   to  bring   artis ts  in,  and  they  would   randomly   select   an  artis t  and  likewise   a  random    group  of  emplo yees  who   would   engage  with   the  artis t.  They  would   put  them   in  a  space    together   and  think   some thing   would   just  emer ge  and  that  the  organization  would   suddenly    become mor e creative.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   3 7   
 By  contrast,  I  designed   the  modern   E.A.T.  program  more  strategically  and  more  purpose fully,   with   ways  to  measur e  impact.   And  again,  I  designed   around   the  modes   of  failur e  I  was  aware  of   and  I  applied   the  concep t  of  a  pre-mort em  to  my  analy sis  and  design   of  the  new  initia tive.  I   spen t  a  lot  of  time   getting  to  know  artis ts,  their   personalities,   their   openness   to  collabor ation   and  their   technologic al  capabilities.   And  the  same   thing   for  the  scien tists  and  engineer s,  so  I   could   mak e  the  right  match.  I  also  had  to  factor  in  deliv erables   and  schedules   of  those   engineer s   and  the  perception  of  their   manag emen t  chain   so  that  we  covered  all  dimensions   of  success   as   much as possible.    I  had  to  think   it  through   as  much   as  I  would   an  actual   technic al  product   that  would   go  to   mark et.  Also   very  import ant  is  the  fact  that  I  viewed  it  as  a  major   cultur al  chang e  initia tive,   which ar e kno wn t o ha ve a high f ailur e rate.   Wha t  were  some   of  the  early   proof  of  value   experiences   you  had,   and  how  did  they  evolve   over time?    In  some   cases,   the  proof  of  value   was  an  exceptionally   insigh tful  conversation  that  comple tely   chang ed our per spectiv e on t echnology and in formed a ne w resear ch dir ection.    From  there  we  developed   whole   new  classes   of  technology–not   just  out  of  the  conversations    with   artis ts  but  also  out  of  the  collabor ations   wher e  artis ts  were  using   our  technology   in  very   different ways.   Technology   is  often  used   differently  than   how  its  inventors  intended.   In  cases   wher e  the   technology   in  ques tion  is  a  creative  tool,  you  get  some   amazing   stories.   The  electric   guitar,  for   example,   was  a  technic al  solution   to  the  very  practic al  problem   of  amplific ation,   but  Jimi   Hendrix   and  other   musicians   created  a  whole   new  sound.   Stevie  Wonder   did  the  same   thing    with   the  synthesiz er,  turning   technic al  and  gimmicky   sounds   into  a  whole   new  artis tic   practice. Wha t were some of the artis t-driv en consequences y ou sa w at Bell Lab s?   One  of  the  earlies t  examples   was  in  the  area  of  wearables.   We  had  asked,  wha t’s  the  next   communic ation  device  after  the  smartphone?   This  was  around   2016.   We  were  looking   10  years   out.  You  had  to  assume   the  smartphone   didn’t   exist  anymor e.  We  started  from  a  technologic al   resear ch  perspectiv e  that  led  to  ideas   of  disag gregating  smart   phone   functionality   so  that  we   could   communic ate,  control  and  sense   the  world  around   us  in  new  ways.  Our  earlies t  designs    and  prototypes   were  very  utilit arian   and  clearly   designed   with   technology   at  the  center.  Then    we brough t in artis ts and appr oached the ques tion fr om c omple tely ne w angles.    One  of  the  first  artis ts  we  collabor ated  with   in  the  modern   E.A.T.  era  was  Jeff  Thomp son.   He   poin ted  out  to  us  that,  even  at  that  time   in  2016,   we  were  all  spending   an  order  of  magnitude    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   3 8   
 more  time   on  our  smartphones   than   we  were  with   the  people   we  mos t  loved  in  the  world!   This   was  an  eye  opening   observation  that  helped   us  comple tely  rethink   the  design   and  developmen t   of  these   new  wearable   concep ts  to  be  more  human   centric.  We  designed   a  wearable   for  your   arm  and  one  for  your  head–the   Sleeve  and  the  Eyebud–tha t  worked  in  combina tion  in  much    more  intuitiv e  and  non-in trusiv e  ways  and  remo ved  the  need   to  keep  looking   at  your   smartphone.   So  our  initial   conversations   focused   on  the  problem   from  a  technologic al   perspectiv e  (solving   the  biggest  tech  challeng es  in  creating  wearables),   but  the  solution   we   ended   up  with   came   directly   out  of  our  artis tic  collabor ations   and  showed  you  could   sense   and   control  the  world  around   you  in  much   more  human   centric  ways,  using   the  more  natural  forms    of your body and le veraging the t echnology in a s ymbiotic w ay.   Wha t about AI?    I  think   there  are  two  main   popular   narratives  surrounding   Machine   Learning   (ML)   and  Artificial    Intelligence   (AI)  at  the  momen t  and  both   stem  from  different  interpr etations   in  the  value   of   automa ting “mundane” t asks.   In  one  argumen t  people   talk  a  lot  about   how  AI  can  be  used   in  indus try  to  enhance    efficiency/pr oductivity   through   automa tion  of  the  mundane   and  the  popular   assump tion  is  that   this  appr oach   will  lead   to  job  losses.   I  think   this  is  probably   a  reasonable   current  assump tion   since   very  few  in  indus try  or  academia   that  are  resear ching   and  developing   these   AI  tools  have   provided   a  strong  counter  argumen t.  It  is  clear   that  current  business   imper atives  are  based   on   cost  savings   and  margin  increases   and  AI  has  the  potential  to  bene fit  companies   across  all   indus tries in tha t regard.   The  second  argumen t  is  that  automa tion  will  free  up  people’ s  time   and  then   they  can  be  more   creative,  productiv e  and  strategic  with   that  time   and  create  more  value.   The  difficulty   with   this   argumen t  is  that  people   can’t  just  become   more  creative/pr oductiv e/strategic–w e  need   to   develop t ools tha t will help them on tha t journe y.   So  either   way,  we  have  a  gap  between  the  bene fits  that  AI  can  provide   and  the  narrative   surrounding   AI  and  its  use  in  indus try.  We  need   to  figur e  out  a  way  to  free  up  people’ s  time    from  the  mundane   tasks  and  help   them   be  more  creative  and  productiv e  with   that  time   to   create  more  value.   The  value   they  create  needs   to  be  more  than   the  savings   created  through    the pot ential of job r eductions.    I’m  also  very  interested  in  counteracting   the  dystopian   narratives  around   AI.  These   negative   stories   are  typic ally  based   on  a  fundamen tal  lack  of  under standing   of  the  technology   and  the   lack  of  under standing   on  the  potential  for  the  technology   to  enhance   human   creativity   and   potential.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   3 9   
 For  example,   at  Bell  Labs  we  wanted  to  showcase  instead,   the  potential  for  AI  to  enhance    human   creativity .  One  project   is  “We  Speak   Music”   and  features  the  beatboxer  Reeps  One.   We   trained   ML  algorithms   on  his  voice  as  he  was  beatboxing  to  the  poin t  wher e  they  started   gener ating  sounds   and  techniques   that  he  had  never  created  in  his  life,  yet  the  AI  voice  kind   of   sounded lik e him.    Prior   to  this  experimen t,  Reeps  One  felt  that  he  had  pushed   the  capability   of  his  voice  to  the   absolut e  limit.   He  didn’t   think   there  was  anything   else  he  could   do  to  augmen t  his  voice  and   had  started  branching   into  other   areas  of  art  to  satisfy  his  creativity   and  curiosity .  But  through    this  experimen t  we  gave  him  wha t  we  called   a  “second  self”,  an  AI  digit al  beatboxing  twin,   for   him  to  collabor ate  with   and  according   to  him  this  enabled   him  to  “level  up”  his  voice  and  he  is   now cr eating ne w sounds and t echniques and c omposing and perf orming in ne w ways.   Think   about   that–we  took  one  of  the  best  beatboxers  that  ever  lived  and  one  of  the  mos t   creative  people   I’ve  had  the  pleasur e  of  working   with   and  we  helped   him  be  more  creative  by   creating  an  AI  digit al  twin   of/for  him  to  collabor ate  with.   Can  you  imagine   the  potential  for  AI  to   enhance the cr eativity of all people if it w as de veloped righ t and f or everyone?    The  seminar   that  I  run  at  U  of  T’s  BMO   Lab  ques tions   the  role  of  technology   in  the  creative   process   and  ther e’s  definitely  a  tension   around   the  ques tion  of  how  much   tech  is  too  much?    At  wha t  poin t  is  it  no  long er  human   creativity?   Is  that  a  good  thing   or  a  bad  thing?   Has  anyone   ever vie wed the idea of an AI-based digit al twin as a dy stopian narr ative?   Never.  I’ve  never  hear d  anyone  even  ques tion  the  experimen t  from  that  perspectiv e.  Wha t  was   import ant  to  me  was  developing   AI  in  a  way  that  involves  actual   humans,   that  took  embodied    cognition   or  embodied   intelligence   into  consider ation  and  wher e  the  technology   was  in  service   to our humanity and not vie wed as a r eplacemen t.   I  belie ve  the  reason   this  ques tion  didn’t   arise   out  of  our  work  is  because   we  collabor ated  with    this in tent from da y 1. The whole poin t of the c ollabor ation w as to dispel this sen timen t.   Wha t’s  interesting  to  me  about   the  work  of  Reeps  One   is  that  it’s  not  about   automa tically   producing   a  piece   of  music   “in  the  style  of,”  like  a  deep   fake,  it’s  about   an  actual   collabor ation   between a human and machine. Can y ou sa y a lit tle mor e about embodied in tellig ence?    I  think   there  is  a  lot  of  work  to  be  done   to  dispel   some   of  the  myths  and  assump tions   in  AI   today.  For  example,   this  notion   that  AI  will  super sede   human   intelligence   is  nonsensic al  to  me   because   the  way  AI  is  developed   today  is  based   on  a  flawed  under standing   of  human    intelligence.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   4 0   
 The  neur al  net  (machine   learning   and  deep   learning)   type   architectur es  of  today  are  based   on   the  mathema tical  models   that  AI  pioneer s  created  30-40   years  ago  based   on  how  they  though t   the  human   brain  worked.  We  know  now  that  the  model   is  more  of  a  metaphor   and  not  how  the   brain  actually   works  today  based   on  neur oscience;   however,  the  model   is  simple   and  pervasive   and w on’t chang e an ytime soon.    The  problem   with   the  neur al  net  model   of  human   intelligence   is  the  aspect   of  disembodimen t   —  the  absence   of  a  human   body .  The  human   brain  on  its  own  has  no  intelligence,   cognition,    creativity   or  consciousness.   It  has  to  be  connect ed  to  the  human   body .  The  brain  is  a   comput ational   pattern  recognition   engine   that  requir es  sensor y  inputs   from  your  physical  body .   Without the body , the br ain is nothing.    But  because   of  the  flawed  founda tions   of  AI,  we  have  this  equally   flawed  idea   of  intelligence    that  encourages  us  to  imagine   we  can  replic ate  or  super sede   human   intelligence,   which   has  all   sorts of pr actic al and e thical implic ations.    I  have  no  doub t  that  we  are  creating  a  new  type   of  intelligence,   which   may  be  able   to  do  things    humans   can’t,   but  it’s  not  going   to  be  more  intelligent  in  the  way  humans   are  intelligent.  It’s   going t o be dif ferent.   Wha t do y ou hope t o achie ve in y our ne w role a t EY?    One  of  the  powerful   lessons   I  learn   everyday  working   with   artis ts  is  to  remember   that  we  are   human,   remember   wha t  is  special   about   humanity   and  keep  that  front  and  center  when    developing t echnology . This is some thing tha t I am v ery excited about with m y ne w role a t EY.   EY  have  invested  in  diverse  communities   for  decades.   For  example,   they  set  up  more  than   10   global   neur odiv erse  centers  of  excellence   and  hired  hundr eds  of  people   from  that  community    showing   the  world  the  immense   value   that  people   with   different  experience   and  skills   can  bring    to an y organization.    I  co-founded   a  new  initia tive  called   the  Cognitiv e  Human   Enterprise.   The  objectiv e  is  to  solve   global-sc ale  human   and  business   challeng es  by  investing  in  massiv ely  interdisciplinar y   collabor ation  and  full-spectrum   diversity  to  create  the  mos t  cognitiv ely  diverse  organizations    possible.   One  aspect   of  acceler ating  towards  that  cognitiv e  diversity  is  to  leverage  the  bene fits   of  fusing   art  and  technology .  Given  EY’s  commitmen t  to  Humans@Cen ter  I  am  excited  to  see   how far w e can take this and deliv er on human-cen tered inno vation.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   4 1   
 Sociology of AI Ethics   Challeng es of AI De velopmen t in Vie tnam: Funding , Talen t and E thics    [Original article b y Ng a Than and Khoa Lam]    Vietnam   in  2020   overtook  Singapor e’s  gross  domes tic  product   (GDP),   and  became   the  third   largest  econom y  in  ASEAN,  the  Associa tion  of  Southeas t  Asian   Nations.   Immedia tely  after  the   new  national    leader ship  was  elect ed  at  the  Communis t  Party  of  Vietnam’ s  Congr ess  in  Januar y   2021,   Presiden t  Nguy en  Xuan  Phuc   signed   an  import ant  documen t  entitled   National   Strategy  on   R&D   and  Applic ation  of  Artificial   Intelligence,   or  the  Strategy  Documen t.  The  14-pag e  documen t   outlines   plans   and  initia tives  for  Vietnam   to  “promot e  resear ch,  developmen t  and  applic ation  of   AI,  making   it  an  import ant  technology   of  Vietnam   in  the  Fourth   Indus trial  Revolution. ”  Vietnam    aims   to  become   “a  center  for  inno vation,   developmen t  of  AI  solutions   and  applic ations   in   ASEAN and o ver the w orld” b y 2030.    With   ambitious   goals,   the  strategy  documen t  provides   some   directions   to  wher e  Vietnam    should   go  in  the  next  decade.   It  shows  that  it  follows  China’ s  and  other   Asian   countries’    foots teps  in  becoming   a  techno-de velopmen tal  state  which   takes  advantage  of  technologic al   chang es  for  economic   developmen ts.  While   outlining   wha t  16  minis tries   and  the  Vietnam    Academ y  of  Science   and  Technology   need   to  do  in  the  next  10  years,  the  documen t  does   not   show  how  other   players  such   as  startup   founder s,  civil  socie ty,  and  bene ficiaries   of  AI,  common    user s  in  Vietnam’ s  AI  econom y  should   do.  It  also  has  no  men tion  of  the  role  of  AI  ethics   in  this   developmen t.  Without   any  consider ation  to  import ant  ethical  issues   such   as  privacy  and   surveillance,   bias  and  discrimina tion,   and  the  role  of  human   judgmen t,  AI  developmen t  in  the   country migh t only bene fit a small gr oup of people, and possibly bring harms t o other s.   In  this  op-ed   we  examine   three  key  issues   regarding   AI  developmen t  that  any  country  would    have to tackle when joining the AI global r ace: Funding , Talen t and E thics.    Funding    AI  developmen ts  need   a  large  amoun t  of  funding   coming   from  a  variety  of  sour ces  such   as   interna tional   venture  capital  firms,   local  venture  capitalists,  governmen t  fundings,   or   companies’   own  profits.   Funding   of  AI  developmen t  in  Vietnam   is  lagging   behind   other    Southeas t  Asian   countries.   In  2019,   Vietnam’ s  AI  investmen t  per  capita  was  under   $1,  while   the   Southeas t  Asian   leader   Singapor e  has  $68  worth   of  AI  investmen t  per  capita.  Venture  capital   investmen t  suffered  in  the  first  half  of  2020   due  to  COVID-19.   However  with   the  governmen t’s   assis tance,   there  has  been   some   sign  of  impr ovemen t  regarding   funding   in  the  near   futur e.  At   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   4 2   
 the  Vietnam   Venture  Summit   2020,   both   foreign   and  domes tic  investors  pledg ed  to  invest  $800    millions   in  Vietnam’ s  startup   ecosystem.  According   to  Crunchbase,   currently,  there  are  155   venture capital investors with in vestmen ts in the c ountry.   Tech  startup s  receiv ed  the  mos t  investmen t  funding   especially   in  e-commer ce,  fintech,  and  AI.   The  governmen t  also  provided   state  funding   at  the  national   and  city  level  to  encourage   entrepreneur ship.   As  a  result,   the  startup   ecosystem  in  cities   like  Ho  Chi  Minh   City  and  Hanoi    thriv ed in 2020, be fore the f ourth w ave of C OVID-19 hit the c ountry in April 2021.    The  strategy  documen t  outlines   the  role  of  the  Minis try  of  Planning   and  Investmen t  to  “to   attract  venture  capital  funds   to  inno vative  AI  start-ups  in  Viet  Nam. ”  The  ques tion  remains   open    as  to  wha t  the  plans   to  bring   interna tional   capital  for  domes tic  technologic al  developmen t  are,   which   specific   areas  of  AI  should   be  the  main   areas  of  investmen t,  how  would   the  capital  be   distribut ed,  and  will  there  be  any  accountability   mechanisms,   and  who   are  these   entities    enforcing acc ountability?    Businesses    The  developmen t  of  AI  in  Vietnam   has  been   driven  primarily   by  private  businesses.   The  strategy   documen t  outlines   a  push   towards  digitiz ation  and  indus try  4.0  to  create  incen tives  for   businesses   to  become   more  aware  of  the  potential  of  data  science   and  AI.  Vietnamese    companies   are  still  in  the  early   stages  of  developmen t.  Only   a  few  large  corpor ations   are   prominen t  in  the  AI  space,   notably  FPT,  Vingr oup,   Zalo,  who   have  the  resour ces  to  invest  in  the   resear ch, de velopmen t, and deplo ymen t of AI.    From  our  conversations   with   professionals   in  the  space,   smaller   companies   run  into  a  key   challeng e:  product -mark et  fit.  To  wha t  extent  is  the  Vietnamese   public   willing   to  adop t  new  AI   solutions   as  opposed   to  existing  solutions?   As  Nam   Nguy en,  the  CTO  of  an  ecommer ce  compan y   in  Ho  Chi  Minh   City,  puts   it:  “If  it  takes  a  lot  of  mone y  to  invest  in  AI,  but  its  economic   bene fits   are  not  yet  signific ant.  Businesses   in  Vietnam   will  not  jump   on  this  AI  bandw agon.  Only   big   companies   with   extra  capital  can  be  in  this  AI  playing  field. ”  This  problem   is  also  prevalen t  in   countries   wher e  AI  developmen t  is  more  mature.  Man y  companies   in  the  US,  for  example,   are   still  struggling   to  scale  AI  solutions   wher e  AI  was  developed   prior   to  finding   customer s  who   are   willing   to  adop t  it.  Vietnamese   companies   also  have  to  compe te  against  foreign   or  import ed  AI   solutions,   and  the  lack  of  venture  capital  investmen t  from  both   domes tic  and  foreign   funds.    Futur e strategy documen ts should addr ess these particular issues in de tail.   Talen t Pool   Ther e  is  no  short age  of  technic al  talen t  in  Vietnam.   However,  AI  educ ation  is  relatively  new  in   Vietnam.   Mos t  of  the  tech  workforce  are  still  working   in  outsour cing.   The  talen t  pool   is  young    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   4 3   
 and  specializ ed:  young   because   the  majority   of  the  talen t  pool   is  IT  gradua tes,  working   data   scien tists,  or  softw are  engineer s  with   few  years  of  experience,   and  specializ ed  because   there  is   a  strong  affinity   to  acquir e  a  technic al  skill  set  in  niche   machine-learning   areas  (e.g.,  deep    learning ,  GANs,   reinforcemen t  learning)—as   opposed   to  a  more  gener al  product   or  project    manag emen t skill se t.   Skilled   talen t  often  look s  for  professional   opportunities   abroad,   wher e  salaries   would   be   drastically  higher .  Furthermor e,  these   opportunities   would   enable   them   to  activ ely  participa te   in  the  resear ch,  developmen t,  and  deplo ymen t  of  state-of-the-art   AI  technologies   in  more   AI-ma ture countries.    Given this landsc ape, ther e are challenging c onditions t o effectiv ely r etain t alen t in Vie tnam:    ●  Salaries   have  to  be  compe titive,  compar ed  to  both   regional   (i.e.,   Southeas t  Asia)   and   global mark ets.   ●  Ther e  have  to  be  professional   developmen t  opportunities   for  talen t  (e.g.,  courses,   interna tional   conferences,   etc.)  wher e  they  can  keep  up-to-da te  with   the  latest  trends    and pr actices in AI de velopmen t.   As  Tuan  Anh,   resear ch  scien tist  at  VinAI,   claims:   “We  need   to  attract  Vietnamese   scien tists  back    to  Vietnam.   The  key  issue   is  still  the  salar y.  It’s  difficult   for  a  Vietnamese-based   compan y  to   compe te with Google, DeepMind, Micr osoft when it c omes t o salar y.”   It  is  worth   men tioning   that  there  is  also  a  languag e  barrier   to  learning   AI.  As  AI  educ ation   material   is  predominan tly  in  English,   it  is  crucial   to  enable   young   talen t  with   the  necessar y   languag e  learning   support   in  addition   to  a  more  technic al  educ ation  in  AI.  “Studen ts  in  special    programs  have  English   curricula.   However,  it  only   accep ts  50-60   studen ts  per  year,”  says  Khoa t   Than, a pr ofessor a t Hanoi Univ ersity of Science & T echnology .   Public P erception of AI and the Missing E thics Con versation   In  Vietnam,   AI  is  viewed  overwhelmingly   positiv ely.  It  is  regarded  as  a  catalytic  force  for   economic   and  technologic al  advancemen t.  In  the  public   mind,   the  concep t  of  wha t  AI  is,  how  it   is  used,   and  who   it  affects  are  not  as  clear .  Due  to  the  push   towards  digitiz ation  and  indus try   4.0,  the  Vietnamese   may  see  AI  only   as  a  tool  reser ved  for  indus tries,   wher e  some    implemen tation  of  natural  languag e  processing   and  comput er  vision   are  used   to  further    business   objectiv es.  However,  these   cases   are  only   among   a  plethor a  of  AI  applic ations   that  the   public   have  already   been   using   in  their   everyday  life.  It  migh t  not  be  immedia tely  obvious   that   the  routes  that  Grab  drivers  use  to  navigate  the  heterogeneous   street  network  in  Saigon  are   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   4 4   
 select ed  by  an  algorithm,   or  that  the  discounted  products   they  see  as  they  log  onto  e-commer ce   websites such as Shopee or Tiki ma y be r ecommended t o them b y an alg orithm.    This  acut e  awareness   is  essen tial  because   it  expands   the  public’ s  perspectiv e  on  the  role  AI  plays   in  bene fiting   or  harming   their   lives.  Amids t  the  COVID-19   pandemic,   “rice   ATMs, ”  automa tic  rice   dispensing   machines,   were  invented  and  deplo yed  in  man y  cities   to  provide   rice  both    contactless   and  free-of -char ge  to  low-income   communities.   Wha t  is  often  left  out  in  the  reports    of  this  story  is  that  facial   recognition   was  also  used   to  ensur e  compliance   with   the  authorities.    This  critic al  emphasis   on  AI  involvemen t  is  the  first  step  in  shaping   the  conversation  around   AI   and  its  impacts   in  Vietnam   as  a  part  of  the  much   larger  global   discourse.  The  public   needs   to   start  having  the  man y  necessar y  conversations   about   AI  around   privacy,  trust,  bias,    cyber security ,  and  ethics,   as  well  as  the  nuances,   risks,  and  trade-of fs  of  these   aspects   (e.g.,   privacy par adox).   Not  only   is  AI  ethics   absent  from  media   and  public   policy   discussions,   it’s  also  missing   in   engineering   educ ation.   Khoa t  Than,   a  professor   at  Hanoi   Univ ersity  of  Science   &  Technology    notes:  “AI  ethics   at  the  colleg e  level  is  lacking   for  engineering   studen ts.  Wha t  studen ts  learn   at   univ ersities   are  still  ethics   in  comput er  science. ”  Colleg es  and  univ ersities   should   invest  in  not   only   learning   from  the  learning   and  teaching   of  this  curriculum,   adop ting  terminologies   from   the  global   discourse,  they  should   also  invest  in  doing   resear ch,  particularly   social   science   that   examines socie tal impacts of t echnology in Vie tnam.    At  the  governmen tal  level,  Vietnam   can  look   to  other   Asian   countries   which   have  drafted   national   strategy  documen ts  that  created  a  frame work  to  mak e  AI  “for  all.”  One  example   is  the   Responsible   AI  for  All  Strategy  Documen t,  recen tly  published   by  Niti  Aayog,  a  premier   think -tank   by  the  Indian   governmen t.  It  outlines   potential  ethical  issues   that  AI  would   create,  and  that   man y  of  those   issues   need   new  legal  frame works  that  different  governmen tal  bodies   need   to   work t ogether t o addr ess.   Conclusion    Vietnam   has  entered  the  early   phase   of  AI  developmen t,  the  strategy  documen t  is  by  no  means    the  last  that  the  governmen t  would   produce.   We  recommend   the  new  leader ship  to  consider    other   aspects   of  AI  developmen t  including   ethical  consider ations,   legal  frame works,  as  well  as   creating  partner ships  with   investors,  civil  socie ty,  and  common   user s  to  create  frame works  to   addr ess  ethical  problems   that  are  native  to  Vietnamese   socie ty.  Vietnam   should   be  in   conversation  with   global   AI  technologis ts  and  ethicis ts  as  AI  developmen t  is  truly   a  global    phenomenon.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   4 5   
 Other   Analy sis  of  the  “Artificial   Intellig ence   governance   principles:   towards   ethical  and  trustworth y  artificial   intellig ence   in  the  European   insur ance    sector”   [  Original documen t  by EIOP A’s Consult ative Expert   Group on Digit al Ethics in insur ance]    [Analy sis  by  Philippe   Dambly   (Senior   Lectur er  at  Univ ersity  of  Liège)  and  Axel  Beelen   (Legal   Consult ant specializ ed in da ta protection and AI)]    Overview  :  After  the  2020   Whit e  Paper   on  Artificial   Intelligence   and  the  Proposal   for  a  new   regula tion  on  AI  of  21  April   2021   published   by  the  European   Commission   in  April   2021,   the   European   Insur ance   and  Occupa tional   Pensions   Authority   («  EIOP A  »)  published,   on  18  June    2021,   a  report   towards  ethical  and  trustworth y  artificial   intelligence   in  the  European   insur ance    sector.  This  is  the  first  AI  EU  regula tion  of  insur ance.   The  report   is  the  result   of  the  intensiv e   work  of  EIOP A’s  Consult ative  Expert   Group  on  Digit al  Ethics   in  insur ance.   The  documen t  aims   in   particular   to  help   insur ance   companies   when   they  implemen t  AI  applic ations/ systems.   The   measur es  proposed   in  this  documen t  are  risk-based   and  cover  the  entire  lifecycle  of  an  AI   applic ation.    Objectiv es of the r eport    The  report   begins   by  first  iden tifying  the  legal  frame work  currently  applied   to  AI  in  the   insur ance   sector  in  the  EU.  Existing  legisla tion  should   indeed   form  the  basis   of  any  AI   governance   frame work,  but  the  different  pieces   of  legisla tion  need   to  be  applied   in  a  systema tic   manner   and  requir e  unpacking   to  assis t  organizations   under stand  wha t  they  mean   in  the   context  of  AI.  Furthermor e,  an  ethical  use  of  data  and  digit al  technologies   implies   a  more   extensiv e  appr oach   than   merely  complying   with   legal  provisions   and  needs   to  take  into   consider ation  the  provision   of  public   goods   to  socie ty  as  part  of  the  corpor ate  social    responsibility   of  firms.   The  existing  frame work  includes,   in  particular ,  the  2009   Solvency   II   Directiv e,  the  2016   IDD  Directiv e,  the  Gener al  Data  Protection   Regula tion  (“GDPR”)   and  the   2002   ePriv acy  Directiv e.  Good   to  know  is  that  the  EIOP A  report   uses   the  definition   of  AI   included in the Pr oposal f or a r egula tion r ecen tly published b y the Eur opean Commission.    Six K ey Principles    The  6  key  principles   iden tified   by  the  report,   along   with   guidance   for  insur ance   companies   on   how to put them in to practice thr oughout the AI s ystem lif ecycle for dif ferent applic ations, ar e:   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   4 6   
 1. the principle of pr oportionality;    2. the principle of f airness and non-discrimina tion;    3. the principle of tr anspar ency and e xplainability;    4. the principle of human o versight;   5. the principle of da ta governance of r ecord keeping and    6. the principle of R obus tness and P erformance.    The  high-le vel  principles   are  accompanied   by  additional   guidance   for  insur ance   firms   on  how  to   implemen t  them   in  practice   throughout   the  AI  system’s  lifecycle.  For  example,   in  order  to   implemen t  the  principle   of  proportionality ,  the  report   develop s  an  AI  use  case  impact    assessmen t  which   could   help   insur ance   firms   under stand  the  potential  outcome   of  AI  use  cases    and  subsequen tly,  determine   in  a  proportiona te  manner   the  “mix ”  of  governance   measur es   necessar y to implemen t ethical and trus tworth y AI s ystems within their or ganizations.    With   regards  to  the  use  of  AI  in  insur ance   pricing   and  under writing ,  the  report   includes    guidance   on  how  to  assess   the  appr opria teness   and  necessity   of  rating  factors,  noting   that   correlation  does   not  imply   causa tion.   From  a  transpar ency   and  explainability   perspectiv e,   consumer s  should   be  provided   with   counterfactual   explana tions,   i.e.  they  should   be  informed    about   the  main   rating  factors  that  affect  their   premium   to  promot e  trust  and  enable   them   to   adop t informed decisions.    Each  of  the  principles   is  analy zed  in  the  light  of  the  principle   of  ethics,   a  transversal  principle   in   AI.  The  report   focuses   on  private  insur ance   (life,  health   and  non-lif e  insur ance).   The  analy sis  of   the  six  principles   by  EIOP A  experts   is  very  rich  and  complemen ted  by  multiple   graphs   and   summar y  tables.   The  possible   issues   of  big  data  and  AI  in  social   insur ance   should   indeed   be   analy zed  separ ately.  The  report   consider s  each   principle   on  the  one  hand   in  its  gener ality   and   then   on  the  other   hand   deepens   it  through   two  or  three  specific   applic ations   of  the  insur ance    sector (such as pricing and under writing , claims manag emen t and fr aud de tection).    Against  this  backgr ound,   several  initia tives  have  proliferated  in  recen t  years  at  interna tional,    European   and  national   level  aiming   to  promot e  an  ethical  and  trustworth y  AI  in  our  socie ty.   EIOP A  also  recogniz es  that  AI  is  an  evolving   technology   with   an  ever-growing   number   of   applic ations   and  continuous   and  in-dep th  resear ch.  This  is  particularly   the  case  in  the  areas  of   transpar ency   and  explainability ,  as  well  as  in  the  areas  of  activ e  fairness   and  non-discrimina tion   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   4 7   
 principles.   As  these   areas  of  applic ation  and  resear ch  evolve,  EIOP A  warns   that  the   recommenda tions included in the r eport ma y ther efore need t o be r evised in the futur e.   Legal value of the r eport?    But  wha t  is  the  legal  value   of  this  report?   Do  European   insur ance   companies   that  want  to   introduce   AI  processes   into  their   systems  (whe ther   opaque   or  not)  have  already   to  take  into   account  its  man y  recommenda tions   on  the  basis   of  the  well-kno wn  principle   “satisfy  or  justify”?   Do the y have to jus tify themselv es if the y want to der ogate from it?    The  report   states,  on  its  page  2,  that  it  was  written  by  member s  of  EIOP A’s  Consult ative  Expert    Group on Digit al Ethics in insur ance.    The  European   regula tor  created  this  working   group  in  2019   to  support   its  work.  This  group  of   experts   was  created  in  order  to  help   the  regula tor  in  its  activities,   but  their   views  are  purely   advisor y.  It  will  therefore  be  necessar y  to  wait  for  the  governing   bodies   of  EIOP A  to  decide   on   the  report   to  know  whe ther   its  content  could   become   manda tory  for  insur ance   companies   or   not.   However,  given  the  excellence   of  the  writing   and  the  import ance   of  the  subject,   there  is  no   doub t  that  EIOP A  will  soon   appr ove  this  documen t  more  formally .  It  is  therefore  of  utmos t   import ance   that  insur ance   companies   (and   other s)  take  note  of  it  and  start  to  implemen t  the  6   analy zed principles tha t we ha ve jus t summariz ed for you.   The Pr oliferation of AI E thics Principles: Wha t’s Ne xt?   [Original article b y Ra vit Dot an]   With   the  rise  of  AI  and  the  recognition   of  its  impacts   on  people   and  the  environmen t,  more  and   more  organizations   formula te  principles   for  the  developmen t  of  ethical  AI  systems.   Ther e  are   now  dozens  of  documen ts  containing   hundr eds  of  principles,   written  by  governmen ts,   corpor ations,   non-pr ofits,   and  academics.   This  proliferation  of  principles   presen ts  challeng es.   For  example,   should   organizations   continue   to  produce   new  principles,   or  should   they  endor se   existing  ones?   If  organizations   are  to  endor se  existing  principles,   which   ones?   And  which   of  the   principles should in form r egula tion?    In  the  face  of  the  proliferation  of  AI  ethics   principles,   it  is  natural  to  seek   a  core  set  of  principles    or  unifying  themes.   The  hope   migh t  be  that  the  core  set  of  principles   would   save  organizations    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   4 8   
 from  reinventing  the  wheel,   prevent  them   from  cherr y-picking   principles,   be  used   for   regula tion,   etc.  In  the  last  few  years,  several  teams   of  resear cher s  have  set  out  to  articula te  such    a set of c ore AI e thics principles.    These   overviews  of  AI  ethics   principles   illumina te  the  landsc ape.   In  addition,   they  highligh t  the   limit ations   of  the  sear ch  for  unifying  themes.   They  help   us  see  that  it  is  unlik ely  that  a  unique    set  of  core  principles   will  be  found.   And  that,  even  if  it  is  found,   univ ersally   applying   it  runs   the   risk of e xacerba ting po wer imbalances.    Five overviews of AI e thics principles    Let’s  start  with   reviewing   five  studies   that  overview  the  landsc ape  of  AI  ethics   principles.   Wha t   is their me thodology? And wha t unif ying themes do the y iden tify?   1.  The  Global   Landsc ape  of  AI  Ethics   Guidelines,   by  Anna   Jobin,   Mar cello   Lenc a,  and  Effy   Vayena   Jobin   et  al.  conduct ed  an  extensiv e  sear ch  and  iden tified   84  paper s  producing   AI  ethics    principles.   The  inclusion   criteria  were  as  follows:  (i)  The  paper   is  written  in  English,   German,    French,   Italian,   or  Greek.   (ii)  The  paper   was  issued   by  an  institutional   entity.  (iii)  The  paper   refers   to  AI  ancillar y  notions   explicitly   in  its  title  or  descrip tion.   And  (iv)  the  paper   expresses   a  moral   preference f or a de fined c ourse of action.    The  team   used   manual   coding   to  iden tify  unifying  themes   and  came   up  with   11  of  them:    transpar ency   (appear ed  in  87%   of  the  documen ts),  justice  and  Fairness   (81%),   non-male ficence    (71%),   responsibility   (71%),   Privacy  (56%),   bene ficence   (49%),   freedom   and  autonom y  (40%),    Trust (33%), sus tainability (17%), dignity (15%), and solidarity (7%).    While   there  is  convergence   on  principles,   Jobin   et  al.  poin t  out  that  there  is  divergence   in  how   the  principles   are  interpr eted,  why  they  are  deemed   import ant,  and  how  they  should   be   implemen ted.   2. A Unified Fr ame work of Fiv e Principles f or AI in Socie ty, by Luciano Floridi and Josh Co wls   Floridi   and  Cowl  iden tify  six  high-pr ofile   and  expert -driv en  AI  ethics   documen ts.  The  selection    criteria  were  as  follows:  (i)  The  documen t  was  published   no  more  than   three  years  before  the   study .  (ii)  The  documen t  is  highly   relevant  to  AI  and  its  impact   on  socie ty  as  a  whole.   And  (iii)   the  documen t  is  highly   reput able,   published   by  an  authorit ative  and  multi-s takeholder    organization  with   at  least  national   scope.   In  sear ching   for  unifying  themes   in  AI  ethics   principles,    the  author s  draw  from  the  four  ethical  principles   commonly   used   in  bioe thics:   bene ficence,    non-male ficence,   autonom y,  and  justice.   They  iden tify  these   same   themes   as  unifying  themes    for AI e thics principles, and the y add a fifth one: e xplic ability .   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   4 9   
 3. Linking Artificial In tellig ence Principles, b y Yi Zeng, Enmeng Lu, and Cunqing Huangfu    Zeng  et  al.  collect ed  27  proposals   of  AI  ethics   principles   and  grouped   them   by  backgr ound:   (i)   Academia,   non-pr ofits,   and  non-g overnmen tal  organizations,   (ii)  governmen t,  and  (iii)  indus try.   The  author s  extracted  principles   from  each   text  and  tracked  common   themes   using   a  keyword   sear ch.  They  started  by  choosing   ten  keywords  as  core  terms:   humanity ,  collabor ation,   shar e,   fairness,   transpar ency ,  privacy,  security ,  safety,  accountability ,  and  AGI  (artificial   gener al   intelligence).   After  iden tifying  these   core  terms,   Zeng  et  al.  comput ationally   expanded   them,    creating  lists  of  related  words  and  expressions.   For  example,   the  “accountability ”  theme   was   expanded   to  include   “responsibility .”  Zeng  et  al.  then   performed   keyword  sear ches   for  all  the   words on the lis ts, ther eby disc overing the fr equency of appear ance of each theme.    The  team   found   that  the  prominence   of  each   theme   depends   on  the  backgr ound   of  the   documen t:   ●  Corpor ations   :  The  top  three  themes   are  humanity ,  collabor ation,   fairness,   transpar ency ,   safety.  They  men tion  privacy  and  security   much   less  than   the  other   institutions   and   men tion A GI and c ollabor ation much mor e.   ●  Governmen ts  :  The  top  themes   are  privacy,  security ,  humanity .  They  men tion   accountability much less than the other kinds of ins titutions.    ●  Academia,   non-pr ofits,   and  non-g overnmen t  :  The  top  categories   are  humanity ,  privacy,   accountability . The y men tion humanity much mor e than the other kinds of ins titutions.    4.  Principled   Artificial   Intellig ence:   Mapping   Consensus   in  Ethical  and  Righ ts-based    Appr oaches   to  Principles   for  AI,  by  Jessic a  Fjeld,   Nele   Achten,  Hannah   Hillig oss,  Adam    Chris topher Nagy , and Madhulik a Srik umar    Fjeld   et  al.  analy zed  36  documen ts.  The  selection   criteria  were  as  follows:  (i)  The  documen t   represen ts  the  views  of  an  organization  or  institution.   (ii)  The  documen t  was  author ed  by   relatively  senior   staff.  (iii)  In  multi-s takeholder   documen ts,  a  breadth   of  experts   were  involved.   (iv)  The  documen t  was  officially   published.   And  (v)  the  documen t  was  written  in  English,    Chinese, Fr ench, German, or Spanish.    The  author s  extracted  ethical  themes   from  these   documen ts  by  manual   coding ,  resulting   in   eigh t  themes:   fairness   and  non-discrimina tion  (appear ed  in  100%   of  documen ts),  privacy  (97%),    accountability   (97%),   transpar ency   and  explainability   (94%),   safety  and  security   (81%),    professional   responsibility   (78%),   human   control  of  technology   (69%),   and  promotion   of  human    values (69%).    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   5 0   
 The  paper   recogniz es  that  other   teams   of  resear cher s  may  iden tify  different  themes.   It  also   poin ts  out  that,  while   there  is  a  convergence   on  the  themes,   the  principles   are  implemen ted   differently in dif ferent documen ts.   5. The E thics of AI E thics: An E valua tion of Guidelines, b y Thilo Hag endorff    Hagendorf f  analy zed  22  major   ethical  guidelines.   The  selection   criteria  were  as  follows:  (i)  The   documen t  was  published   no  more  than   three  years  before  the  study .  (ii)  The  documen t  refers  to   more  than   a  national   context  or  has  signific ant  interna tional   influence.   (iii)  The  documen t   addr esses   AI  ethics   gener ally,  not  on  specific   aspects   of  AI.  (iv)  The  principles   are  not  corpor ate   policies unless the y have bec ome w ell-kno wn thr ough media c overage.   Hagendorf f  iden tified   eigh t  themes:   privacy  protection   (appear ed  in  82%   of  documen ts),   fairness,   non-discrimina tion,   justice  (82%),   accountability   (77%),   transpar ency/ openness   (73%),    safety,  cyber -security   (73%),   common   good,   sustainability ,  well-being   (73%),   human   oversight,   control, auditing (54%), and solidarity , inclusion, social c ohesion (50%).    Hagendorf f  also  iden tified   that  mos t  of  the  author s  of  the  documen ts  were  men   and  that  only    one  documen t  included   notes  on  the  technic al  applic ation  of  the  principles,   and  even  those    were few and limit ed.   Limit ations of the sear ch for unif ying themes in AI e thics principles    1. Is it lik ely t o iden tify a unique se t of c ore AI e thics principles?    As  you  can  see,  the  different  overviews  result ed  in  different  sets  of  unifying  themes.   Such    differences   are  expect ed  since   the  overviews  differ  on  their   choice   of  documen ts,  methodology ,   and applic ation of me thodology .   Wha t  shall   we  do  with   the  resulting   multiplicity   of  unifying  themes?   One  appr oach   is  to  seek    unifying  themes   in  the  proposed   unifying  themes.   The  hope   migh t  be  to  iden tify  the  “core”  of   the  core  AI  ethics   principles.   However,  it  seems   unlik ely  that  such   efforts  will  yield   a  unique   set.   We  will  once   again  need   to  ask:  Which   sets  of  unifying  themes   should   be  included?   Which    methodology   should   be  chosen?   And  how  should   it  be  applied?   Just  as  different  overviews  of  AI   ethics   principles   produced   different  unifying  themes,   it  is  likely  that  overviews  of  the  overviews   will pr oduce dif ferent sets of “unif ying unif ying themes. ”   Ther efore, finding a unique se t of c ore AI e thics principles seems unlik ely.   2. Suppose tha t a core set of principles w ere to be f ound, should it be univ ersally adop ted?   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   5 1   
 Even  if  a  core  set  of  AI  ethics   principles   were  to  be  found   in  the  existing  AI  ethics   principles,    univ ersally   adop ting  it  is  problema tic  because   of  the  lack  of  diversity  in  the  perspectiv es  that   gener ated the principles.    To  start,  the  vast  majority   of  the  existing  AI  ethics   documen ts  were  written  in  North   Americ a   and Eur ope, as some of the o verviews highligh t.   Mor eover,  even  within   the  global   north,   the  perspectiv es  that  are  represen ted  in  the  existing  AI   ethics   documen ts  are  limit ed.  As  Hagendorf f  iden tified,   the  documen ts  were  written  by  men   for   the  mos t  part.   We  do  not  have  statistics  on  the  participa tion  of  other   relevant  iden tity   categories,   such   as  race,  religion,   and  sexual  orien tation.   However,  the  author s  of  the  AI  ethics    documen ts ar e probably r elatively homog enous along these a xes as w ell.   Further ,  the  voice  of  those   impact ed  by  AI  systems  is  likely  to  be  underr epresen ted.  Zeng  et  al.   suggest  that  AI  ethics   documen ts  migh t  reflect   the  interests  and  needs   of  the  institutions   that   author ed  them.   For  example,   Zeng  et  al.  show  that  corpor ations   men tion  privacy  and  security    less  than   other   types   of  institutions.   The  reason   migh t  be  that  privacy  and  security   are  sensitiv e   topics   for  them.   Similarly ,  governmen ts  men tion  accountability   less,   and  academia,   non-pr ofits,    and  non-g overnmen tal  organizations   men tion  collabor ation  less.   The  reason   migh t  be  that   these   are  sensitiv e  topics   for  them.   Which   institutions   represen t  the  interests  and  needs   of  the   broader ,  global   public   impact ed  by  AI  systems?   How  influen tial  are  they  in  the  production   of  AI   ethics principles?    Given  the  lack  of  diversity  in  the  perspectiv es  involved  in  gener ating  AI  ethics   principles,   they   seem   to  represen t  the  preferences   and  interests  of  a  select ed  few.  If  a  core  set  of  principles    were  to  be  found   among   them,   it  would   represen t  these   select ed  few  as  well.  Ther efore,   univ ersally   adop ting  unifying  themes   found   in  the  existing  AI  ethics   principles   would   run  the  risk   of  subjug ating  broad  popula tions   to  principles   that  were  formula ted  by  a  small   elite,  thereby   exacerba ting e xisting po wer imbalances.    Wha t’s next?   Overviews  of  existing  AI  ethics   principles   help   us  see  that  it  is  unlik ely  that  a  core  set  of   principles   will  be  found   and  that,  even  if  it  were  to  be  found,   univ ersally   adop ting  runs   the  risk   of  exacerba ting  power  imbalances.   That  brings   us  back   to  the  ques tions   with   which   we  started.   How  do  we  navigate  the  proliferation  of  AI  ethics   principles?   Wha t  should   we  use  for  regula tion,    for  example?   Should   we  seek   to  create  new  AI  ethics   principles   which   incorpor ate  more   perspectiv es?  Wha t  if  it  doesn’t   result   in  a  unique   set  of  principles,   only   increasing   the   multiplicity   of  principles?   Is  it  possible   to  develop   appr oaches   for  AI  ethics   governance   that   don’t r ely on g ener al AI e thics principles?    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   5 2   
 Represen tation and Imagina tion f or Pr eventing AI Harms    [Original article b y Sean McGr egor]   The  AI  Inciden t  Database   launched   publicly   in  November   2020   by  the  Partner ship  on  AI  as  a   dashboar d  of  AI  harms   realiz ed  in  the  real  world.   Inspir ed  by  similar   databases   in  the  aviation   indus try,  its  chang e  thesis   is  deriv ed  from  the  Santayana  aphorism,   “Those   who   cannot    remember   the  past  are  condemned   to  repea t  it.”  As  a  new  and  rapidly   expanding   indus try,  AI   lacks  a  formal   history  of  its  failur es  and  harms   were  beginning   to  repea t.  The  AI  Inciden t   Database   thus   archives  inciden ts  detailing   a  passport   imag e  check er  telling   Asian   people   their    eyes  are  closed,   the  gender   biases   of  languag e  models,   and  the  death  of  a  pedes trian   from  an   autonomous   car.  Making   these   inciden ts  discoverable   to  futur e  AI  developer s  reduces   the   likelihood of r ecurr ence.    Wha t Have We Learned?    Now  with   a  large  collection   of  AI  inciden ts  and  a  new  inciden t  taxonom y  feature  from  the   Center  for  Security   and  Emer ging   Technology ,  we  have  a  sense   of  our  history  and  two  statistics   are worth highligh ting.    CSET Harm T ype T axonom y   First,  the  harm   types   seen   in  the  real  world  are  highly   varied.   Existing  socie tal  processes   (e.g.,   formal   lab  tests  and  independen t  certific ation)   are  prepar ed  to  respond   to  just  the  24  percent   of  inciden ts  related  to  physical  health   and  safety.  While   an  autonomous   car  poses   obvious    safety  challeng es,  the  harms   to  social   and  politic al  systems,   psychology ,  and  civil  liberties    represen t  more  than   half  of  the  inciden ts  recorded  to  date.  These   inciden ts  are  likely  either    failur es  of  imagina tion,   or  failur es  of  represen tation.   Let’s  dial  into  “failur es  of  imagina tion”   with    the  observation  that  the  majority   of  inciden ts  are  not  distribut ed  evenly   across  all   demogr aphics within the popula tion.    Uneven dis tribution of harms basis    Of  these   “une venly   distribut ed”  harms,   30  percent  are  distribut ed  according   to  race  and  19   percent  according   to  sex.  Man y  of  these   inciden ts  could   have  been   avoided   without   needing   an   example   in  the  real  world  if  the  teams   engineering   the  systems  had  more  varied   demogr aphic    iden tity.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   5 3   
 So  is  represen tation  a  panacea   to  the  harms   of  intelligent  systems?   No.  Even  were  it  possible   to   have  all  iden tities   represen ted,  there  will  still  be  inciden ts  proving  the  limits   of  our  collectiv e   imagina tion.   For  these   “failur es  of  imagina tion” ,  the  AI  Inciden t  Database   stands   ready   to   ensur e the y can only happen once.    Wha t is ne xt?   If  you  compar e  the  AI  Inciden t  Database   to  the  Common   Vulner abilities   and  Exposur es  database    and  the  US  Aviation  Acciden t  Database   both   have  extensiv e  softw are,  processes,   community    integrations,   and  authorities   accumula ted  through   decades   of  private  and  public   investmen t.   Compar atively,  the  AI  Inciden t  Database   is  only   at  the  beginning   of  its  work  ensuring   AI  is  more   socially   bene ficial.   Three  thema tic  areas  are  particularly   import ant  for  building   on  the  early    successes of the AIID in its curr ent form. These include,    1)  Governance   and  Process.   The  AIID   oper ates  within   a  space   lacking   established   and  broadly    accep ted  definitions   of  the  technologies,   inciden t  response   processes,   and  community   impacts.    Regularizing   these   elemen ts  with   an  oversight  body   composed   of  subject   matter  experts   in  the   space   ensur es  quality   work  product   and  adop tion  across  the  corpor ate  and  governmen tal   arenas.    2)  Expanding   Technic al  Depth.  The  AI  Inciden t  Database   does   not  offer  one  canonic al  sour ce  of   truth   regarding   AI  inciden ts.  Indeed,   reasonable   parties   will  have  well-founded   reasons   for  why   an  inciden t  should   be  report ed  or  classified   differently.  Consequen tly,  the  Database   supports    multiple   perspectiv es  on  inciden ts  both   by  ingesting  multiple   reports   (to  date,  1,199   author s   from  547  public ations),   and  by  supporting   multiple   taxonomies   for  which   the  CSET   taxonom y  is   an  early   example.   The  AIID   taxonomies   are  flexible   collections   of  classific ations   manag ed  by   expert   individuals   and  organizations.   The  taxonomies   are  the  means   by  which   socie ty   collectiv ely  works  to  under stand  both   individual   inciden ts,  as  well  as  the  popula tion-le vel   statistics  for  these   classific ations.   Well  structur ed  and  rigorously   applied   AI  inciden t  taxonomies    have  the  ability   to  inform  resear ch  and  policy   making   priorities   for  safer  AI,  as  well  as  help    engineer s  under stand  the  vulner abilities   and  problems   produced   by  increasingly   comple x   intelligent systems.    The  CSET   taxonom y  is  a  gener al  taxonom y  of  AI  inciden ts  involving   several  stages  of   classific ation  review  and  audit   to  ensur e  consis tency   across  annot ators.  The  intention  behind    the  CSET   taxonom y  is  to  inform  policy   mak ers  of  impacts.   Even  with   the  success   of  the  CSET    taxonom y  for  policy   mak ers,  the  AIID   still  lacks  a  rigorous  technic al  taxonom y.  Man y  technic al   classific ations   informing   wher e  AI  is  likely  to  produce   futur e  inciden ts  are  not  currently   captured.  Iden tifying  unsa fe  AI  and  motiv ating  the  developmen t  of  safe  AI  requir es  technic al   classific ation.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   5 4   
 3)  Expand   Database   Breadth.   The  AI  Inciden t  Database   is  built   on  a  documen t  database   and  a   collection   of  serverless   browser  applic ations.   This  means   that  the  database   is  highly   extensible    to  new  inciden t  types   and  scalable   to  a  very  large  number   of  inciden t  reports.   In  short,   the   database   architectur e  anticipa tes  the  need   to  record  an  increasing   number   of  highly   varied   and   comple x  AI  inciden ts.  While   a  large  number   of  inciden ts  currently  in  the  database   have  been    provided   by  the  open   sour ce  community ,  we  know  we  are  currently  missing   man y  inciden ts  that   should   be  included   in  the  current  criteria.   This  is  one  area  wher e  everyone  has  a  role  in  the   success ful de velopmen t of our c ollectiv e per spectiv e into AI inciden ts.   How can y ou help?    The  AI  Inciden t  Database   will  not  succeed   without   your  input   of  inciden ts  and  analy sis.  When    encountering   an  AI  Inciden t  in  the  world,   we  implor e  you  to  submit   a  new  inciden t  record  to  the   database.   We  additionally   ask  that  softw are  engineer s  and  resear cher s  work  with   the  codebase    and da taset to engineer a futur e for humanity tha t ma ximally bene fits fr om in telligent systems.    Evolution   in  Age-Verific ation  Applic ations:   Can  AI  Open   Some   New   Horiz ons?    [Original article b y Az far Adib]    Have  you  ever  been   asked  to  prove  your  age  or  verify  your  iden tity  while   you  tried   to  buy  any   product   or  service?   Man y  reader s  here  may  answer  yes  to  this  ques tion.   Whe ther   it’d  be  buying    a  bottle  of  wine   or  signing   up  for  our  first  driving   lesson,   age-verific ation  requir emen ts  have   existed  for  long.   Similarly ,  a  wide   range  of  online   applic ations   now  requir es  age-verific ation   before  providing   service  or  content  access   to  user s.  So,  in  this  digit al  age,  this  has  become   a   broad ar ea of r esear ch and pr oduct de velopmen t.   According   to  a  report   by  Mark etsandMark ets,  the  global   mark et  of  iden tity  verific ation  is   expect ed  to  grow  from  USD   7.6  billion   in  2020   to  USD   15.8   billion   by  2025   [1].  In  its  recen t   directory,  “Digit al  ID  &  Authen tication  Council   of  Canada”   (DIA CC)  has  enlis ted  73  member    companies   providing   a  variety  of  digit al  authen tication  services   in  a  continuously   growing    mark et [2].    Age  verific ation  is  becoming   crucial   in  various   dimensions.   An  increasingly   under -18  digit al   popula tion  has  heigh tened   pressur e  from  regula tory  and  non-r egula tory  entities   on  service   provider s  to  implemen t  more  string ent  assur ances,   so  that  childr en  are  kept  safe  online.   Also ,   age  verific ation  is  no  long er  just  limit ed  to  segregation  between  adult   and  under -aged  ones,   it   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   5 5   
 has  other   applic ations   too.  For  instance,   estima ting  the  age  of  uniden tified   patients  in  hospit al   emer gencies c an be a crucial decision f or ph ysicians.    So  accur acy  and  consis tency   of  age  verific ation  tools  are  quite  crucial.   Equally   import ant  is  to   main tain  their   ethical  standar ds;  particularly   privacy,  bias-a voidance   and  data  security .   Advancemen t  of  AI  is  playing  a  key  role  here.  Let  us  have  a  look   that  at  some   key  transforma tion   occurring in this ar ena:    From paper -based t o digit al:   Paper   based   iden tification  documen ts  (like-  driving   license,   health   card  or  other   governmen t   issued   ID  documen ts)  have  been   the  mos t  prevalen t  verific ation  scheme   during   in-per son   transactions.   For  digit al  transactions   and  often  during   in-per son  transactions   also,  their    paperless v ersions (pictur e of ID documen ts) ar e widely used.    The  concurr ent  trend,   particularly   fueled   by  the  COVID-19   pandemic,   is  inclining   more  towards   the  paperless   version  and  beyond.   In  a  survey  carried   out  by  Interac  in  Augus t  2020   among    adult   Canadians,   the  majority   of  the  responden ts  expressed   hygiene   concerns   around   physical   ID.  They  were  also  concerned   about   keeping   their   iden tity  data  safe  online,   and  felt  it  risky   to   take  a  pictur e  of  a  physical  ID  [3].  Fraudulen t  activities   are  indeed   becoming   a  major   concern    here  for  both   user s  and  service  provider s.  This  indic ates  the  need   for  more  secur ed  and   seamless digit al iden tification schemes.    Emer gence of biome trics    Biome tric  iden tification,   being   widely   used   across  the  globe,   enables   automa ted  recognition   of   individuals   through   certain  physiologic al  char acteristics  like-  facial   imag e,  fingerprin t,  iris,  voice,    gait,  signa ture,  heart   signal,   gait  etc.  A  resear ch  by  Juniper   has  predict ed  that  95  percent  of   mobile   user s  will  adop t  biome trics   for  authen tication  by  2025.   Interestingly ,  as  this  resear ch   showed,  face  biome trics   expansion   was  not  slowed  down  by  increased   face  mask   usag e  during    COVID-19 pandemic [4].    Cutting-edg e  machine   learning   schemes   have  been   playing  a  pivotal  role  in  enhancing   accur acy   in  biome trics.   However,  biome trics   are  a  form  of  determinis tic  data,  wher e  people   are  iden tified    by  matching   with   their   previously   stored  record,  mos tly  through   super vised   algorithms.   While    facial   recognition   often  claims   high   classific ation  accur acy  (over  90%),   these   outcomes   may  not   be  univ ersal.  Some   resear ch  (like  the  2018   “Gender   Shades”   project   resear ch  carried   out  by   MIT  Media   Lab  and  Micr osoft   Resear ch)  exposes   increasing   error  rate  of  facial   recognition    across  marginaliz ed  demogr aphic   groups,  with   the  poor est  accur acy  consis tently  found   in   subjects   who   are  female,   Black,   and  18-30   years  old.  Overcoming   such   bias  remains   a   continuous endea vor for resear cher s-de veloper s [5].    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   5 6   
 Despit e  increasing   levels  of  implemen tation,   some   people   still  remain   reluct ant  to  use   biome trics,   especially   when   it  involves  their   imag e  capturing ,  voice  recording   or  physical  touch.    So  contactless,   non- vocal  and  non- facial   biome tric  schemes   are  also  being   developed   as   alterna tes.  A  very  recen t  example   is  finger  vein-based   biome trics   to  verify  a  person’s  COVID-19    vaccina tion s tatus, being de veloped b y Hit achi and K yushu Univ ersity in Japan [6].    Non-anon ymous iden tification t o anon ymous iden tification   As  user s  are  becoming   more  conscious   regarding   their   data  and  privacy,  they  are  preferring    more  controls  over  wher e  their   data  is  shar ed.  In  a  July  2021   survey  conduct ed  by  Liminal,   a   quart er  of  consumer s  stated  that  they  activ ely  avoided   using   fingerprin t  or  facial   biome trics   on   smartphones   due  to  privacy  concerns.   Three-quart ers  of  consumer s  desir ed  to  control  and   revoke access t o their iden tity da ta any time [7].    Such   demand   from  user s  puts   forth   the  relatively  novel  concep t  of  anon ymous   age  verific ation.    In  existing  technologies,   age  verific ation  is  actually   a  part  of  a  holis tic  authen tication  scheme,    wher e  individuals   get  comple tely  iden tified   based   on  their   previously   stored  creden tials.   The   purpose   of  anon ymous   age  verific ation  is  to  estima te  any  person’s  age  (or  age  range)  instantly   from  certain  biome trics   data,  without   any  prior   info  about   them,   thus   avoiding   their   comple te   iden tification.    From  technologic al  and  biologic al  perspectiv es  this  remains   a  daun ting  task,  but  resear ch  on   this  is  continuing ,  mos tly  through   a  combina tion  of  super vised   and  unsuper vised   algorithms.   In   a  study   jointly  carried   out  in  Michig an  State  Univ ersity  and  Beihand   Univ ersity  based   on  GAN    (Gener ative  Adversarial   Network),   resear cher s  have  estima ted  age  progression   through   facial    analy sis  with   accur acies   of  over  99%   [8].  Age  estima tion  is  also  being   attemp ted  from  ECG   (Electr ocardiogr am), which has r ecen tly emer ged as a pr omising biome tric scheme [9].    As  age-verific ation  tools  keep  evolving ,  artificial   intelligence   will  obviously   continue   its  dominan t   role  in  this  journe y  of  continuous   enhancemen t.  Ethical  consider ations   need   to  be  at  the  core  of   this  progress.  And  we  may  not  be  far  away  from  a  world  wher e  just  an  instant  biome tric  signal    will be enough t o iden tify our ag e, without an y documen t or prior da ta.   References    1.   https://w ww.mark etsandmark ets.com/Mark et-Reports/iden tity-verific ation-mark et-178660742.    html   2. https://diacc.c a/member ship/diacc-member s/   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   5 7   
 3. https://ne wsroom.in terac.ca/core-principles- for-building-digit al-iden tity-in-c anada/    4. https://w ww.juniperr esear ch.com/whit epaper s/ho w-to-ma ximise-mobile-pa ymen t-security    5. http://pr oceedings.mlr .press/v81/buolam wini18a/buolam wini18a.pdf    6.   https://w ww.biome tricupda te.com/202110/hit achi-trials- vein-biome trics-t o-verify-vaccina tion-r    ecords   7. https://liminal.c o/articles/bio-shock -is-biome tric-a voidance-mor e-than-digit al-luddism/    8. https://ieee xplor e.ieee.or g/documen t/8772083    9. https://w ww.nature.com/articles/ s41467-021-25351-7    Managing   Human   and  Robots   Together   –  Can  That  Be  a  Leader ship    Dilemma?    [Original article b y Az far Adib]    Have  we  all  written  poems   some time   at  some   poin t  in  our  lives?  Some   of  us  may  have  tried   so,   some   may  not.  Poetry  is  mos tly  consider ed  as  a  gifted  talen t.  Now,  can  we  expect   robots   to  be   poets? W ell, tha t is quit e possible no w.   On  26  November   2021,   Ai-Da   (the  world’ s  first  ultra-realis tic  humanoid   robot   artis t)  gave  a  live   demons tration  of  her  poetry  [1].  The  event  took  place   at  the  Univ ersity  of  Oxford’s  famous    Ashmolean   Museum,as   part  of  an  exhibition   marking   the  700th   death  anniv ersary  of  the  great   Italian   poet  Dante.  Ai-Da   produced   poems   there  as  an  instant  response   to  Dante’s  epic  “Divine    Comedy ”,  which   she  consumed   entirely  ,  used   her  algorithms   to  analy ze  Dante’s  speech    patterns, and then cr eated her w ork utilizing her o wn w ord collection.    This  is  another   solid   example   of  the  marvelousness   of  artificial   intelligence.   In  the  previous    mon th  also  (Oct ober   2021),   Ai-Da   participa ted  in  the  “Forever  is  Now  “,  an  historic  art  exhibition    in  the  Giza  Pyramids   ,  jointly  arranged  by  UNE SCO  and  the  Governmen t  of  Egyp t.  Prior   to  that   event,  one  inciden t  got  quite  highligh ted.  While   entering   into  Egyp t,  ,  Ai-Da   was  seized  by   border  agents  who   feared  her  robotics   may  have  been   hiding   covert  spy  tools  !  After  10  days   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   5 8   
 she  got  released,   thank s  to  continuous   effort  in  diploma tic  and  other   channels   for  that  [2].   Egyp tian  border  guar ds  basic ally  raised   security   concerns   about   modem   and  camer a  set  inside    Ai-Da.   While   the  modem   could   be  tempor arily   remo ved,  remo ving  the  camer a  was  not  an   option  as  those   were  crucial   componen ts  for  the  robot ’s  vision.   “I  can  ditch  the  modems,   but  I   can’t  really   gouge  her  eyes  out,”-  this  was  the  remark   by  Aidan   Meller ,  creator  of  the  robot    Ai-Da [3].    While   this  was  an  unfortuna te  and  discr ete  occurr ence   in  a  particular   context,  it  does   spark    some   interesting  perspectiv e.  Can  the  same   laws-regula tions   ,  which   are  used   for  humans,   can   be  applied   for  robots?   That  will  not  be  realis tic,  as  we  clearly   find  in  the  above  example.    Carr ying  a  camer a  in  a  particular   place   may  be  prohibit ed  for  humans.   However,  a  camer a  is  a   basic or gan for a r obot, without tha t it c an not oper ate.   Let  us  consider   a  more  familiar   scenario   to  us,  for  instance   a  workplace.   We  are  obviously    habitua ted  to  human   co-workers  in  workplaces.   Wha t  will  be  the  scenario   if  there  are  robot    co-workers?  In  mos t  organizations,   there  remains   certain  policies-pr ocedur es  for  human    resour ces. Will w e need separ ate policies f or robots?    For  leader s  in  the  organization,   it  can  be  an  interesting  challeng e.  Effectiv e  leader s  always  try  to   engage  and  motiv ate  people   to  bring   out  the  best  from  them.   Some times   they  need   to  lead   in  a   diverse  multicultur al  scenario ,  wher e  also  they  may  lead   success fully   following   some   basic    values.   But  how  can  they  adap t  to  a  scenario   when   they  will  have  both   humans   and  robots   in   their   workforce?  Do  they  need   to  focus   equally   on  the  robots   to  facilit ate  performance,   or  can   they  just  consider   those   as  mechanic al  devices?Can   they  evalua te  performance   and  provide    feedback   to  the  robots   in  the  same   manner   they  do  for  their   human   colleagues?   In  summar y,   can the y manag e humans and r obots similarly and simult aneously?    We  may  need   to  wait  some time   to  find  the  answers  to  these   ques tions.   It  migh t  be  a  better   appr oach   to  emphasiz e  on  how  humans   and  robots   can  empo wer  each   other   in  a  working    environmen t. Few though ts ar ound tha t are men tioned belo w:   Cross-tr aining    On-the-job   learning   is  always  an  effectiv e  way  to  acquir e  practic al  working   skills.   It  is  actually   a   career-long   process   while   we  continuously   learn   from  our  own  tasks,  from  our   colleagues-s takeholder s  along   with   traditional/   digit al  learning   mediums.   Learning   from  robot    co-workers  can  add  new  dimensions   for  the  human   workforce,  and  vice- versa.  However,  it  also   depends   on  the  learning   mode.   In  a  recen t  study   carried   out  by  a  group  of  resear cher s  from   MIT  and  US  Air  Force,  it  was  shown  that  AI  agents  became   frustrating  teamma tes  for  human    players  while   playing  a  card  game   Hanabi.   The  study   also  showed  that  human   players  preferred   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   5 9   
 the  classic   and  predict able   rule-based   AI  systems  over  comple x  Reinforcemen t  Learning   (RL)   based   systems.   So  choosing   the  right  mode   of  cross-learning   between  humans   and  robots   is   alway quit e import ant.   Increased Pr oductivity    Several  studies   have  demons trated  increased   productivity   in  certain  indus tries   (particularly    manuf acturing)   when   humans   and  robot   workers  success fully   collabor ate.  A  study   in  MIT   demons trated  85%   reduction   in  manuf acturing   idle  time   when   people   worked  collabor atively   with   a  human-a ware  robot,   compar ed  to  when   working   in  an  all-human   team   [5].  Another   study    by  “Advanced   Robotics   for  Manuf acturing ”  found   that  a  collabor ative  appr oach   cuts  cycle  time    by  almos t  two-thir ds  compar ed  to  a  fully   manual   appr oach   [6].  In  man y  scenarios   it  is  quite   effectiv e  to  deplo y  a  hybrid   mechanism   of  shar ed  functionalities   between  humans   and  robots,    rather than a fully manual or fully aut oma ted appr oach.    Dealing With Un favourable Cir cums tances    Robots   have  been   used   for  a  long   time   to  perform  risky   tasks  in  unfavourable   scenarios,   which    humans   can  not  attemp t.  Such   examples   include   fire  hazards,  natural  disas ters  (flood,    earthquak e,  snowfall),  nuclear   hazards  etc.  During   COVID-19   pandemic,   robots   often  became    essen tial  workers  by  providing   crucial   support   in  cleaning-sanitiz ation  along   with    testing-scr eening   procedur es  [7].  Last  year  a  Canadian   construction   compan y  deplo yed  a  robot    dog  in  one  of  their   sites  in  Mon treal,  which   they  declar ed  as  the  first  robot   worker  in  the  world   being   fully   activ e  on  a  daily   basis   [8].  So  it  will  always  remain   quite  natural  for  human   workers  to   trust their r obot c olleagues t o be tter deal with risky w orking c onditions.    Preferring R obots as Manag ers   It  may  sound   surprising   enough,   but  some   studies   have  shown  that  people   have  often  preferred   robots   as  their   manag ers  rather   than   human   beings   !  In  a  study   carried   out  last  year  by  Oracle   and  Workplace   Intelligence,   involving   more  than   12,000   people   across  11  countries   ,68%    responden t  say  that  during   stress  or  anxiety  they  would   prefer  talking   to  a  robot   than   their   own   manag er.  This  opinion   may  not  indic ate  robots   as  better  manag ers  for  all.  However,  it  does   show   that  there  remains   signific ant  impr ovemen t  scope  for  manag ers  (or  “human   manag ers”)  to   support   their   emplo yees,  or  there  can  be  some   better  practices   which   can  be  learn t  from  robots    in this r egard.   Time   will  ultima tely  tell  us  whe ther   managing   humans   and  robots   together   may  turn   out  as  a   dilemma   for  leader s.  In  any  scenario ,  it  always  remains   as  a  better  option  for  leader s  to  combine    the  consis tency ,  precision   and  speed   of  robots   with   the  flexibility ,  creativity   and  emotional    intelligence of human w orkers.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   6 0   
 References    https://w ww.ai-dar obot.c om   https://w ww.cnn.c om/2021/11/27/t ech/ ai-da-r obot -intl-scli-gbr/inde x.html   https://w ww.bbc.c om/ne ws/world-us-c anada-58993682    https://arxiv .org/pdf/2107.07630.pdf    https://dspace.mit.edu/handle/1721.1/63034    https://w ww.veobot.c om/blog /2020/2/18/fr eemo ve-in-a-c ollabor ative-palle tizing-c ase-s tudypa    rt-2   https://w ww.forbes.c om/sites/saibala/2021/01/26/r obots-ha ve-bec ome-an-essen tial-part -of-th   e-war-against-covid-19/?sh=1207f3ab5e f3   https://mon trealgazette.com/ne ws/loc al-ne ws/spot -the-r obot -mak es-debut -on-quebec-c onstru   ction-sit e   https://w ww.forbes.c om/sites/tr acybr ower/2020/10/07/ study -sho ws-people-pr efer-robot -over-   their -boss-6-w ays-to-be-a-leader -people-pr efer/?sh=445c661a45f4    “Welcome t o AI”; a t alk giv en to the Mon treal In tegrity Ne twork   [Original article b y Connor W right]   Overview  :  In  a  talk  given  to  the  Mon treal  Integrity   Network,  Connor   Wright  (Partner ships   Manag er)  introduces   the  field   of  AI  Ethics.   From  an  AI  dem ystifier   to  a  facial   recognition    technology use c ase, AI is seen as a s word tha t we should wield, but only with pr oper tr aining.    Introduction    In  a  talk  given  to  the  Mon treal  Integrity   Network,  I  set  about   offering   an  overview  of  the  AI   Ethics   field   and  the  issues   it  contains.   Stretching   from  defining   AI,  to  doughnuts,   to  facial    recognition   technology   (FRT)  and  current  laws,  I  aimed   to  provide   a  fruitful   introduction   to  the   field. Lik e an y good pr esen tation, it all s tarts with some de finitions.    An AI dem ystifier    I  men tioned   how  AI  is  not  just  limit ed  to  the  stereotypic al  view  of  killer   termina tor  robots   or   anthropomorphic   AI.  Instead,   depending   on  how  you  define  AI  (which   is  highly   fluid),   you  can   possess the t echnology in y our v ery hand.    With some cen tral themes of m y talk emer ging , I se t about de fining wha t AI is.    How can AI learn?    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   6 1   
 Machine learning    As  a  subset  of  AI,  machine   learning   provides   a  more  technic al  explana tion  of  how  an  AI  mak es   its  predictions.   Here,  machine   learning   describes   AI  as  algorithms,   wher eby  a  human   sets  the   parame ters  and  desir able   features  of  the  data  that  it  will  receiv e  as  input.   For  example,   let’s  say   we’re  designing   an  AI  with   the  goal  of  iden tifying  pictur es  of  cats.  I  would   set  the  parame ters   (like  the  ‘rules   of  a  game’)   for  the  AI  to  act  towards  this  goal  by  iden tifying  the  desir able    features  (such   as  whisk ers).  The  algorithm   will  be  able   to  impr ove  its  predictions   with   the  more   input da ta (phot os of c ats) tha t I pr ovide it.    Deep learning    As  a  subset  of  machine   learning ,  deep   learning   algorithms   set  the  desir able   features   themselv es,  unlik e  machine   learning   itself .  Requiring   large  data  sets  and  computing   power,  the   algorithm   goes  about   learning   which   features  of  the  data  it  receiv es  are  conduciv e  to  achie ving   its goal.   For  example,   if  we  started  a  doughnut -ranking   business,   we  could   set  the  deep   learning    algorithm   to  try  and  discover  the  mos t  popular   Krispy  Kreme   doughnut   in  the  world.   The  data   supplied   to  it  would   contain  every  different  type   of  doughnut   sold  in  the  world,   and  it  would    then   set  about   iden tifying  which   features  help   it  best  to  come   to  a  decision.   In  this  way,  it  would    start  elimina ting  all  the  doughnuts   that  Krispy  Kreme   doesn’t   supply ,  considering   that  popularity    means ho w man y are sold, e tc.   Machine   learning   vs  deep   learningIn   this  way,  we  can  come   to  a  critic al  difference   between   machine   learning   and  deep   learning.   Here,  machine   learning   requir es  ‘structur ed  data’  (data   with   labels   set  for  the  algorithm   to  learn   and  then   use  to  iden tify  objects   pertaining   to  its  goal).    On  the  other   hand,   deep   learning   uses   unstructur ed  data  (data  without   labels,   which   it  then    creates  to  iden tify  the  objects   conduciv e  to  its  goal).   Here,  machine   learning   needs   to  have  it   poin ted  out  that  cats  have  whisk ers  to  recogniz e  imag es  of  them,   wher eas  deep   learning   creates   its o wn label f or whisk ers to iden tify imag es as c ats.   AI in business    Chatbots    I  touched   upon   how  chatbots   (through   their   use  of  natural  languag e  processing   which   allows   them   to  recognise   words)  are  a  straightforward  way  to  leverage  AI  technology   in  business.   They   help   free  up  resour ces  to  be  dedic ated  elsewher e  and  can  act  as  part  of  a  24/7   customer    response ser vice.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   6 2   
 I  then   tackled   the  ques tion  of  wha t  mak es  a  good  chatbot?   Part  of  the  answer  process   is  the   right  balance   between  anthropomorphism   and  truth.   Here,  a  more  ‘natural’  sounding    conversation  will  help   keep  customer s  engaged  (such   as  asking   wha t  the  customer ’s  name   is),   but  this  should   not  be  taken  to  the  poin t  wher e  the  chatbot   is  confused   with   being   a  human.    Furthermor e,  a  decen t  knowledg e  of  colloquialisms   and  the  ability   to  adap t  to  typos   are  also   key.  At  times,   these   two  aspects   may  flumo x  the  chatbot   and  that’s  alrigh t,  so  long   as  a  human    agent can be in troduced in to the c onversation quickly .   The hiring pr ocess    AI  can  also  be  used   in  business   to  help   streamline   the  hiring   process.   Top  businesses   such   as   Hilton  use  AI  to  help   deal   with   the  thousands   of  applic ations   they  receiv e  a  day.  For  example,    Hilton  uses   the  end  to  end  AI  recruitmen t  softw are  of  AllyO   to  help   schedule   final   interview  calls   for  call  centre  applic ations.   AI  in  this  capacity   can  again  help   dedic ate  finite  resour ces  to  other    tasks  which   requir e  a  more  personal   touch,   instead  of  having  to  manually   send   thousands   of   emails and schedule thousands of c alls. Ho wever, this is not without its pr oblems.    Issues and c oncerns    Problems with learning    An  unfortuna tely  popular   avenue   for  problems   within   AI  is  contained   within   the  learning    process   of  AI  itself .  I  men tioned   how  with   machine   learning   and  deep   learning ,  the  sour cing  of   data  is  vital.  Large  data  sets  are  requir ed  in  order  to  better  train  the  models   being   designed   and   create  a  more  accur ate  product   at  the  end  of  it.  However,  how  this  data  is  sour ced  can  be   problema tic,  with   the  consen t  of  the  ‘producer ’  of  the  data  (such   as  a  Facebook   user)   not   always being achie ved.   AI bias    I  took  AI  Bias  to  be  the  systema tic  prioritiz ation  of  arbitr ary  char acteristics  in  a  model   that  leads    to  unfair  outcomes.   An  AI  is  then   biased   if  it  mak es  decisions   that  favour  or  penaliz e  certain   groups  for  reasons   that  are  not  valid  criteria  for  decision-making   or  for  factors  that  are   spuriously   correlated  with   the  outcome.   For  example,   within   predictiv e  policing ,  an   unrepresen tative  data  set  fed  into  the  algorithm   (such   as  featuring   more  criminal   records  of  one   race  over  another)   would   be  more  likely  to  predict   a  dispr oportiona tely  higher   likelihood   to   commit a crime f or some r aces o ver other s.   AI fairness    I  commen ted  on  how  algorithmic   fairness   is  the  principle   that  the  outputs   of  an  AI  system   should   be  uncorrelated  with   certain  char acteristics  such   as  gender ,  race,  or  sexuality .  Ther e  are   man y  possible   ways  to  consider   a  model   fair.  Common   appr oaches   include   equal   false  positiv es   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   6 3   
 across  sensitiv e  features,  equal   false  negatives  across  sensitiv e  char acteristics,  or  minimising    “worst  group  error”,  the  algorithm’ s  number   of  mistakes  on  the  least  represen ted  group.   Being    able   to  best  evalua te  an  AI’s  fairness   is  to  know  wher e  and  how  it  went  wrong,  preventing  the   proliferation of “black bo x” alg orithms.    Facial r ecognition t echnology (FR T) use c ase   Wha t are the kinds of e thical issues in volved in FR T? I w as able t o men tion the f ollowing:    FRT needs specifics    FRT  does   not  like  any  “noise”   presen t  when   it’s  trying  to  study   phot os  (such   as  loads   of  different   objects   in  the  backgr ound).   This  doesn’t   bode   too  well  for  socie ty,  which   is  a  busy  place   and   isn’t alw ays posing f or a clear phot o.   FRT is liable t o bias    Wha t  you  give  FRT,  you  get  out.  As  with   all  AI,  the  dataset  given  to  the  algorithm   or  softw are  is   wha t  it  feeds   off  and  learns   from  (like  a  newborn   baby).  For  example,   if  we  taugh t  a  newborn    that  the  first  letter  of  the  alphabe t  is  Z,  it  will  continue   to  treat  it  as  such.   Likewise,   if  we  presen t   the  FRT  with   a  dataset  that  only   comprises   of  whit e  male   faces,   it  will  only   be  able   to  accur ately   iden tify  whit e  male   faces.   This  could   be  a  result   of  human   error,  or  just  a  lack  of  awareness   of   wha t the da tabase c onsis ts of .   The le vel of trus t   It  has  become   a  ‘malpr actice’   to  ques tion  decisions   made   by  technology .  The  technologic al   mindse t  that  has  shaped   our  sear ches   for  solutions   to  problems,   I  argued,   made   it  almos t   frowned   upon   to  ques tion  the  results   of  the  technology .  Technology   is  seen   as  some thing   that,   and  with   proof,  is  more  accur ate  than   humans   can  ever  wish   for.  However,  statistical  accur acy  is   different  to  contextual  accur acy  and  the  human   experience   in  gener al.  In  this  sense,   technology    has de finitely pr oven it c an be trus ted, but it mus t also w arrant the trus t.   Difficulty with op ting out:    Just  like  with   website  cookies,   wher e  it’s  a  lot  easier   just  to  “Accep t  all”  or  opt-in  to  wha tever  ad   analy sis  they  want  to  do  to  get  rid  of  that  anno ying  pop  up,  it’s  a  lot  easier   just  to  consen t  to  the   use of FR T.  If y ou do not op t-in, y ou ar e very much seen as an inc onvenience.    How  FRT  affects  our  social   beha viourA   talk  of  mine   would   not  be  so  without   a  little  bit  of   philosoph y.  Here,  I  made   sure  to  men tion  how  the  vigilance   aspect   of  FRT  could   end  up   affecting   how  we  conduct   ourselves  in  social   spaces.   We  could   begin   to  become   hyper   aware  of   how  we  act,  and  whe ther   this  attends   to  the  standar ds  of  the  people   behind   the  vigils.   Here,  we   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   6 4   
 could   now  be  treated  as  some thing   ‘to  be  monit ored’  and  ‘to  be  tracked’  like  with   FRT  being    used in My anmar t o track pr otestors earlier this y ear.   The need f or indus try ethics    In  this  section,   given  the  ethical  issues   at  play,  I  highligh ted  the  need   for  indus try  ethics.   I   men tioned   Amaz on’s  Rekognition   moratorium   in  2020,   as  well  as  IBM  and  Facebook   cancelling    their v entures in to FR T.   Curr ent laws surr ounding AI    I  made   sure  to  give  my  audience   a  flavour  of  wha t  AI  regula tion  is  currently  activ e.  I  specific ally   men tioned   the  Bolstering   Online   Transpar ency   Act  in  Calif ornia,   as  well  as  the  Illinois   video    analy sis  law.  I  also  men tioned   how  age-old   laws  like  the  Civil  Righ ts  Act  can  still  play  a  part  in   algorithmic   design,   making   sure  the  technology   can’t  discrimina te  on  age,  race,  marit al  status   etc.   Between the lines    My  conclusion   centres  in  how  while   AI  is  a  sword  to  be  wielded,   it  requir es  training   to  be  used    correctly .  AI  in  the  business   world  can  serve  as  a  great  tool,  but  its  potential  issues   are  clear   for   all  to  see.  However,  through   the  study   of  the  AI  basics   and  the  technology ’s  current  state,  I   belie ve tha t the righ t training c an be pr ovided in or der t o appr opria tely utiliz e AI.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   6 5   
 2. Analy sis of the AI E cosystem   Introduction  by Connor W right, Partner ship s Manag er,  Mon treal AI E thics Ins titut e   Such   a  ques tion  as  the  one  above  could   gener ate  hour s  of  discussion   yet  only   scratch  the   surface  of  the  discipline.   In  recen t  years,  AI  Ethics   has  been   moulded   into  its  own  field   through    increasing   recognition   of  its  import ance   and  sheer   comple xity.  Nevertheless,   essen tial  to  note  is   while   AI  Ethics   is  now  an  established   branch,   it  stretches   into  and  requir es  input   from  its   surrounding   fields.   As  men tioned   in  my  resear ch  summar y  on  how  we  cannot   have  AI  Ethics    without   Ethics   ,  the  domain   does   not  exist  in  a  vacuum.   It  should   be  incorpor ated  as  a  standar d   practice   in  different  fields,   rather   than   a  stand-alone   subject   (rather   than   some thing   to  ‘tick  off   the  list’).  Its  independence   requir es  interdependence   on  a  diverse  and  multidisciplinar y   appr oach, not jus t on the individual le vel but also on the or ganisa tional.    The  whole   corpor ate  involvemen t  within   AI  Ethics   is  paramoun t.  Establishing   the  link  between   those   at  the  top  of  the  tree  and  those   closes t  to  the  AI  product   will  be  critic al  to  transferring    informa tion.   Open   communic ation,   here,  can  lead   to  hones t  feedback   on  how  the  technology   is   performing.   Not  only   that  but  involving   those   closes t  to  the  applic ation  creates  the   empo wermen t  of  emplo yees  ,  allowing   them   to  take  owner ship  of  the  solutions   they  are   making.   For  example,   should   the  compan y  be  involved  in  automa ted  hiring ,  emplo yees  will  be   able t o ans wer the ques tion c onfiden tly, “wh y are you cr eating  automa ted hiring t ools  ?”.   Furthermor e,  involving   different  business   sections   can  avoid  the  broken  part  fallacy   ,  focussing    on  a  broken  part  in  a  problem   that  you  think,   once   fixed,  resolv es  the  issue.   However,  this   dismisses   the  systemic   nature  of  the  problem.   Hence,   before  looking   for  the  broken  part,   we   should   ask  ourselves  how  it  got  there.  To  be  sure  your  decisions   can  comba t  this  issue,    academia and educ ation f orm a crucial part of the AI E thics iden tity.   We  have  explor ed  how  scenario   analy sis  and  real  case  studies   are  the  best  way  to  educ ate  on   the  issues   involved.  Rather   than   treat  AI  Ethical  issues   as  theor etical  and  intangible,   basing   them    on  practic al  consider ations   help s  anchor   the  practice   in  the  here  and  now.  In  this,   academia    serves  as  a  rich  resour ce  from  which   to  encounter  the  latest  necessar y  concerns.   Comba ting   racial   issues   ,  ackno wledging   nonhuman   consider ations   and  more  are  issues   that  can  be  brough t   to  the  fore  through   academia’ s  involvemen t.  Wha t  mus t  now  be  consider ed  is  wher e  or,  better   yet, to whom is the implemen tation of these c onsider ations dedic ated to?   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   6 6   
 In  terms   of  their   job  title,   an  AI  Ethicis t  is  the  person  in  char ge  of  putting  theor y  into  practice.    However,  AI  ethics   isn’t   just  for  an  ethicis t  (even  if  it’s  seman tically  similar).   Instead,   they  have   to  act  as  engineer s,  data  scien tists,  comput er  programmer s,  and  more  to  best  represen t  their    view.  To  do  so  may  involve  putting  the  brakes  on  an  initia tive  that  is  pressing   and  will  mak e  a   compan y a health y profit. Y ou ar e, at times, the   only   person sa ying no   .   The  time frame   to  say  no  should   certainly   not  be  limit ed  to  purely  the  design   stage.  Instead,    saying  no  can  also  involve  decisions   on  products   already   presen t,  such   as  Facebook   shut ting   down  their   use  of  facial   recognition   amid   privacy  concerns,   alongside   IBM’ s  bias  concerns   .   Monit oring   the  AI  product   in  these   early   stages  after  deplo ymen t  is  crucial   to  mitig ating   potential pr oblems, especially unin tended c onsequences.    Such   decisions   often  include   thinking   of  unin tended   consequences   ,  a  difficult   task  made   even   more  so  when   some   unin tended   consequences   may  actually   be  bene ficial.   For  example,   the   true   visit  of  dolphins   to  the  waters  of  Venice   in  March  2021   thank s  to  the  city’s  lockdown   restrictions.   In  this  way,  thinking   of  all  scenarios   is  hard  and  the  field   may  be  seen   as  a  constant   uphill push, but the k ey is t o  work t ogether   .   As  men tioned   above,  the  AI  Ethics   space   is  incredibly   encompassing   and  the  bene fit  of  including    diverse  perspectiv es  is  appar ent  for  all  to  see.  From  our  perspectiv e  here  at  MAIEI,   nothing    expresses   this  more  than   our  Learning   Community   2021   Cohort.   Featuring   experts   in  minorities,    disability   and  gender   from  the  corpor ate,  academic   and  politic al  scene,   participan ts  engaged  in   topics   from  digit al  labour   to  emotional   recognition.   Alongside   the  geogr aphic al  variation  in  the   group,   the  discussions   were  both   enligh tening   and  rich  in  content,  showing   the  true   potential  of   an  interdisciplinar y  discussion.   If  anyone  ever  asks  for  a  one  sentence   answer  to  the  ques tion   “wha t does AI E thics in volve?” interdisciplinarity is impossible t o omit.    One  of  the  man y  reasons   for  this,   in  my  view,  is  diversity’s  way  of  tackling   the  difficult   task  if  one   person  being   able   to  consider   all  perspectiv es.  It  may  be  that  the  AI  Ethicis t  is  to  assume   the   role  of  designer ,  programmer   and  so  on,  but  this  is  still  in  desper ate  need   of  other   peoples’    inputs.   As  I  men tioned   in  my  TEDxY outh   Talk  in  November   2020,   AI  Ethics   is  to  involve  allowing    the  public   to  become   author s  of  the  AI  story  .  That  is  to  say,  those   directing   AI’s  current   traject ory  cannot   account  for  every  single   different  experience,   meaning   without   making   our   own  unique   experiences   known  the  AI  Ethics   space   will  be  worse  off.  It  is  the  space’ s  ability   to   be  enriched   by  all  those   involved,  no  matter  wha t  qualific ation  you  possess,   that  mak es  it  truly    unique.    I  hope   the  following   chap ter  help s  to  deepen   wha t  I  have  already   men tioned,   as  well  as   highligh t  the  import ant  role  all  can  play  in  this  constantly  evolving   space.   It  may  be  a  lonely    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   6 7   
 practice   at  times,   but  the  bene fits  of  working   alongside   each   other   to  achie ve  a  solution   that   bene fits all is truly en ticing. So , please join!    Connor Wright (  @csi_wright  )   Partnerships Manager   Montreal AI Ethics Institute   Connor  is  the  Partnerships  Manager  at  the  Montreal  AI  Ethics  Institute  and  is   currently  pursuing  a  philosophy  degree  at  the  University  of  Exeter.  He  has   featured  on  panels  on  the  topics  of  facial  recognition  technology  and   post-pandemic  education,  while  currently  working  on  the  relationship  between   anthropomorphism  and  AI.  His  main  passion  lies  in  the  form  of  the  cross-section  between  the   Sub-Saharan African philosophy of Ubuntu and AI, stemming from his upbringing in South Africa.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   6 8   
 Go Deep: Research Summaries   The V alues Enc oded in Machine Learning R esear ch   [  Original   paper   by  Abeba   Birhane,   Pratyusha   Kalluri,   Dallas   Card,  William   Agne w,  Ravit  Dotan,   Michelle Bao]    [Resear ch Summar y by Abhishek Gup ta]   Overview  :  Machine   learning   is  often  portr ayed  as  a  value-neutr al  endea vor;  even  when   that  is   not  the  exact  position   taken,  it  is  implicit   in  how  the  resear ch  is  carried   out  and  how  the  results    are  communic ated.  This  paper   undert akes  a  qualit ative  analy sis  of  the  top  100  mos t  cited   paper s  from  NeurIPS   and  ICML   to  uncover  some   of  the  mos t  prominen t  values   these   paper s   espouse and ho w the y shape the pa th forward.   Introduction    As  we  get  a  higher   proliferation  of  AI  in  various   aspects   of  our  lives,  critic al  scholar s  have  raised    concerns   about   the  negative  impacts   of  these   systems  on  socie ty.  Yet,  mos t  technic al  paper s   published   today  pay  little  to  no  attention  to  the  socie tal  implic ations   of  their   work.  And  this  is   despit e  emer ging   requir emen ts  like  “Broader   Impact   Statemen ts”  that  have  become   manda tory   at  several  conferences.   Through   the  manual   analy sis  of  100  paper s,  this  resear ch  surfaces   trends    that  support   this  position   and  articula tes  that  machine   learning   is  not  value-neutr al.  They   annot ate  sentences   in  the  paper s  using   a  custom  schema,   making   open   sour ce  annot ated   versions   of  the  paper s  and  their   schema   and  code.   The  resear cher s  used   an  inductiv e-deductiv e   appr oach   to  capture  the  values   that  are  represen ted  in  the  paper s.  The  resear cher s  found   that   mos t  of  the  technic al  paper s  focused   on  performance,   gener alizability ,  and  building   on  past   work  to  demons trate  continuity .  Ther e  has  also  been   a  rising   trend  in  the  affiliations   and   funding   sour ces  for  the  author s  of  these   paper s  to  come   from  Big  Tech  and  elite  univ ersities.    Through   these   findings,   the  author s  hope   that  technic al  resear ch  can  become   more   self-reflectiv e to achie ve socially bene ficial out comes.    Methodology    The  author s  choose   NeurIPS   and  ICML   as  the  sour ce  of  their   paper s  because   they  have  the   highes t  impact   (as  quan tified   by  the  median   h-5  inde x  on  Google   Scholar),   and  conference    submissions   are  a  bellw ether   to  judg e  wher e  the  field   is  headed   and  wha t  areas  of  the  field    resear cher s  care  about   and  focus   their   efforts  on  as  they  form  critic al  evalua tive  factors.  Man y   of  these   paper s  are  written  to  win  appr oval  from  the  community   and  the  reviewers  drawn  from   that  community   to  achie ve  these   goals.   The  annot ation  appr oach   includes   examining   the   content  of  the  paper   and  creating  a  justificatory  chain   with   a  rating  on  the  degr ee  to  which    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   6 9   
 technic al  and  socie tal  problems   serve  as  the  motiv ation  for  the  work.  They  also  pay  attention  to   the  discussion   of  the  negative  impacts   of  the  work  as  stated  in  those   paper s.  The  author s   ackno wledg e  that  this  methodology   is  limit ed  because   it  is  manual   and  can’t  be  easily   scaled.    They  justify  that  by  poin ting  out  that  automa ted  annot ation  would   be  limiting   in  that  the   categories   would   be  pre-enc oded   and  subtleties  will  be  lost  that,  for  the  time   being ,  only    human r eviewers would be able t o pick up on.    Findings    Values   related  to  user   rights  and  stated  in  ethical  principles   rarely  occurr ed,  if  at  all,  in  the   paper s.  Other   moral  values   like  autonom y,  justice,   and  respect   for  people   were  also  noticeably    absent.  Mos t  of  the  justifications   provided   for  carrying  out  resear ch  poin t  to  the  needs   of  the   ML  community   with   no  relation  to  the  socie tal  impacts   or  problems   that  they  are  trying  to   solve.  The  negative  potential  of  these   works  was  also  conspicuous   by  their   absence.   Though,    some   of  those   omissions   migh t  be  the  result   of  the  taxonom y  and  awareness   of  the  socie tal   impacts   of  AI  being   more  recen tly  discussed,   especially   related  to  the  analy sis  of  the  paper s   from 2008-09.    In  terms   of  performance,   the  typic al  char acterization  for  it  is  average  performance   over   individual   data  poin ts  with   equal   weigh ting.   This  is  a  value-laden   move  as  it  deprioritiz es  those    who   are  underr epresen ted  in  the  datasets.  In  choosing   the  data  itself ,  building   on  past  work  to   show  impr ovemen ts  on  benchmark s  is  the  dominan t  appr oach,   but  this  presupposes   a   particular   way  of  char acterizing   the  world  that  migh t  not  be  accur ate,  as  demons trated  with    man y  datasets  codifying  socie tal  biases.   The  emphasis   on  large  datasets  also  moves  to  centralize   power  because   it  shifts   control  over  to  those   who   can  accumula te  such   data  and  then   dictate   wha t  is  included   and  wha t  is  not.  The  reliance   on  ground-truth   labels   in  this  case  also  codifies    the  assump tions   that  there  is  necessarily   a  correct  ground-truth   that  is  single- valued,   which   is   not the c ase.   Represen tational   harms   arise   from  the  excessiv e  focus   on  gener alization  capabilities   of  the   systems  since   it  moves  to  disregard  context  and  enforce  a  particular   view  of  the  world  onto  the   rest  of  the  incoming   data  in  the  interest  of  gener alization.   Efficiency   is  another   value   that  is   emphasiz ed,  but  it  is  rarely  discussed   in  the  context  of  accessibility   which   could   create  more   equity .  Instead,   it  focuses   on  using   fewer  resour ces  and  scalability   as  the  values   that  are  the   mos t  import ant.  The  focus   on  novelty  and  building   on  previous   work  also  entrenches   existing   positions   further   with   limit ed  critic al  examina tion  of  that  prior   work  in  the  interest  of  continuity    and  demons trating  impr ovemen ts  on  existing  benchmark s  rather   than   ques tioning   if  the   benchmark s are represen tative in the fir st place.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   7 0   
 Finally ,  the  increasing   influence   of  Big  Tech  and  elite  univ ersities   on  the  state  of  resear ch  is   another   avenue   through   which   ethical  principles   are  being   sidelined   and  a  specific   set  of  values,    as  highligh ted  above,  are  being   pushed   into  the  resear ch  and  developmen t  of  machine   learning.    The  current  trend  of  treating  machine   learning   as  neutr al  creates  insula tion  for  the  field   in   terms of critiquing the v alues tha t it espouses, both implicitly and e xplicitly .   Between the lines    This  meta-analy sis  of  the  state  of  affairs  in  machine   learning   resear ch  is  a  signific ant   contribution   to  better  under standing   wher e  we  are  headed.   In  particular ,  the  author s’   contribution   of  a  data  annot ation  schema   and  the  set  of  annot ated  paper s  will  be  help ful  for   futur e  analy sis  and  resear ch.  Some   developmen ts  that  I’d  like  to  see  building   on  this  work  would    be  finding   a  way  to  scale  this  appr oach   to  have  a  more  real-time   analy sis  of  wher e  the  field   is   headed   and  self-correct  as  we  go  along.   A  broader   test  of  the  inter-resear ch  agreemen t  on  the   annot ations   would   also  be  help ful.  While   the  author s  do  indic ate  a  high   degr ee  of   inter-reviewer  agreemen t  through   the  Cohen   Kappa   coefficien t,  it  would   be  interesting  to  see   how  that  chang es  (if  at  all)  when   you  get  a  broader   set  of  people   to  take  a  look,   especially   those    coming   from  a  variety  of  fields   (even  though   the  author s  themselv es  are  quite  diverse  in  the   composition of this t eam).    Comba ting An ti-Blackness in the AI Community    [  Original paper   by De vin Guillor y]   [Resear ch summar y by Connor W right]   Overview  :  Racism   has  the  potential  to  establish   itself   in  every  corner   of  socie ty,  with   the  AI   community   being   no  different.  With   a  mix  of  observations   and  advice,   the  paper   harbour s  a   need   for  chang e  alongside   the  potential  for  the  academic   environmen t  to  manif est  it.  While    some of the s teps involved carry risk, the dang er of not doing so is e ven gr eater.   Introduction    How  badly   does   the  AI  community   need   diversity?   Wha t  is  academia’ s  role  in  this  process?    Themes   surrounding   how  those   in  the  AI  community   can  help   to  comba t  systemic   racial    injus tice  are  lightly  touched   upon   through   an  academic’ s  lens.   Here,  ackno wledging   how  racism    permea tes  into  every  corner   of  our  socie ty  is  an  import ant  first  step.  However,  further    disparities   and  chang es  are  still  left  unturned,   with   the  AI  community   suffering   as  a  result   if   nothing is done.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   7 1   
 Discr epancies in r esour ces   The  AI  field   boas ts  some   invisible   barrier s  to  entry  between  candida tes  of  different  ethnicities    wanting access. These pr esen t themselv es in 3 dif ferent categories:    Physical  discr epancies.   Disparities   in  resour ces,  such   as  comput ers,  are  accen tuated  in  a  field    that  often  requir es  large  computing   power  to  participa te.  Also   included   is  the  valuable   asse t  of   time.    Social   discr epancies.   Man y  AI  jobs  are  now  accessed   through   social   networking   and  referrals.   With   there  being   gaps  in  physical  resour ces,  having  access   to  the  networking   environmen ts   requir ed varies hug ely.   The  Measur es  used   –  SAT  systems  have  been   seen   to  dispr oportiona tely  disadv antage  Black    studen ts.   In  the  admissions   process,   the  disparity   in  terms   of  social   and  physical  resour ces  becomes   even   more  appar ent.  Academia’ s  role,  and  the  potential  problems   it  can  propag ate,  become   even   more critic al with its r elation t o the AI c ommunity .   Academia as a w ell of in forma tion   Academia   and  resear ch  are  a  direct  feeder   into  the  AI  community ,  so  any  poisoning   found   in  the   field   will  propag ate  to  other   parts   of  socie ty.  In  this  way,  academic   faculties   will  have  to   wholeheart edly   buy  into  the  effort  of  comba ting  the  issues   at  their   root.  One  way  to  do  this  is   through f eedback.    The import ance of f eedback    Any  positiv e  chang e  will  need   to  be  grounded   in  informa tion  from  those   who   have  gone   through   the  system.  The  experiences   of  those   who   have  been   discrimina ted  against  can  provide    crucial   insigh ts  into  how  the  system  can  chang e.  Without   such   feedback,   the  same   procedur es   and the same discrimina tion will c ontinue t o be pr esen t.   Given  the  need   for  chang e,  the  paper   also  offers  views  on  wha t  can  be  done.   The  first  of  which    involves jumping in to the unkno wn.   Taking risk s   Starting   to  accep t  candida tes  with   different  applic ations   to  years  gone  by  can  be  a  first  step   toward  comba ting  any  effects  of  systemic   racism.   Looking   at  other   institutions   from  which    studen ts  come   from  or  prioritising   different  char acteristics  in  success ful  candida tes.  Not   emphasising   the  need   for  experience,   a  clear   consequence   of  having  lesser   social   and  physical   discr epancies, as much c an be one w ay of doing this.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   7 2   
 Reflecting on y our en vironmen t   As  a  resear cher ,  reflecting   on  which   studen ts  you  are  men toring   can  also  bring   up  observations    about   the  current  diversity  level  in  your  environmen t.  Furthermor e,  collabor ating  with   different   people   than   you  usually   can  also  help   promot e  diversity  by  experiencing   different  ways  of   thinking.    Wha t div ersity brings    Such   alterna tive  views  are  not  the  only   thing   that  diversity  brings.   Having  a  faculty   with   varied    backgr ounds   can  also  allow  studen ts  of  similar   experiences   to  better  relate  to  their   professor s.   Some   studen ts  may  feel  that  they  can  only   talk  about   specific   problems   with   professor s  of   similar   backgr ounds,   with   such   a  presence   bringing   great  comfort  to  the  academic   experience.    However,  this  is  not  to  say  that  underr epresen ted  member s  are  solely   though t  of  in  the  value    they  add  to  an  in-gr oup  institution.   Instead,   the  bene fits  of  diversity  should   be  a  consequence    of the div erse pr ofessor s’ value.    Between the lines    The  potential  academia   possesses   to  influence   the  proliferation  of  discrimina tory  practices   in   the  AI  community   is  extensiv e.  Seen   as  the  seed   for  the  AI  community ,  taking   risks  to  effect   chang e  is  a  signific ant  step  for  me.  Nevertheless,   any  form  of  chang e  will  not  be  easy,  especially    if  it  involves  self-reflection   about   your  environmen t.  However,  not  taking   these   steps  could    further   drive  any  form  of  diversity  away,  which   is  simply   a  move  that  the  AI  community   can  no   long er afford.   Achie ving   a  ‘Good   AI  Socie ty’:  Comparing   the  Aims   and  Progress  of  the  EU   and the US    [  Original   paper   by  Huw   Roberts   ,  Josh   Cowls,  Emmie   Hine   ,  Francesc a  Mazzi,   Andr eas   Tsamados , Mariar osaria T addeo , Luciano Floridi]    [Resear ch Summar y by Andr ea P edeferri]    Overview  :  Governmen ts  around   the  world  are  formula ting  different  strategies   to  tackle   the   risks  and  the  bene fits  of  AI  technologies.   These   strategies   reflect   the  norma tive  commitmen ts   highligh ted  in  high-le vel  documen ts  such   as  the  EU  High-Le vel  Expert   Group  on  AI  and  the  IEEE,    among   other s.  The  paper   “Achie ving  a  ‘Good   AI  Socie ty’:  Comparing   the  Aims   and  Progress  of   the  EU  and  the  US  compar es  strategies   and  progress  made   in  the  EU  vs  the  US.  The  paper    concludes   by  highligh ting  areas  wher e  impr ovemen t  is  still  needed   to  reach   a  “Good   AI  Socie ty”   are “autonomous, in teractiv e, and adap tive”.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   7 3   
 Introduction    Recen tly,  we  have  focused   on  how  designer s  should   implemen t  values   in  AI  systems  and  how   design   choices   can  become   more  ethical.  Now  is  the  time   to  turn   to  the  role  of  policymak ers   and  governmen ts  in  shaping   strategies   and  regula tions   to  tackle   the  risks  and  the  bene fits  of  AI   technologies.   The  paper   Achie ving  a  ‘Good   AI  Socie ty’:  Comparing   the  Aims   and  Progress  of  the   EU  and  the  US  compar es  the  strategies   and  the  progress  made   in  the  EU  vs  the  US.  The  paper    concludes b y highligh ting ar eas wher e impr ovemen t is s till needed.    Key Insigh ts   At  MAIEI,   we  have  look ed  at  some   recen t  resear ch  on  AI  governance   in  China.   Similarly ,  the   current  paper   gives  us  the  chance   to  look   at  how  AI-governance   is  shaping   up  in  the  US  and  in   the  EU.  Why  EU  and  US  only?   As  the  author s  of  the  paper   explain,   “We  chose   to  focus   on  the  EU   and  US  in  particular   because   of  their   global   influence   over  AI  governance,   which   far  exceeds    other   countries   (excluding   China).   Mor e  substantively,  the  EU  and  the  US  mak e  for  an   interesting  compar ative  case  study   because   of  their   often-touted  politic al  alignmen t  over   guiding   values,   such   as  represen tative  democr acy,  the  rule  of  law  and  freedom. ”  Hence,   the   goal  of  the  paper   is  to  analy ze  those   governmen ts’  “visions   for  the  role  of  AI  in  socie ty”,  and  in   particular ho w the y intend t o de velop a ‘ Good AI Socie ty’.   When   making   a  compar ative  analy sis  of  ethics-r elated  issues,   it  is  crucial   to  keep  in  mind   that   different  socie ties  and  cultur es  may  subscribe   to  different  values   and  have  a  different   under standing   of  wha t  developing   a  ‘good  AI  socie ty’  actually   means.   At  the  same   time,   the   author s  rightly  poin t  out  that,  “to  consider   no  values   as  inher ently  ‘good’   is  a  form  of  extreme    metaethical  relativism   (Brandt,   2001),   according   to  which   nothing   of  substance   can  ever  be  said   justifiably   about   the  respectiv e  merits   of  different  visions. ”  The  author s’  view  on  this  is  that  we   should   adop t  a  form  of  “ethical  pluralism” .  As  they  explain   it,  there  are  “man y  different  valid   visions   of  a  ‘Good   AI  Socie ty’,  but  [ …]  each   one  needs   to  be  underpinned   by  a  set  of  values   that   are  viewed  at  national   and  interna tional   levels  as  desir able.   Such   values   are  likely  to  include    democr acy,  justice,   privacy,  the  protection   of  human   rights,  and  a  commitmen t  to   environmen tal  protection. ”  Thus,   while   they  want  to  avoid  adop ting  ethical  absolutism,   the   author s also v oice the need t o avoid the tr ap of e thical relativism.    AI Go vernance in the Eur opean Union    In  particular   since   2016,   European   countries   have  worked  quite  hard  to  find  ways  to  regula te  AI.   They  have  put  forward  some   high   level  requir emen ts  for  a  trustworth y  AI  (e.g.  robus tness,    transpar ency).   Mos t  recen tly,  the  EU  has  released   the  draft  Artificial   Intelligence   Act  “which    proposes   a  risk-based   appr oach   to  regula ting  AI.”  As  the  author s  explain,   “the   EU’s  long-t erm   vision   for  a  ‘Good   AI  Socie ty’,  including   the  mechanisms   for  achie ving  it,  appear s  coher ent.  The   vision   for  governing   AI  is  underpinned   by  fundamen tal  European   values,   including   human    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   7 4   
 dignity ,  privacy  and  democr acy.  [ …]  The  risk-based   appr oach,   which   combines   hard  and  soft   law,  aims   to  ensur e  that  harms   to  people   are  minimiz ed,  while   allowing   for  the  business   and   socie tal bene fits of these t echnologies. ”   However, this vision has some not able g aps:   ●  No  reference   is  made   to  the  “contribution   of  training   AI  models   to  increased   greenhouse    gas emissions. ”   ●  Not  enough   to  “support   collectiv e  interests  and  social   values”   (e.g.  no  right  to   group-priv acy)   ●  Not  enough   emphasis   on  “how  to  addr ess  systemic   risk” .  The  draft  focuses   on  “the   risk   to  individuals   from  specific   systems”   but  does   not  really   look   at  “the   potential  of  AI  to   cause wider socie tal disrup tions. ”   ●  No clear position on “the use of AI in the milit ary domain. ”   ●  “The  aim  of  boos ting  the  EU’s  indus trial  capacity   is  hams trung   by  the  current  funding   of   the  EU  AI  ecosystem,  which   has  been   criticiz ed  as  being   inadequa te  when   compar ed  to   the US ’s and China’ s”   ●  No  clear   path  to  tackle   disparities   among   European   countries:   “Some   Member   States,   typic ally  in  Western  Europe,   have  developed   AI  strategies,   yet  this  is  mos tly  not  the  case   in Eastern and Southern Eur ope” .   The  languag e  around   risk  and  risk-assessmen t  in  the  draft  is  “vague   and  not-commit tal”.  “As  a   result,   effectiv e  protection   from  high-risk   systems  will  be  largely  relian t  on  interpr etations   by   standar ds  bodies   and  effectiv e  internal   compliance   by  companies,   which   could   lead   to   ineffectiv e or une thical out comes in pr actice. ”   The US appr oach t o AI   In  2016,   two  broad  US  reports   on  AI  were  released:   “Preparing   for  the  Futur e  of  Artificial    Intelligence’   and  the  ‘National   Artificial   Intelligence   Resear ch  and  Developmen t  Strategic  Plan’ .   These   and  other   documen ts  released   in  the  last  few  years  focus   mos tly  on  making   sure  US   leader ship  in  AI  is  preser ved  while   limiting   regula tory  overreach.   When   it  comes   to  ensuring   a   ‘Good   AI  Socie ty’,  the  documen ts  focus   on  ethical  principles   such   as  privacy,  fairness   and   transpar ency .  These   principles,   however,  do  not  transla te  into  a  real  AI  governance   strategy  and   the  tendency   is  to  emphasiz e  self-regula tion  by  indus try  (as  for  instance,   IBM’ s  recen t  initia tives   to  ensur e  a  trustworth y  design   and  use  of  AI).  The  problem   is  that,  as  the  author s  poin t  out,   “the   lack  of  specific   regula tory  measur es  and  oversight  can  lead   to  practices   such   as  ethics    washing   (introducing   superficial   measur es),  ethics   shopping   (choosing   ethical  frame works  that   justify  actions   a  posteriori)   and  ethics   lobb ying  (exploiting   digit al  ethics   to  dela y  regula tory   measur es).”   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   7 5   
 The  US  strategy  is  much   more  hand-on   when   it  comes   to  interna tional   relations   that  concern    the  use  and  developmen t  of  AI.  For  instance,   the  author s  explain   that  “The  Americ an  AI   Initia tive  states  the  need   to  promot e  an  interna tional   environmen t  that  opens   mark ets  for   Americ an  AI  indus tries,   protects  the  US’s  technologic al  advantage  and  ensur es  that   interna tional   cooper ation  is  consis tent  with   ‘Americ an  values’ .”  This  has  transla ted  into  a  clear    effort  to  frame   AI  as  a  “a  defense   capability   that  is  essen tial  for  main taining   technologic al,  and   therefore  oper ational,   superiority   over  the  adversary.”  However,  the  overall  assessmen t  is  that   “US  has  not  gone  far  enough   in  protecting   its  AI  capacities,   including   its  data  sets  and  stopping    the illicit tr ansfer of t echnologies” (e. g. sur veillance t echnology).    Between the lines    The  paper   concludes   that  when   it  comes   to  AI  governance,   “the   EU’s  appr oach   is  ethically   superior ”  as  it  strives  to  protect  its  citizens  by  implemen ting  regula tory  mechanisms.   The  US  has   mainly   focused   on  making   sure  that  “the   governance   of  AI”  is  placed   “in  the  hands   of  the   private  sector”.  Wha t  we  have  not  seen   discussed   in  the  paper ,  though,   is  the  role  an   independen t  auditing   of  AI  systems  could   play  in  both   the  US  and  the  EU.  It  would   be  import ant   to  see  how  and  whe ther   independen t  auditing   in  AI  could   be  applied   in  the  US’  and/ or  the  EU’s   regula tory systems, and wha t could be the adv antages and disadv antages of doing so.    Responsible Use of T echnology: The IBM Case Study    [  Original   paper   by  World  Economic   Forum   and  the  Markk ula  Center  for  Applied   Ethics   at   Santa Clar a Univ ersity]    *  Conflict  of  interest:  Marianna   is  currently  collabor ating  with   a  resear ch  team   at  IBM   led  by   Francesc a Rossi   [Resear ch Summar y by Marianna Ganapini]    Overview  :  In  recen t  summaries,   we  have  stressed   the  fact  that  at  times   private  companies   have   taken  the  lead   in  providing   guidelines   for  the  responsible   use  and  developmen t  of  AI   technologies.   The  World  Economic   Forum   and  the  Markk ula  Center  for  Applied   Ethics   at  Santa   Clara  Univ ersity  are  collabor ating  in  surveying  the  work  of  these   companies,   and  they  recen tly   have  focused   on  IBM.   In  this  summar y,  we  will  go  through   the  main   poin ts  of  their   mos t  recen t   whit e paper , discussing the import ance and no velty of the appr oach t aken b y IBM.    Introduction    Some   tech  companies   have  taken  the  lead   in  providing   guidelines   for  the  responsible   use  and   developmen t  of  AI  technologies,   especially   wher e  governmen ts  and  public   institutions   are   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   7 6   
 failing   to  establish   clear   guidelines   and  regula tions.   The  World  Economic   Forum   and  the   Markk ula  Center  for  Applied   Ethics   at  Santa  Clara  Univ ersity  are  collabor ating  in  surveying  the   work  of  these   companies,   and  in  this  paper ,  they  have  focused   on  IBM.   In  wha t  follows,  we  will   go  through   the  main   poin ts  of  their   recen t  whit e  paper   discussing   the  import ance   and  novelty   of the appr oach t aken b y IBM.    Key Insigh ts   One  of  the  key  momen ts  in  IBM’ s  developmen t  of  AI  Ethics   strategy  was  the  public ation  of  IBM   5 key commitmen ts to “accountability , compliance and e thics in the ag e of smart machines”:    ●  Creating  an  IBM  AI  Ethics   Boar d  to  “discuss,   advise   and  guide   (eventually   govern)  the   ethical  developmen t  and  deplo ymen t  of  AI  systems  (by  IBM  and  its  clien ts)”  —  since    2019   it  is  co-chair ed  by  Chris tina  Mon tgomer y  (IBM   Chie f  Privacy  Officer)   and  Francesc a   Rossi (IBM Global Leader of AI E thics)    ●  Designing a “ compan y-wide educ ational curriculum on the e thical de velopmen t of AI”    ●  Creating  the  IBM  “Artificial   Intelligence,   Ethics   and  Socie ty  program”:   “a  multidisciplinar y   resear ch  programme   for  the  ongoing   explor ation  of  responsible   developmen t  of  AI   systems aligned with the or ganization’ s values”    ●  Establishing   an  ongoing   “participa tion  in  cross-indus try,  governmen t  and  scien tific   initia tives and e vents on AI and e thics”    ●  Establishing   a  “regular ,  ongoing   IBM-hos ted  engagemen t  with   a  robus t  ecosystem  of   academics,   resear cher s,  policy -mak ers,  non-g overnmen tal  organizations   (NGOs)   and   business leader s on the e thical implic ations of AI    How  are  these   commitmen ts  being   implemen ted  in  practice?   To  under stand  some   of  the  recen t   key  decisions   of  the  IBM  AI  Ethics   Boar d,  we  need   to  first  zoom   in  on  the  fact  that  Trust  &   Trustworthiness   are  central  concep ts  to  the  current  IBM  strategy,  and  they  emer ge  out  of  5   “pillar s of trus t”: Explainability , Fairness, R obus tness, T ranspar ency , Priv acy.   These   are  the  key  values   that  IBM  pledg es  to  follow  in  its  design   strategies,   starting   with   the   creation  of  ethics-sensitiv e  technologies   followed  by  close   monit oring   of  downstream   effects  of   the use of these t echnologies.    These   pillar s  have  been   tackled   at  IBM  by  first  developing   some   technic al  tools  to  ensur e  trust   for their clien ts and f or the public a t large. Le t’s see wha t the y are:   ●  Explainability   :  when   AI  is  involved  in  a  decision-making   process,   the  reasons   for  the   decisions   are  to  be  made   available.   IBM  AI  Explainability   360  toolkit   aims   at  tackling    some of the t echnic al challeng es of ensuring e xplainability    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   7 7   
 ●  Fairness   :  together   with   the  IBM  Cloud   Pak  for  Data,  the  IBM  AI  Fairness   360  toolkit   for   detecting   biases   in  AI  is  a  tool  that  could   help   avoid  discrimina tion  and  unequal    treatmen t in the design of AI t echnologies.    ●  Robus tness   :  to  shield   from  adversarial   attacks  the  IBM  Adversarial   Robus tness   360   toolbo x is a v aluable de fense t ool   ●  Transpar ency   :  the  IBM  AI  FactShee ts  360  and  the  Uncert ainty  Quan tification  360  toolkit    are  ways  for  AI  developer s  to  documen t  key  aspects   of  their   models   to  ensur e   transpar ency    ●  Privacy  :  IBM  pledg es  that  “[o]nly   necessar y  data  should   be  collect ed,  and  consumer s   should ha ve clear access t o controls o ver ho w their da ta is being used. ”   Going   beyond  the  technic al  tools,   to  oper ationaliz e  those   5  pillar s  and  ensur e  trust,  IBM  adop ts   an  “ethics   by  design”   appr oach.   In  our  under standing ,  that  should   mean   that  the  above  values    or  pillar s  are  embedded   in  the  design   of  AI  technology   not  only   in  the  initial   design   phase   but   also  in  considering   the  downstream   consequences   and  potential  misuse   of  that  technology .  In   some c ases, tha t ma y requir e a c ompan y to re-design or chang e the t echnology alt ogether.   IBM  seems   commit ted  to  embedding   values   in  this  way,  as  shown,  for  instance,   by  their    willingness   to  re-think   the  use  and  production   of  their   facial   recognition   softw are.  Mor e   specific ally,  according   to  the  report,   IBM  is  taking   compan y-wide   practic al  steps  to  implemen t  its   “ethics b y design” appr oach. Some of these import ant steps are:   ●  Internal   curriculum-de velopmen t  and  repea ted  training   activities   to  promot e  ethics    sensitiv e design (IBM Gar age)   ●  Fostering div ersity, inclusion and equality in the w orkplace and a t the HR le vel   ●  Stakeholder s  engagemen t  with   the  goal  of  bringing   together   “AI  corpor ations   with   civil   socie ty  groups  for  conversations   on  the  best  practices   for  bene ficial   AI”  (e.g.  the   collabor ation with P AI)   ●  Stakeholder s  engagemen t  through   partner ships  with   univ ersities   (e.g.  Notr e  Dame-IBM    Tech E thics Lab)    ●  Involvemen t  in  governmen tal  discussion   on  AI  (e.g.,  Francesc a  Rossi’s  involvemen t  in  the   European Commission’ s High-Le vel Expert Gr oup on AI)    ●  Promoting   AI  for  social   good  (e.g.  the  Science   for  Social   Good   initia tive;  IBM  signed   the   Vatican’s Rome Call f or AI E thics in 2020).    These   are  some   of  the  concr ete  initia tives  taken  by  IBM  to  drive  the  compan y  toward  a  more   ethical  and  trustworth y  design   and  use  of  AI.  They  can’t  do  it  alone,   though:   private  companies    can  be  fully   trustworth y  only   if  they  are  part  of  a  broader   value-sensitiv e  environmen t  that   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   7 8   
 includes   independen t  oversight  organizations,   a  clear   legisla tive  frame work,  and  an  engaged   and in formed public.    Between The Lines    IBM  has  taken  the  lead   in  setting  the  standar ds  for  private  corpor ations’   involvemen t  in   promoting   AI  Ethics,   trying  to  learn   from  their   past  mistakes  while   looking   for  new  ways  to   ensur e  a  trustworth y  AI  for  their   clien ts  and  socie ty  at  large.  We  hope   to  see  more  of  that  kind    of  engagemen t  and  commitmen t  from  the  private  sector  going   forward.  Mor e  broadly ,  we   belie ve tha t to reach a trus tworth y AI w e need t o put mor e effort in to the f ollowing:    ●  Precise and t argeted g overnmen t’s AI r egula tions    ●  A priv ate sect or tha t genuinely c ommit ted to a trus tworth y AI   ●  Independen t oversight organizations & fr ame works (e.g., Independen t audit s ystems)    ●  Civic c ompe tence-pr omoting initia tives and or ganizations    U.S.-EU  Trade  and  Technology   Council   Inaugur al  Joint  Statemen t  –  A  look    into wha t’s in s tore for AI?    [  Statemen ts and R eleases   from The Whit e House Brie fing  Room]    [Resear ch Summar y by Angshuman K aushik]    Overview  :  This  write-up   focuses   on  the  conclusions   reached   at  the  inaugur al  mee ting  of  the   U.S.-EU  Trade  and  Technology   Council   (“TTC”)  held   in  Pittsbur gh,  Penns ylvania   on  September    29,  2021.   It  concerns   only   those   aspects   in  the  mee ting  that  deal   with   the  use  of  AI,  its  effects,    and the ar eas of c ooper ation en visioned, g oing f orward.   Introduction    The  timing   of  the  inaugur al  mee ting  of  the  TTC  couldn’t   have  been   better,  with   the  Facebook    saga  unfolding   before  the  world.   The  reason   I  say  this  is  because,   the  TTC’s  Inaugur al  Joint   Statemen t  discusses   outcomes   in  respect   of  five  key  areas,  one  of  them   being   developmen t  and   implemen tation  of  AI  systems  that  are  trustworth y,  and  those   that  respect   univ ersal  human    rights.  Set  in  motion   by  Presiden t  Joe  Biden,   Presiden t  of  the  European   Commission   Ursula  von   der  Leyen  and  European   Council   Presiden t  Charles   Michel   at  the  US-EU  Summit   in  June   2021,    TTC  comprises   10  Working   Groups,  with   AI  falling   within   the  Technology   Standar ds  Working    Group.   The  import ance   of  the  TTC  can  be  gauged  by  the  fact  that  both   the  US  and  the  EU  have   appoin ted  some   of  their   senior   officials   to  spearhead   it.  The  US  side  is  led  by  Secr etary  of  State   Antony  Blink en,  US  Trade  Represen tative  Katherine   Tai  and  Secr etary  of  Commer ce  Gina    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   7 9   
 Raimondo.   EU  Commissioner   for  Compe tition   Margrethe  Vestager  and  Commissioner   for  Trade   Valdis Dombr ovskis ar e represen ting Brussels.    Statemen t on AI    Coming   to  the  material   contents  of  the  Joint  Statemen t,  with   regards  to  AI,  it  talks  about   the   belie f  of  both   the  sides   in  the  potential  of  AI  to  bring   about   substantial  bene fits  to  their    respectiv e  socie ties,   and  tackle   various   challeng es.  One  signific ant  aspect   of  the  joint  statemen t   is  the  ackno wledg emen t  from  both   US  and  the  EU  of  the  risks  associa ted  with   AI-enabled    technologies   that  are  either   not  developed   and  deplo yed  responsibly ,  or  misused.   Further ,  they   assert   their   willingness   and  intention  to  develop   and  implemen t  trustworth y  AI,  and  their    commitmen t  to  a  human-cen tered  appr oach   that  buttresses   shar ed  democr atic  values   and   respects   univ ersal  human   rights.  The  key  here  is  the  choice   of  words  i.e.,  trustworth y  AI  and   human-cen tered  appr oach.   In  fact,  the  EU  has  already   shown  the  way  to  the  world,   by  putting   out  the  Proposal   for  a  Regula tion  of  the  European   Parliamen t  and  of  the  Council   Laying  Down   Harmoniz ed  Rules   on  Artificial   Intelligence   and  Amending   Cert ain  Union   Legisla tive  Acts   on  April    21,  2021   (“Artificial   Intelligence   Act”).  The  aforemen tioned   Proposal   deliv ers  on  the  politic al   commitmen t  by  Presiden t  Ursula  von  der  Leyen  that  the  Commission   would   bring   about    legisla tion  for  a  coordina ted  European   appr oach   on  the  human   and  ethical  implic ations   of  AI.  In   pursuance   of  the  above,  the  Commission   published   the  Whit e  Paper   on  Artificial   Intelligence   –  A   European   appr oach   to  excellence   and  trust  on  Februar y  19,  2020.   The  Whit e  Paper   sets  out  the   policy   routes  on  how  to  achie ve  the  twin   goals  of  promoting   the  uptake  of  AI  and  of  addr essing    the risk s associa ted with cert ain uses of such t echnology .   This  proposal   aims   to  implemen t  the  second  goal  for  the  developmen t  of  an  ecosystem  of  trust   by  proposing   a  legal  frame work  for  trustworth y  AI.  Mor eover,  both   the  US  and  the  EU  are  the   founding   member s  of  Global   Partner ship  on  AI,  which   brings   together   a  group  of  like-minded    partner s  seeking   to  support   the  responsible   developmen t  of  AI  that  is  based   on  human   rights   and  socie tal  bene fit.  The  joint  statemen t  also  opposed   and  reflect ed  its  signific ant  concern,    regarding   the  social   scoring   systems  deplo yed  by  authorit arian   governmen ts  (without   naming    any  country,  in  particular)   with   the  aim  to  implemen t  social   control  at  scale.  They  reiterated  that   these   systems  pose   threats  to  fundamen tal  freedoms   and  the  rule  of  law,  which   includes    silencing   speech,   punishing   peace ful  assembly   and  unla wful  surveillance.   The  statemen t  also   emphasiz ed  that  the  policy   and  regula tory  measur es  should   be  based   on  and  proportiona te  to   the  risks  posed   by  the  different  uses   of  AI.  Mor eover,  the  US  noted  the  European   Commission’ s   proposal   for  a  risk-based   regula tory  frame work  for  AI  and  the  fact  that,  EU  supports   a  number    of  resear ch  projects   on  trustworth y  AI,  as  part  of  its  AI  strategy.  The  EU  also  noted  the  US   governmen t’s  developmen t  of  an  AI  Risk  Manag emen t  Frame work,  as  well  as  ongoing   projects    on  trustworth y  AI  as  part  of  the  US  National   AI  Initia tive.  Further ,  the  joint  statemen t  reiterated   the  commitmen t  of  both   the  sides   to  work  together   to  foster  responsible   stewardship   of   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   8 0   
 trustworth y  AI  and  provide   resear ch-based   methods   to  advance   trustworth y  appr oaches   to  AI   that ser ve people in bene ficial w ays.   Areas of c ooper ation   The  statemen t  also  men tions   areas  of  cooper ation  between  both   the  US  and  the  EU.  The   objectiv e  is  to  transla te  shar ed  common   values   into  tangible   action   and  cooper ation  for  mutual    bene fit.  It  goes  without   saying  that  the  commitmen t  to  the  responsible   stewardship   of   trustworth y  AI  seems   to  be  on  top  of  the  agenda   for  both   US  and  the  EU,  as  they  seek   to   develop   a  mutual   under standing   on  the  principles   underlying   ‘trus tworth y  and  responsible   AI’.   In  this  regard,  they  intend  to  discuss   measur emen t  and  evalua tion  tools  and  activities   to  assess    the  technic al  requir emen ts  for  trustworth y  AI,  concerning ,  for  example,   accur acy  and  bias   mitig ation.   Further ,  they  also  expressed   their   desir e  to  collabor ate  on  projects   furthering   the   developmen t  of  ‘trus tworth y  and  responsible   AI’  to  explor e  better  use  of  machine   learning   and   other   AI  techniques   towards  desir able   impacts.   The  above  quite  clearly   poin ts  toward  the  fact   that,  both   sides   are  concerned   about   the  damaging   effects  of  certain  algorithms   on  socie ty.   Both   the  US  and  EU  also  expressed   their   intention  to  explor e  cooper ation  on  AI  technologies    designed   to  enhance   privacy  protections,   in  full  compliance   with   their   respectiv e  rules,   coupled    with   additional   areas  of  cooper ation  to  be  defined   through   dedic ated  exchang es.  Further ,  they   also  stressed   on  upholding   and  implemen ting  the  OECD  Recommenda tion  on  AI.  They  also   intend  to  jointly  undert ake  an  economic   study   examining   the  impact   of  AI  on  the  futur e  of  their    workforces,  with   attention  to  outcomes   in  emplo ymen t,  wages  and  disper sion  of  labor   mark et   opportunities.   They  also  expressed   their   willingness   to  inform  appr oaches   to  AI  consis tent  with    an  inclusiv e  economic   policy   that  ensur es  that  the  bene fits  of  technologic al  gains  are  broadly    shar ed b y workers.   Between the lines    As  stated  above,  this  mee ting  assumes   a  lot  of  signific ance,   considering   the  developmen ts   taking   place   globally   against  the  detrimen tal  effects  of  AI  on  individuals   in  particular ,  and  the   socie ty,  in  gener al.  Undoub tedly,  it  is  a  herculean   task  to  resolv e  the  issue   of  bias  and   discrimina tion  creeping   into  the  AI  systems,   but  someone   has  to  mak e  a  start  some wher e.   Further ,  the  aforesaid   problem   is  exacerba ted  by  interpr etability   and  explainability   issues    associa ted  with   certain  ‘black   box’  algorithms.   Governmen ts  around   the  world,   either    individually ,  or  in  a  collectiv e  manner   (as  is  the  case  here)  can  only   enact   policies   and  laws  to   enforce  responsible   and  trustworth y  AI,  but  self-regula tion  by  the  entities   accountable   for  the   developmen t  and  deplo ymen t  of  AI  is  crucial.   The  latest  example   of  governmen t  stepping   in  is   the  non-binding   resolution   passed   by  the  European   Parliamen t  on  October   6,  2021   calling   for  a   ban  on  police   use  of  facial   recognition   technology   in  public   places   and  on  predictiv e  policing.   It   also  called   for  a  ban  on  private  facial   recognition   databases   in  law  enforcemen t  and  support ed   the  European   Commission’ s  recommenda tions   to  put  an  end  to  social   scoring   systems.   As  far  as   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   8 1   
 self-regula tion  by  the  aforesaid   entities   are  concerned,   Francis   Haug en  (Facebook    whis tleblo wer),  during   her  testimon y  before  the  Sena te  Subc ommit tee  on  Consumer    Protection,   Product   Safety  and  Data  Security   within   the  Commit tee  on  Commer ce,  Science   and   Transport ation  stated  –  “the y  (she   was  referring   to  Facebook)   have  a  100%   control  over  their    algorithms” ,  which   means   that  companies   who   mak e  use  of  AI,  have  100%   control  over  their    algorithms.   Ther efore,  it  is  imper ative  that  both   the  governmen t  and  the  corpor ates  need   to   join  hands   in  order  to  arrest  the  further   occurr ence   and  spread  of  bias,   discrimina tion,   hatred   and disin tegration of the socie ty caused b y “toxic alg orithms” .   Public Str ategies f or Artificial In tellig ence: Which V alue Driv ers?   [  Original paper   by Gianluigi Viscusi, Anc a Rusu, and   Marie-V alen tine Florin]    [Resear ch Summar y by Connor W right]   Overview  :  Different  nations   are  now  catching   on  to  the  need   of  national   AI  strategies   for  the   good  of  their   futur es.  However,  wha t  really   drives  AI  and  whe ther   this  is  in  line  with   the  current   fundamen tal values a t the heart of dif ferent nations is a dif ferent ques tion.    Introduction    With   the  release   of  the  UK  Governmen t’s  AI  strategy,  the  ques tion  of  wha t  is  driving   its  design    comes   to  mind.   AI  has  long   been   touted  as  a  sure  way  to  impr ove  public   service  deliv ery  and   adminis tration  efforts.   However,  ques tioning   wha t  kind   of  values   are  currently  driving    governmen t  initia tives  has  not  been   too  visited.  Are  the  private  and  public   spher es  motiv ated   by  different  values?   Does   humanity   itself   enter  into  the  AI  conversation?   Is  AI  being   treated  in   too much of an ins trumen tal maner? I will no w explor e these thr ee ques tions in turn.    Do the priv ate and public sect ors diff er?   With   different  end  goals  and  different  audiences,   the  private  and  public   sectors  can  differ  on   man y  aspects   of  governance.   However,  “accountability ,  expertise,   reliability ,  efficiency ,  and   effectiv eness”   (p.g.  2)  were  found   to  be  held   in  common   between  the  two  spher es.  Other    aspects   like  “professionalism” ,  “efficiency ”,  “openness”   and  “inclusion”   have  been   common   to   both   as  well  (p.g.  2).  However,  I  belie ve  wha t  can  differentiate  the  two  are  the  interpr etations   of   the  values   listed.  Efficiency   will  differ  from  business   to  business,   especially   in  terms   of  wha t  is   deemed   as  the  threshold.   Mor eover,  “inclusion”   (p.g.  2)  and  who   is  subject ed  to  it  can  vary   widely   in  terms   of  extent,  depth  and  wha t  inclusion   entails.  Being   included   in  the  AI  governance    could   range  from  participa ting  in  its  design   or  just  occasionally   being   informed   on  the  chang es   being made t o an AI. Mos t of the time, a lack of inclusion is witnessed.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   8 2   
 A lack of f ocus on humanity    Principles   such   as  “transpar ency ”,  “priv acy”  and  “responsibility ”  are  men tioned   in  AI   governance   strategies   more  so  than   “Human   dignity ”  (pg.  4).  AI  is  often  touted  in  the  media   as   the  golden   ticket  towards  greater  ‘prosperity ’  for  humanity ,  but  humanity ’s  role  in  this   prosperity   is  often  left  untouched.   To  illustrate,  there  were  fewer  risks  and  challeng es  iden tified    in  the  AI  strategies   of  each   country  than   there  were  values.   So,  wha t  are  these   values    contributing   towards  if  governmen ts  do  not  articula te  the  problem?   From  my  reading   of  the   report, AI is f ar mor e accep ted as a t ool than as a socially designed and sensitiv e technology .   Instrumen tal norma tivity o ver social norma tivity    Phrases   in  the  shape   of  privacy,  efficiency   and  transpar ency   often  are  preferred  by  governmen ts   instead  of  words  such   as  democr acy.  One  way  to  express  this  substitution   is  by  ackno wledging    the  tension   between  impr oving  adminis trative  features  and  simult aneously   focusing   on  socie tal   issues.   Result antly,  the  values   held   at  the  core  of  constitutions   and  manif estos  are  often   sidelined   when   thinking   about   AI,  like  with   the  lack  of  reference   to  democr acy.  Ques tions   then    arise   of  whe ther   values   serve  as  sound   guiding   principles   for  AI  at  all,  or  are  they  another    example of a t okenistic gesture.   Between the lines    Wha t  is  crucial   for  me  to  consider   is  how  wha t  is  valued   varies   so  differently  across  different   nations.   As  explained   in  one  of  our  event  summaries   on  AI  in  different  national   contexts,  the   cultur al  interpr etation  of  different  values   can  vary  widely .  However,  fragmen tation  is  not  always   a bad thing if each c ountry adher es appr opria tely t o their giv en in terpr etation.    The  next  topic   of  deba te  comes   from  how  the  fundamen tal  values   that  the  entities   in  the  report    expounded   were  not  concurr ent  with   the  values   shown  in  the  AI  strategies.   Commit ting  to  AI   values   beyond  simply   writing   them   down  is  a  known  struggle  within   the  space.   For  me,  writing    down  the  value   and  wha t  problem   this  value,   if  championed   correctly ,  help s  to  prevent  is  one   way  of  contextualizing   and  focussing   efforts  on  the  issue   at  hand.   The  more  context  and   grounding   in  wha t  AI  is  being   deplo yed  to  do,  the  more  the  equally   valuable   social   values   we   hold c an appear .   Artificial In tellig ence: the global landsc ape of e thics guidelines    [  Original paper   by Anna Jobin, Mar cello Ienc a, Effy  Vayena]    [Resear ch Summar y by Avantika Bhandari]    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   8 3   
 Overview  :  Man y  private  companies,   resear ch  institutions,   and  public   sectors  have  formula ted   guidelines   for  ethical  AI.  But,  wha t  constitut es  “ethical  AI,”  and  which   ethical  requir emen ts,   standar ds,  and  best  practices   are  requir ed  for  its  realiz ation.   This  paper   investigates  whe ther    there  is  an  emer gence   of  a  global   agreemen t  on  these   ques tions.   Further ,  it  analy zes  the  current   corpus of principles and guidelines on e thical AI.    Introduction    Ther e  has  been   continuous   and  vigorous  deba te  around   AI  technologies   and  their    transforma tive  impact   on  socie ties.   While   mos t  studies   establish   that  AI  brings   man y   advantages,  they  also  underline   numer ous  ethical,  legal,  and  economic   concerns   primarily    relating  to  human   rights  and  freedoms.   Then   there  are  concerns   that  AI  may  “jeopar dize  jobs   for  human   workers,  be  exploit ed  by  malicious   actors,  or  inadv ertently  dissemina te  bias  and   thereby undermine f airness. ”   National   and  interna tional   organizations   are  looking   at  solutions   to  tackle   the  risks  associa ted   with   the  developmen t  of  AI  by  developing   ad  hoc  expert   commit tees.  Examples   include:   the   High-Le vel  Expert   Group  on  Artificial   Intelligence   appoin ted  by  the  European   Commission,   the   Advisor y  Council   on  the  Ethical  Use  of  Artificial   Intelligence   and  Data  in  Singapor e,  and  the   select   commit tee  on  Artificial   Intelligence   of  the  Unit ed  King dom   (UK)   House   of  Lords.  Private   companies   like  Google,   and  SAP  have  also  released   their   principles   and  guidelines   on  AI.   Professional   associa tions   and  non-g overnmen tal  organizations   such   as  the  Associa tion  of   Computing   Machiner y  (ACM),   Access   Now,  and  Amnes ty  Interna tional   have  come   forward  with    their   own  recommenda tions.   Activ e  involvemen t  of  different  stakeholder s  in  issuing   AI  policies    and  guidelines   proves  the  strong  interest  in  shaping   the  ethics   of  AI  in  order  to  mee t  their    respectiv e priorities.    The r esear cher s pose the f ollowing ques tions:    ●  Are  these   groups  converging   on  wha t  ethical  AI  should   be,  and  the  ethical  principles   that   will de termine the de velopmen t of AI?    ●  And, if the y div erge, then wha t are these dif ferences, and c an the y be r econciled?    Results    The  resear cher s  conduct ed  a  review  of  the  existing  corpus   of  guidelines   on  ethical  AI.  The   sear ch iden tified 84 documen ts containing e thical principles or guidelines f or AI.    ●  Data  reveal  a  signific ant  increase   in  the  number   of  public ations,   with   88%   having  been    released a fter 2016.    ●  Mos t  documen ts  were  produced   by  private  companies   (  22.6%)   and  governmen tal   agencies   respectiv ely  (21.4%),   followed  by  academic   and  resear ch  institutions   (10.7%),    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   8 4   
 intergovernmen tal  or  supr a-na tional   organizations   (9.5%),   non-pr ofit  organizations,   and   professional   associa tions/ scien tific  socie ties  (  8.3%   each),   private  sector  alliances   (4.8%),    resear ch  alliances   (  1.2%),   science   founda tions   (  1.2%),   feder ations   of  worker  unions    (1.2%)   and  politic al  parties   (1.2%).   Four  documen ts  were  issued   by  initia tives  belonging    to  more  than   one  of  the  above  categories   and  four  more  could   not  be  classified   at  all   (4.8% each).    ●  In  terms   of  geogr aphic   distribution:   a  signific ant  represen tation  came   from  more   economic ally  developed   countries   (MEDC).   The  USA  (23.8%)   and  the  UK  (16.7%)    together   account  for  more  than   a  third  of  all  ethical  AI  principles,   followed  by  Japan    (4.8%), German y, France, and Finland (3.6% each).    ●  Ethical  values   and  principles:   Eleven  (11)  overarching   ethical  values   and  principles   have   emer ged  from  the  content  analy sis.  These   are  by  frequency   of  the  number   of  sour ces  in   which   they  appear ed:  transpar ency ,  justice,   and  fairness,   non-male ficence,    responsibility ,  privacy,  bene ficence,   freedom   and  autonom y,  trust,  dignity ,  sustainability ,   and solidarity .   ●  The  resear cher s  found   that  no  single   ethical  principle   was  found   common   to  the  entire   corpus   of  documen t,  however,  an  emer ging   convergence   was  found   around   the   following   principles:   transpar ency ,  justice  and  fairness,   non-male ficence,   responsibility ,   and priv acy.   Discussion    ●  The  proportion   of  documen ts  issued   by  the  public   and  private  sectors  indic ate  that   ethical  challeng es  of  AI  concern   both   the  stakeholder s.  However,  there  is  a  notable    divergence in the solutions pr oposed.    ●  Further ,  there  seems   to  be  an  underr epresen tation  of  geogr aphic   areas  such   as  South    and  Central  Americ a,  Afric a,  and  Asia  which   insinua tes  that  the  interna tional   deba te  on   AI  may  not  be  happening   in  equal   measur es.  It  seems   that  MEDC   is  shaping   this  deba te,   which   raises   concerns   about   “neglecting   local  knowledg e,  cultur al  pluralism   and  global    fairness. ”   ●  Ther e  is  an  emer gence   of  a  cross-s takeholder   convergence   on  promoting   the  ethical   principles   of  transpar ency ,  justice,   non-male ficence,   responsibility ,  and  privacy.  However,   the  thema tic  analy sis  shows  divergences   in  four  (4)  areas:  1)  how  ethical  principles   are   interpr eted,  2)  why  they  are  deemed   import ant,  3)  wha t  issue,   domain   or  actors  they   pertain  to,  and  4)  how  they  should   be  implemen ted.  It  remains   ambiguous   as  to  which    ethical  principle   should   be  prioritiz ed,  how  the  conflicts   between  the  principles   should    be  resolv ed,  the  enforcemen t  mechanism   on  AI,  and  how  institutions   and  resear cher s   can comply with the r esulting guidelines.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   8 5   
 The  resear ch  indic ates  an  emer ging   consensus   around   the  promotion   of  some   ethical  principles,    however  thema tic  analy sis  provides   a  complic ated  narrative  as  “ther e  are  critic al  differences   in   how  these   principles   are  interpr eted  as  well  as  wha t  requir emen ts  are  consider ed  to  be   necessar y for their r ealiz ation. ”   Between the lines    It  seems   that  the  different  stakeholder s  seem   to  converge  on  the  import ance   of  transpar ency ,   responsibility   non-   non-male ficence,   and  privacy  for  the  developmen t  and  deplo ymen t  of   ethical  AI.  However,  the  resear cher s  also  call  for  underr epresen ted  ethical  principles   such   as   solidarity ,  human   dignity ,  sustainability   that  would   mos t  likely  result   in  better  articula tion  of  the   ethical  landsc ape  for  AI.  Mor eover,  it  is  high   time   there  is  a  shift   in  focus   from   principle- formula tion  into  actual   practice.   Finally ,  a  global   scheme   for  ethical  AI  should   “balance    the  need   for  cross-na tional   and  cross-domain   harmoniz ation  over  the  respect   for  cultur al   diversity and mor al plur alism. ”   NOTE  :  The  resear cher s  ackno wledg e  limit ations   in  the  study .  First,  the  guidelines   and  soft-law   documen ts  are  an  example   of  gray  literature,  and  thereby  not  inde xed  in  conventional    databases.   Second,   a  languag e  bias  may  have  skewed  the  corpus   towards  English   results.   Finally ,   given  the  rapid   frequency   of  public ation,   there  is  a  possibility   that  new  policies   were  published    after the r esear ch w as comple ted.   UK’s roadmap t o AI supr emacy: Is the ‘ AI W ar’ hea ting up?    [  Documen t  from the UK Go vernmen t]   [Resear ch Summar y by Angshuman K aushik]    Overview  :  The  UK’s  first  National   Artificial   Intelligence   Strategy  was  presen ted  to  the   Parliamen t  by  Nadine   Dorries,   Secr etary  of  State  for  Digit al,  Cultur e,  Media   and  Sport   by   Command   of  Her  Majes ty  on  September   22,  2021.   The  highligh t  of  the  strategy  is  the  highly    ambitious   ten-year  plan   ‘to  mak e  Britain  a  global   AI  superpo wer’.  Further ,  according   to  Dorries,    ‘this   strategy  will  signal   to  the  world  the  UK's   intention  to  build   the  mos t  pro-inno vation   regula tory environmen t in the w orld’ .   Introduction    The  celebr ated  Kai-Fu   Lee  in  his  seminal   book,   ‘AI  Super -Powers  China,   Silicon  Valley,  and  the   New  World  Order’,  observed,  ‘Harnessing   the  power  of  AI  today  –  the  “electricity ”  of  the   twenty-first  century  –  requir es  four  analog ous  inputs:   abundan t  data,  hungr y  entrepreneur s,  AI   scien tists,  and  an  AI-friendly   policy   environmen t’.  The  UK  which   has  always  been   at  the   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   8 6   
 vanguar d  of  AI  –  from  Alan   Turing   to  the  presen t  –  seems   to  have  taken  the  perspicacious   words   of  ‘the  indisput able   rock  star  of  China’ s  technology   scene’   earnes tly,  and  has  decided   to  follow   them   to  the  hilt,  going   forward.  In  fact,  in  the  recen t  past,  it  has  come   up  with   a  surfeit  of   Reports,   Policy   Paper s  etc.,  all  touching   upon   the  various   facets  of  AI.  One  amongs t  them,   is  the   House   of  Lords  Select   Commit tee  Report   on  Artificial   Intelligence   published   in  2018   titled   ‘AI  in   the  UK:  ready ,  willing   and  able? ’,  followed  by  the  House   of  Lords  Liaison   Commit tee  Report    called   the  ‘AI  in  the  UK:  No  Room   for  Complacency .’,  published   in  2020.   Ther e  are  several  other    reports   also.   This  strategy,  which   is  the  latest  entrant  into  the  scene,   owes  enormously   to  the  AI   Council’ s  16  recommenda tions   to  help   the  governmen t  develop   a  UK  National   AI  Strategy.  In   fact,  the  strategy  ackno wledg es  the  said  fact  by  men tioning   that  the  Council   has  played  a   central  role  in  gathering   evidence   to  inform  its  developmen t.  It  further   goes  on  to  say  that  the   governmen t  remains   grateful  to  the  AI  Council   for  its  continued   leader ship  of  the  AI  ecosystem.   The  strategy  centers  around   three  pillar s  with   short,   medium   and  long   term  timelines   for   achie ving the delinea ted tasks.   The Thr ee Pillar s   Pillar 1: In vesting in the long-t erm needs of the AI ec osystem   The  first  pillar   focuses   on  investing  in  the  long-t erm  needs   of  the  AI  ecosystem.  Some   of  the  key   ways in which the g overnmen t intends t o achie ve the same is outlined belo w:   ●  continue   to  develop   the  brigh test  and  the  mos t  diverse  workforce,  considering   that  UK   suffers from AI skills g ap;   ●  Unit ed  King dom   Resear ch  and  Inno vation  (UKRI)   will  support   the  transforma tion  of  the   UK’s  capability   in  AI  by  launching   a  National   AI  Resear ch  and  Inno vation  (R&I)    Programme;    ●  continue   to  use  Official   Developmen t  Assis tance   to  support   R&D   partner ships  with    developing c ountries;    ●  publish   a  policy   frame work  in  autumn   2021,   setting  out  its  role  in  enabling   better  data   availability in the wider ec onom y;   ●  consult   on  the  potential  value   of  and  options   for  a  UK  capability   in  digit al  twinning   and   wider ‘ cyber -physical infrastructur e’;   ●  continue   to  publish   authorit ative,  open   and  machine-r eadable   data  on  which   AI  models    for both public and c ommer cial bene fit can be tr ained.    ●  the  Office  for  AI  will  also  work  with   teams   across  governmen t  to  consider   wha t  valuable    datasets  governmen t  should   purpose fully   incen tivize,  that  will  acceler ate  the   developmen t of AI applic ations;    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   8 7   
 ●  to  better  under stand  the  UK’s  futur e  AI  computing   requir emen ts,  the  Office  for  AI  and   UKRI   will  evalua te  the  UK’s  computing   capacity   needs   to  support   AI  inno vation,    commer cializ ation and deplo ymen t;   ●  continue   to  evalua te  the  state  of  funding   specific ally  for  inno vative  firms   developing   AI   technologies acr oss e very region of the UK; and    ●  include   provisions   on  emer ging   digit al  technologies,   including   AI,  in  the  governmen t’s   new trade deals.    Pillar 2: Ensuring AI bene fits all sect ors and r egions    Signific ant modes t o be pur sued include:    ●  the  Office  for  AI  will  publish   resear ch  later  this  year  into  the  drivers  of  AI  adop tion  and   diffusion;    ●  to  stimula te  the  developmen t  and  adop tion  of  AI  technologies   in  high-pot ential,  low-AI   maturity sect ors the Of fice f or AI and UKRI will launch a pr ogramme;    ●  the  Office  for  AI  will  work  closely   with   the  Office  for  Science   and  Technology   Strategy   and  governmen t  departmen ts  to  under stand  the  governmen t’s  strategic  goals  and   wher e AI c an pr ovide a c atalytic contribution;    ●  through   its  leader ship  in  interna tional   developmen t  and  diplomacy ,  the  governmen t  of   UK  will  work  to  ensur e  that  interna tional   collabor ation  can  unlock   the  enormous    potential  of  AI  to  acceler ate  progress  on  global   challeng es,  from  clima te  chang e  to   poverty;    ●  launch   a  draft  National   Strategy  for  AI  in  Health   and  Social   Care  in  line  with   the  National    AI  Strategy.  This  will  set  the  direction   for  AI  in  health   and  social   care  up  to  2030,   and  is   expect ed to launch in early 2022; and    ●  publish   the  Defence   AI  Strategy,  which   will  include   the  establishmen t  of  a  new  Defence    AI Cen ter.   Pillar 3: Go verning AI e ffectiv ely   Some of the t asks to be t aken up ar e as s tated belo w:   ●  the  Office  for  AI  will  develop   UK’s  national   position   on  governing   and  regula ting  AI,   which will be se t out in a Whit e Paper in early 2022;    ●  the  governmen t  will  continue   to  work  with   its  partner s  around   the  world  to  shape    interna tional   norms   and  standar ds  relating  to  AI,  including   those   developed   by   multila teral and multis takeholder bodies a t global and r egional le vel;   ●  to  support   the  developmen t  of  a  mature  AI  assur ance   ecosystem,  the  CDEI   is  publishing    an AI assur ance r oadmap;    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   8 8   
 ●  the  governmen t  is  working   with   the  Alan   Turing   Institut e  to  upda te  the  guidance   on  AI   Ethics   and  Safety  in  the  Public   Sect or  in  order  to  provide   the  public   servants  with   the   mos t curr ent informa tion about the s tate of the art in r esponsible AI inno vation;    ●  the  Minis try  of  Defence   has  rigorous  codes   of  conduct   and  regula tion  which   uphold    responsible   AI  use,  and  is  working   closely   with   the  wider   governmen t  on  appr oaches   to   ensur e clear alignmen t with the v alues and norms of the socie ty;   ●  to  ensur e  that  citizens  have  confidence   and  trust  in  how  data  is  being   processed   and   analy zed  to  deriv e  insigh ts,  the  Central  Digit al  and  Data  Office  (CDDO)   is  conducting    resear ch  with   a  view  to  developing   a  cross-g overnmen t  standar d  for  algorithmic    transpar ency in line with the c ommitmen t in the Na tional Da ta Str ategy; and    ●  the  Office  for  AI  will  coordina te  with   cross-g overnmen t  processes   to  accur ately  assess    long   term  AI  safety  and  risks,  which   will  include   activities   such   as  evalua ting  technic al   expertise in g overnmen t and the v alue of r esear ch in frastructur e.   Next steps   The  presen t  strategy  talks  about   public ation  of  a  plan   to  execut e  the  vision   set  out  therein,  in   the  near   futur e.  Further ,  mechanisms   will  be  put  in  place   to  monit or  and  assess   progress.  The   governmen t  also  intends   to  publish   a  set  of  quan titative  indic ators,  given  the  ‘far-ranging ’  and   ‘hard-to-de fine’   impacts   AI  will  have  on  the  econom y  and  the  socie ty.  These   indic ators  will  be   published   separ ately  and  at  regular   intervals  to  provide   transpar ency   and  accountability .  It  is  the   Office  for  AI  that  will  be  responsible   for  overall  deliv ery  of  strategy,  monit oring   progress  and   enabling its implemen tation acr oss g overnmen t, ac ademia, indus try and civil socie ty.   Between the lines    On  March  12,  2021   Oliver  Dowden,   the  then   Secr etary  of  State  for  Digit al,  Cultur e,  Media   and   Sport   and  the  predecessor   of  Nadine   Dorries   announced   the  Governmen t’s  Ten  Tech  Priorities.    One  of  the  priorities   also  included   a  commitmen t  to  publish   a  National   AI  Strategy  (the  presen t   strategy).   Dowden  said  ‘Unleashing   the  power  of  AI  is  a  top  priority   in  our  plan   to  be  the  mos t   pro-tech  governmen t  ever.  The  UK  is  already   a  world  leader   in  this  revolutionar y  technology   and   the  new  AI  Strategy  will  help   us  seize  its  full  potential  –  from  creating  new  jobs  and  impr oving   productivity   to  tackling   clima te  chang e  and  deliv ering   better  public   services’ .  Now,  that  the   strategy is published, w e will ha ve to wait and w atch the e xecution of the vision c ontained in it.    Putting AI e thics t o work: ar e the t ools fit f or purpose?    [  Original paper   by Jacqui A yling and Adriane Chapman]    [Resear ch Summar y by Ra vit Dot an]   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   8 9   
 Overview  :  This  paper   map s  the  landsc ape  of  AI  ethics   tools:   It  develop s  a  typology   to  classif y  AI   ethics   tools  and  analy zes  the  existing  ones.   In  addition,   the  paper   iden tifies   two  gaps.  First,  key   stakeholder s,  including   member s  of  marginaliz ed  communities,   under -participa te  in  using   AI   ethics   tools  and  their   outputs.   Second,   there  is  a  lack  of  tools  for  external   auditing   in  AI  ethics,    which   is  a  barrier   to  the  accountability   and  trustworthiness   of  organizations   that  develop   AI   systems.    Introduction    As  more  and  more  AI  ethics   tools  are  developed,   it  becomes   difficult   to  get  a  handle   on  the   terrain.  This  paper   addr esses   the  challeng e  by  mapping   and  analy zing  the  existing  AI  ethics   tools   (as of the end of 2020).    The  author s  conduct ed  a  thor ough   sear ch  and  iden tified   169  AI  ethics   documen ts.  Of  those,   39   were  found   to  include   concr ete  AI  ethics   tools.   Each  of  the  39  tools  was  classified   using   the   following   ques tions:   (i)  Wha t  sector  are  the  documen t’s  author s  from?   And  wha t  sector  are  the   user s  of  the  tools  from?   (ii)  Which   stakeholder s  would   either   use  the  tool  or  engage  with   the   results?   (iii)  Wha t  type   of  tool  is  it?  Which   strategy  does   it  emplo y?  (iv)  Were  these   tools  for  use   internally ,  or  did  they  have  external   elemen ts?  (v)  In  which   stage  in  the  AI  production   and  use   chain w as the t ool used? (vi) W as the t ool appr opria te for addr essing the model, da ta, or both?    The  paper   presen ts  statistics  char acterizing   the  tools  using   these   ques tions.   Among   its  findings,    the  paper   uncovers  that  a  wide   stakeholder   base,   involving   customer s,  the  broader   public,   and   the  environmen t,  is  typic ally  not  a  part  of  AI  ethics   evalua tion  processes.   Mor eover,  the  paper    finds   that  almos t  all  AI  ethics   tools  are  used   internally ,  without   external   oversight.  The  author s   emphasiz e  that  these   char acteristics  stand  in  the  way  of  accountability   and  trustworthiness   of   organizations tha t develop AI s ystems.    The map of AI e thics t ool landsc ape   The paper divides AI e thics t ools in to thr ee categories:    Impact assessmen t tools    Impact   assessmen t  is  a  fact-finding   and  evalua tion  process   that  precedes   or  accompanies   the   production   of  artifacts,   systems,   or  resear ch.  Ex-ante  assessmen ts  are  used   in  the  use  case   developmen t  and  testing  stages.  Ex  post  assessmen ts  are  used   post-deplo ymen t,  in  the   monit oring   stage,  to  capture  the  impacts   of  the  system.  The  mos t  predominan t  tools  for  impact    assessmen ts in AI e thics ar e checklis ts and ques tionnair es.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   9 0   
 Technic al and design t ools    These   tools  are  typic ally  developed   by  the  ML  community .  Some   of  them   are  comput ational,    e.g.,  comput ationally   iden tifying  and  mitig ating  bias.   Other s  are  design   processes,   e.g.,   workshop-s tyle  events  for  raising   awareness   in  design   teams   or  participa tory  design   processes.    These   tools  are  used   along   the  whole   process   and  can  facilit ate  impact   assessmen t  and   auditing.    Auditing t ools    An  audit   is  an  examina tion  of  evidence   of  a  process   or  activity   against  some   standar ds  or   metrics.   To  ensur e  transpar ency   and  to  be  able   to  place   liabilities,   the  auditing   process   needs   to   be  independen t  of  the  assessmen t  process   and  from  the  day-to-da y  manag emen t  of  the   audit ee.  AI  ethics   auditing   tools  are  used   in  the  late  stages  of  the  production   process,   when    testing  and  monit oring   the  AI  system.  The  focus   of  these   tools  is  on  appr opria te  documen tation   for verific ation and assur ance. Checklis ts ar e also used f or auditing , but less so.    Some   statistics  :  The  paper   finds   that  tools  for  AI  ethics   are  developed   mos tly  by  the  private  and   academic   sectors.  However,  the  private  and  the  public   sectors  are  the  ones   that  mos tly  use  the   tools.   The  paper   also  finds   that  more  tools  are  developed   for  the  early   stages  of  the  production    process,   namely   the  use  case  and  design   stages.  Overall,  AI  ethics   tools  focus   more  on   addr essing models, as opposed t o addr essing da ta.   A gap in s takeholder participa tion   Typically,  AI  ethics   tools  are  directly   used   by  those   who   develop   the  AI  system  (e.g.,   developmen t,  deliv ery,  quality   assur ance).   The  outputs   of  the  AI  ethics   tools  are  typic ally  used    by  decision-mak ers,  such   as  elect ed  officials   and  boar d  member s.  Ther e  is  typic ally  little   participa tion  in  the  assessmen t  and  audit   processes   by  traditionally   marginaliz ed  groups,  the   user s  of  the  developed   services,   and  vested  interest  stakeholder s  such   as  citizens,  shar eholder s,   and in vestors.   The  paper   emphasiz es  the  relation  between  participa tion  in  AI  ethics   processes   and  power   dynamics.   The  two  are  linked  because   participa tion  has  to  do  with   who   has  the  power  to  mak e   decisions,   who   is  invited  to  the  table,   and  whose   views  and  goals  are  prioritiz ed.  The  paper    recommends   integrating  a  wider   stakeholder   base   in  AI  ethics   assessmen ts  and  audits.   It  also   recommends   focusing   the  conversation  on  power  relations   rather   than   strictly   on  participa tion.    Focusing on participa tion alone runs the risk of giving rise t o “participa tion w ashing. ”   A gap in auditing    Nearly   all  the  AI  ethics   tools  are  for  internal   self-assessmen t  only.  Ther e  are  gener ally  no   requir emen ts  or  processes   for  publishing   the  outputs   externally .  The  author s  emphasiz e  that   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   9 1   
 external   oversight  is  requir ed  for  the  trustworthiness   of  organizations   developing   the  systems.    Without   robus t  oversight,  there  is  a  risk  that  organizations   that  develop   AI  systems  would   fall   into  a  “checklis t  men tality ”  and  would   settle  for  performa tive  gestures  that  constitut e  “ethics    washing. ”   Between the lines    This  paper   gives  us  languag e  to  talk  about   the  different  AI  ethics   tools  that  are  out  there.  In   doing   so,  it  help s  in  under standing   the  comple x  landsc ape  of  AI  ethics.   The  iden tification  of  the   participa tion and auditing g aps invites the r eader t o seek solutions.    One  topic   for  further   explor ation  is  which   strategies   are  appr opria te  for  external   oversight  in  the   case  of  AI  ethics.   It  migh t  be  temp ting  to  think   of  auditing   processes   familiar   from  the  financial    and  other   sectors.  However,  in  the  case  of  AI  ethics,   the  participa tion  from  a  wider   stakeholder    base   in  external   oversight  seems   especially   import ant  given  that  ethical  evalua tions   involve  the   values   and  the  perspectiv es  available   to  the  evalua tor.  Can  sufficien t  participa tion  be  introduced    into  familiar   auditing   processes,   and  if  so,  how?  Alterna tively,  would   it  be  better  to  design    different oversight procedur es for AI e thics? If so , wha t should the y look lik e?   UNE SCO’s Recommenda tion on the E thics of AI    [  Original documen t  by UNE SCO]   [Resear ch Summar y by Angshuman K aushik]    Overview  :  The  Director-Gener al  of  the  Unit ed  Nations   Educational,   Scien tific  and  Cultur al   Organization  (UNE SCO)  convened   an  Ad  Hoc  Expert   Group  (AHE G)  for  the  prepar ation  of  a  Draft   Text  of  a  Recommenda tion  on  the  Ethics   of  Artificial   Intelligence   (“her eina fter  the   Recommenda tion”)   and  submit ted  the  draft  text  of  the  Recommenda tion  to  the  special    commit tee  mee ting  of  technic al  and  legal  experts,   designa ted  by  Member   States.  The  special    commit tee  mee ting  revised   the  draft  Recommenda tion  and  appr oved  the  presen t  text  for   submission   to  the  Gener al  Conference   at  its  41st  Session   for  adop tion.   Consequen tly,  it  was   unanimously adop ted b y all its 193 Member St ates on 24.11.2021.    Introduction    The  Recommenda tion  addr esses   ethical  issues   related  to  AI  to  the  extent  that  they  are  within    UNE SCO’s  manda te.  Mor eover,  a  signific ant  feature  of  the  Recommenda tion  is  that,  it  does   not   provide   one  single   definition   of  AI,  since   such   a  definition   would   need   to  chang e  over  time,   in   accordance   with   technologic al  developmen ts.  Rather,  its  ambition   is  to  addr ess  those   features   of  AI  systems  that  are  of  central  ethical  relevance.   Ther efore,  this  Recommenda tion  appr oaches    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   9 2   
 AI  systems  as  systems  which   have  the  capacity   to  process   data  and  informa tion  in  a  way  that   resembles   intelligent  beha viour ,  and  typic ally  includes   aspects   of  reasoning ,  learning ,   perception,   prediction,   planning   or  control.  Further ,  the  aim  of  the  Recommenda tion  is  to   provide   a  basis   to  mak e  AI  systems  work  for  the  good  of  humanity   and  to  prevent  harm.   To  add   to  the  above,  it  also  aims   at  ‘stimula ting  the  peace ful  use  of  AI  systems’ .  The  Recommenda tion   probably   refers  to  the  use  of  AI  systems  in  milit ary  warfare  but,  wha tever  that  means,   it  needs    elucida tion.    Core aspects of the r ecommenda tions    The  Recommenda tion  states  that  the  policy   actions   proposed   in  it  are  all  directed  at  promoting    trustworthiness   in  all  the  stages  of  the  AI  system  life  cycle.  Its  values   and  principles   are  outlined    belo w;   Values:-    ●  Human   rights  and  fundamen tal  freedoms   mus t  be  respect ed,  protected  and  promot ed   throughout the lif e cycle of AI s ystems;    ●  All  actors  involved  in  the  life  cycle  of  AI  systems  mus t  comply   with   laws,  standar ds,   practices   etc.,  designed   for  environmen tal  and  ecosystem  protection   and  restoration,    and sus tainable de velopmen t;   ●  Respect,   protection   and  promotion   of  diversity  and  inclusiv eness   should   be  ensur ed   throughout   the  life  cycle  of  AI  systems,   consis tent  with   interna tional   law,  including    human righ ts law; and    ●  AI  actors  should   play  a  participa tive  and  enabling   role  to  ensur e  peace ful  and  just   socie ties.    Principles:-    ●  The  use  of  AI  systems  shall   be  governed   by  the  principle   of  ‘necessity   and   proportionality ’.  AI  systems,   in  particular ,  should   not  be  used   for  social   scoring   or  mass    surveillance purposes;    ●  Safe  and  secur e  AI  systems  shall   be  prioritiz ed  and  any  threat  emana ting  from  such    systems shall be addr essed t o ensur e human and en vironmen tal well-being;    ●  AI  actors  shall   safeguar d  fairness   and  non-discrimina tion  and  also  ensur e  that  the   bene fits of AI t echnologies ar e available t o all;    ●  The  continuous   assessmen t  of  the  human,   social,   cultur al,  economic   and  environmen tal   impact   of  AI  technologies   should   be  carried   out  to  ascert ain  whe ther   they  are  in   conformity   with   the  sustainable   goals,   such   as,  those   currently  iden tified   in  the  Unit ed   Nations Sus tainable De velopmen t Goals (UNSDGs);    ●  Privacy shall be pr otected thr oughout the lif e cycle of the AI s ystems;    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   9 3   
 ●  Member   States  to  ensur e  that  it  is  always  possible   to  attribut e  ethical  and  legal  liability    arising   out  of  AI  systems  to  humans.   Further ,  as  a  rule,   life  and  death  decisions   should    not be ceded t o AI s ystems;    ●  Efforts  need   to  be  made   to  enhance   transpar ency   and  explainability   of  AI  systems,    including those ha ving e xtra-territ orial e ffect, t o support democr atic governance;    ●  Appr opria te  oversight,  impact   assessmen t,  audit   and  due  diligence   mechanisms,    including   whis tle-blo wers’  protection,   should   be  developed   to  ensur e  accountability   for   AI systems;    ●  Public   awareness   and  under standing   of  AI  technologies   should   be  promot ed  through    open   and  accessible   educ ation,   civic   engagemen t,  AI  ethics   training   etc.,  so  that  people    can  take  informed   decisions   regarding   their   use  of  AI  systems  and  be  protected  from   undue in fluence; and    ●  States  shall   be  able   to  regula te  the  data  gener ated  within   or  passing   through   their    territ ories,   and  take  measur es  towards  effectiv e  regula tion  of  data  in  accordance   with    interna tional   law.  Further ,  measur es  should   be  taken  to  allow  for  meaningful    participa tion b y mar ginaliz ed gr oups.   Areas of policy action    The  policy   actions   men tioned   in  the  policy   areas  oper ationaliz e  the  values   and  principles   set  out   in  the  Recommenda tion.   It  calls  for  member   states  to  put  in  place   effectiv e  measur es,  such   as,   policy   frame works  and  to  ensur e  that  stakeholder s,  such   as  private  sector  companies,   academic    and  resear ch  institutions   and  civil  socie ty  adher e  to  them   by  encouraging   them   to  develop    ethical  impact   assessmen t,  due  diligence   tools  etc.,  in  line  with   guidance,   including   the  Unit ed   Nations Guiding Principles on Business and Human Righ ts. Lis ted belo w ar e the policy ar eas:   ●  Policy   Area  1:-  The  Member   States  shall   introduce   frame works  for  impact   assessmen ts,   such   as  ethical  impact   assessmen ts,  to  iden tify  and  assess   bene fits,  concerns   and  risks  of   AI systems;    ●  Policy   Area  2:-  The  Member   States  shall   ensur e  that  AI  governance   mechanisms   are   inclusiv e, transpar ent, multidisciplinar y, multila teral and multi-s takeholder .   ●  Policy   Area  3:–  The  Member   States  shall   develop   data  governance   strategies.   Further ,   privacy  shall   be  respect ed,  protected  and  promot ed  throughout   the  life  cycle  of  AI   systems.    ●  Policy   Area  4:–  Both   the  Member   States  and  transna tional   corpor ations   shall   prioritiz e  AI   ethics   by  including   discussions   on  the  topic   in  relevant  interna tional,   intergovernmen tal   and  multi-s takeholder   forums.   Further ,  the  Member   States  shall   work  to  promot e   interna tional   collabor ation  on  AI  Resear ch  and  inno vation,   particularly   in  the  area  of  AI   ethics.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   9 4   
 ●  Policy   Area  5:–  The  Member   States  and  businesses   shall   assess   the  direct  and  indir ect   impact   on  the  environmen t  throughout   the  life  cycle  of  an  AI  system.  They  shall   also   ensur e compliance with en vironmen tal law, policies and pr actices b y all the AI act ors.   ●  Policy   Area  6:–  The  Member   States  shall   ensur e  that  the  potential  of  AI  systems  to   contribut e  to  achie ving  gender   equality   is  fully   maximiz ed,  and  further ,  they  mus t  also   ensur e  that  the  human   rights  and  fundamen tal  freedoms   of  girls  and  women,   and  their    safety and in tegrity ar e not viola ted a t any stage of an AI s ystem lif e cycle.   ●  Policy   Area  7:-  The  Member   States  are  encouraged  to  incorpor ate  AI  systems,   wher e   appr opria te,  in  the  preser vation,   enrichmen t,  under standing ,  promotion,   manag emen t   and  accessibility   of  cultur al  herit age,  including   endang ered  languag es  as  well  as   indig enous languag es and kno wledg e.   ●  Policy   Area  8:–  The  Member   States  shall   work  with   interna tional   organizations,    educ ational   institutions   and  private  and  non-g overnmen tal  entities   to  provide   adequa te   AI  literacy  educ ation  to  the  public   in  order  to  empo wer  people   and  reduce   the  digit al   divide and digit al access inequalities r esulting fr om the wide adop tion of AI s ystems.    ●  Policy   Area  9:–  The  Member   States  shall   use  AI  systems  to  impr ove  access   to  informa tion   and  knowledg e.  This  shall   include   support   to  resear cher s,  academia,   journalis ts,  the   gener al public and de veloper s to enhance fr eedom of e xpression e tc.   ●  Policy   Area  10:–   The  Member   States  shall   assess   and  addr ess  the  impact   of  AI  systems   on labor mark ets.   ●  Policy   Area  11:–   The  Member   States  shall   endea vor  to  emplo y  effectiv e  AI  systems  for   impr oving  human   health   and  protecting   the  right  to  life,  including   mitig ating  disease    outbr eaks.  Further ,  they  shall   implemen t  policies   to  raise  awareness   about   the   anthropomorphiz ation of AI t echnologies.    The  Recommenda tion  also  directs  the  Member   States  to  credibly   and  transpar ently  monit or  and   evalua te  policies,   programmes   and  mechanisms   related  to  ethics   of  AI,  using   a  combina tion  of   quan titative  and  qualit ative  appr oaches,   according   to  their   specific   conditions,   governing    structur es  and  constitutional   provisions.   The  Recommenda tion  further   directs  that  processes   for   monit oring   and  evalua tion  should   ensur e  broad  participa tion  of  all  stakeholder s,  including ,  but   not limit ed to, vulner able people or people in vulner able situa tions.    Between the lines    Although   the  Recommenda tion  is  volun tary  and  non-binding ,  it  signifies   ‘consensus   ad  idem’    amongs t  all  the  UNE SCO  Member   States.  However,  there  are  suggestions   which   requir e   elabor ation.   For  instance,   it  is  recommended   that  “Member   States  and  business   enterprises    should   implemen t  appr opria te  measur es  to  monit or  all  phases   of  an  AI  system  life  cycle”.  Now,   guidance   on  the  meaning   and  mode   of  oper ation  of  the  term  ‘appr opria te  measur es’  is   imper ative.  Another   case  in  poin t  is  that  the  Recommenda tion  states  that  the  “Member   States   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   9 5   
 that  acquir e  AI  systems  for  human   rights-sensitiv e  use  cases,   such   as  ……… ..the  independen t   judiciar y  system  should   provide   mechanisms   to  monit or  the  social   and  economic   impact   of  such    systems  by  appr opria te  oversight  authorities,   including   independen t  data  protection    authorities,   sectoral  oversight  and  public   bodies   responsible   for  oversight”.  A  couple   of   ques tions   that  emer ge  here  are  firstly,  can  an  independen t  judicial   system  be  subject ed  to   oversight  by  say,  a  data  protection   authority?   Secondly ,  wouldn’t   subjecting   a  court   to  further    oversight  when   it  is  already   under   the  super visor y  control  of  a  superior   court,   lead   to  an  issue   of   overlapping   of  jurisdiction?   Ther e  are  other   key  areas  also  that  need   to  be  clarified.   Further ,  it  is   interesting  to  note  that  China   being   a  member   of  UNE SCO  has  adop ted  the  Recommenda tion   which   follows  its  formula tion  of  the  AI  Ethics   Code.   However,  the  US  is  not  a  signa tory  to  the   Recommenda tion  as  it  is  not  an  UNE SCO  Member   State.  Now,  wha t  needs   to  be  seen   is  how  the   Member   States  incorpor ate  and  oper ationaliz e  the  various   guidelines   enshrined   in  the   Recommenda tion in the futur e.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   9 6   
 Go Wide: Article Summaries  (summariz ed b y Abhishek   Gup ta)   Americ a's  global   leader ship   in  human-cen tered  AI  can't  come   from   indus try alone    [Original article b y  The Hill   ]   Wha t  happened   :  Li,  one  of  the  people   behind   the  famed   Imag eNet  dataset  and  the  co-Dir ector   of  the  Stanford  HAI  center  talks  about   her  work  on  the  National   AI  Resear ch  Resour ce  Task   Force.  As  a  part  of  this  work,  they  are  seeking   to  realiz e  the  vision   of  human-cen tered  AI  by   democr atizing   access   to  AI  systems  and  educ ation  to  build   capacity   in  the  ecosystem  for  more   people   to  build   these   systems.   This  will  be  support ed  through   the  construction   of  a  National    Resear ch  Cloud   with   the  goal  of  making   comput e  and  data  storage  resour ces  available   to  a   wider   set  of  people.   Ther e  is  vast  inequity   between  those   who   are  back ed  by  large  indus trial   and  academic   labs  and  those   who   are  not  in  terms   of  the  resear ch  work  that  they  can  carry  out   in the space, some thing tha t we ha ve highligh ted in our w ork her e as w ell.   Why  it  matters  :  To  achie ve  more  responsible   AI  systems  as  well,  widespr ead  access   to  the   necessar y  underlying   infrastructur e  to  run  experimen ts  and  do  resear ch  will  be  essen tial,  quite   in  line  with   the  mission   of  the  Mon treal  AI  Ethics   Institut e  as  well.  Mor e  import antly,  with    dedic ated  resour ces  being   alloc ated  by  the  feder al  governmen t  and  a  firm  commitmen t  from   them   to  mak e  the  National   Resear ch  Cloud   a  reality   showcases   a  positiv e  step  in  really   making    AI  some thing   that  will  empo wer  a  lot  more  people   to  build   solutions   for  problems   that  are  close    to them.    Between  the  lines   :  Wha t  is  particularly   heart ening   is  to  see  someone   of  Li’s  caliber   and   expertise   being   a  part  of  the  Task  Force,  especially   given  the  deeply   technic al  nature  of  the  work   that  will  be  involved  in  making   this  a  reality .  Additionally ,  she  is  someone   who   is  championing    human-cen tered  AI  through   her  work  at  Stanford  and  elsewher e  which   hope fully   will  become   a   central tenet in the final s tructur e tha t the Na tional R esear ch Cloud manif ests in.    Training self -driving c ars for $1 an hour    [Original article b y  Rest of W orld  ]   Wha t  happened   :  The  article   highligh ts  the  abysmal   rates  that  are  paid   out  to  workers  who   help    to  power  the  mos t  lucrative  and  well-funded   sub-indus tries   within   AI:  self-driving   vehicles.    Given  that  the  dominan t  paradigm   for  getting  these   systems  to  work  effectiv ely  still  requir es   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   9 7   
 large  amoun ts  of  labeled   data,  it  is  not  surprising   that  loads   of  mone y  gets  pour ed  into  building    up  datasets  that  can  be  used   by  companies   to  train  their   systems.   Some   interesting  highligh ts   from  the  article   showcase  how  the  demand   for  this  has  reshaped   the  crowdsour ced  work   indus try  demanding   much   higher   rates  of  accur acy  from  workers,  super vision   officer s  and   check ers  over  the  data  labeler s,  and  finally ,  additional   distancing   between  those   who   provide    these t asks and those who c omple te them.    Why  it  matters  :  Fair  compensa tion  for  work,  especially   for  work  that  is  crucial   to  the  existence    and  continua tion  of  the  self-driving   indus try  is  the  least  that  can  be  done,   particularly   when    such   companies   are  extremely   well-funded.   The  platform  called   Remot eworks  discussed   in  the   article   is  owned   by  ScaleAI,   a  giant  AI  compan y  valued   at  close   to  $7b.   The  AI  engineer s  who    develop   models   are  typic ally  based   in  places   like  SF  while   the  workers  who   pains takingly    construct   the  datasets  to  power  these   systems  work  in  the  Global   South   with   none   of  the   bene fits of fered to the emplo yees of the or ganizations tha t contract out this w ork.   Between  the  lines   :  The  evolving   requir emen ts  put  forth   as  the  demands   for  dataset   construction   for  these   systems  become   more  rigorous  will  herald  a  further   reshaping   of  the   crowdsour ced  work  indus try.  One  of  the  examples   provided   in  the  article   talks  about   a  new   label   category  called   “atmospherics”   that  requir es  labeling   rain  drops  in  an  imag e  so  that  the   powerful   camer as  onboar d  the  vehicle   which   capture  those   raindr ops  in  their   imag es  don’t    mistake  them   for  obstacles.   The  tasks  are  only   going   to  become   more  tedious   and  will  mak e  the   pace of such da taset construction unsus tainable in the long run.    How Da ta Brokers Sell Access t o the Backbone of the In terne t   [Original article b y  Vice   ]   Wha t  happened   :  Netflow  data  is  the  data  that  tracks  reques ts  over  the  interne t  from  one   device  to  another .  Piece   enough   of  this  together   and  you  can  learn   about   the  patterns   of   communic ation  of  any  individual   or  an  organization.   In  this  article,   we  learn   more  about   Team    Cymru,   a  firm  that  build   products   based   on  netflow  data  that  it  purchases   from  various   interne t   service  provider s  (ISPs)  which   are  then   sold  on  to  other   cyber security   firms   and  organizations    who w ant to perf orm analy sis for in telligence, sur veillance, and man y other purposes.    Why  it  matters  :  This  is  import ant  because   it  allows  tracking   even  through   virtual   private   networks  (VPNs)   stripping   away  anon ymity   on  the  interne t  even  further   than   it  already   is.  Ther e   is  an  inher ent  conflict  with   the  collection   of  such   data  in  the  sense   that  on  the  one  hand   it  is   intrusiv e  and  strips  away  privacy  but  it  also  enables   some   great  cyber security   work  that  help s   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   9 8   
 protect  against  virtual   and  physical  threats.  As  with   all  other   dual-purpose   technologies,   this   one  requir es  a  thor ough   analy sis  of  the  pros  and  cons,  especially   the  potential  for  misuse   as  the   data stored with T eam Cymru migh t fall in to the hands of bad act ors.   Between  the  lines   :  Some thing   that  caugh t  my  attention  was  how  the  Citiz en  Lab  declined   to   commen t  on  their   use  of  a  product   from  Team   Cymru   for  a  resear ch  report   that  they  published.    Given  the  strong  upholding   of  rights  and  transpar ency   that  the  Citiz en  Lab  engages  in,  it  seemed    odd  to  not  commen t  on  the  story.  In  addition,   the  Wyden  reques ts  for  informa tion  to  the   Departmen t  of  Defense   for  their   purchase   and  use  of  interne t  metadata  would   also  be   interesting  to  examine   to  gain  an  under standing   of  the  extent  to  which   people’ s  interne t   activities ar e monit ored.   How ne w regula tion is driving the AI g overnance mark et   [Original article b y  VentureBea t  ]   Wha t  happened   :  The  article   highligh ts  the  trend  in  the  current  mark et  towards  a  greater   adop tion  of  AI  governance   solutions,   frame works  and  tools,   that  will  multiply   the  mark et  value    of  such   solutions   to  almos t  10x  the  current  amoun t  over  the  next  6  years.  This  is  being   driven  by   incoming   regula tions,   mos tly  from  Europe  with   burgeoning   efforts  in  the  US,  combined   with    increasing   consumer   savvy  around   data  privacy  and  other   harms   like  biases   in  algorithmic    systems  as  they  mak e  purchase   and  use  decisions   for  the  various   products   and  services   around    them.    Why  it  matters  :  As  highligh ted  in  a  report   from  the  Berk eley  Center  for  Long-T erm   Cyber security ,  AI  governance   has  under gone  3  major   stages  since   2016:   developmen t  of   high-le vel  principles,   consensus   on  those   principle   sets,  and  transla ting  principles   into  practice.    The  trend  observed  here  in  terms   of  mark et  value   is  just  a  natural  extension   of  the  final   stage   wher e  a  demand   for  solutions   that  can  materializ e  the  principles   will  be  sough t  by   organizations.    Between  the  lines   :  I  belie ve  that  there  is  another   era  that  we’re  entering   with   this  trend  which    is  going   to  involve  imma ture  solutions   that  claim   to  solve  AI  governance   problems   proliferating   the  mark et  (the  current  phase),   followed  by  a  culling   of  players  who   aren’t  able   to  deliv er  on   lofty   promises,   and  finally   an  establishmen t  of  more  mature  companies   that  will  cemen t  their    positions in v arious niches of the AI g overnance landsc ape selling mor e ba ttle-tested solutions.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   9 9   
 How open-sour ce softw are shapes AI policy    [Original article b y  Brookings   ]   Wha t  happened   :  This  article   dives  into  details  about   how  the  open-sour ce  softw are  (OSS)    ecosystem  oper ates  and  the  implic ations   that  it  has  on  how  policies   are  developed   for  the   governance   of  AI  systems.   It  iden tifies   a  gap  in  the  current  policy   initia tives  on  examining   the   role  of  OSS  in  the  power  dynamics   of  the  AI  ecosystem.  Notably,  mos t  policy   discussions   today   focus   on  technology ,  data,  talen t,  funding ,  but  rarely  do  they  look   at  how  the  OSS  ecosystem   impacts   all  of  these   factors.  OSS  provides   bene fits  like  speeding   up  AI  adop tion,   bringing   more   transpar ency   to  the  code  bases   used   in  products   and  services,   and  help s  to  acceler ate   fundamen tal  advances   in  a  lot  of  fields   by  making   AI  capabilities   more  accessible.   But,  this  also   has  negative  impacts   in  terms   of  the  compe titiveness   in  the  mark et  for  AI  solutions   and  sets   standar ds  implicitly ,  oper ating  outside   the  purview  of  standar ds  setting  bodies   that  would    typic ally  help   to  bring   counterweigh t  to  the  developmen t  of  the  tools  and  methodologies   in  the   domain.    Why  it  matters  :  The  article   highligh ts  how  the  current  ecosystem  for  AI  frame works  is   domina ted  by  Google   and  Facebook   through   Tensorflo w  and  PyTorch  respectiv ely.  It  is  not  a   new  phenomenon   since   both   these   companies   have  also  published   the  popular   Angular .js  and   React.js   that  domina te  frontend  web  developmen t  frame works.  Wha t  is  interesting  on  closer    examina tion  is  that  mos t  of  the  core  developer s  on  Tensorflo w  and  PyTorch  still  come   from   Google   and  Facebook   giving   them   a  much   stronger  implicit   say  in  how  the  code  develop s  in  the   futur e  and  thus   potentially   shaping   also  the  standar ds  that  migh t  follow  since   we  would   be   locked in to ho w these fr ame works structur e and oper ate.   Between  the  lines   :  OSS  contribut ors  need   to  be  paid   and  the  funding   for  that  needs   to  come    from  some wher e.  A  lot  of  OSS  projects   end  up  being   abandoned   or  suffer  when   there  isn’t    adequa te  funding   to  compensa te  the  contribut ors  for  their   efforts  and  they  choose   to  work  on   other   things   that  help   them   pay  their   bills.   If  we  talk  about   true   democr atization  of  tooling   in   OSS,   we  need   to  strongly   consider   whe ther   we  can  reshape   the  structur e  of  the  ecosystem  as  it   exists  today  towards  some thing   wher e  there  are  perhap s  external   grants  that  are  more  widely    available   that  allow  anyone  to  sustainably   contribut e  to  such   projects   helping   to  bring   more   diversity  to  the  contribut ors  list.  Until  then,   we  at  least  do  have  access   to  such   tooling    bene fitting fr om the in vestmen ts made b y corpor ate bene factors.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 0 0   
 AI  indus try,  obsessed   with   speed,   is  loathe  to  consider   the  ener gy  cost  in   latest MLP erf benchmark    [Original article b y  ZDNe t  ]   Wha t  happened   :  In  the  latest  MLP erf  benchmark   results,   a  benchmark   that  compar es  hardware   performance   in  the  domain   of  AI,  with   the  rise  in  performance   capabilities   of  the  chip s   submit ted,  there  was  a  notable   drop  in  the  report ed  values   for  ener gy  consump tion.   The  article    posits   that  manuf actur ers  are  more  interested  in  selling   the  high   performance   of  their   systems,    and  consider   the  ener gy  efficiency   as  a  secondar y  outcome,   hence   when   asked  to  mak e   tradeof fs, the y choose t o lean t owards the f ormer .   Why  it  matters  :  We’ve  spok en  about   the  environmen tal  impact   of  AI,  in  our  State  of  AI  Ethics    Report   and  in  “The  Imper ative  for  Sustainable   AI  Systems” ,  and  informa tion  about   the  ener gy   consump tion  of  the  physical  infrastructur e  used   to  power  AI  applic ations   is  essen tial  in  guiding    the  actions   of  the  practitioner s  in  choosing   an  appr opria te  solution,   some thing   that  we’ve   highligh ted  in  “The  current  state  of  affairs  and  a  roadmap   for  effectiv e  carbon-acc ounting   tooling   in  AI.”  Without   that  informa tion,   it  is  difficult   to  assess   the  ener gy  efficiency   of  different   systems  without   trying  them   all  out,  instrumen ting  them,   and  reporting   the  results,   an  exercise   that manuf actur ers can do f or a fr action of the c ost.   Between  the  lines   :  With   an  overhea ted  mark et  for  the  very  essen tial  hardware  that  powers  the   booming   AI  mark et,  it  is  under standable   that  manuf actur ers  want  to  emphasiz e  the   performance   of  their   hardware,  rather   than   draw  attention  to  the  massiv e  ener gy  consump tion   of  these   chip s.  Wha t  lies  in  our  control  though   is  to  demand   that  we  be  provided   that   informa tion,   and  use  our  purchasing   power  to  shape   the  mark et  by  rewarding   those    manuf actur ers  who   do,  in  essence   setting  a  new  status  quo  wher e  reporting   ener gy  figur es   becomes the norm.    Facebook   Rolls  Out  News  Feed  Chang e  That  Block s  Watchdogs   from   Gathering Da ta   [Original article b y  The Mark up  ]   Wha t  happened   :  In  yet  another   blow  to  resear cher s  who   utiliz e  data  from  Facebook   to  study   its   impacts   on  socie ty,  the  platform  has  rolled   out  code  chang es  by  injecting   superfluous   elemen ts   into  its  website  that  mak e  it  even  more  difficult   for  resear ch  projects   to  oper ate  and  gather   the   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 0 1   
 necessar y  data  that  fuels   their   efforts  to  study ,  for  example,   how  disin forma tion  spreads   on  the   platforms   and  biases   in  the  kinds   of  advertisemen ts  shown  to  people   of  different  demogr aphic    groups.  The  chang e  has  impact ed  the  Citiz en  Browser  project   from  The  Mark up,  the  team   at  the   NYU Ad Ob servatory amongs t other s.   Why  it  matters  :  Not  only   are  resear cher s  affected,  but  screen  reader s,  that  rely  on  the  HTML    tags  that  have  now  been   inject ed  with   junk   code  to  foil  these   attemp ts,  have  seen   performance    problems   making   the  site  much   less  accessible   for  the  visually   impair ed  who   rely  on  screen   reader s,  and  consequen tly  the  tags  to  navigate  the  website.  This  is  not  the  first  time   that  a   chang e  in  the  Facebook   website  code  base   has  had  an  impact   on  accessibility .  This  viola tes   some   of  the  tenets  of  accessible   web  design   all  in  the  interest  of  decr easing   transpar ent  access    to ad dis tribution on the pla tform and r educing the e fficacy of ad block ers tha t people use.    Between  the  lines   :  One  of  the  poin ts  highligh ted  in  the  article   aptly  sums   up  the  current  state   of  affairs:  Facebook   is  working   against  resear cher s  rather   than   with   them,   and  this  is  only   going    to  mak e  problems   worse.  As  poin ted  out  by  the  article,   there  was  also  another   instance   this   year  when   Facebook   corrected  previously   supplied   data  about   misin forma tion  on  the  platform   only   after  someone   noticed   a  discr epancy   in  a  report   published   by  Facebook   and  the  open   data   that  they  had  made   available,   this  potentially   has  impacts   on  years  of  resear ch  efforts.   Moving   away  from  an  adversarial   dynamic   will  be  essen tial  if  we  want  to  achie ve  the  goal  of  having  a   healthier ec osystem.   Ther e's a Multibillion-Dollar Mark et for Your Phone's Loc ation Da ta   [Original article b y  The Mark up  ]   Wha t  happened   :  We  all  have  tons  of  apps  on  our  phones   and  for  those   of  us  who   are  more   privacy-minded,   we  turn   off  location  services.   But,  there  exists  a  massiv e  mark et  for  one’ s   location  data,  and  often  some   apps  not  only   don’t   have  the  option  to  disable   collecting   location   data,  there  are  some   who   collect   that  data  surreptitiously .  And  this  mark et  for  location  data  is   worth   billions   of  dollar s  with   man y  players  who   trade  in  billions   of  data  poin ts  on  millions   of   individuals   selling   that  data  for  pennies.   Often  these   are  companies   that  we  don’t   hear   about    very  often.  Data  brokers  are  requir ed  to  register  in  Vermon t  and  Calif ornia,   but  in  man y  other    places, the y oper ate under dif ferent guises.    Why  it  matters  :  Ther e  have  been   man y  past  cases   wher e  people   have  been   iden tified   for  their    sexual  iden tity  or  religious   belie fs  based   on  location  data  that  was  obtained   illicitly   from  such    data  brokers.  The  data  brokers  oper ate  in  a  shado wy  world,   buying   and  selling   from  any  and  all   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 0 2   
 sour ces,  trading   informa tion  to  continue   building   richer   datasets  that  unlock   more  informa tion   about   each   person.   This  happens   through   some thing   called   the  mosaic   effect  wher e  dispar ate   pieces   of  informa tion  can  be  combined   to  fill  in  the  blank s  about   our  lives  and  mak e  inferences    about our iden tities and beha viors.   Between  the  lines   :  In  a  paper   published   in  2018,   the  role  of  data  brokers  was  poin ted  out  as   being   even  more  insidious   in  a  world  wher e  biome tric  data  about   us  also  becomes   more  widely    available   through   systems  for  DNA   tests,  facial   recognition   technology ,  and  other s.  Without    robus t  guar antees  for  the  security   of  that  data  (which   is  always  a  challeng e!),  and  without   more   string ent  measur es  on  how  data  brokers  oper ate,  we  will  continue   to  exacerba te  risks  for   people   in  our  socie ty  whose   data  can  be  weaponiz ed  against  them.   And  this  is  a  bigger  problem    in regimes wher e ther e are even fewer civil righ ts pr otections.    Chinese   AI  gets  ethical  guidelines   for  the  first  time,   aligning   with   Beijing ’s   goal of r eining in Big T ech   [Original article b y the   SCMP   ]   Wha t  happened   :  The  Chinese   Minis try  of  Science   and  Technology   released   more  focused    guidelines   on  AI  ethics   that  place   human   control  over  the  technology   at  its  center.  It  has  brough t   the  broader   Beijing   AI  Principles   published   earlier   much   more  in  line  with   the  emphasis   that  the   Chinese   Governmen t  has  placed   on  reigning   in  Big  Tech.   Some   of  the  other   values   emphasiz ed   in  the  documen t  include   impr oving  human   well-being ,  promoting   fairness   and  justice,    protecting priv acy and sa fety, and r aising e thical lit eracy.   Why  it  matters  :  With   the  emphasis   on  human   control,  the  guidelines   set  a  strong  example   in   terms   of  how  the  interaction   between  humans   and  machines   will  take  place.   In  particular ,  the   men tion  of  the  ability   of  humans   to  exit  the  interaction   with   an  AI  system  at  any  time,    discontinuing   the  AI  system,  and  accep ting  to  interact  with   the  AI  system  in  the  first  place   will   have  severe  consequences   for  the  large  number   of  AI-in fused   products   and  services   that  are   used   daily   across  the  mos t  popular   apps  in  China.   How  this  comes   into  effect  and  how  strict  the   enforcemen t will be will de termine t o wha t extent the guidelines achie ve their in tended g oals.    Between  the  lines   :  Given  the  manda te  at  the  Mon treal  AI  Ethics   Institut e,  it  is  very  interesting   to  see  “raising   ethical  literacy”  be  included   as  a  core  consider ation  in  the  AI  ethics   guidelines.    We  belie ve  that  achie ving  AI  ethics   in  practice   will  requir e  educ ation  and  empo wermen t  of  all   stakeholder s,  not  just  having  guidelines   and  enforcing  regula tions   for  those   who   develop   and   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 0 3   
 deplo y  AI  systems.   Perhap s  this  is  a  harbing er  of  other   countries   adop ting  this  as  a  core   consider ation as w ell.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 0 4   
 3. Priv acy   We  have   a  special   contribution   in  Spanish   for  the  first  time   in  the  State  of  AI  Ethics   Report   as  we   mak e  in-roads   towards   making   our  report   more   multilingual.   We  hope   you  enjo y  this   contribution! (An E nglish version f ollows the Spanish t ext.)   Introduction  by  Idoia   Salaz ar,  Cofounder ,  Observatory  of  the  Social   and  Ethical  Impact   of   Artificial In tellig ence    La import ancia de la priv acidad en un mundo dominado por los da tos y la IA    El  impact o  masiv o  de  la  Inteligencia   Artificial   y  el  Big  Data  que  circula  a  nues tro  alrededor   está   provocando   la  necesidad   de  un  cambio   import ante  en  nues tra  sociedad.   Un  cambio   de   men talidad   para  adap tarnos   a  nues tra  nueva  realidad.   Vivimos   rodeados   de  datos.  A  cada  paso    que  damos   gener amos   datos.  Y  cada  vez  más  procesos   serán  realiz ados   y  ejecut ados   en  base   a   estos  datos,  con  ayuda   de  la  inteligencia   artificial.   Pero  nosotr os,  como   humanos,   no  estamos    acostumbr ados   esta  realidad   digit al.  A  esta  vida  híbrida   en  la  que  no  solo  cuen tan  los  pasos    físicos  que  damos   en  nues tra  rutina   diaria.   Sino   también   la  huella   digit al  que  vamos   dejando ,  y   que  es,  igualmen te,  parte  de  nosotr os  mismos.   Nues tro  gemelo   digit al.  Poco  a  poco,  a  medida    que  vamos   compartiendo   nues tros  datos,  nos  vamos   volviendo   cada  vez  más  transpar entes  a   nues tro  entorno.   ¿Esto  es  bueno   o  malo?   ¿seremos   capaces   de  aprender   a  vivir   con  ello  como    humanidad?    La  privacidad   es  import ante  para  las  personas,   pero  también   depende   de  lo  que  signific a  este   concep to  para  cada  una  de  ellas,   y  no  siempr e  es  lo  mismo.   Suele   variar   en  función   de  la  cultur a   o  de  tus  propias   experiencias   personales.   No  todos   le  damos   la  misma   import ancia,   en  cada   caso  concr eto.  Pero  lo  que  es  un  hecho   es  que,   actualmen te,  todos   sufrimos   o  disfrutamos   de   las  consecuencias   del  "mundo   de  datos"  en  el  que  vivimos.   Un  ejemplo ,  muy   ilustrativo,  ocurrió    ya  en  2012,   cuando   un  padr e  puso   una  queja   a  la  cadena   de  grandes   almacenes    estadounidenses   Target.  Aleg aba  que  había   recibido   publicidad,   a  nombr e  de  su  hija,   sobr e   product os  de  bebés   .  Le  parecía  ridículo   que  se  lo  enviaran  porque  la  chica  aún  estaba 5  terminando   Bachiller ato.  Pero  result ó  que  esta  cadena   de  supermer cados,   a  través  de  los  datos,   había   detectado  que  su  hija  cumplía   con  el  perfil   de  "embar azada"   y,  siguiendo   su  polític a   comer cial,  enviaba   esta  publicidad.   En  efecto,  la  hija  estaba  embar azada  y  Target  lo  sabía   antes   que su padr e.   5  K. Hill. "How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did". Forbes, 2012.   https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-   figured-out-a-teen-girl-was-pregnant-before-her-father-did/.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 0 5   
 En  el  mundo   labor al  este  seguimien to  es  habitual.   Cada   vez  hay  más  empr esas   que  investigan   perfiles   de  sus  candida tos  para  contratar  en  redes   sociales,   antes  de  tomar   una  decisión,   y  no  es   raro  que,   si  ven  algo  que  no  les  agrada  ahí,  toman   una  decisión   negativa  .  Tampoc o  es  raro  que 6  nos  llegue   publicidad   online   sobr e  viajes   a  un  país,   sin  haberlo   solicit ado,  simplemen te  porque   hemos   hecho   alguna   búsqueda   en  el  ordenador   el  día  anterior .  Se  podría   decir   que  es  un  mal   necesario ,  ya  que  poca  gente  está  disponible   a  pagar  para  leer  las  noticias,   usar   apps,  como    Google   Map s,  Facebook,   Instagram,  etc.  Aunque   no  nos  guste,  hasta  ciert o  punto,  podríamos    llegar a en tender por qué e xplot an nues tros da tos.   Sin  embar go,  hay  algo  que  debe   preocupar   aún  más:   los  casos   de  práctic as  ilegales  con  los  datos   personales.   Aunque   haya  muchísimos   casos   documen tados,   el  más  emblemá tico,  hasta  la  fecha,    ha  sido  el  escándalo   de  Cambridg e  Analy tica  y  los  datos  de  Facebook   de  decenas   de  millones   de   usuarios   de  todo  el  mundo ,  ocurrido   en  marz o  de  2018   .  En  este  caso,  un  investigador   de  esta 7  empr esa  transfirió,  de  maner a  ilegal  (es  decir   en  contra  de  la  polític a  de  datos  de  Facebook),    estos  datos  a  la  empr esa  Cambridg e  Analy tica,  especializ ada  en  el  mark eting  polític o  online   ,   sobr e todo en c ampañas elect orales  ,  usando t ecnologías   de  Big Dat a  .   La import ancia de anonimiz ar los da tos   En  cualquier   caso,  y  aunque   sean   práctic as  habituales   estos  intercambios   de  datos,  es  muy    import ante  en  la  actualidad   man tener   y  cuidar   la  privacidad   de  las  personas.   Para  ello,  cada  vez   es  más  común   que  empr esas   y  organizaciones,   públic as  y  privadas,   que  utiliz an  o  desarr ollan   IA,   usen   datos  "anonimiz ados".   Así,  los  datos  son  some tidos   a  un  proceso   en  el  que  se  desvincula    los  datos  personales   (nombr e,  IP,  númer o  de  la  Seguridad   Social,   númer o  de  cuen ta  banc aria...)    del  resto  de  valores.  De  esta  maner a,  en  principio ,  no  se  podrían   volver  a  asociar   de  forma    directa  a  campos   consider ados   personales   para  iden tificar  directamen te  a  la  persona   individuo.    Esto  permit e  garantizar  la  privacidad   del  individuo   en  cues tión,   a  la  vez  que  utiliz as  sus  datos   para  la  mues tra.  Este  proceso   de  "anonimiz ación"   es  import ante  porque  los  datos  personales    están  protegidos,   en  toda  Europa,   por  la  Regulación   Gener al  de  Protección   de  Datos  (la  GDPR,    por sus siglas en inglés). Una v ez que "desper sonaliz as" y a no habría pr oblema   . 8  Una r esponsabilidad individual    En  cualquier   caso,  y  a  pesar   de  tener   estas  protecciones   legales,   es  cada  vez  más  import ante   que  cada  persona   tome   cada  vez  más  consciencia   de  su  responsabilidad   individual.   Por   supues to,  la  empr esa  que  produce   o  comer cializ a  un  product o  o  servicio   con  IA  debe   cumplir    unos   requisit os  de  ética  y  legalidad,   pero  también   el  que  lo  consume,   en  función   de  sus   8  Benjamins, R; Salazar, I (2020): El Mito del Algoritmo: cuentos y cuentas de la inteligencia Artificial. Anaya.  7  C. Cadwalladr y E. Graham-Harrison. "Revealed: 50 million Facebook profiles harvested for Cambridge Analytica in   major data breach". The Guardian, 2018.   https://www.theguardian.com/news/2018/mar/17/cambridge-analytica-facebook-influence-us-election.  6  M. Wood. "Not Getting Any Job Offers? Your Social Media Activity Could Be The Reason". Forbes, 2017.   https://www.forbes.com/sites/allbusiness/2017/06/22/not-getting-any-job-offers-your-social-media-activity-could-   be-the-reason/.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 0 6   
 principios   y  de  su  propio   criterio,  debe   asumir   esa  responsabilidad.   Para  ello,  es  import ante  que   nos  formemos   convenien temen te  para  tener   criterios   solidos   ante  las  distintas  situaciones,   ya   sean   fáciles   o  difíciles.   Si  un  algoritmo   de  IA  nos  hace   una  recomendación,   es  solo  eso,  una   opción   para  nosotr os.  Somos   nosotr os,  como   personas   y/o  profesionales   con  criterio,  los  que   debemos   decidir   seguir   lo  marcado  por  el  sistema   de  IA  o  no.  Por  tanto,  la  responsabilidad   recae   sobr e  el  consumidor ,  no  sobr e  el  algoritmo   de  IA  .  Igualmen te,  debemos   de  ser  toda  la  sociedad  9  los  que  podamos   decidir ,  de  una  forma   sencilla,   borr ar  nues tros  datos  cuando   así  lo   consider emos.   El  consumidor ,  la  persona   individual,   debe   ser  el  responsable   y  dueño   absolut o   de  los  datos  que  gener a.  Y  para  que  así  lo  pueda   entender   la  educ ación   en  estas  materias   es  la   base fundamen tal par a cimen tar un futur o prósper o que c ada v ez es más pr esen te.   English t ext:   The import ance of priv acy in a w orld domina ted b y da ta and AI    The  massiv e  impact   of  Artificial   Intelligence   and  Big  Data  circulating  around   us  is  provoking   the   need   for  a  major   chang e  in  our  socie ty.  A  chang e  of  men tality   to  adap t  to  our  new  reality .  We   live  surrounded   by  data.  Every  step  we  take  we  gener ate  data.  And  more  and  more  processes    will  be  performed   and  execut ed  based   on  this  data,  with   the  help   of  artificial   intelligence.   But   we,  as  humans,   are  not  used   to  this  digit al  reality .  To  this  hybrid   life  in  which   not  only   the   physical  steps  we  take  in  our  daily   routine   count.  But  also  the  digit al  footprin t  we  leave  behind,    which   is  also  part  of  ourselves.  Our  digit al  twin.   Little  by  little,  as  we  shar e  our  data,  we  become    more  and  more  transpar ent  to  our  surroundings.   Is  this  good  or  evil?  Will  we  be  able   to  learn   to   live with it as humanity?    Privacy  is  import ant  to  people,   but  it  also  depends   on  wha t  this  concep t  means   to  each   person,   and  it  is  not  always  the  same.   It  often  varies   according   to  cultur e  or  your  own  personal    experiences.   We  do  not  all  attach  the  same   import ance   to  it,  in  each   individual   case.  But  wha t   is  a  fact  is  that,  nowadays,  we  all  suffer  or  enjo y  the  consequences   of  the  "data  world"   in  which    we  live.  A  very  illustrative  example   occurr ed  already   in  2012,   when   a  father   complained   to  the   US  departmen t  store  chain   Target.  He  alleg ed  that  he  had  receiv ed  advertising   in  his  daugh ter's   name   for  baby  products   .  He  though t  it  was  ridiculous   that  they  sent  it  to  him  because   the  girl 10  was  still  finishing   high   school.   But  it  turned   out  that  this  supermark et  chain,   through   the  data,   had  detected  that  her  daugh ter  met  the  profile   of  "pregnan t"  and,   following   its  commer cial   10  K. Hill. "How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did". Forbes, 2012.   https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-   figured-out-a-teen-girl-was-pregnant-before-her-father-did/.  9  Salazar, I; Benjamins, R (2021): ‘El algoritmo y yo: GuÍA de convivencia entre seres humanos y artificiales’. Anaya.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 0 7   
 policy ,  sent  this  advertising.   In  fact,  the  daugh ter  was  pregnan t  and  Target  knew  it  before  her   father did.    In  the  professional   world,   such   monit oring   is  commonplace   .  Mor e  and  more  companies   are 11  resear ching   profiles   of  their   candida tes  for  recruitmen t  on  social   networks  before  making   a   decision,   and  it  is  not  uncommon   for  them   to  mak e  a  negative  decision   if  they  see  some thing    they  don't   like  there.  It  is  also  not  uncommon   to  receiv e  unsolicit ed  online   advertisemen ts   about   travel  to  a  country,  simply   because   we  have  done   some   resear ch  on  our  comput er  the   day  before.  You  could   say  it's  a  necessar y  evil,  as  few  people   are  willing   to  pay  to  read  the  news,   use  apps,  like  Google   Map s,  Facebook,   Instagram,  etc.  Even  if  we  don't   like  it,  to  a  certain   extent, we migh t come t o under stand wh y the y exploit our da ta.   However,  there  is  some thing   that  should   be  even  more  worrying:   cases   of  illegal  practices   with    personal   data.  Although   there  are  man y,  man y  documen ted  cases,   the  mos t  emblema tic,  to   date,  has  been   the  Cambridg e  Analy tica  scandal   and  the  Facebook   data  of  millions   of  user s   around   the  world,   which   occurr ed  in  March  2018   .  In  this  case,  a  resear cher   from  this  compan y 12  illegally  (i.e.  against  Facebook's   data  policy)   transferred  this  data  to  the  compan y  Cambridg e   Analy tica,  which   specialises   in  online   politic al  mark eting,  especially   in  election   campaigns,   using    Big Da ta technologies.    The import ance of anon ymising da ta   In  any  case,  although   such   data  exchang es  are  common   practice,   it  is  very  import ant  nowadays   to  main tain  and  protect  people's   privacy.  To  this  end,   it  is  increasingly   common   for  companies    and  organisa tions,   both   public   and  private,  that  use  or  develop   AI  to  use  "anon ymised"   data.   Thus,   data  are  subject ed  to  a  process   in  which   personal   data  (name,   IP,  Social   Security   number ,   bank   account  number ...)  are  separ ated  from  the  rest  of  the  values.   In  this  way,  in  principle,   they   can  no  long er  be  directly   associa ted  with   fields   consider ed  personal   in  order  to  directly   iden tify   the  individual   person.   This  mak es  it  possible   to  guar antee  the  privacy  of  the  individual   in   ques tion, while using his or her da ta for the sample   . 13  This  "anon ymisa tion"   process   is  import ant  because   personal   data  is  protected  throughout    Europe  by  the  Gener al  Data  Protection   Regula tion  (GDPR).   Once   you  "deper sonalise"   it  is  no   long er a pr oblem.    13  Benjamins, R; Salazar, I (2020): El Mito del Algoritmo: cuentos y cuentas de la inteligencia Artificial. Anaya.  12  C. Cadwalladr y E. Graham-Harrison. "Revealed: 50 million Facebook profiles harvested for Cambridge Analytica   in major data breach". The Guardian, 2018.  11  M. Wood. "Not Getting Any Job Offers? Your Social Media Activity Could Be The Reason". Forbes, 2017.   https://www.forbes.com/sites/allbusiness/2017/06/22/not-getting-any-job-offers-your-social-media-activity-could-   be-the-reason   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 0 8   
 An individual r esponsibility    In  any  case,  and  despit e  having  these   legal  protections,   it  is  increasingly   import ant  that  each    person  becomes   more  and  more  aware  of  his  or  her  individual   responsibility .  Of  course,  the   compan y  that  produces   or  mark ets  a  product   or  service  with   AI  mus t  comply   with   ethical  and   legal  requir emen ts,  but  also  the  consumer ,  according   to  his  or  her  own  principles   and  criteria,    mus t  assume   this  responsibility .  To  this  end,   it  is  import ant  that  we  are  properly   trained   to  have   solid   criteria  in  different  situa tions,   whe ther   they  are  easy  or  difficult.   If  an  AI  algorithm   mak es   us  a  recommenda tion,   it  is  just  that,  an  option  for  us.  It  is  up  to  us,  as  people   and/ or   professionals   with   criteria,   to  decide   whe ther   to  follow  the  AI  system's   recommenda tions   or   not.  Ther efore,  the  responsibility   lies  with   the  consumer ,  not  with   the  AI  algorithm   .  Equally ,  it 14  should   be  socie ty  as  a  whole   that  can  decide,   in  a  simple   way,  to  dele te  our  data  when   we   consider   it  appr opria te.  The  consumer ,  the  individual   person,  mus t  be  the  absolut e  owner   and   responsible   for  the  data  he  or  she  gener ates.  And  in  order  for  them   to  under stand  this,    educ ation  in  these   matters  is  the  fundamen tal  basis   for  cemen ting  a  prosper ous  futur e  that  is   increasingly our NO W.   Idoia Salazar   Co-founder and President   Observatory  of  the  Social  and  Ethical  Impact  of  Artificial   Intelligence (OdiseIA)   Salazar  is  the  co-founder  and  president  of  the  Observatory  of  the  Social  and   Ethical  Impact  of  Artificial  Intelligence  (OdiseIA).  She  is  Principal  Investigator  of   the  SIMPAIR  Research  Group  (Social  Impact  of  Artificial  Intelligence  and   Robotics)I.  She  is  a  specialist  in  Ethics  in  Artificial  Intelligence  and  a  professor  in  international  degrees  at   CEU  San  Pablo  University.  She  has  authored  the  following  books:  'The  Algorithm  and  I:  Guide  to   coexistence  between  human  and  artificial  beings',  'The  Myth  of  the  Algorithm:  Tales  and  truths  of   Artificial  Intelligence  (co-author  with  Richard  Benjamins),'  The  Revolution  of  robots:  How  Artificial   Intelligence  and  robotics  affect  our  future  'and'  The  depths  of  the  Internet:  Access  information  that   search  engines  cannot  find  and  discover  the  intelligent  future  of  the  Internet  '  (written  in  spanish),  as   well  as  scientific  and  informative  articles  oriented  to  investigate  and  raise  awareness  about  the  impact  of   Artificial  Intelligence.  She  is  in  the  list  of  experts  to  assist  the  European  Parliament´s  Artificial  Intelligence   Observatory  (EPAIO);  member  of  the  Board  of  Directors  of  the  Association  ‘Arco  Atlántico  para  la   Seguridad  y  el  entorno  Digital’;  founding  member  of  Springer  AI  and  Ethics  journal  and  member  of  the   Global AI Ethics Consortium.   14  Salazar, I; Benjamins, R (2021): ‘El algoritmo y yo: GuÍA de convivencia entre seres humanos y artificiales’. Anaya.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 0 9   
 Go Wide: Article Summaries  (summariz ed b y Abhishek Gup ta)   The  Limits   of  Differential  Privacy  (and   Its  Misuse   in  Data  Release   and   Machine Learning)    [Original article b y  ACM Mag azine   ]   Wha t  happened   :  We  pitch  the  concep t  of  differential  privacy  as  a  silver  bulle t  to  solve  our   struggle  with   wanting  to  shar e  data  (in  the  interest  of  building   publicly   bene ficial   technologies)    with   the  desir e  to  have  strong  privacy  protections.   Yet,  there  is  no  free  lunch.   Differential   privacy  has  short comings:   the  more  substantial  the  privacy  protections,   the  less  utility   we  get   from  the  data  as  tuned   by  the  epsilon   parame ter  in  the  differentially   private  analy sis.  As  per  the   original   paper ,  we  bring   meaningful   privacy  protections   when   epsilon   values   stay  less  than   1   (lower  values   are  better  for  privacy).   Still,   a  lot  of  current  uses   of  differential  privacy  use  values    as  high   as  30.  In  addition,   the  fundamen tal  formula tion  of  differential  privacy  works  to  protect   individual   records  in  a  pool   of  records  of  man y  individuals.   We  viola te  this  basic   notion   when   we   apply   differential  privacy  to  healthc are  data  from  devices   in  conjunction   with   feder ated  learning    because all the r ecords coming fr om a single de vice belong t o the same per son.    Why  it  matters  :  This  sort  of  in-dep th  technic al  analy sis  and  challenging   dominan t  assump tions    in  the  field   is  crucial   if  we  want  to  achie ve  responsible   AI  in  practice   rather   than   just  pay   lip-ser vice t o it b y articula ting a se t of principles.    Between  the  lines   :  Even  though   there  is  a  some wha t  valid  diatribe   against  technic al   practitioner s  proposing   solutions   to  addr ess  ethical  challeng es  in  AI,  we  cannot   work  without    their   expertise   and  help.   We  risk  creating  requir emen ts  and  legisla tions   with   a  limit ed   under standing of the limits of pr oposed solutions, leading t o mor e harm in the long-run.    Should F amilies’ Sur veillance Camer as Be Allo wed in Nur sing Homes?    [Original article b y  The Mark up  ]   Wha t  happened   :  A  surveillance   camer a  installed   in  a  nursing  home   captured  a  death  showing    gruesome   details  with   the  victim   crying  out  for  help   and  the  nursing  staff  idling   and,   in  some    cases   laughing   at  the  patient.  It  spark ed  a  massiv e  deba te  on  whe ther   it  was  legal  to  moun t   such   camer as  in  nursing  homes.   Arguably   there  are  privacy  concerns,   and  it  showcases   a   distrust  in  the  staff  at  the  nursing  homes,   while   other s  have  argued   that  it  is  a  way  to  hold   them    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 1 0   
 accountable.   As  is  usually   the  case  with   technologic al  solutions   to  sociologic al  problems,   such   a   solution   fails  to  addr ess  the  underlying   issues   of  under -compensa ted  and  overworked  nursing   staff, amongs t other pr oblems with the healthc are system.   Why  it  matters  :  Particularly   as  we  start  to  automa te  the  video   processing   captured  from  these    camer as,  the  issues   raised   in  this  case  will  become   even  more  pertinen t  to  the  overall  discussion    of  surveillance.   As  the  case  made   its  way  through   the  legisla tive  process,   the  courts   settled  on   deeming   it  ok  to  have  visible   camer as  so  that  the  people   being   surveilled   are  aware  of  the   presence of a c amer a. In c ontrast, the use of hidden c amer as w as pr ohibit ed.   Between  the  lines   :  We  see  a  collision   of  technology   and  socie ty  in  ways  that  we  couldn’t   have   anticipa ted.  Issues   such   as  the  use  of  camer as  (which   have  become   cheaper   to  deplo y)  coupled    with   the  use  of  AI  to  automa tically  process   the  video   feeds   can  help   in  unbur dening   the  staff   from  having  to  constantly  monit or  patients,  such   as  automa tically  detecting   if  a  patient  has   fallen.   On  the  other   hand,   it  begins   to  normaliz e  automa ted  surveillance   as  an  accep ted  part  of   our socie ty which will ha ve much mor e profound e ffects in the long t erm.    Huge da ta leak sha tters the lie tha t the innocen t need not f ear sur veillance    [Original article b y The Guar dian]    Wha t  happened   :  The  firm  NSO   that  is  known  to  have  sold  surveillance   softw are  to   organizations,   including   governmen ts  around   the  world,   has  come   under   fire  for  its  softw are   Pegasus   which   is  under   investigation  for  its  implic ation  in  the  surveillance   on  not  just  typic al   targets  of  spy-craft  around   the  world  but  also  everyday  citizens  Over  the  coming   weeks,  The   Guar dian’ s  investigative  team   in  partner ship  with   other   news  organizations   around   the  world   will  be  releasing   the  names   of  the  people   who   have  been   a  target  of  the  softw are  and  the   compr omises   facilit ated  by  it.  The  case  that  they  seek   to  mak e  is  that  anybody   is  suscep tible   to   these   intrusions   given  our  over-reliance   on  our  phones   and  why  privacy  as  a  core  tenet  of   functioning in our digit al socie ty needs t o be some thing tha t we pa y a lot mor e attention t o.   Why  it  matters  :  While   there  are  some   regula tions   perhap s  in  the  use  of  surveillance   technology    when   governmen t  agencies   deplo y  them   in  their   intelligence   oper ations   (though   a  lot  of  that   was  debunk ed  with   the  Snowden  leaks  in  2013),   the  oper ations   of  a  player  like  the  NSO   and   their   ability   to  sell  their   tools  and  services   to  anyone  on  the  mark et  chang e  the  equa tion   signific antly  in  terms   of  wha t  privacy  guar antees  we  can  hope   to  have  as  individuals   spending   a   chunk of our liv es in the digit al realm.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 1 1   
 Between  the  lines   :  The  seriousness   of  the  matter  is  under scored  by  some   phrases   in  the  article    that  highligh t  the  degr ee  of  protections   that  the  staff  behind   this  story  had  taken  including   not   having  any  of  their   phones   around   during   their   mee tings,   sour ces,  etc.  They  position   this  as   some thing   that  will  be  as  monumen tal  as  their   investigation  and  public ation  of  results   at  the   time   of  the  Snowden  leaks  that  moved  the  needle   of  under standing   for  the  public   on  wha t   spy-craft  capabilities   exist  and  how  they  are  being   used.   Now,  that  conversation  will  expand   to   include an yone in the w orld, innocen t or not.    The Ine vitable W eaponiz ation of App Da ta Is Her e   [Original article b y  Vice   ]   Wha t  happened   :  A  Substack  public ation  called   The  Pillar   bough t  location  data  from  a  data   broker  and  combining   it  with   data  from  the  Grindr   app  outed  a  pries t  as  potentially   gay  which    led  to  his  resigna tion.   Even  anon ymiz ed  data  without   names   attached   to  a  specific   person  can   be  used   to  obtain  informa tion  about   a  specific   person.   A  very  small   number   of  location  poin ts   are  requir ed  to  uniquely   iden tify  a  person  because   of  the  patterns   that  we  all  follow  in  the   places w e visit and wher e we spend mos t of our time: our homes and of fices.    Why  it  matters  :  Apps  like  Grindr   defend  themselv es  by  saying  that  wha t  was  men tioned   in  the   article   that  let  to  the  ouster  of  the  pries t  is  “technic ally  infeasible” ,  the  problem   is  that  there  are   plen ty  of  companies   that  offer  data  consulting   services   towards  “iden tity  resolution”   as  a  way  of   unearthing da ta about specific individuals fr om tr oves of da ta tha t are sold b y data brokers.   Between  the  lines   :  Wha t  was  previously   the  domain   of  highly -resour ced  organizations   is   some thing   that  anyone  with   a  little  bit  of  mone y  and  motiv ation  can  execut e  with   ease.   Data   brokers  collect   large  amoun ts  of  data  from  all  the  apps  that  have  any  sort  of  in-app   advertising    and  then   pack age  and  sell  that  data  over  to  anyone  willing   to  fork  over  a  few  dollar s.  This  is  still   a  largely  unregula ted  indus try  and  calls  from  Sena tors  like  Wyden  in  the  US  to  bring   the  force  of   the  FTC  to  regula te  this  domain   are  essen tial  if  we  want  to  get  rid  of  the  scourge  of  sensitiv e   data exposing in tima te de tails of our liv es.   Amaz on will pa y you $10 in cr edit f or your palm prin t biome trics    [Original article b y  TechCrunch   ]   Wha t  happened   :  Amaz on  is  rolling   out  paymen ts  in  their   physical  stores  using   a  contactless    palm   scanner   and  are  offering   $10  in  store  credit  to  those   who   enrol  in  the  service.   Contactless    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 1 2   
 paymen ts  do  seem   attractiv e  in  a  pandemic   but  there  are  the  obvious   cyber security   concerns,    especially   those   of  data  privacy  of  immut able   personally   iden tifiable   informa tion,   that  is,  your   biome trics which c annot be chang ed unlik e your phone number or home addr ess.   Why  it  matters  :  Amaz on  doesn’t   have  a  great  track  record  when   it  comes   to  the  use  of   biome trics,   we  all  remember   their   Rekognition   program  and  the  ensuing   problems,   including    inaccur acies   in  the  inferences   gener ated  by  that  system.  In  addition,   they  are  a  private  compan y   which   can  turn   around   and  potentially   sell  this  data  to  data  brokers  who   can  collate  that  with    other   informa tion  about   you  that  is  floating  around   on  the  interne t  to  disas trous  consequences.    My  paper   from  2018   “The  Evolution   of  Fraud:   Ethical  Implic ations   in  the  Age  of  Large-Scale  Data   Breaches   and  Widespr ead  Artificial   Intelligence   Solutions   Deplo ymen t”  talked  about   some    horrif ying c onsequences tha t migh t arise fr om leak ed biome tric da ta.   Between  the  lines   :  While   the  headline   is  sensa tionalis t  in  its  presen tation  of  the  scenario ,   namely   that  it  brings   focus   to  the  mone tary  aspect   of  how  much   customer s  will  be   compensa ted  for  the  use  of  biome trics,   it  detracts  from  the  more  import ant  issue   of  how  and   when   biome trics   should   be  used   and  wha t  regula tions   we  need   to  develop   to  ensur e  their   safe   usag e.   Apple W alks a Priv acy Tigh trope t o Spot Child Abuse in iCloud    [Original article b y  Wired  ]   Wha t  happened   :  Apple   has  introduced   a  new  feature  for  the  devices   that  use  iCloud   which   will   scan  imag es  to  determine   if  there  is  any  child   sexual  abuse   material   (CSAM)   in  them.   This  is   being   heralded   as  a  win  in  the  fight  against  child   abuse   online   while   some   privacy  activis ts   belie ve  that  this  weakens  the  privacy  protections   offered  by  the  Apple   ecosystem  to  its  user s.   The  determina tion  process   is  split   between  the  device  and  the  cloud   wher e  hashes   are   comput ed  on  the  imag es  and  these   are  compar ed  against  a  known  database   of  CSAM  that  is   downloaded   through   a  blinding   process   to  the  user ’s  device.   This  prevents  a  user   from  reverse   engineering   all  the  hashes   to  prevent  abuse   and  evasion   of  the  detection   system.  It  also  uses    some thing   called   Neur alHash   that  is  robus t  to  alterations   in  the  imag es  that  abuser s  can  use  to   evade  detection.   It  also  uses   the  notion   of  privacy  set  intersection   to  only   alert   the  system  when    hashes   are  matched   other wise   resting  silen t  and  preventing  Apple   from  gaining   access   to   hashes of all y our imag es.   Why  it  matters  :  Online   services   have  certainly   made   it  easier   to  spread  CSAM  and  this  move  by   Apple   is  a  huge  win  in  comba ting  this  scourge.  But,  the  concerns   raised   by  privacy  activis ts  also   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 1 3   
 have  some   merit   in  terms   of  wha t  other   demands   migh t  the  compan y  accede   to  in  the  futur e  in   the  interest  of  law  enforcemen t.  The  setting  of  the  preceden t  is  wha t  scares  the  privacy  scholar s   and  activis ts  more  so  than   this  particular   instance   which   is  quite  clearly   bene ficial   for  the  health    of our in forma tion ec osystem.   Between  the  lines   :  The  engineering   solutions   proposed   to  tackle   this  problem   of  CSAM   detection   in  a  privacy-preser ving  fashion   will  have  lots  of  other   positiv e  downstream   usag e  and   it  is  a  net  win  overall  in  designing   technology   that  can  keep  harm   at  bay  while   still  main taining    fundamen tal  rights  and  expect ations   of  user s  like  privacy.  Public   discussion   of  such   technology    through   open-sour ce  examina tion  migh t  be  another   way  to  boos t  the  confidence   that  people    have  in  the  solutions   being   rolled   out  while   also  potentially   poin ting  out  holes   in  the  technology    leading t o an o verall mor e robus t solution.    Privacy in the Br ain: The E thics of Neur otechnology    [Original article b y  Technology Ne tworks  ]   Wha t  happened   :  The  article   poin ts  out  the  emer ging   regula tory  challeng es  faced   in  the  domain    of  neur otechnology   as  the  quan tified   wellness   indus try  has  taken  off  with   man y  mass-mark et   devices   like  the  Muse   headband   claiming   that  they  are  able   to  tap  into  brain  signals   to  deliv er   better  neur otech-enabled   experiences   to  enhance   productivity   and  impr ove  medit ation   practices.   Wha t  was  previously   the  domain   of  the  DIY  community   is  now  going   mains tream   and   one  of  the  interviewees  in  the  article   explic ates  that  without   under going   the  same   string ent   standar ds  governed   by  the  FDA  as  we  have  for  other   medic al  devices,   we  risk  causing    irreversible harm f or those who use these no vel de vices in an e xperimen tal fashion.    Why  it  matters  :  The  demons tration  from  Musk’ s  Neur alink   certainly   brough t  neur otechnology    to  the  attention  of  man y  more  people   than   before.  Pitched   as  being   able   to  augmen t  the  brain   and  our  ways  of  communic ating  with   each   other   media ted  by  machines,   in  addition   to  the  more   immedia te  (and   perhap s  realis tic)  bene fits  of  helping   those   vision   and  speech   problems,   the   ethical  concerns   of  commer cial  technologies   without   medic al-gr ade  appr ovals  is  very  unner ving.    The  privacy  implic ations,   especially   in  the  era  of  continual   cyber attacks  is  another   exacerba ting   factor.   Between  the  lines   :  While   physical  damag e  like  skin  burns   from  wrong  usag e  of  transcr anial    direct  current  stimula tion  (tDCS)   devices   can  be  measur ed  to  a  certain  extent,  there  is  potential   for  damag e  that  is  hidden   or  alters  the  subjectiv e  experience   of  a  person  that  can’t  be   quan tified.   In  such   a  case,  under standing   the  burden  of  liability   is  tricky   to  resolv e,  especially    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 1 4   
 with   incomple te  informa tion  and  this  has  some   bearing   on  a  lot  of  scenarios   we  encounter  in   the domain of AI e thics as w ell.   This  is  the  real  story  of  the  Afghan   biome tric  databases   abandoned   to  the   Taliban    [Original article b y  MIT T echnology R eview  ]   Wha t  happened   :  As  the  US  forces  exited  Afghanis tan,  the  technic al  infrastructur e  and  data  trails   left  behind   are  causing   problems   as  they  fall  into  the  hands   of  the  Taliban.   In  particular ,  the   article   dives  into  the  details  of  the  Afghan   Personnel   and  Pay  System  (APPS)   used   by  the   Minis try  of  the  Interior   and  the  Minis try  of  Defense   that  has  access   to  highly   sensitiv e   informa tion  including   biome trics   of  security   personnel   and  their   networks.  The  implic ations   of   that  data  extend  beyond  just  breaches   of  privacy:  there  are  real-w orld  security   concerns   with    how tha t data migh t be used ag ainst those who support ed the pr evious r egime.    Why  it  matters  :  Aside   from  the  sensitiv e  and  immut able   nature  of  the  data  that  resides   in  these    databases   that  have  now  been   taken  over  by  the  Taliban,   this  is  an  unfortuna te  example   of  wha t   happens   when   extensiv e  data  gathering   oper ations   are  conduct ed  without   regard  to  wha t  may   happen   when   the  informa tion  falls  into  the  hands   of  malicious   actors.  In  this  case,  there  are   incoming   reports   that  men tion  how  the  data  migh t  have  potentially   been   used   to  target   individuals   still  within   the  country  that  had  support ed  the  previous   regime.   Given  the   immut ability   of  the  biome trics,   the  people   who   are  captured  in  that  data  have  no  chance   of   erasing or esc aping.    Between  the  lines   :  While   the  data  is  usually   gathered  under   the  guise   of  providing    adminis trative  services   like  access   to  governmen t  and  social   security   bene fits,  without    appr opria te  cyber security   protections,   and  some times   even  with   them,   when   data  falls  into  the   hands   of  those   who   can  misuse   that  data  or  target  individuals   based   on  their   iden tity  or  activity ,   there  is  little  that  people   can  do  to  escape  the  consequences   of  their   presence   in  those    datasets.  This  is  one  of  the  strongest  reasons   in  support   of  data  minimiz ation  and  purpose    limit ation.   As  men tioned   in  the  article,   creating  national   ID  schemes   based   on  biome tric  data  is   not  the  best  way  to  go  about   it,  and  this  is  one  example   wher e  we  see  how  this  can  go  horribly    wrong.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 1 5   
 The Do wnside t o Sur veilling Y our Neighbor s   [Original article b y  The Mark up  ]   Wha t  happened   :  Apps  like  Citiz en  and  Nextdoor   that  claim   to  mak e  neighborhoods   safer  by   empo wering   residen ts  with   more  informa tion  through   the  deplo ymen t  of  surveillance    infrastructur e  are  under   intense   scrutin y  as  they  have  become   platforms   rife  with   racism   and   vigilan tism.   Wha t  is  different  compar ed  to  previous   iterations   of  such   neighborhood   monit oring    solutions   is  that  these   are  now  often  tied  in  with   official   police   departmen ts,  giving   them   a  live   feed  into  the  happenings   of  a  neighborhood.   Exacerba ting  the  problems   is  the  fact  that   report ed  inciden ts  can  get  blown  out  of  proportion   as  residen ts  migh t  engage  in  racist  beha vior   that dispr oportiona tely t argets BIPOC member s.   Why  it  matters  :  Some   apps  like  Citiz en  are  used   in  over  60  cities   in  the  US.  While   a  lot  of  apps   have  now  severed  connections   with   official   police   departmen ts  being   able   to  read  feeds    directly ,  it  doesn’t   prohibit   police   forces  from  having  non-of ficial   accounts  to  monit or  the  feeds.    Mor e  so,  the  problems   of  content  moder ation  and  health   of  the  informa tion  ecosystem  that   they main tain suf fers from the same challeng es tha t other social media pla tforms suf fer fr om.   Between  the  lines   :  This  is  a  piece   of  technologic al  solution   that  doesn’t   solve  the  real  problem    that  it  claims   to  provide   a  solution   for.  It  just  normaliz es  extreme   views  that  some   residen ts   migh t  hold   on  their   neighbor s  by  bringing   those   views  out  into  the  open   and  garnering   support    from  other   isola ted  pock ets  (hope fully)   of  the  same   viewpoin ts.  By  being   loud   and  vocal  on   those   platforms,   they  can  spur   hate  and  mistrust  amongs t  the  communities   countering   the  goal   of  having  safe  and  harmonious   living.   Ultima tely,  safer  communities   migh t  just  requir e  steering    away from t echnologic al fix es and mor e so f ocusing on c ommunity building IRL.    Leak ed  Documen ts  Show  How  Amaz on's   Astro  Robot   Tracks  Everything   You   Do   [Original article b y  Vice   ]   Wha t  happened   :  Amaz on  has  unveiled   a  new  robot   dubbed   Astro  that  integrates  with   Alexa   Guar d  and  Ring   (other   products   from  Amaz on)  to  provide   automa ted  home   security   solutions.   It   is  a  $999   robot   that  will  patrol  the  home   of  the  user   and  constantly  surveil  it  for  inciden ts  that   warrant  the  attention  of  the  owner   in  the  case  of  unusual   activity   and  also  to  monit or  strangers   inside   the  house.   If  this  doesn’t   already   spook   you,  the  article   men tions   a  leaked  memo   that   details the numer ous fla ws in the s ystem tha t go counter to the mark eting e ffort fr om Amaz on.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 1 6   
 Why  it  matters  :  Cons tant  surveillance,   data  privacy,  targeted  advertising ,  and  the  list  goes  on  in   terms   of  concerns   that  arise   from  the  use  of  a  persistent  sentry  moving  around   the  mos t  private   spaces   of  our  lives:  our  homes.   Wha t  is  even  more  problema tic  is  that  people   who   have  worked   on  Astro  poin t  out  that  the  system  is  flawed  in  its  person  recognition   capabilities,   struggles  to   navigate  spaces,   and  is  sold  as  an  accessibility   enhancer   within   the  home   though   it  has  notable    failur e  modes   wher e  it  is  known  to  get  into  jams.   Wha t’s  even  more  interesting  is  that  Amaz on   doesn’t   have  a  policy   to  allow  for  returning   broken  Astros  as  men tioned   in  the  article    referencing the leak ed memo.    Between  the  lines   :  From  a  practic al  and  technic al  standpoin t,  there  are  man y  challeng es  in   getting  social   robots   right:  above  all,  getting  it  to  oper ate  in  line  with   human   expect ations   mos t   of  the  time   to  be  welcome   in  their   mos t  private  spaces.   The  demons trated  failur es  of  Astro   along   with   the  almos t  insurmoun table   combina tion  of  challeng es  of  being   able   to  respond   to   myriad   voice  commands   from  the  owner ,  navigating  a  comple x,  dynamic,   and  uncert ain   environmen t,  and  interacting   with   dynamic   live  and  static  elemen ts  in  that  environmen t  mak es   it  highly   unlik ely  that  Astro  succeeds   in  winning   a  place   in  people’ s  home.   As  consumer s  become    savvier  about   privacy  and  other   ethical  concerns   regarding   some   of  the  tech  that  is  requir ed  to   power  the  Astro,  Amaz on  will  have  to  provide   very  robus t  guar antees  before  people   are  going    to bring one home.    “The  power  to  surveil,  control,  and  punish”:   The  dystopian   dang er  of  a   manda tory biome tric da tabase in Me xico   [Original article b y  Rest of W orld  ]   Wha t  happened   :  Mexico,  empo wered  by  a  loan   from  the  World  Bank,   is  pushing   hard  to   implemen t  a  unified   national   iden tity  scheme   with   a  view  to  mak e  access   to  governmen t   services   and  public   bene fits  linked  through   a  single   system.  Similar   initia tives  have  been   funded    by  the  World  Bank   in  countries   around   the  world,   and  in  man y  places   the  implemen tation  of   such   national   iden tity  schemes   has  led  to  less  than   desir ed  outcomes.   In  particular ,  biome trics    associa ted  with   the  iden tities   tend  to  result   in  failur es  at  the  poin t  of  receiving   the  services   due   to  problems   with   the  technology   that  is  deplo yed  to  ascert ain  iden tity,  such   facial   recognition    and fing erprin t scanning.    Why  it  matters  :  Once   such   a  system  is  put  in  place,   it  is  incredibly   difficult   to  extricate  the   provision   of  the  services   from  such   a  system.  In  a  country  wher e  crime   infiltrates  various   levels   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 1 7   
 of  governmen t  and  wher e  there  is  a  risk  for  cyber security   breaches,   potentially   compr omising    all  the  iden tities   and  biome trics   of  the  people   who   have  enrolled   in  the  scheme,   such   a  system   migh t  create  more  problems   than   it  solves.  Given  all  these   problems,   and  the  potential  that  all   iden tities   linked  into  a  single   scheme   can  cause   a  single   poin t  of  failur e  and  give  too  much    authorit arian po wer to governmen ts to sur veil, w e need t o be c areful be fore proceeding.    Between  the  lines   :  One  of  the  things   that  stands   out  in  the  article   is  that  such   national   iden tity   schemes   are  pushed   heavily  in  the  Global   South   under   the  guise   of  impr oving  access   to  public    bene fits  and  governmen t  services,   but  this  migh t  not  be  an  ideal   appr oach   if  there  isn’t    adequa te  supporting   infrastructur e  that  can  ensur e  that  such   a  scheme   is  implemen ted  with    privacy  safeguar ds  and  security   measur es  in  place.   Also ,  we  need   to  be  cogniz ant  of  the  fact   that  there  continue   to  exist  alterna tive  ways  for  people   to  access   services   who   aren’t  able   to   enroll  in  the  scheme   or  experience   failur es  in  the  verific ation  process   at  the  poin t  of  service   provision   due  to  hardware  failur es,  denying  them   access   to  essen tial  services   like  free  rations    and healthc are in cert ain places.    The  Popular   Family   Safety  App  Life360   Is  Selling   Precise   Location  Data  on   Its Tens of Millions of User s   [Original article b y  The Mark up  ]   Wha t  happened   :  Data  brokers  are  organizations   that  oper ate  in  the  shado ws,  away  from  much    scrutin y  and  regula tions,   furnishing   a  mark et  worth   billions   of  dollar s  with   aggregated  data  from   across  various   sour ces  to  fuel  targeted  advertising   amongs t  other   services   that  are  mean t  to   strip  away  at  the  consumer ’s  agency ,  pock et,  and  autonom y.  The  article   dives  into  details  of   Life360,   a  compan y  that  allows  families   to  track  the  locations   of  their   kids,   pitched   as  a  safety   product.   Wha t  is  buried   in  the  fine  print,  that  parents  some times   gloss   over,  is  that  such   data  is   sold  downstream   to  3rd  parties,   including   governmen t  agencies.   While   they  recen tly  put  in   place   a  policy   to  not  sell  the  data  to  law  enforcemen t,  they  have  been   doing   so  for  man y  years   already , meaning da ta has pot entially tr aveled f ar.   Why  it  matters  :  While   such   apps  do  provide   a  degr ee  of  comfort  and  utility   to  parents  to   monit or  their   kids,   the  cost  is  potentially   too  high,   location  data  used   to  create  a  rich  profile   of   their   childr en  that  will  follow  them   for  the  rest  of  their   lives  as  these   data  brokers  enrich   such    datasets  with   more  informa tion  and  sell  that  downstream   for  targeted  advertising   all  the  way   up  to  changing   insur ance   premiums   and  other   higher   stake  situa tions.   Given  that  such   data  is   collect ed  directly   by  the  app  and  then   sold  later  once   it  is  centralized,  typic al  appr oaches   used    by  privacy  resear cher s  hunting  for  code  that  shows  signs   of  linking   to  common   data  brokers   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 1 8   
 doesn’t   really   work  here.  Clien ts  of  Life360   have  been   flagged  for  problema tic  beha viour   by   man y concerned en tities, including Sena tor W yden’ s office.    Between  the  lines   :  Wha t  is  appalling   about   the  whole   situa tion  is  that  such   companies   use   instances   like  the  pandemic   and  the  veneer   of  providing   a  public   service  by  gathering   this  data   and  selling   it  to  organizations   doing   good  work  like  the  CDC  to  draw  attention  away  from  the   fact  that  they  are  also  selling   this  data  to  other   unscrupulous   parties.   Mor e  so,  they  take   advantage  of  parents’  fears  of  child   safety  as  a  Trojan   horse  to  more  invasive  data  gathering    which   fuels   their   bottom  lines,   comple tely  out  of  sync  with   their   stated  values.   The  business    goals  and  stated  values   are  often  in  conflict  with   the  current  trend  indic ating  that  business   goals   are winning b y a mile.    Sing apor e’s  tech-ut opia   dream   is  turning   into  a  surveillance   state   nigh tmar e   [Original article b y  Rest of W orld  ]   Wha t  happened   :  Technology   is  seen   as  an  instrumen t  to  enable   a  fulfilling   and  meaningful   life   in  Singapor e,  with   large  leeway  provided   to  the  governmen t  to  impose   technologic al  solutions    in  service  of  building   a  utopia.   But,  as  the  pandemic   rolled   on,  citizens  in  Singapor e  are  chafing   against  the  intrusions   that  constant  surveillance   and  digit al  intrusions   are  now  imposing   on  their    lives.  The  inclina tion  for  technology   remains   so  high   in  Singapor e  that  the  governmen t  offers   regula tory  sandbo xes  so  that  companies   can  experimen t  with   novel  technologies   that  can  then    be  brough t  into  mains tream   socie ty.  Things   like  smart   lamp   posts  to  monit or  traffic,   environmen tal  conditions,   and  people’ s  movemen ts,  robots   for  elder   care,  biome tric  databases    to  process   people   at  borders  and  impr oving  security   at  bank s  and  public   services.   With   apps  like   TraceT ogether   and  SafeEntry  becoming   manda tory  and  combined   into  a  single   experience,    movemen t tracking in the in terest of curbing the pandemic bec ame ubiquit ous.    Why  it  matters  :  These   apps  are  quite  detailed   in  the  amoun t  of  data  that  they  collect,   especially    when   we  consider   that  they  are  linked  to  the  national   iden tity  system  in  Singapor e.  Even  though    the  governmen t  provided   assur ances   that  such   technology   would   only   be  used   for  contact   tracing ,  it  was  later  revealed   that  the  data  had  been   shar ed  with   law  enforcemen t  and  cases    have  used   this  informa tion  as  evidence.   People   have  been   slower  to  ques tion  and  raise   concerns   because   of  the  strict  informa tion  ecosystem  in  Singapor e  with   laws  like  POFMA   and   FICA ,  which   are  mean t  to  protect  against  misin forma tion,   being   used   to  tightly  control  wha t  is   said about the g overnmen t’s technology use.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 1 9   
 Between  the  lines   :  Migr ant  workers  in  Singapor e,  who   form  the  backbone   of  physical  labor   that   has  been   used   to  build   the  infrastructur e  powering   and  enabling   everything ,  are  the  targets  of   technologic al  experimen tation  and  are  often  forced  to  live  in  situa tions   with   very  limit ed  rights.   Once   the  technology   is  refined,   it  is  deplo yed  en  masse   to  the  rest  of  the  Singapor e  popula tion.    Bundling   solutions   together   and  tying   them   with   national   iden tity  solutions   migh t  have  helped    Singapor e  return   to  normalcy   faster  from  the  pandemic,   but  it  has  come   with   an  astronomic al   cost  of  introducing   extremely   pervasive  and  intrusiv e  surveillance   technology   that  doesn’t   show   any signs of g oing a way.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 2 0   
 4. Bias    Introduction  by  Abhishek   Gup ta,  Founder   and  Principal   Resear cher ,  Mon treal  AI  Ethics    Institut e   Bias  mitig ation  continues   to  pose   challeng es  to  those   who   are  designing   and  developing   AI   systems  but  also  to  those   who   are  procuring   these   systems  and  integrating  them   into  their   own   products   and  services.   The  opening   article   details  a  frame work  from  the  National   Institut e  of   Standar ds  and  Technology   (NIST)  which   has  a  3-step  appr oach   for  bias  mitig ation  focussed   on   the  phases   of  pre-design,   design   and  developmen t,  and  deplo ymen t.  Tying  bias  mitig ation   appr oaches   explicitly   to  various   stages  of  the  AI  lifecycle  mak es  it  so  that  we  have  actions   that   more closely map t o the activities c arried out b y practitioner s everyday in their w ork.   Carr ying  on  with   practic al  appr oaches   to  bias  mitig ation,   the  next  piece   in  this  chap ter  walks   through   co-designed   checklis ts,  a  piece   from  Micr osoft   Resear ch  that  highligh ts  resear ch  work   that  draws  from  an  under standing   of  how  practitioner s  appr oach   fairness   concerns   today,  wha t   are  their   desider ata  for  fairness   checklis ts,  and  how  they  want  them   to  be  implemen ted.  The   checklis t  follows  a  similar   appr oach   to  the  NIST  piece   in  the  sense   that  there  is  a  corresponding    mapping   to  the  AI  lifecycle,  as  articula ted  in  six  steps:  envision,   define,   prototype,   build,   launch,    and  evolve.  Though   they  conclude   by    recognizing   that  a  procedur e  alone   cannot   overcome   the   value t ensions and inc ompa tibilities in e thical pr actice.    Social   media   platforms   have  become   the  place   wher e  people   self-organize  to  raise  issues   that   are  import ant  to  them.   In  this  chap ter,  we  see  how  selectiv e  filtering   and  content  moder ation   policies   can  dispr oportiona tely  negatively  impact   minorities.   We  see  an  example   wher e  making    small   chang es  to  one’ s  profile   on  TikTok  to  include   phrases   like  “Black   Lives  Matter”  can  lead   to   flags   but  it  isn’t   the  same   with   whit e  supr emacy .  Such   biases   are  not  just  limit ed  to  profile    informa tion,   for  example,   on  Facebook,   videos   with   Black   men   were  wrongly   tagged  with    “gorilla”   making   recommenda tions   to  watch  videos   with   animals.   This  is  a  stark  demons tration   of  how  AI  systems,   even  ones   built   by  organizations   on  data  reposit ories   with   huge  amoun ts  of   user -uploaded c ontent can fail spect acularly when ther e aren’t appr opria te guar drails in place.    Datasets  like  C4  that  are  used   to  train  really   large  machine   learning   models   like  T5  and  Switch   Transformer   are  known  now  to  have  under gone  severe  filtering   that  dispr oportiona tely   remo ved  content  from  LGBTQ+  communities.   This  has  direct  impacts   in  terms   of  how  strictly    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 2 1   
 content  with   that  material   migh t  be  policed   since   it  won’t  have  good  represen tation  when   these    pretrained models ar e used in c ontent moder ation.    Bias  remains   an  extremely   import ant  area  of  resear ch  and  is  deeply   contextual  requiring   lots  of   interdisciplinar y  resear ch  before  we  get  to  a  place   wher e  we  can  effectiv ely  put  these   ideas   into   practice.   This  chap ter  has  some   fascina ting  examples   from  the  work  of  Cynthia  Dwork  on   “fairness   through   awareness”   and  work  that’s  been   done   at  Vimeo   to  uncover  biases   in  sear ch   and  recommenda tion  systems.   I  hope   you  find  this  chap ter  insigh tful  and  wide-r anging   beyond   the mos t commonly c overed ar eas in the discussions about bias in AI s ystems.    Abhishek Gupta (  @atg_abhishek  )   Founder, Director, & Principal Researcher   Montreal AI Ethics Institute   Abhishek  Gupta  is  the  Founder,  Director,  and  Principal  Researcher  at  the   Montreal  AI  Ethics  Institute.  He  is  a  Machine  Learning  Engineer  at  Microsoft,   where  he  serves  on  the  CSE  Responsible  AI  Board.  He  also  serves  as  the  Chair   of the Standards Working Group at the Green Software Foundation.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 2 2   
 Go Deep: Research Summaries   “A  Proposal   for  Iden tifying   and  Managing   Bias   in  Artificial   Intellig ence” .  A   draft from the NIS T   [  Original paper   by Reva Sch wartz, Leann Do wn, Adam   Jonas, Elham T abassi]    [Resear ch summar y by Connor W right]   Overview  :  Wha t  does   bias  in  an  AI  system  look   like?  Is  it  obvious?   How  can  we  mitig ate  such    threats?  The  NIST  provides   a  3-stage  frame work  for  mitig ating  bias  in  AI,  with   it  being   seen   as   key  to  building   public   confidence   in  the  technology .  Not  only   can  such   mitig ation  help   us  better   reduce   the  effects  of  AI,  but  it  can  also  help   us  better  under stand  it,  and  the  NIST  wants  to  do   just tha t.   Introduction    Wha t  does   bias  in  an  AI  system  look   like?  If  we  saw  it,  would   we  be  able   to  mitig ate  it?  The   National   Institut e  of  Standar ds  and  Technology   (NIST)  tries   to  answer  both   of  those   ques tions   as   part  of  their   pursuit  for  a  frame work  for  responsible   and  trustworth y  AI.  Mitig ation,    transpar ency ,  and  public   engagemen t  are  widely   accep ted  as  popular   notions   for  building   public    trust  in  AI.  For  me,  the  mos t  exciting   poin ts  in  the  NIST’s  draft  are  their   interaction   with   bias  as  a   concep t  and  their   3-stage  frame work.  With   bias  proving  one  of  AI’s  biggest  problems,   such    frame works can be tter expose this pr oblem and be tter under stand it.    The pr oblem of bias    It’s  import ant  to  note  how  automa ted  biases   can  spread  more  quickly   and  affect  a  wider    audience   than   human   biases   on  their   own.  Rather   than   being   confined   to  those   you  interact   with,   the  presence   of  AI  systems  that  stretch  across  the  globe   means   that  those   affected  by  its   negative  consequences   are  more  numer ous.   Its  effects  are  then   heigh tened   through   AI’s   presence   (and   further   potential  presence)   in  our  lives.  For  example,   the  proliferation  of  facial    recognition   technology   and  AI  being   used   in  job  screening.   As  a  result,   the  NIST  finds   it   necessar y to investigate ho w this c an come about, and I wholeheart edly agr ee.   Why is this the c ase?    Bias  can  be  seen   to  creep  in  when   the  object   of  study   can  only   be  partially   captured  by  the  data,   such   as  a  job  applic ation.   Here,  aspects   such   as  the  value   gained   from  work  experience   and  how   it transla tes in to the ne w role c annot be acc ounted for by jus t a simple k eyword sear ch.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 2 3   
 At  times,   bias  also  enters  into  the  fray  through   AI  decisions   being   made   using   accessible   rather    than   suitable   data.  Here,  resear cher s  are  said  to  “go  wher e  the  data  is”  and  formula te  their    ques tions   once   they  get  there,  rather   than   taking   comple te  account  of  the  necessar y  data  for  an   informed   and  represen tative  AI  system.  For  example,   it  would   be  as  if  you  were  to  look   at  a   colleg e  applic ation  and  solely   focus   on  the  academic   data  (grades)   available,   rather   than   also   looking a t the e xtra-curricular activities the c andida te has undert aken.   To  try  and  tackle   this,   the  NIST  proposes   a  3-stage  lifecycle  to  better  locate  how  AI  can  enter  the   pictur e.   Stage 1: Pr e-design    Here,  the  technology   is  “devised,   defined   and  elabor ated,  “  which   includes to  involve  then    framing   the  problem,   the  resear ch,  and  the  data  procur emen t.  Essen tial  notions   to  consider   can   then   be  seen   in  iden tifying  who’ s  responsible   for  making   the  decisions   and  how  much   control   they  have  over  the  decision-making   process.   This  allows  for  a  more  eviden t  tracking   of   responsibility   in  the  AI’s  developmen t  and  exposes   the  presence   of  any  “fire,  ready ,  aim”    strategies.   Wha t  is  mean t  by  this  play  on  words  is  how,  at  times,   AI  systems  are  often  deplo yed   before  they’ve  been   adequa tely  tested  and  scrutinised.   The  second  stage  then   becomes   even   more relevant.   Stage 2: Design and de velopmen t   Usually   involving   data  scien tists,  engineer s  and  the  like,  this  stage  consis ts  in  the  engineering ,   modelling   and  evalua tion  of  the  AI  system.  Here,  the  context  in  which   the  AI  will  be  deplo yed   mus t  be  taken  into  account.  Simply   deplo ying  an  accur ate  model   does   not  automa tically   mitig ate  any  problem   of  bias  without   this  essen tial  componen t.  This  is  to  say,  a  facial    recognition   system  could   be  95%   accur ate  in  iden tifying  the  faces   of  childr en  between  5-11    years old, but being deplo yed in an adult c ontext will r ender it useless.    In  this  sense,   techniques   such   as  “cultur al  effectiv e  challeng e”  can  be  pursued.   This  is  a   technique   for  creating  an  environmen t  wher e  technology   developer s  can  activ ely  participa te  in   ques tioning   the  AI  process.   This  better  transla tes  the  social   context  into  the  design   process   by   involving   more  people   and  can  prevent  issues   associa ted  with   “target  leakage”.  To  explain,    “target  leakage”  is  wher e  the  AI  trains  on  data  that  prepar es  it  for  an  alterna tive  job  than   the   one  it  initially   intended   to  comple te.  To  illustrate,  training   on  past  judicial   data  and  learning   the   decision-making   pattern  of  the  judg es  and  not  the  reasons   for  conviction.   If  such   problems   can   then   be  avoided,   the  deplo ymen t  stage  will  be  less  likely  to  run  into  any  issues.   However,  this  is   not alw ays the c ase.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 2 4   
 Stage 3: Deplo ymen t   The  deplo ymen t  stage  is  probably   the  mos t  likely  stage  for  any  harm ful  bias  to  emer ge,   especially   given  how  the  public   now  starts  to  interact  with   the  technology .  Given  AI’s   accessibility ,  such   interaction   can  also  include   malicious   use  on  behalf   of  an  unin tended    audience,   such   as  using   chatbot   technology   to  spread  fake  news  online.   Even  if  this  wasn’t    intentional,   the  gener al  interaction   by  the  public   could   also  expose   any  problems   to  do  with   the   technology .   This  shouldn’t   be  the  case,  however.  Any  such   problems   should   instead  be  dealt   with   in  the  2   previous   stages,  but  the  current  AI  ecosystem  is  geared  towards  treating  the  deplo ymen t  phase    as  the  testing  phase.   While   this  continues   to  be  the  case,  the  response   to  AI  bias  will  not  be   mitig ation but r ather a dela yed reaction.    Between the lines    For  me,  gener ating  this  kind   of  frame work  is  definitely  the  right  way  to  go.  Having  defined    stages  of  the  AI  lifecycle  can  mak e  the  iden tification  of  responsible   parties   easier   to  manag e  and   better  expose   how  bias  enters  into  the  process.   In  my  view,  any  appr oach   to  mitig ating  bias  has   to  then   involve  the  member s  of  the  social   context  in  which   it  will  be  deplo yed.  Such    involvemen t  can  then   lead   to  a  more  elabor ate  and  deeper   under standing   of  the  socie tal   implic ations   of  AI,  rather   than   leaving  that  up  to  a  select   few  in  the  design   process.   This   technology   is  at  its  best  when   it’s  represen tative  of  all,  rather   than   simply   trying  to  represen t  all   through the e yes of the f ew.   Co-Designing   Checklis ts  to  Under stand  Organizational   Challeng es  and   Opportunities ar ound F airness in AI    [  Original   paper   by  Michael   A.  Madaio ,  Jennif er  Wortman   Vaughan,   Luke  Stark  and  Hanna    Wallach]    [Resear ch Summar y by Anne Boily]    Overview  :  Among   the  burgeoning   literature  on  AI  ethics   and  the  values   that  would   be   import ant  to  respect   in  the  developmen t  and  use  of  artificial   intelligence   systems  (AIS),   fairness    comes   up  a  few  times,   perhap s  as  an  echo   of  the  very  current  notion   of  social   justice.   Author s   Madaio ,  Vaughan,   Stark  and  Wallach   (Micr osoft   Resear ch)  have  co-de veloped   a  checklis t  that   seek s  to  ensur e  fairness,   while   recognizing   that  a  procedur e  alone   cannot   overcome   the  value    tensions and inc ompa tibilities in e thical pr actice.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 2 5   
 Introduction    Anyone  interested  in  the  ethics   of  artificial   intelligence   is  aware  of  this:   a  plethor a  of  position    paper s  on  AI  ethics   have  emer ged  in  the  last  five  years.  They  come   from  private  companies,   civil   socie ty, univ ersities, as w ell as g overnmen tal and in terna tional or ganizations.    Author s  Michael   A.  Madaio ,  Jennif er  Wortman   Vaughan,   Luke  Stark,  and  Hanna   Wallach    (Micr osoft   Resear ch)  noted  this  sort  of  buzz   around   AI  ethics,   while   remarking   that  the  level  of   abstraction of man y of these s tatemen ts posed pr oblems f or their pr actic al applic ation (p.1).    To  avoid  this  pitfall,  these   resear cher s  participa ted  in  the  developmen t  of  an  equity   checklis t   with   48  AI  practitioner s  from  a  dozen  companies   working   in  a  variety  of  AI  applic ations   (pp.1,   5).   Through   semi-s tructur ed  interviews  as  well  as  “[ … ]  an  iterative  co-design   process   [ …]”  (p.1),    the author s were guided b y thr ee resear ch ques tions:    “RQ1: What are practitioners’ curren t proc esses f or iden tifying and mitigating AI f airness issues?    RQ2: What are practitioners’ desiderat a and c oncerns regarding AI f airness checklis ts?   RQ3:  How  do  practitioners   envision   AI  fairness   checklis ts  migh t  be  implemen ted  within   their    organiz ations? ” (emphasis in the t ext, p.4)    Key Insigh ts   The  disconnect   between  principles   and  practice   is  a  criticism   that  has  been   repea tedly   leveled    at AI e thical guidelines. Madaio e t al. ob viously in tended t o avoid this pitf all.   But  how  do  we  avoid  this  gap  between  ethics   and  technic al  practice   (p.1)?   Even  with   all  the   goodwill   in  the  world,   checklis ts  for  ethical  AI  developmen t  and  deplo ymen t  may  be  poorly    followed  by  practitioner s  or  ignor ed  altogether.  Even  more,  the  items  on  the  checklis t  may   prove inc ompa tible in pr actice (p.2), c onflicting in pot entially irr epar able w ays.   The  author s  are  well  aware  of  this,   so  much   so  that  they  admit   that  “[ … ]  AI  ethics   principles   can   place   practitioner s  in  a  challenging   moral  bind   by  establishing   ethical  responsibilities   to  different   stakeholder s  without   offering   any  guidance   on  how  to  navigate  tradeof fs  when   these    stakeholder s’ needs or e xpect ations c onflict. ” (p.2)    Would   the  solution   to  this  challeng e  of  compr omise   lie  in  a  “technologiz ation”   of  ethics?    Madaio   et  al.  do  not  think   so  (p.2.)   One  should   not  imagine   that  a  simple   answer  to  a  binar y   ques tion  (p.3)   captures  the  comple xity  of  the  ethical  dilemma   in  which   the  developer   may  find   himself .   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 2 6   
 Several  contextual  elemen ts  mus t  be  consider ed,  for  example   the  sector  of  activity   or  resear ch   (public   or  private),  or  the  size  of  the  compan y  (p.10).   Ther e  is  often  disagr eemen t  about   the   definition   of  the  concep ts  themselv es  (p.3).   In  the  context  of  developing   their   equity   checklis t,   Madaio   et  al.  explicitly   ackno wledg e  that  the  very  concep t  of  equity   can  be  under stood   differently  in  different  contexts:  “Fairness   is  a  comple x  concep t  and  deeply   contextual  [and]   [ …]   Ther e  is  no  single   definition   of  fairness   that  will  apply   equally   well  to  different  applic ations   of   AI” (p.15). F or example, equity c an be under stood in per sonal or or ganizational t erms (p.5).    The  checklis t  proposed   by  Madaio   et  al.,  based   on  a  previously   developed   checklis t  model   (p.4)    and  modified   according   to  the  co-design   workshop s  with   the  participan ts,  consis ts  of  six  main    steps,  which   roughly   correspond   to  the  developmen t  of  an  artificial   intelligence   system   (pp.16-20):    1. “En vision”    2. “De fine”    3. “Pr ototype”    4. “Build”    5. “Launch”    6. “E volve”   At  all  stages  of  the  checklis t,  it  is  necessar y  to  ensur e  that  the  criterion   of  fairness   can  be   respect ed  or,  if  compr omises   are  necessar y,  to  documen t  them   and  to  consider   dropping   the   project   if  this  would   be  preferable   (pp.16-20).   This  proposal   guar antees  a  great  hones ty  in  the   developmen t  of  AIS,  orien ting  the  appr oach   not  only   towards  the  maximiz ation  of  efficiency ,   but also t owards the c ommon g ood, enc apsulated for these author s in the v alue of equity .   For  Madaio   et  al,  dialogue   is  central  to  the  use  of  this  list  (p.16).   The  discussion   mus t  involve   stakeholder s  as  diverse  as  the  people   who   will  use  the  technology ,  who   will  be  affected  by  it,   the  practitioner s  who   develop   it,  their   teams,   and  experts   to  consult   at  different  stages  of  the   process.   Hence,   the  author s  suggest  that  “[ … ]  the  mos t  bene ficial   outcome   of  implemen ting  an   AI  ethics   checklis t  may  be  to  promp t  discussion   and  reflection   that  migh t  other wise   not  take   place. ” (p.3)    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 2 7   
 Another   advantage  of  the  checklis t,  according   to  the  author s,  is  that  it  would   mak e  it  possible   to   establish   a  preventive  rather   than   a  reactiv e  ethic,   which   would   have  the  mission   of  anticipa ting   ethical  glitches,   while   being   adap ted  to  the  oper ating  modes   of  the  practitioner s.  With   such   a   tool, w e could possibly see a r eduction in an xiety among de veloper s (p. 6-7).    That  said,   the  use  of  the  checklis t  does   not  guar antee  the  eradication  of  equity   problems,   but   their   prevention  and  mitig ation,   as  much   as  possible   (p.15).   In  other   words,  one  cannot    eradicate  tensions   or  value   clashes   in  practice,   but  one  can  seek   to  minimiz e  the  negative   effects  of  the  trade-of f  one  has  found   (cf.  Blattberg  2018,   151).   This  view  is  not  unlik e  the   philosophic al  school   of  “value   pluralists”  such   as  Isaiah   Berlin,   Bernar d  Williams,   or  Stuart    Hamp shire.   One  caveat,  noted  by  the  participan ts  in  this  study ,  is  that  the  checklis t  may  be  used   merely  as  a   formal,   “minimal”   process   (p.  8),  rather   than   as  a  means   of  gener ating  deep   conversations    about   the  ethical  implic ations   of  the  technology   being   developed   (p.8).   It  is  import ant  to  clarif y   the  role  of  the  checklis t.  While   it  serves  as  a  tool  for  discussion   in  the  implemen tation  of  ethics,    it  is  not  under stood  in  a  fully   procedur al  way.  Indeed,   this  procedur al  under standing   could   be   problema tic,  as  one  study   participan t  noted:  “[ … ]  ‘I’m  a  little  bit  suspicious   of  the  checklis t   appr oach.   I  actually   tend  to  think   that  when   we  have  highly   procedur al  processes   we  wind   up   with r eally pr ocedur al under standings of f airness’” (p.8).    The  dang er  is  there  and,   basic ally,  it  is  difficult   to  reduce   the  richness   of  a  concep t  such   as   equity   to  a  definition   and  a  procedur e.  The  author s  hear d  these   concerns   and  adap ted  their    model   accordingly:   “[ … ]  our  checklis t  items  are  intended   to  promp t  critic al  conversations,   using    words  like  ‘scrutiniz e’  and  asking   teams   to  ‘define  fairness   criteria’   rather   than   including   specific    fairness crit eria or thr esholds t o mee t” (p.8).    Between the lines    Some   will  be  skeptical  of  a  proposal   such   as  the  Madaio   et  al.’s  checklis t,  since   it  does   not   appear   to  “fix  the  problem”   of  AI  ethics   once   and  for  all.  On  the  contrary,  this  checklis t  would    rather   refer  to  “[ … ]  a  way  to  spur   ‘good  tension, ’  promp ting  critic al  conversations   and  prying   open   discussion   about   AI  fairness   [ …]”  (p.10).   These   conversations   will  be  made   possible   by  the   organizational cultur e – an elemen t tha t should not be o verlook ed (p.10).    To  try  to  solve  all  ethical  problems   in  advance   by  means   of  a  procedur e  is  probably   idealis tic.  As   the  author s  rightly  suggest,  “[t]her e  are  seldom   clear -cut  answers.  It  is  therefore  import ant  to   documen t  your  processes   and  consider ations   (including   priorities   and  tradeof fs),  and  to  seek    help when needed” (p.15).    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 2 8   
 Does   this  mean   that  AI  ethics   is,  in  a  certain  way,  a  moving  target  that  no  practitioner ,  theor y,  or   procedur e  can  immobiliz e  once   and  for  all?  If  that  is  the  case,  then   dialogue   does   seem   to  be  a   good  solution   to  practice   ethics   in  the  specificity   of  each   context.  I  would   add  to  this  that  the   virtue of prudence w ould be a g ood guide f or this type of discussion.    Note  :  To  facilit ate  the  location  of  the  original   informa tion,   the  page  number s  for  the  checklis t   are those of the PDF documen t (the c ontinua tion of the article).    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 2 9   
 Go Wide: Article Summaries  (summariz ed b y Abhishek   Gup ta)   How  TikTok’s  hate  speech   detection   tool  set  off  a  deba te  about   racial   bias   on the app    [Original article b y  Vox  ]   Wha t  happened   :  TikTok  is  under   fire  again  (as  covered  in  a  previous   newsletter  wher e  it   involun tarily   chang ed  people’ s  faces)   for  flagging   content  that  dispr oportiona tely  impacts   Black    creators.  The  creator  featured  in  the  article   men tions   how  while   editing   his  bio  on  the  platform,    his  content  was  flagged  as  inappr opria te  for  including   phrases   like  “Black   Lives  Matter.”  Phrases    like  “whit e  supr emacy ”  didn’t   have  a  similar   effect.  The  creator  called   for  strikes  and  his  videos    have  had  more  than   a  million   views  with   fellow  Black   creators  under standably   agitated  about    dispr oportiona te  harm.   The  compan y  explained   that  such   content  wasn’t   against  policy   but   their c ontent moder ation s ystems needed impr ovemen t to addr ess these challeng es.   Why  it  matters  :  In  a  COVID-19   world  wher e  a  lot  of  activism   is  taking   place   online,   such    inciden ts  impact   historically  marginaliz ed  people   even  more  by  stripping   away  their   ability   to   organize  and  express  their   views.  Mor e  so,  it  showcases   how  current  automa ted  systems  for   content  moder ation  are  quite  brittle  and  unable   to  handle   variances   in  the  text  wher e   seemingly inappr opria te content migh t actually be used t o highligh t and r espond t o key issues.    Between  the  lines   :  With   the  rise  of  people   using   social   media   platforms,   human   content   moder ation  is  only   going   to  decr ease   over  time   since   it  is  infeasible   to  check   all  the  content  that   goes  up  on  these   platforms   every  minut e.  We  need   to  have  resear ch  into  more  robus t   automa ted  methodologies   and  rely  on  community -driv en  moder ation  as  an  intermedia te  to  still   have some human-in-the-loop elemen ts.   We tested AI in terview tools. Her e’s wha t we found.    [Original article b y  MIT T echnology R eview  ]   Wha t  happened   :  With   a  lot  of  uphea val  in  the  job  mark et  since   the  start  of  the  pandemic,   and   limit ed  staff  capacities   on  the  recruitmen t  side  of  things   for  companies,   man y  have  resort ed  to   the  use  of  automa ted  hiring   tools.   The  author s  of  the  article   put  two  such   systems  to  the  test,   CuriousThing   and  MyIn terview  to  gain  an  under standing   on  how  good  they  are  in  mee ting  their    claims.   To  their   disappoin tmen t  and  no  surprise,   they  found   these   two  tools  to  be  opaque   in   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 3 0   
 wha t  they  evalua ted.  As  an  example,   even  when   reading   out  paragraphs   in  German,   the  tool   determined   that  the  interviewee,  one  of  the  author s,  was  quite  fluen t  in  English.   It  also   evalua ted them on the Big Fiv e per sonality tr aits and other a ttribut es with v arying r esults.    Why  it  matters  :  These   tools  are  touted  as  a  way  to  reduce   bias  in  the  hiring   process   and  often   the  results   from  such   tools  are  not  the  only   data  poin ts  in  hiring   decisions.   But,  they  have  the   potential  to  skew  the  process,   especially   when   they  have  glaring   flaws  and  high   sensitivity   in   how  they  evalua te  some   of  these   psychologic al  traits  using   things   like  the  intonation  of   someone’ s voice r ather than the c ontent of wha t the y are saying.    Between  the  lines   :  While   the  founder   of  the  compan y  defended   that  the  system  is  not  mean t  to   be  used   with   German   and  hence   the  flawed  results,   it  still  raises   an  interesting  ques tion  on  wha t   the  degr ee  of  robus tness   of  these   systems  is,  particularly   when   they  are  used   in  the  wild,   as   opposed   to  a  controlled   experimen t  in  this  article,   wher e  there  migh t  not  be  an  opportunity   to   review  how  a  certain  person  responded.   This  can  lead   to  pre-emp tive  dismissal   in  a  large  pool   of   applic ants  in  the  interest  of  expediency ,  particularly   affecting   those   who   don’t   fit  the  mold   that   is de termined t o be ideal b y the aut oma ted system.   How Humans Can F orce the Machines t o Pla y Fair   [Original article b y  Quan ta Mag azine   ]   Wha t  happened   :  In  this  insigh tful  interview  with   the  inventor  of  the  notion   of  differential   privacy,  we  learn   about   the  new  challeng es  that  Dwork  is  embarking   on  in  her  recen t  work.   Tackling   fairness   in  AI-in fused   systems,   Dwork  talks  about   her  work  titled   “Fairness   through    Awareness”   which   takes  into  account  both   individual   and  group  fairness   and  how  to  achie ve   both.   She  also  talks  about   how  this  is  a  much   more  difficult   challeng e  compar ed  to  her  work  in   privacy  but  advocates  taking   a  “sunshine”   appr oach   to  the  resear ch  work  in  this  space.   The   article   also  has  several  great  examples   of  how  applying   individual   fairness   isn’t   enough   and  how   her  experience   with   piano   practice   reinforced  the  import ance   of  consider ations   for  fair   affirma tive action t o achie ve group f airness.    Why  it  matters  :  The  field   of  AI  ethics   is  inunda ted  with   work  on  how  to  best  achie ve  fairness   in   machine   learning.   Yet,  a  lot  of  it  struggles  to  articula te  how  to  account  for  tradeof fs  that  are   bound   to  occur   when   offering   preferential  treatmen t  to  some   over  other s  in  the  interest  of   achie ving  fairness.   Dwork’s  work  and  her  history  in  providing   clear   metrics   and  methodologies    for  addr essing   challeng es  in  achie ving  more  responsible   statistical  systems  is  a  good  preceden t   and beac on for us t o mak e meaningful pr ogress in this space.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 3 1   
 Between  the  lines   :  Bias  and  fairness   are  extremely   challenging   concep ts  when   it  comes   to   machine   learning   because   they  don’t   have  clear   metrics   as  is  the  case  with   privacy  wher e  we   are  fairly   certain  of  wha t  the  outcomes   need   to  be.  In  this  case,  there  is  widespr ead   disagr eemen t  even  about   wha t  fair  outcomes   look   like  presen ting  us  with   graver  challeng es.   Work  that  claims   to  provide   easy  solutions   is  certainly   some thing   to  guar d  against,  especially   in   the  face  of  this  space   becoming   commer cializ ed  with   startup s  and  tools  being   offered  that   addr ess, or w orse “fix ”, bias in machine learning.    The Secr et Bias Hidden in Mortg age-Appr oval Alg orithms    [Original article b y  The Mark up  ]   Wha t  happened   :  Ther e  are  strong  biases   against  people   of  color  in  lending   decisions   made   by   financial   institutions   in  the  US  as  found   out  by  a  recen t  study   conduct ed  by  The  Mark up  on  data   from  2019.   They  found   that  even  after  controlling   for  new  factors  that  are  supposed   to  tackle    racial   disparities,   the  differences   persisted.  Even  those   with   very  high   income   levels  ($100,000+)    with   low  debt  were  reject ed  over  Whit e  applic ants  with   similar   income   levels  but  higher   debt.   This  analy sis  was  sent  to  the  Americ an  Bank ers  Associa tion  and  the  Mortg age  Bank ers   Associa tion  both   of  whom   denied   the  results   from  the  study   citing   that  there  were  missing    slices   of  informa tion  in  the  public   data  used   by  The  Mark up  thus   making   the  results   incorrect.   But,  they  didn’t   poin t  out  specific   flaws  in  the  analy sis.  Some   of  that  data  is  not  possible   to   include   in  the  analy sis  because   the  Consumer   Financial   Protection   Bureau  strips  it  to  protect   borr ower priv acy.   Why  it  matters  :  While   there  are  laws  like  the  Equal   Credit  Opportunity   Act  and  the  Fair  Housing    Act  that  are  supposed   to  protect  against  racial   discrimina tion,   with   organizations   like  Freddie    Mac   and  Fannie   Mae   driving   how  loans   are  appr oved  through   their   opaque   rating  systems,   it  is   very  difficult   to  override   decisions   made   by  automa ted  systems  as  men tioned   in  the  case  of  the   person  discussed   in  the  article   who   was  denied   a  loan   at  the  last  momen t,  unresolv able   by  15  or   so loan of ficer s who also look ed a t the loan applic ation.    Between  the  lines   :  The  algorithms   being   used   by  these   organizations   date  back   to  more  than   15   years  and  reward  more  heavily  traditional   credit  which   Whit e  people   have  more  access   to.  They   also  unfairly   penaliz e  structur al  elemen ts  like  missing   paymen t  reports   filed   by  payday  lender s   who   are  dispr oportiona tely  presen t  in  neighborhoods   with   people   of  color  thus   skewing   the   data  on  bad  financial   beha vior  while   ignoring   good  financial   beha vior  such   as  paymen t  on  time    of  utility   paymen ts.  The  lack  of  transpar ency   on  the  part  of  organizations   like  Freddie   Mac   and   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 3 2   
 Fannie   Mae,   and  the  protections   that  they  have  in  not  disclosing   outcomes   from  their   systems   in  public   data  and  the  opaqueness   around   their   evalua tion  methodology   will  continue   to   exacerba te the pr oblem.    Facebook Apologiz es Aft er A .I. Puts ‘Prima tes’ Label on Video of Black Men    [Original article b y  NYTimes   ]   Wha t  happened   :  Facebook   provides   automa ted  recommenda tions   for  videos   and  other   content   on  its  platforms   as  user s  consume   content.  In  a  particularly   egregious   error,  the  platform   showed  a  messag e  promp ting  the  user   if  they  wanted  to  see  more  “keep  seeing   videos   about    Prima tes”  when   the  video   that  they  were  watching   was  in  fact  a  video   of  a  few  Black   men    having  an  altercation  with   some   Whit e  people   and  police   officer s.  The  video   had  nothing   to  do   with   prima tes  wha tsoe ver.  The  spok esper sons   for  the  compan y  said  that  they  are  doing   a  root   cause analy sis to see wha t migh t have gone wr ong.    Why  it  matters  :  The  use  of  an  AI  system  trained   without   guar drails,  especially   when   there  are   known  issues   of  bias  and  racism   due  to  the  outputs   from  the  system  shows  that  there  isn’t   yet   enough   being   done   to  mitig ate  these   issues   that  can  instantly  impact   millions   of  people   due  to   their   scale  and  pace.   Facebook   as  a  platform  has  the  largest  reposit ory  of  user -uploaded    content  and  it  uses   that  to  train  its  AI  systems.   But,  this  recen t  inciden t  demons trates  that  there   is  much   more  to  be  done   before  this  becomes   some thing   that  we  can  safely  deplo y,  if  we  ever   get ther e.   Between  the  lines   :  Given  the  large  number   of  problems   that  automa ted  recommenda tion   systems  have  today,  it  is  a  bit  surprising   that  these   are  still  used   in  deplo ymen t.  Wha t  would   be   interesting  to  analy ze  is  the  extent  to  which   these   companies   are  willing   to  overlook   such    inciden ts  in  the  interest  of  the  gains  that  they  get  from  keeping   people   engaged  on  the  platform   when   the  recommenda tions   do  work.  Because   the  external   resear ch  community   and  the  public    have  no  visibility   on  this  tradeof f,  it  is  increasingly   difficult   to  hold   such   organizations    accountable   for  such   errors  and  to  recommend   correctiv e  actions   when   it’s  not  entirely  clear    how  the  system  is  being   built   and  oper ated  and  wha t  the  internal   costs  and  bene fits  analy sis   look s like.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 3 3   
 Minority   Voices   ‘Filtered’  Out  of  Google   Natural  Languag e  Processing    Models    [Original article b y  Unit e  ]   Wha t  happened   :  The  article   spotligh ts  some   findings   from  a  recen tly  published   report   that   analy zed  the  filters  that  went  into  creating  the  C4  (Colossal   Clean   Crawled   Corpus)   dataset,  a   subset  of  the  much   larger  Common   Crawl  (CC)  dataset.  The  C4  was  used   to  train  Google’ s  T5   and  Switch  Transformer ,  two  massiv e  languag e  models   that  are  used   in  downstream   products    and  services.   The  essence   of  the  findings   were  that  in  creating  a  non-t oxic  dataset,  the   aggressiv e  filtering   excluded   material   related  to  LGBTQ+  communities   in  non-se xual  and   non-of fensiv e  contexts  along   with   a  heavy  filtering   of  colloquial   and  ethnicity -specific   dialects    like Afric an-Americ an and Hispanic-aligned English.    Why  it  matters  :  One  of  the  reasons   for  poor   performance   of  large  languag e  models   on   non-politic al,  non-of fensiv e,  non-se xual  material   that  discusses   LGBTQ+  communities   is  that   there  is  no  represen tation  of  them   in  these   curated  datasets,  or  when   it  is  there,  it  is  heavily   filtered.  This  has  the  impact   of  much   stronger  automa ted  content  moder ation  applied   to  that   content  compar ed  to  other s  on  social   media   platforms.   Other   products   and  services   that  also   consume   such   pretrained   models   for  oper ations   then   suffer  from  biases   because   data  related  to   these   areas  and  dialects   is  excluded,   rather   than   coming   up  with   better  appr oaches   to   moder ation tha t don’t jus t rely on a banned lis t of w ords.   Between  the  lines   :  One  of  the  striking   things   about   the  resear ch  efforts  that  led  to  the  report   is   that  they’ve  made   the  raw  data  available   for  C4  and  provided   different  versions   of  it  with    various   levels  of  filtering   applied   for  people   to  further   analy ze  the  data.  Even  though   the  original    author s  of  C4  (from  Google)   made   the  scrip ts  available   for  that  dataset,  the  comput ational   costs   are  so  high,   that  recreating  the  C4  from  CC  would   be  out  of  reach   for  man y  resear cher s.  Not   only   do  these   minority   communities   suffer  from  the  biases   against  them   in  content  moder ation,    because   of  such   misdir ected  filtering ,  they  stand  to  miss   out  on  legitima te  bene fits  from  ML  like   machine   transla tion  and  sear ch.  As  the  author s  of  the  study   rightly  poin ted  out,  we  need   to  do   better in t erms of ho w we process da ta bec ause it has signific ant downstream e ffects.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 3 4   
 Facebook,   Citing   Socie tal  Concerns,   Plans   to  Shut   Down  Facial   Recognition    System   [Original article b y  NYTimes   ]   Wha t  happened   :  In  a  move  to  “find   the  right  balance, ”  Facebook   is  going   to  be  deactiv ating   facial   recognition   technology   within   its  ecosystem  citing   concerns   with   how  this  technology   is   used   and  wha t  it  powers.  With   the  recen t  rebrand  to  Meta,  Facebook   is  on  a  warpa th  to  set  its   public   imag e  right.  The  feature  was  used   to  power  automa tic  tagging   of  people   in  uploaded    pictur es  and  videos,   some thing   that  would   help   the  network  deepen   connections   and  mak e  it   more  frictionless   for  user s  to  associa te  their   account  with   visual   asse ts  on  the  site.  The   technology   was  also  used   to  power  capabilities   to  detect  if  someone   migh t  be  imper sona ting   you  on  the  site  and  to  provide   accessibility   features  like  reading   out  descrip tions   of  phot os  for   blind user s.   Why  it  matters  :  Given  the  fines   that  the  compan y  faced   from  the  FTC  and  the  state  of  Illinois    citing   viola tions   of  privacy,  this  is  a  win  for  privacy  advocates  to  get  Meta  to  shut   down  this   feature.  Appr oxima tely  1  billion   facial   recognition   templa tes  will  also  be  dele ted  from  the  site   and  there  are  talks  about   controlling   pictur es’  visibility   as  well  to  limit   how  external   companies    like PimE yes and Clear viewAI c an use these asse ts to train f acial r ecognition t echnology .   Between  the  lines   :  Despit e  this  announcemen t,  some thing   of  note  in  the  article   is  that  Meta   has  not  ruled   out  comple tely  the  use  of  facial   recognition   technology   in  futur e  products.   Though    the  recen tly  released   glasses   in  partner ship  with   Ray-Ban   don’t   have  it,  this  doesn’t   mean   that   futur e  products   will  never  again  have  facial   recognition   technology .  We  also  need   to  continue   to   pay  attention  to  how  this  data  that  has  been   collect ed  will  be  remo ved  and  how  other   policies    chang e  on  Meta  and  related  sites  like  Instagram  which   continue   to  be  the  largest  reposit ories   of   facial imag es in the w orld.    Uncovering bias in sear ch and r ecommenda tions    [Original article b y  Vimeo Engineering   ]   Wha t  happened   :  The  team   at  Vimeo ,  the  video   streaming   platform,   talks  about   their   work  in   assessing   bias  in  the  sear ch  results   and  the  recommenda tions   that  they  provide   to  the  user s  of   their   platform.   They  do  so  for  gender   bias  as  an  entry  poin t  to  this  assessmen t  and  utiliz e  LTR   (Learning   to  Rank)   and  BM25   appr oaches   as  the  underlying   sear ch  results   ranking   comparing    results   from  gender -neutr al  sear ch  terms   and  checking   for  the  presence   of  gender ed  terms   in   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 3 5   
 the  returned   result   list  and  the  ordering   of  those   results.   Chock -full  of  technic al  details,  one  of   the  things   that  stand  out  in  the  article   is  an  interesting  challeng e  on  ground   truth   labels,   which    are  hard  to  get  because   the  relevance   of  sear ch  results,   especially   for  items  saved  in  a  library   are  highly   specific   to  the  user   themselv es  and  hence   it  is  difficult   to  gener alize  to  the  broader    user   base   from  that.  So,  they  used   click s  to  form  the  signal   for  the  ground   truth   in  the   super vised learning t ask.   Why  it  matters  :  The  way  the  experimen ts  are  run  (e.g.  presen ting  two  variations   of  blue   text  for   buttons  and  judging   which   user s  prefer,  only   provides   informa tion  about   user s’  preferences   for   blue   buttons  and  nothing   about   green  buttons)   and  how  data  related  to  interactions   is  collect ed   can  have  a  tremendous   impact   on  downstream   tasks  that  migh t  use  this  data.  In  addition,   the   small   sample   sizes  of  self-declar ed  gender   pronouns   on  the  Vimeo   user   base   and  drawing    conclusions   from  that  to  apply   to  the  broader   user   base   also  poses   challeng es.  For  example,    some   migh t  not  choose   to  iden tify,  the  limit ed  options   of  gender   pronoun   iden tification  offered   by  Vimeo   are  also  ackno wledg ed  by  the  team   that  did  this  analy sis.  But,  this  does   offer  a  great   starting poin t for diving in to ho w bias ma y manif est in sear ch results and r ecommenda tions.    Between  the  lines   :  For  platforms   that  are  even  larger  than   Vimeo ,  the  impact   of  bias  in  wha t   kind   of  results   pop  up  when   a  user   types   in  a  sear ch  result,   and  particularly   how  those   sear ch   results   are  ordered  (think   back   to  how  man y  times   you  navigate  beyond  the  first  page  of  sear ch   results   on  Google)   have  the  potential  to  amplif y  gender   and  other   biases   signific antly  if  click s   and  other   user -driv en  metrics   are  used   to  drive  the  modeling   of  relevance   for  any  downstream    tasks.  Having  more  studies   conduct ed  by  platforms   themselv es  instead  of  by  external    organizations   has  the  upside  that  the  platforms   have  the  deepes t  access   to  all  the  metrics   and   interactions;   of  course,  this  comes   with   the  caveat  that  negative  outcomes   from  such   an   investigation ma y be suppr essed f or business in terests.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 3 6   
 5. Social Media and Pr oblema tic In forma tion   Introduction  by  Abhishek   Gup ta,  Founder   and  Principal   Resear cher ,  Mon treal  AI  Ethics    Institut e   The  scourge  of  problema tic  informa tion  continued   throughout   the  second  half  of  2022,   not   much   has  chang ed  unfortuna tely  since   the  public ation  of  our  last  report   .  One  of  the  biggest   players  in  the  space,   Facebook   rebranded   as  Meta  and  domina ted  quite  a  few  news  cycles   towards  the  end  of  2021   with   its  pivot  towards  the  metaverse  (I  am  personally   still  unclear   on   wha t  it  means   exactly   so  if  you  have  a  clear   explana tion  for  it,  feel  free  to  drop  me  a  line  at   abhishek@mon treale thics.ai   )   The  opening   piece   in  this  chap ter  talks  about   deep fakes  which   continue   to  form  a  chunk   of  the   problema tic  informa tion  spreading   around   on  social   media   and  it  mak es  the  case  for  finding    those   with   capabilities   to  better  analy ze  these   deep fakes  and  not  only   find  countermeasur es   but also w ays to curb their spr ead in the fir st place.    The  piece   on  how  targeted  ads  can  divide   us  even  when   they’re  not  politic al  was  eye-opening;    mos t  of  the  time   our  focus   is  on  politic al  ads  but  harms   can  emer ge  in  more  subtle  and  equally    pernicious   ways  elsewher e  too.  That  said,   it  goes  without   saying  that  politic al  ads  still  caused   a   ton  of  problems   as  a  couple   of  pieces   poin t  out  in  this  chap ter.  One  of  the  pieces   demons trates   the  sheer   scale  of  problema tic  informa tion  wher e  more  than   140  million   people’ s  attention  was   grabbed   by  troll  farms.   For  context,  that  is  almos t  half  the  popula tion  of  the  US.  Documen ting   and  analy zing  the  kind   of  content  spreading   on  Facebook   Meta  isn’t   easy,  especially   as  they   revoked  access   for  groups  studying   problema tic  informa tion  on  the  platform.   The  chap ter   shar es  some   details  on  independen t  efforts  like  the  one  being   run  by  The  Mark up  called   the   Citiz en  Browser  that  gives  them   access   to  untouched   Facebook   feeds   as  participa ting  user s  view   them.   The  analy ses  coming   out  of  that  project   are  definitely  ones   to  keep  your  eyes  on  in  case   you  want  to  go  beyond  wha t  is  just  published   by  the  Meta  team   about   the  state  of  problema tic   informa tion on their pla tform.    Finally ,  three  other   pieces   really   caugh t  my  attention  in  this  chap ter.  The  first  one  on  how  the   underlying   business   models   of  both   Google   and  Facebook   that  rewards  content  creators  steers   malicious   actors  into  areas  wher e  they  not  only   have  politic al  motiv ations   but  also  financial    ones   (enough   to  sustain  their   oper ations!)   that  exacerba tes  the  problem   of  problema tic   informa tion  on  the  platform.   An  initia tive  by  the  US  Governmen t  that  hired  influencer s  to   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 3 7   
 comba t  the  spread  of  vaccine-r elated  misin forma tion  was  particularly   exciting   since   it  taps  into   the  native  dynamics   of  the  platform  to  bring   about   some   positiv e  chang e  rather   than   solely    relying   on  policy   mechanisms.   A  heartbr eaking   piece   concludes   the  chap ter  wher eby  the  queer    interne t  is  gradually   being   erased   comple tely  from  China   and  it  documen ts  the  efforts  of   volun teers  who   dedic ate  their   time   and  face  potential  persecution   in  attemp ts  to  try  and   preser ve tha t and c ontinue t o main tain their social c onnections.    Abhishek Gupta (  @atg_abhishek  )   Founder, Director, & Principal Researcher   Montreal AI Ethics Institute   Abhishek  Gupta  is  the  Founder,  Director,  and  Principal  Researcher  at  the   Montreal  AI  Ethics  Institute.  He  is  a  Machine  Learning  Engineer  at  Microsoft,   where  he  serves  on  the  CSE  Responsible  AI  Board.  He  also  serves  as  the  Chair   of the Standards Working Group at the Green Software Foundation.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 3 8   
 Go Wide: Article Summaries  (summariz ed b y Abhishek   Gup ta)   The W orld Needs Deep fake Experts t o Stem This Chaos    [Original article b y  Wired  ]   Wha t  happened   :  In  Myanmar ,  a  video   made   claims   that  amplified   corrup tion  char ges  against   Aung   San  Suu  Kyi,  but  because   of  its  grainy  quality   and  the  gener al  distrust  in  governmen t,   people   decried   it  as  being   a  deep fake.  People   used   online   deep fake  detectors  to  figur e  out  the   video’ s  authen ticity ,  and  social   media   quickly   made   this  opinion   popular .  The  author   of  this   article   poin ts  out  concerns   in  how  malicious   agents  can  easily   manipula te  untrained   everyday   citizens in to belie ving wha tever the y want as the quality of deep fakes incr ease.    Why  it  matters  :  While   the  risk  from  deep fakes  remains   highes t  for  unwanted,  nonc onsensual    sexual  imag es,  their   use  for  politic al  manipula tion  is  on  the  rise.   Everyday  citizens  unaware  of   the  limit ation  of  deep fake  detection   run  the  risk  of  counter-forensic   techniques   that  inject    artifacts  into  videos   to  confound   these   free,  online   tools.   Encouraging   ama teur  forensics   online    can  lead   people   down  conspir acy  rabbit   holes   exacerba ting  the  problem   of  misin forma tion   online.    Between  the  lines   :  Sam   rightly  poin ts  out  that  more  advanced   capabilities   are  limit ed  to  elite   circles  of  academia,   governmen t,  and  indus try  in  Europe  and  North   Americ a.  We  need   more   funding   and  sharing   of  knowledg e  and  tools  with   other   parts   of  the  world,   especially   those    vulner able   to  such   attacks.  Inequity   in  the  distribution   of  these   capabilities   will  deepen   the   digit al divide acr oss r egions.    After  Repea tedly   Promising   Not  to,  Facebook   Keeps  Recommending    Politic al Gr oups to Its User s   [Original article b y  The Mark up  ]   Wha t  happened   :  In  The  Mark up’s  Citiz en  Browser  project,   which   tracks  the  Facebook   feeds   of   user s  paid   by  The  Mark up  to  send   them   data,  resear cher s  discovered  that  despit e  promises    made   by  Facebook   that  they  will  stop  recommending   politic al  groups  to  user s,  they  haven’t   done   so  yet.  In  several  responses   to  governmen t  agencies   and  in  public,   Facebook   has  claimed    that  they  have  applied   measur es  to  elimina te  such   recommenda tions   but  have  let  slip  on   occasion tha t the y cannot do so en tirely.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 3 9   
 Why  it  matters  :  As  documen ted  in  the  article,   about   two-thir ds  of  the  people   landing   on   politic ally-motiv ated  Facebook   groups  arriv e  there  through   the  recommenda tions   made   by  the   platform  to  its  user s.  If,  even  after  making   public   commitmen ts  to  remedy   that,  we  don’t   see   chang es, then tha t is a c ause f or se vere concern.    Between  the  lines   :  As  we’ve  men tioned   in  this  newsletter  before,  work  from  organizations   like   The  Mark up  can  help   to  hold   companies   like  Facebook   accountable.   But,  this  requir es  funding    and  inno vative  resear ch  methods,   especially   when   there  aren’t  broad-access   APIs   available   to   resear cher s to scrutiniz e the activity on the pla tform.    Targeted  ads  isola te  and  divide   us  even  when   they’re  not  politic al  –  new   resear ch   [Original article b y  The Con versation  ]   Wha t  happened   :  When   we  think   about   divisiv e  ads,  politic al  ads  always  come   first  to  our  minds.    This  article   argues   that  commer cial  ads  post  an  equally   pernicious   threat  to  the  epistemic    integrity   of  our  informa tion  ecosystem  online.   Drawing   on  an  interesting  example   regarding    body   positivity   in  the  London   under ground,   passeng ers  complained   to  the  regula tor  that  the   ads  promot ed  unhealth y  stereotypes   and  promp ted  action   from  the  regula tor  in  taking   down   the  ad.  Yet,  out  of  the  hundr eds  of  thousands   of  passeng ers,  only   387  filed   such   a  complain t,   presumably   some   were  stirred  by  the  graffiti  on  those   ads  promp ting  them   to  take  action   as   well.   Why  it  matters  :  In  the  online   world,   we  are  neatly  segmen ted  into  various   categories   (whe ther    accur ate  and  reflectiv e  of  us  or  not)  that  mak e  it  difficult   to  under stand  and  gain  collectiv e   knowledg e  about   whe ther   some   ads  migh t  be  causing   us  harm   without   us  even  realizing ,  as  was   the  case  with   the  London   under ground   example   which   manif ested  in  the  physical  world.    Commer cial  ads  can  cause   harm   both   through   their   targeted  messaging   towards  vulner able    popula tions   like  showing   gambling   addicts   ads  for  casinos   and  omission   of  ads,  say  job  postings    to only a cert ain g ender .   Between  the  lines   :  Given  that  mos t  of  our  focus   remains   on  tackling   the  problem   of  politic al  ads   on  platforms,   this  article   presen ts  a  compelling   case  for  thinking   more  deeply   about   the  impact    that  commer cial  ads  have  on  us.  Ther e  are  certain  policies   and  regula tions,   in  the  US  for   example   there  are  regula tions   around   disability ,  housing ,  and  emplo ymen t  that  migh t  mak e   some   ads  or  omission   of  ads  illegal,  but  for  the  mos t  part  it  remains   an  under studied   area.  This   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 4 0   
 problem   is  exacerba ted  by  the  fact  that  it  is  incredibly   difficult   to  obtain  the  necessar y   informa tion  across  a  broad  swathe  of  user s  without   enrolling   them   in  a  study   which   can  cost  a   lot of mone y, as is the c ase with the Citiz en Br owser pr oject fr om The Mark up.   Facebook   Tells  Biden:   ‘Facebook   Is  Not  the  Reason’   Vaccina tion  Goal   Was   Missed    [Original article b y  NYTimes   ]   Wha t  happened   :  The  US  had  planned   to  have  70%   of  their   popula tion  vaccina ted  by  July  4,  but   it  fell  short   of  the  target  and  the  Biden   adminis tration  laid  some   of  that  blame   on  the   misin forma tion  spread  on  Facebook   as  a  cause   for  the  vaccine   hesit ancy   in  the  US.  The  platform   responded   by  saying  that  they  have  undert aken  man y  measur es  that  have  helped   to  inform  the   user s  of  Facebook   about   vaccina tion  such   as  notices   and  eradication  of  anti-vaccina tion  ads  on   their   platform.   An  adversarial   dynamic   is  emer ging   between  the  adminis tration  and  the  social    media   platform  as  they  are  frustrated  with   each   other ’s  under standing   of  the  efforts  being    made.    Why  it  matters  :  While   it  is  not  uncommon   for  such   a  divergence   to  emer ge,  the  lack  of   transpar ency   in  the  impacts   of  the  efforts,   especially   in  response   to  the  continued   concerns   that   misin forma tion  is  still  spreading   rapidly   on  the  platform  through   groups.  This  is  a  continual    platform  wher e  people   who   are  already   belie vers  in  conspir acy  theories   and  other   false  content   are  suggested  anti-vaccina tion  groups  given  the  meta-alignmen t.  But,  this  only   exacerba tes  the   problem.   A  lot  of  engagemen t  happens   in  these   groups  and  until  the  platform  is  able   to   drama tically  reduce   these   recommenda tions   in  addition   to  its  other   efforts,   we  will  continue   to   see the pr oblem pr evail.   Between  the  lines   :  We  need   to  find  better  ways  of  engaging   the  technology   and  governmen t   stakeholder s  in  our  informa tion  ecosystem.  The  stronger  the  adversarial   dynamic,   the  more  the   risk  of  irreconcilable   differences   and  non-r esolution   of  the  core  issues.   Mor e  structur ed   experimen ts  and  transpar ency   around   the  results   from  the  efforts  undert aken  by  the  platform   will  help   us  build   a  better  under standing   of  wha t  actions   are  going   to  be  effectiv e  in  our  fight   against the in fodemic which will ultima tely help us figh t the pandemic.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 4 1   
 To Figh t Vaccine Lies, Authorities R ecruit an ‘In fluencer Arm y’   [Original article b y  NYTimes   ]   Wha t  happened   :  While   mos t  of  the  time   in  this  public ation  we  talk  about   the  negative  effects   of  social   media   and  the  spread  of  disin forma tion  and  misin forma tion  that  it  facilit ates,  this   article   highligh ts  a  great  example   of  governmen t  effort  to  recruit   the  power  of  influencer s  in   spreading   “positiv e  informa tion”   to  get  people   vaccina ted.  The  Whit e  House   is  working   with    influencer s  on  TikTok,  YouTube,   and  other   platforms   to  get  them   to  shar e  the  messag e  of   vaccina tion with their lar ge follower bases.    Why  it  matters  :  The  vaccina tion  rates  in  the  US  have  been   higher   in  the  older   demogr aphic   than    the  young er  ones.   This  is  the  exact  audience   that  can  be  reached   through   influencer s  on  social    media.   As  poin ted  out  by  a  survey  cited  in  the  article,   people   tend  to  be  better  persuaded   by   content  creators  that  they  watch  /  listen  to  on  social   media   than   other   public ation  outle ts.  This   is  then   the  perfect  channel   to  quash   rumor s  and  answer  ques tions   about   vaccina tion,   urging    people t o go out ther e and g et the jab.    Between  the  lines   :  Borr owing   on  tactics   that  were  used   for  politic al  mobiliz ation  during   the   Biden   campaign,   the  Whit e  House   is  now  using   the  same   insigh ts  and  appr oach   to  get  an   import ant  messag e  out  to  the  people   to  get  vaccina ted.  As  DiResta  poin ts  out  in  the  article   that   those   looking   to  spread  disin forma tion  are  more  motiv ated  and  organic  reach   can  exceed    targeted  measur es  like  these,   it  is  still  a  good  first  step  in  countering   “neg ative  informa tion”    with   some   action   rather   than   just  trying  to  suppr ess  misin forma tion  and  disin forma tion  on   these   platforms.   A  multi-pr onged  appr oach   will  always  be  more  effectiv e  in  countering    pervasive problems lik e this one.    Let’s Keep the V accine Misin forma tion Pr oblem in P erspectiv e   [Original article b y  Wired  ]   Wha t  happened   :  An  insigh tful  article   that  talks  about   the  comple xities   in  trying  to  disen tangle    the  effects  of  misin forma tion,   which   are  myriad,   from  that  of  other   confounding   factors  when   it   comes   to  the  low  rates  of  vaccina tion  amongs t  certain  demogr aphics   in  the  Unit ed  States.  In  a   study   cited  in  the  article,   they  find  that  there  is  a  strong  correlation  between  vaccine   hesit ancy    and  a  gener al  mistrust  in  mains tream   institutions   which   pushes   these   people   towards  getting   their   news  from  social   media   rather   than   more  trusted  and  reput able   sour ces.  The  recen t  clash    between  the  Whit e  House   and  Facebook   on  the  role  that  Facebook   has  played  in  enhancing    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 4 2   
 vaccine   hesit ancy   and  the  US  falling   behind   in  mee ting  its  vaccina tion  goals  is  a  demons tration   of  how  collap sing  the  issue   into  easy  to  reason   with   binaries   mak es  it  a  tough   problem   to  solve.   In  particular ,  the  article   poin ts  out  how  some   are  advocating  for  dispensing   this  politic al  capital   in  a  better  fashion   to  perhap s  spark   vaccine   manda tes  by  offices   and  schools   and  to  treat   misin forma tion’ s role in this as a br oader issue tha t needs mor e long-t erm solutions.    Why  it  matters  :  As  is  the  case  with   any  socio-t echnic al  issue,   there  are  a  range  of  variables   that   impact   the  inputs   and  outputs   of  a  problem.   Wha t  we  see  here  is  that  a  lack  of  transpar ency   on   the  parts   of  Facebook   and  YouTube  for  example   limiting   the  ability   of  resear cher s  to  gather    adequa te  data  about   the  degr ees  of  correlation  between  vaccine   hesit ancy   and  the  content  that   they  see  online   along   with   the  efficacy  of  the  measur es  undert aken  by  these   platforms   to  limit    the spr ead of misin forma tion, specific ally her e, related to vaccines.    Between  the  lines   :  The  recen t  denial   of  access   to  resear cher s  studying   the  Facebook   platform  is   yet  another   blow  that  will  only   deepen   the  chasm   between  positiv e  public   health   outcomes   and   the  potential  role  that  a  compan y  like  Facebook   can  and  is  playing  in  that.  The  Klobuchar   bill   men tioned   in  the  article   serving  as  a  messaging   bill  is  a  first  step  in  establishing   some   baselines    on  how  to  tackle   the  issue   but  it  raises   even  more  ques tions   and  issues   than   it  tries   to  solve:   namely ,  transferring   over  the  arbitr ation  of  wha t  is  and  is  not  misin forma tion  from  the  platform   and  their   community   of  moder ators  to  the  governmen t,  which   would   certainly   raise  concerns    around the viola tion of the Fir st Amendmen t.   Troll  farms   reached   140  million   Americ ans  a  mon th  on  Facebook   before   2020 election, in ternal r eport sho ws   [Original article b y  MIT T echnology R eview  ]   Wha t  happened   :  In  a  perhap s  not  so  shocking   report,   a  former   senior -level  data  scien tist   revealed   that  troll  farms   continue   to  have  signific ant  audiences   who   are  deeply   engaged  on   Facebook.   The  report   highligh ted  3  key  short comings   in  the  existing  platform  design   that   allowed  pages  run  by  these   troll  farms,   that  have  never  engaged  with,   nor  have  knowledg e  of   the  communities   that  they  influence,   to  shape   their   though ts.  Facebook   doesn’t   penaliz e  pages   that  post  unoriginal   content,  allowing   previously   viral  content  to  merely  be  copied   and  go  viral   again,  perpe tuating  disin forma tion.   Engaging   content  from  pages  that  user s  don’t   even  follow   can  still  show  in  their   feeds   when   a  friend   interacts  with   that  piece   of  content.  And  finally ,  more   engaging   content  is  pushed   up  higher   in  the  newsfeed  no  matter  wha t  the  type   of  content  or   sour ce. This incen tivizes politic ally divisiv e and clickbait c ontent to rise t o the t op.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 4 3   
 Why  it  matters  :  In  an  ecosystem  wher e  a  large  number   of  people   get  their   news  upda tes  from   social   media   platforms   rather   than   traditional   media   outle ts,  the  combina tion  of  the  above   three  forces  entails  a  signific ant  problem   in  our  ability   to  main tain  a  health y  informa tion   ecosystem.  Mor e  so,  with   a  blatant  disregard  for  the  type   of  content  and  merely  utilizing   its   engagemen t  rates  to  disbur se  it,  the  platform  specific ally  encourages  the  worst  kind   of  beha vior   that  troll  farms   in  places   like  Kosovo  and  Macedonia   are  able   to  leverage  for  financial   rather    than politic al gains.    Between  the  lines   :  The  report   also  provides   some   suggestions   on  how  we  can  comba t  this   scourge:  using   some thing   called   Graph  Authority ,  one  can  get  an  under standing   of  how   authen tic  and  relevant  a  piece   of  content  is  based   on  the  number   of  reput able   in  and  out  links,   some thing   that  Google   has  done   for  several  years  already .  Yet,  as  per  the  report,   such   efforts   within   Facebook   have  largely  been   ignor ed  and  it  continues   to  prioritiz e  content  that  has  the   highes t  likelihood   of  engagemen t  driving   usag e  on  the  platform  rather   than   the  quality   of  the   content itself .   The F acebook whis tleblo wer sa ys its alg orithms ar e dang erous. Her e’s wh y.   [Original article b y  MIT T echnology R eview  ]   Wha t  happened   :  Frances   Haug en,  the  primar y  sour ce  for  The  Facebook   Files   included   in  the   WSJ  investigative  series   on  the  compan y,  testified   in  a  Sena te  hearing   confirming   a  lot  of  things    that  people   assumed   about   how  Facebook   oper ates  and  wher e  it  falls  short   in  terms   of   practic ally  addr essing   problems   on  its  platform.   One  of  the  main   argumen ts  put  forward  by   Haug en  in  the  Sena te  hearing   was  that  the  compan y  knew  about   the  problems,   and  didn’t   act   on  them.   Mor e  so,  the  emphasis   on  content  moder ation  as  a  tool  for  creating  a  healthier    informa tion  ecosystem  is  inher ently  flawed  and  instead  we  should   be  focusing   on  the  design   of   the  algorithms   powering   the  platform  to  addr ess  the  root  causes   of  the  problems   plaguing   the   platform.    Why  it  matters  :  Scathing   in  its  criticism   of  the  platform  and  wha t  it  is  doing   to  addr ess  the   challeng es  including   misin forma tion,   polariz ation,   addictiv e  engagemen t,  data  misuse   for   targeted  advertising ,  and  other s,  the  fact  that  existing  mechanisms   like  content  moder ation   because   of  limit ations   in  their   languag e  and  context  capabilities   are  just  proverbial   band-aids   on   a  broken  dam   are  the  call-to-action   that  should   spur   Facebook   to  mak e  more  investmen ts  in   reshaping   the  platform  to  mitig ate  the  emer gence   of  such   problems   in  the  first  place.   Wha t  this   also  does   is  shows  that  presen ted  evidence   of  investmen ts  into  content  moder ation,   we  should    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 4 4   
 be  more  cogniz ant  of  the  actual   impact   that  such   measur es  will  have  in  solving   the  root   problems a t the heart of the pla tform.    Between  the  lines   :  At  the  center  of  all  the  proposed   mechanisms,   the  problems,   proposed    regula tions,   and  everything   else  to  create  a  more  health y  ecosystem  is  a  fundamen tal  tension:    the  business   incen tives  of  the  platform  in  realizing   profits   are  stacked  against  the  interests  of   the  user s  of  the  platform.   Yes,  there  migh t  be  ways  of  giving   one  side  more  of  an  edge  but  the   tension   remains   because   of  the  business   model   which   ultima tely  drives  a  lot  of  the  activity   on   the  platform’ s  design,   developmen t,  and  deplo ymen t.  Without   ackno wledging   that  more  fully,   and  working   towards  resolving   that  tension,   the  solutions   will  only   addr ess  the  root  problems   in   a piece-meal f ashion.    How W e Investigated F acebook’ s Mos t Popular Con tent   [Original article b y  The Mark up  ]   Wha t  happened   :  The  team   at  The  Mark up  reviewed  the  recen tly  published   “Widely   Viewed   Content  Report ”  from  Facebook,   a  documen t  that  is  published   by  the  compan y  in  the  interest  of   transpar ency ,  to  see  how  content  disper ses  online,   wha t  the  frequency   of  that  content  is  on  the   platform,   and  how  the  rankings   of  various   websites  chang e  based   on  wha t  kind   of  methodology    is  used.   They  utiliz ed  their   Citiz en  Browser  project   to  simula te  the  calcula tions   done   by  the  team    at  Facebook,   and  applying   statistical  methods,   determined   that  the  sample   size  and  appr oach    that  they  are  using   lines   up  quite  well  to  draw  statistically  signific ant  conclusions   about   the   performance of t op perf orming c ontent on the pla tform.    Why  it  matters  :  The  article   dives  into  methodologic al  details  that  are  well  worth   reviewing ,  but   more  import antly,  they  highligh t  the  lack  of  transpar ency ,  ironic   given  the  purpose   of  the  report,    in  the  methodology   published   accompan ying  the  report   from  Facebook.   The  focus   of  that   report   was  solely   on  the  views  for  the  content  and  much   less  so  on  the  frequency   with   which    that  content  migh t  have  appear ed  in  the  newsfeed  of  a  user.  This  is  an  import ant  distinction   to   mak e,  since   the  frequency   with   which   someone   sees   a  piece   of  content,  the  chances   that  they   assimila te  its  messag e  increases,   and  it  also  correlates  with   the  probability   of  them   sharing   that   piece   of  content,  thus   amplif ying  its  reach.   Without   that  level  of  granularity ,  we  get  a  poor    facsimile of the actual dis tribution and in fluence of c ontent on the pla tform.    Between  the  lines   :  In  an  effort  to  provide   transpar ency ,  Facebook’ s  report   is  a  great  first  step,   but  as  the  investigation  done   by  The  Mark up  poin ts  out,  the  report   obscures  quite  a  bit,   especially   given  the  limit ed  informa tion  about   the  underlying   methodology   that  was  used   to   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 4 5   
 arriv e  at  the  final   number s.  Mor e  so,  some thing   that  is  now  back ed  by  empiric al  evidence   as  per   the  work  done   by  The  Mark up,  is  that  sensa tionalis t  content  and  opinion   sites  do  outperf orm   mains tream   news  content  on  the  platform  and  this  isn’t   appar ent  in  the  report   from  Facebook    because of the w ay the c alcula tions ar e carried out.    How  social   media   companies   help   Afric an  governmen ts  abuse    “disin forma tion la ws” to target critics    [Original article b y  Rest of W orld  ]   Wha t  happened   :  The  article   describes   how  the  combina tion  of  vaguely   defined   disin forma tion   laws  in  countries   like  Kenya,  Uganda,   Mala wi,  and  Nigeria,   instead  of  clamping   down  on   disin forma tion  actually   restricts   legitima te  speech   more.  This  is  exacerba ted  by  the  fact  that   social   media   platforms   are  also  limit ed  in  their   appr oach   of  addr essing   disin forma tion  problems    on  their   platforms,   such   as  narrow  appr oaches   like  simply   taking   down  content.  Some times,    these   vague   laws  have  also  led  to  interne t  shut downs  in  Afric an  nations.   In  the  mids t  of  all  this,    the  regula tions   mos tly  serve  the  interests  of  the  governmen t  while   the  policies   of  social   media    platforms   mos tly  serve  the  companies   themselv es.  The  fundamen tal  rights  of  end  user s  are   mos tly ignor ed.   Why  it  matters  :  The  article   does   poin t  to  some   fundamen tal  texts  in  the  space   like  the  Santa   Clara  Principles   (MAIEI   provided   commen ts  to  it)  that  can  serve  as  guides   on  effectiv e  regula tion   of  the  problem   of  disin forma tion  such   that  fundamen tal  rights  of  end  user s  are  still  protected.   Concr etely,  creating  some thing   that  is  soft  law  in  the  beginning   based   on  these   guidelines   and   then   determining   which   parts   of  it  work  and  which   don’t,   can  be  moved  into  the  hard  law   territ ory.   Between  the  lines   :  A  shar ed  responsibility   model   wher e  we  have  man y  actors  who   are  jointly   responsible   for  governing   how  the  disin forma tion  challeng e  is  addr essed   on  social   media    platforms   is  going   to  be  essen tial.  Mor e  so,  I  belie ve  that  elevating  media   and  digit al  literacy  will   offer  yet  another   effectiv e  avenue   to  comba t  this  problem,   further   bolstering   the  efforts  that   emer ge on the t echnic al and policy fr onts to addr ess these challeng es.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 4 6   
 The Me taverse Is Mark Zuck erber g’s Mobile Do-Ov er   [Original article b y  Wired  ]   Wha t  happened   :  The  metaverse  has  taken  the  world  of  tech-r elated  discussions   by  storm  ever   since   the  rebranding   announcemen t  from  Facebook   becoming   Meta.  The  article   dives  into  the   details  of  previous   attemp ts  by  Meta  in  trying  to  establish   itself   in  the  leagues   of  other    companies   that  have  a  stronger  grip  on  the  underlying   infrastructur e  and  plumbing   that  enables    us  to  enjo y  all  the  apps  and  other   services   built   on  top  of  them,   so  things   like  OSes,   devices,   and   platforms   (which   is  its  domain   for  now).  The  article   examines   past  endea vors  from  the   companies   in  trying  to  introduce   mobile   OS,  a  Facebook   phone,   the  Facebook   Home   that  was   supposed   to  become   the  central  thing   on  your  phone,   and  finally   wha t  they’re  trying  to  achie ve   with their vision f or the me taverse and ho w the y are appr oaching it.    Why  it  matters  :  If  the  metaverse  is  some thing   that  takes  hold   (though   some   argue  that  we  are   already   in  a  metaverse  with   all  the  other   online   activities   that  we  are  engaged  in  and  how  we   define  the  metaverse  in  the  first  place!),   Meta  argues   that  it  will  only   become   success ful  if  it   involves  open   standar ds  and  other   companies   providing   services   and  solutions   that  can  all  plug    into  a  single   ecosystem.  Of  course,  there  are  under currents  to  this  appr oach   in  that  Meta  would    be  deligh ted  if  it  is  based   on  their   vision   and  platform  +  infrastructur e  that  would   mak e  them   a   central pla yer in this futur e if it c omes t o pass.    Between  the  lines   :  With   all  the  scrutin y  that  the  compan y  has  faced   in  the  US,  the  rebranding    and  moving  away  from  the  social   media   platform  to  some thing   more  nebulous   like  the   metaverse  migh t  seem   like  a  mechanism   for  drawing   away  attention.   But,  as  technology    becomes   more  ubiquit ous,  and  the  possibility   of  realizing   the  metaverse,  at  least  in  the  form   that  Meta  imagines   it,  becomes   more  likely,  this  is  a  good  call  for  our  community   to  start   thinking   about   wha t  the  ethical  consequences   of  this  migh t  be  so  that  we  are  prepar ed  and   respond pr oactiv ely.   How Facebook and Google fund global misin forma tion   [Original article b y  MIT T echnology R eview  ]   Wha t  happened   :  Algorithmic   amplific ation  of  problema tic  informa tion  online   is  nothing   new  to   the  reader s  of  this  newsletter.  We’ve  covered  it  time   and  again.  But,  this  article   sheds   a  new   light  on  the  machiner y  that  feeds   this  informa tion  ecosystem,  in  particular ,  it  highligh ts  some   of   the  funding   mechanisms   that  power  malicious   actors  to  continue   their   activities.   In  particular ,   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 4 7   
 the  introduction   of  Instant  Articles   by  Facebook   brough t  into  the  fray  additional   incen tives,   financial   ones,   that  directed  the  ener gies  of  other wise   undir ected  malicious   actors  into  the   politic al  arena,   given  the  high   engagemen t  rates  of  politic al  content  on  the  website.  Wha t  this   mean t  is  that  not  only   were  there  politic ally  motiv ated  malicious   actors,  but  also  those   who    aren’t  really   connect ed  with   any  politic al  objectiv es  and  are  seeking   to  eke  out  a  profit  by   milking the c ontent dissemina tion machiner y tha t Facebook and Google pr offer.   Why  it  matters  :  While   addr essing   the  algorithmic   basis   of  how  informa tion  spreads   online   is   one  way  of  going   about   tackling   the  proliferation  of  problema tic  informa tion,   we  need   to  also   focus   on  the  underlying   business   mechanisms.   Especially   as  highligh ted  by  this  article   when   new   tools  like  Instant  Articles   propel   clickbait   and  non-mains tream   media   outfits   to  outcompe te  and   overcome   the  platform  when   it  comes   to  the  content  that  is  viewed  and  engaged  with   by  the   user s.  The  fact  that  the  informa tion  ecosystem  is  domina ted  by  a  few  giants  and  that  wha t   happens   on  one  platform  (say  YouTube)   has  a  drama tic  impact   on  content  that  shows  up  on  and   domina tes  another   platform  (videos   trending   on  Facebook),   tells  us  that  we  also  need   to   examine wha t such a monopoliz ation means f or the health of the in forma tion ec osystem.   Between  the  lines   :  Adding   financial   incen tives  to  an  already   char ged  ecosystem  wher e  there   are  man y  motiv ations   for  adversaries   and  malicious   actors  to  pollut e  the  informa tion  ecosystem   demons trates  a  worsening   state  of  affairs.  Having  higher   transpar ency   on  who   is  paid   out  and   how  much   from  mone tization  mechanisms,   along   with   access   to  external   audit ors  and   resear cher s  (who   have  had  their   access   taken  away  from  conducting   independen t  resear ch  on   Facebook)   and  demons tration  of  action   on  the  recommenda tions   that  are  provided   by  civil   socie ty  organizations   and  individual   watchdogs   is  going   to  be  essen tial  to  curb   the  spread  of   misin forma tion  online,   and  reduce   the  very  real  harms   inflicted  on  people   as  a  result   of  this   proliferation as seen in My anmar amongs t other places.    China’ s queer in terne t is being er ased    [Original article b y  Rest of W orld  ]   Wha t  happened   :  In  July  2021,   some   of  the  mos t  prominen t  and  well-connect ed  social   media    accounts  for  member s  from  the  LGBTQI  communities   were  banned,   disconnecting   folks  from   across  the  nation  who   relied   on  these   to  coordina te  their   online   activities   and  exchang e  on   issues   they  face.  Some   of  the  people   interviewed  for  the  article   men tioned   that  they  saw  such   a   ban  coming   given  the  slow  erosion   of  safe  spaces   for  them   both   offline   and  online   to  organize.   In  the  early   days,  there  was  support   from  organizations   like  univ ersities   who   support ed  these    online   accounts  as  a  way  to  showcase  that  they  were  open   and  progressiv e.  But,  the   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 4 8   
 communities   started  to  get  called   out  for  anti-na tional   sentimen ts  and  these   were  used   as   reasons   to  start  censoring   them.   Even  apps  like  Blued   that  alleg edly   served  the  needs   of  the   communities   have  started  acting   in  alignmen t  with   national   governmen t  interests;  embarking    on  things   like  assigning   credits   for  good  beha viour   on  the  platform  and  stripping   those   credits    away  as  the  accounts  posted  anything   that  had  the  potential  to  draw  ire  from  the  governmen t   censor s.   Why  it  matters  :  Given  the  continued   taboo   around   LGBTQI  iden tity  in  China,   online   spaces,    through   their   anon ymity ,  offered  a  safe  space   to  explor e  and  discuss   issues   while   moving   towards  securing   greater  rights  to  be  more  open   about   this  subject.   But,  as  men tioned   by  the   interviewees  in  the  article,   anything   that  relates  to  rights  is  some thing   that  is  quick er  to  attract   the  censor s  and  has  a  higher   likelihood   of  getting  their   accounts  shut   down.  Compr omise   in  the   form  of  using   their   accounts  (at  least  the  ones   that  are  still  activ e)  as  a  medium   to  shar e   resour ces  on  sexual  health   and  other   topics   that  aren’t  rights  related  is  still  a  way  to  convene   these c ommunities without r eceiving outrigh t bans.    Between  the  lines   :  As  more  and  more  of  everyone’ s  activities   leave  digit al  traces,   the  situa tion   in  China   doesn’t   bode   well  for  how  people   organize  around   interests  and  iden tities   that  aren’t   accep table   to  the  governmen t.  This  is  particularly   problema tic  in  areas  wher e  there  are  few   people   that  iden tify  as  one  does   in  the  LGBTQI  community ,  making   it  incredibly   difficult   to  find   other s  to  shar e  their   struggles  with.   This  has  the  potential  to  mak e  things   worse  even  from  a   men tal  health   perspectiv e  for  those   who   are  not  able   to  find  a  like-minded   community .  Moving   towards  offline,   in-per son  community   gatherings   is  a  way  to  counter  this  force,  but  it  comes    with the c ost of loss of anon ymity , and with the ong oing pandemic, ele vated health risk s.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 4 9   
 6. AI Design and Go vernance    Introduction  by  Michael   Klenk,   Assis tant  Professor ,  Philosoph y,  Delft   Univ ersity  of   Technology    We  lived  in  a  beautiful,   century-old  house   at  a  Dutch  canal.   The  problem   was,  we  had  mice   in   the  building.   Their   squeaking   and  shuf fling   woke  us  up  at  nigh t  in  some times   eerie   and  always   anno ying  episodes.   I  had  designed   things   before,  like  a  kitchen   table,   but  now  I  needed   an   elabor ate mechanism t o catch the crit ters. I had m y design pr oblem cut out f or me.    The  design   of  mouse   traps  shar es  its  two  fundamen tal  ques tions   with   any  design   activity ,   including   the  design   of  AI.  What   problem   should   be  solved?  And  how  should   the  problem   be   solved?  As  much   as  I  wanted  to  get  rid  of  the  mice,   I  did  not  want  a  lethal  trap,  nor  did  I  wish   to   turn   the  whole   house   into  a  warzone.   Looking   back,   I  recognise   how  I  implicitly   combined   the   obvious   functional   requir emen ts  for  a  trap  with   social   and  ethical  requir emen ts.  I  balanced   the   condition t o  'catch mic e a.s.a.p.   ' with   'unnotic eable   in daily lif e  ’ and '   not le thal  .'   Design   is  an  inher ently  norma tive  activity .  Designing   some thing   means   deciding   how  the  world   should   be  like.  Your  individual   decision   may  be  small   and  inconsequen tial.  For  example,   in  the   case  of  my  humane   mouse   trap,  little  was  at  stake  except  for  the  mice   and  my  sleep   quality .  But   take  together   all  our  design   choices,   however  small,   and  you  see  them   giving   shape   to  our   world,   including   our  social   world.   For  example,   imagine   how  the  AI  co-worker  discussed   in  the   report belo w ma y influence and chang e ho w you e xperience y our w ork.   As  a  designer ,  you  orien t  yourself  in  a  design   space   that  contains  everything   you  could   do.  No   design   problem   has  a  single   solution.   Ther e  are  always  multiple   options   for  thinking   about   a   problem   and  different  resolutions   (other wise,   I  would   not  deem   it  a  design   activity).   Obviously ,   that  you  could   design   some thing   in  some   way  does   not  imply   that  you  should   do  it  that  way.  For   example,   I  could   have  burned   down  the  house   to  get  rid  of  the  mice,   but  that  option  is  patently   absurd.  Possible,   but  not  wha t  ough t  to  be  done.   Ther e  are  so  man y  ways  our  world  could   be,   and na turally, some ar e be tter than other s. At this poin t, design mee ts ethics.    The  norma tivity   of  design   means   that  we  mus t  be  critic al  about   the  problems   we  want  to  solve   with   AI.  When   a  design   problem   is  given  to  you  (say,  at  work),   it  may  seem   like  the  only   ques tion   is  how  to  solve  it.  But  in  that  case,  the  norma tive  ques tion  –  what   problem   should   be  solved?  -   has  already   been   answered  for  you.  The  ques tion  may  often  escape  our  view,  but  it  is  there   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 5 0   
 none theless.   Design   thinking   capitalises   on  this  simple   insigh t  into  the  norma tive  nature  of   design.   It  is  never  the  case  that  even  the  problem   definition   is  certain.  Ther e  is  always  an  open    ques tion:   Is  that  the  problem   we  should   solve?  Good   designer s  ask  that  ques tion.   When   you   read  the  summaries   belo w,  you  can  ask  yourself  whe ther   AI  is  being   used   to  solve  a  problem    that  we  ough t  to  solve.  For  example,   wha t  problem   does   the  AI  co-worker  solve,  and  is  it  a  good   problem tha t needs a solution?    Mor eover,  the  criteria  we  should   use  in  assessing   our  design   decisions   are  not  set  in  stone  and   certainly   not  limit ed  to  technic al  and  economic   consider ations.   The  surging   attention  to  AI   Ethics   is  based   on  a  fundamen tally  similar   realisa tion.   Rather   than   myopic ally  focusing   on   technic al  and  financial   factors,  people   now  realise   that  ethical,  social,   and  ecologic al   consider ations   determine   wha t  problems   we  ough t  to  design   AI  for  and  how  we  should   solve   them.   The  summaries   of  this  report   illustrate  this  in  several  ways.  For  example,   the  call  for   sustainable   AI  is  a  specific   design   requir emen t  that  constrains  the  design   space   for  AI  solutions    by  demanding   that  AI  training   and  execution   be  done   sustainably .  Another   example   is  the  AI   certific ation  summarised   belo w.  Notwiths tanding   the  value   of  such   certific ations,   the   requir emen t  that  AI  mee t  specific   ethical  requir emen ts  shapes   which   options   in  design   space    are feasible.    The  deep   and  hard  work  is  to  find  out  the  proper   criteria  that  we  can  use  to  evalua te  our  design    choices.   This  is  mos t  explicitly   discussed   in  the  literature  on  Value   Sensitiv e  Design   and  Value    Alignmen t  in  AI.  In  both   fields   of  literature,  you  will  re-disc over  varian ts  of  our  two  fundamen tal   design   ques tions:   Wha t  should   we  design   AI  for?  And  how  can  we  ensur e  it  aligns   with   the   targets  we  chose?   I  am  glad   to  see  the  summar y  of  DeepMinds   work  on  the  topic.   It  illustrates   that  serious   attention  is  given  to  this  topic   in  academia   and  indus try.  The  summar y  of  mapping    AI4Good   principles   for  a  Value   Sensitiv e  Design   appr oach   is  also  help ful.  It  illustrates  how  design    principles   are  at  their   heart   attemp ts  to  suss   out  which   options   in  design   space   are  feasible   for   us.  Going   forward,  we  need   more  integration  of  philosophic al  perspectiv es  about   wha t  values    are to mak e progress on the v alue alignmen t problem.    If  we  bracket  for  the  momen t  the  technic al  ques tion  of  how  we  can  align   AI  with   given  values,    we  first  need   to  find  out  wha t  these   values   are.  And  that  is  a  ques tion  that  philosoph y  has  done    much   to  clarif y.  So  we  should   use  this  resour ce.  I  am  often  amaz ed  how  my  studen ts,  mos t  of   whom   have  a  STEM   backgr ound   and  come   to  my  courses  at  best  scep tical  about   the  value   of   philosoph y,  turn   into  epistemologis ts  and  metaphysicians   within   a  few  sessions.   They  ask   difficult   and  excellen t  ques tions   about   the  nature  of  the  values   we  are  supposed   to  be  aligning    AI  with   and  the  possibilities   for  finding   out  about   them.   For  example,   are  values   discoverable   by   science,   and  how  should   we  deal   with   value   disagr eemen ts?  I  suspect   that  the  scep ticism    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 5 1   
 expressed   by  AI  practitioner s  about   the  widespr ead  implemen tation  of  AI  Ethics   summarised    belo w is partly a r eflection of the dif ficulty of these ques tions.    Some   progress  can  be  made   even  when   a  final   answer  is  not  yet  settled  from  a  philosophic al   perspectiv e.  For  example,   the  idea   that  ethics   applies   to  design   decisions   in  the  first  place   is  not   trivial   and  still  support ed  by  several  lines   of  philosophic al  argumen t.  And  one  thing   I  know  for   sure.  Our  shar ed  aim  of  using   AI  to  shape   the  world  positiv ely  is  not  helped   by  simplif ying   things.   On  the  one  hand,   it  appear s  that  there  is  a  strong  tendency   in  AI  Ethics   to  oversimplif y  in   a  subjectivis t  direction.   AI  Ethicis ts  often  seem   overly  impr essed   by  appar ently  deep,    unassailable   differences   in  values.   But,  of  course,  there  are  legitima te  ques tions   about   how   deep   these   disagr eemen ts  really   are.  We  should   not  be  led  to  deny  that  there  are  better  or   worse  aims   to  strive  for  in  the  design   of  AI.  On  the  other   hand,   we  mus t  not  oversimplif y  in  the   direction   of  objectivism   and  uncritic ally  suggest  that  a  certain  set  of  principles   is  eviden tly  clear    and  set  in  stone.   In  any  case,  we  need   a  good  explana tion  for  why  specific   values   hold   and  why   they should shape our design choices.    When   we  see  the  design   of  AI  for  wha t  it  is,  an  inher ently  norma tive  activity ,  we  can  ask  the   right  ques tions.   Wha t  problems   should   we  solve?  And  how  should   we  do  it?  With   these    ques tions   in  clear   sight,  we  can  start  seeking   answers.  I  urge  that  we  take  the  philosophic al   fundamen tals  of  these   ques tions   seriously .  Though   there  are  no  neat  and  straightforward   answers  to  be  expect ed,  we  will  avoid  repea ting  mistakes  that  years  of  philosophic al  theorising    have  laid  bare.  Wha tever  happened   to  the  mice   in  our  house,   however,  you'll   have  to  ask  me  in   person.    Michael Klenk   Assistant Professor, Philosophy   Delft University of Technology   Klenk  is  Assistant  Professor  of  Philosophy  at  Delft  University  of  Technology.  His   research  is  part  of  the  ERC-funded  Value  Change  project  and  the  Ethics  of   Socially  Disruptive  Technologies  research  programme.  He  is  a  member  of  the   Delft  Design  for  Values  Institute  and  as  an  advisor  for  Ethical  Intelligence.  He   also works as a columnist at 3Quarks Daily.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 5 2   
 Go Deep: Research Summaries   Experts   Doub t  Ethical  AI  Design   Will  Be  Broadly   Adop ted  as  the  Norm    Within the Ne xt Dec ade   [  Original paper   by Lee Rainie, Janna Ander son and   Emily A . Vogels]   [Resear ch Summar y by Connor W right]   Overview  :  How  would   you  answer  the  following   ques tion:   “By  2030,   will  mos t  of  the  AI  systems   being   used   by  organizations   of  all  sorts   emplo y  ethical  principles   focused   primarily   on  the  public    good? ”  An  overwhelming   majority   (68%)   say  no,  and  there  are  more  than   just  ethical  reasons    why this is the c ase.   Introduction    How  would   you  answer  the  following   ques tion:   “By  2030,   will  mos t  of  the  AI  systems  being   used    by  organizations   of  all  sorts   emplo y  ethical  principles   focused   primarily   on  the  public   good? ”  A   resounding   68%   of  the  experts   involved  in  this  resear ch  paper   answered  no.  Positiv es  are  few   and  far  between  within   the  resear ch  presen ted,  despit e  some   clear   examples.   So,  let’s  look   into   why tha t is the c ase.   Ethics is both v ague and subjectiv e   One  prevalen t  theme   throughout   this  piece   is  the  frustratingly   vague   and  subjectiv e  nature  of   ethics.   Ther e  is  no  consensus   over  wha t  ethical  AI  look s  like,  nor  is  there  any  agreemen t  over   wha t  is  a  moral  outcome.   In  this  sense,   it  could   be  rightly  said  how  our  ethical  frame works  are   only   ‘half -writ ten  book s,  missing   some   crucial   pages  and  chap ters  to  guide   us.  As  a  result,   ethics    turns   out  to  be  an  iterative  rather   than   dogma tic  process,   requiring   us  to  be  okay  with   not   knowing   the  potential  outcomes   and  answers  of  a  situa tion.   Unfortuna tely,  this  does   not  bode    well with tr ying t o enc ode e thical systems in to AI.    Wha t  I  mean   by  this  is  how  real-lif e  situa tions   can  be  seen   as  being   too  situa tional   to   programme   into  an  ethical  AI  frame work,  wher eby  actual   ethical  dilemmas   do  not  possess   any   correct  answers.  For  example,   views  of  wha t  is  ethical  differ  worldwide,   wher e  countries   such   as   China   values   social   stability   more  than,   say,  Western  countries.   Thus,   when   AI  is  applied   (such   as   in  warfare),  it  is  unlik ely  that  both   sides   of  the  conflict  would   emplo y  the  same   ethical   frame work.  Hence,   finding   a  common   ethical  thread  can  better  help   fuse   a  potentially   fractur ed   AI regula tion appr oach, which I belie ve lies in iden tifying the human in the AI pr ocess.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 5 3   
 Iden tifying the human in the pr ocess    Here,  the  paper   rightly  poin ts  out  the  false  claim   that  technologic al  solutions   are  better  than    human   solutions   as  they’re  based   on  ‘cold  computing ’  and  not  ‘emotiv e  human   responses’ .   Instead,   it  should   be  noted  how,  perhap s,  when   we  talk  about   AI  ethics,   we  should   referring   to   human   ethics   media ted  through   AI.  By  this,   I  mean   how  there  are  no  inher ently  good  or  evil   mathema tical  functions,   wher eby  it  is  rather   the  human   presence   that  determines   the  ethical   propensity   of  the  AI  applic ation.   The  oblig ation  to  be  moral  lies  in  the  hands   of  corpor ations   and   system designer s rather than in wha t the AI does.    As  a  result,   the  role  the  human   plays  in  ‘feeding   and  nurturing ’  their   AI  is  to  be  ackno wledg ed.   Supplying   the  system  with   adequa te  data  for  it  to  train  on  and  proper   privacy  protections   are   two  ways  in  which   this  role  can  be  carried   out  meaningfully .  Without   such   measur es  in  place,   AI   then   has  the  potential  to  become   the  medium   through   which   our  lack  of  under standing   of   human   bias  and  bias  in  itself   is  expressed.   One  environmen t  in  which   this  has  become   too   appar ent is in AI inno vation.    Ethics doesn’t driv e AI inno vation   Effectiv e  AI  has  been   seen   to  be  prioritised   over  ethical  AI.  Looking   at  facial   recognition   systems   such   as  Amaz on’s  Rekognition   and  IBM,   it  becomes   clear   that  companies   are  prioritising   the  ‘E’   word,  but  not  the  one  that  should   be  emphasised.   Thus,   Techno-po wer  has  become   the  main    driver  behind   the  pursuit  of  AI  instead  of  ethical  consider ations.   As  a  consequence,   those   few  at   the  helm   of  AI  inno vation  have  proliferated  the  techno-solutionis t  mindse t  throughout   the   practice,   allowing   AI  to  be  used   as  the  new  manif estation  to  masquer ade  and  hide   the  business    interests  and  biases   of  the  institutions   and  people   involved.  In  this  sense,   AI  has  become   the   digit al  represen tation  of  the  collectiv e  corpor ate  mindse t,  meaning   that,  as  some   experts   in  the   paper   observed,  so  long   as  AI  is  owned   those   who   have  access   to  it  will  bene fit  and  those   who    do not will suf fer the c onsequences.    In  this  sense,   perhap s  taking   the  view  of  seeing   the  wood  for  the  trees  and  observing  wha t  AI  is   at its c ore is no w worth e xploring.    Taking AI as it r eally is    One  of  the  lures  of  AI  is  how  it  almos t  creates  its  own  separ ate  reality ,  filled   with   the  promise   of   wha t  can  be  in  a  different  world  separ ate  from  the  current  reality .  However,  this  distracts  from   wha t  AI  is  in  essence.   For  example,   AI  applic ations   in  different  sectors  such   as  law  enforcemen t   do  wha t  they’re  told  to  do.  It  does   not  possess   a  moral  compass   nor  social   awareness.   In  this   sense,   AI  can  be  seen   to  lack  contextual  under standing   as  it  sets  out  to  achie ve  its  goal.  To   illustrate,  the  paper   included   how  an  AI  tasked  to  keep  you  dry  would   not  be  fussed   about    stealing   an  umbr ella  from  an  old  lady  in  the  street  when   it  starts  to  rain.  In  this  sense,    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 5 4   
 recognising   AI  as  a  tool,  or  even  yet,  potentially   going   as  far  as  saying  that  it’s  an  elong ation  of   previous   statistical  techniques   and  inno vations,   could   serve  to  help   cut  away  the  confusing   mist   surrounding   such   technology .  Perhap s  viewing   it  as  a  tool  can  then   help   to  influence   the  futur e   applic ations of such a t ool, including in the incen tives to action it brings with it.    The pr oblem of incen tives   One  potential  way  to  correct  the  men tioned   corpor ate  prioritisa tion  of  efficiency   could   then   be   to  look   into  wha t  incen tivises   businesses   to  act  this  way.  In  this  sense,   the  experts   involved  in   the  paper   observe  how,  in  its  current  state,  the  corpor ate  world  is  not  offered  any  bene fits  from   ethically  coordina ting  AI,  with   businesses   tending   to  prioritise   efficiency ,  scale  and  automa tion,    rather   than   augmen tation,   inclusion   and  local  context.  If  this  can  be  achie ved,  there  certainly   is   a brigh t side t o AI.    The positiv es   AI  has  been   showing   promise   in  its  use  in  educ ation  and  health,   allowing   the  prioritisa tion  of   accessible   and  necessar y  digit al  skills   in  educ ation  programmes,   as  well  as  impr oving  the   accur acy  of  certain  diagnoses.   In  this  way,  it  has  been   observed  in  the  paper   how  the  more  we   develop   AI,  the  more  we  appr eciate  the  unique   traits  and  special   qualities   of  humans   that  are  so   hard  to  code.   Such   qualities   such   as  compassion,   contextual  under standing   and  decision-making    are  common   throughout   the  human   world,   meaning   that  AI  could   also  prove  the  median    through   which   we  are  able   to  bridg e  the  conversation  between  countries.   While   these   positiv es   are few in the paper , the y are worth k eeping in mind none theless.    Between the lines    From  my  perspectiv e,  wha t  kind   of  humans   we  want  to  be  should   be  reflect ed  in  how  we  go   about   designing   our  AI  systems.   In  this  sense,   there  should   be  a  lack  of  cheap   and  subversive   techniques   to  avoid  complic ated  issues   like  justice,   with   the  social   good  and  social   infrastructur e   over  inno vation  and  the  good  of  the  governmen ts.  For  me,  this  comes   through   ackno wledging    the  human   in  the  process,   both   in  its  role  as  the  protagonist  in  the  AI  process,   as  well  as  the   eventual r ecipien ts of both its positiv es and its neg atives.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 5 5   
 The Logic of Str ategic Asse ts: Fr om Oil t o AI   [  Original paper   by Jeffrey Ding and Allan Da foe]   [Resear ch Summar y by Connor W right]   Overview  :  Does   AI  qualif y  as  a  strategic  good?   Wha t  does   a  strategic  good  even  look   like?  The   paper   aims   to  provide   a  frame work  for  answering   both   of  these   ques tions.   One  thing ’s  for  sure;   AI is not as s trategic as y ou ma y think.    Introduction    Is  AI  a  strategic  good  for  countries?   Wha t  is  strategic  nowadays?  The  theor y  proposed   serves  to   aid  policymak ers  and  those   on  the  highes t  level  to  iden tify  strategic  goods   and  accur ately   interpr et  the  situa tion.   Wha t  a  strategic  good  involves  will  now  be  discussed,   both   in  terms   of   the import ance of e xternalities and whe ther AI qualifies.    Wha t is a s trategic g ood?    The  crux   of  the  paper   centres  on  the  problem   of  accur ately  iden tifying  a  strategic  good.   The   paper   suggests  that  such   goods   “requir e  attention  from  the  highes t  levels  of  the  state  to  secur e   national   welfare  against  interstate  compe tition” .  While   this  may  be  wide-r eaching ,  the  author s   offer the f ollowing f ormula:    “Strat egic le vel of asse t = Import ance x Ext ernality x Nationaliz ation”    The  import ance   of  the  asse t  is  based   on  both   milit ary  and  economic   terms.   For  example,   oil  that   fuels a c ountry’s naval flee t vs cotton being used t o manuf actur e high-end f ashion.    The  externality   part  is  about   positiv e  externalities.   Here,  the  more  positiv e  externalities    produced,   the  more  strategic  the  product.   Private  actors  are  discouraged  from  investing  in  the   good  as  they  cannot   receiv e  all  the  positiv e  externalities   exclusiv ely.  For  example,   wind   turbines    offer positiv e externalities in clean ener gy, but priv ate act ors can’t e xclusiv ely o wn this.    Nationalisa tion  then   focuses   on  how  localised   the  externalities   are.  The  good  becomes   less   strategic if the e xternalities deriv ed fr om it c an spr ead t o other c ountries.    Strategic g oods in t erms of e xternalities    The  externalities   brough t  by  strategic  goods   can  be  classed   in  three  ways:  cumula tive-strategic   logics, in frastructur e-strategic logics and dependency -strategic logics:    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 5 6   
 Cumula tive-strategic  logics   term  how  strategic  goods   are  to  possess   high   barrier s  to  entry.  This   leads   to  low  mark et  investmen t  and  the  need   for  governmen t  consen t  for  the  product   to  be   purchased   (such   as  aircraft  engines).   On  the  other   hand,   Uranium   isn’t   a  cumula tive-strategic   logic as a c ountry’s pur chasing of ur anium doesn’t put up barrier s to en try for other s.   Infrastructur e-strategic  logics   note  how  strategic  goods   in  the  form  of  fundamen tal   technologies   tend  to  upgr ade  socie ty.  The  diffuse   positiv e  externalities   produced   echo    throughout the c ommunity and the milit ary, such as the s team tr ain in the Indus trial R evolution.    Dependency -strategic  logics   focus   on  whe ther   extra  mark et  forces  and  few  substitut es   determine   the  supply   of  a  good  or  not.  For  example,   the  good  becomes   more  strategic  if  a   nation c an cut supplies of a specific g ood t o other c ountries (such as lithium).    As  a  result,   a  strategic  good  is  based   on  the  good  itself   and  the  country’s  strategy  with   it.  For   example,   the  US’s  use  of  oil  in  1941   allowed  them   to  be  the  supplier   of  80%   of  Japan’ s  oil.   Hence,   when   the  US  decided   to  cut  the  oil  supply   to  Japan   as  part  of  the  war  effort,  it  had   devastating e ffects on the Japanese milit ary.   It’s  import ant  to  note  how  the  good’ s  positiv e  externalities   mus t  be  both   import ant  and   strategic,   as  seen   in  this  case.  For  example,   oil  was  able   to  produce   positiv e  externalities   in  the   form  of  modernising   travel.  However,  standar d-issue   milit ary  rifles   can  be  necessar y  for  a   country’s  milit ary,  but  not  strategic.   They  are  easy  to  manuf actur e  (cannot   produce   a   dependency -strategic  logic),   do  not  have  high   barrier s  to  entry,  and  do  not  chang e  socie ty  too   much. Hence, the mor e logics emplo yed a t the same time, the mor e strategic the g ood is.    Wha t this theor y means f or strategic g oods    A  strategic  asse t  is  then   wher e  “ther e  is  an  externality   that  is  both   import ant  and  rivalrous   [(strategic)]. ”.  Strategic  goods   are  no  long er  based   on  milit ary  signific ance,   wher e  a  good  would    be  strategic  if  it  could   be  used   in  the  war  effort.  Under   this  frame work,  such   goods   would   not   requir e  a  high   level  of  attention,   so  they  would   not  be  classed   as  strategic.   Instead,   the   import ant  and  rivalrous  externalities   deriv ed  from  technology   that  can  reduce   CO2  emissions    solely in the c ountry tha t uses it c an be t agged as s trategic.    The s trategic aspect of the de velopmen t of AI    AI  then   becomes   an  interesting  case  in  determining   whe ther   it  is  a  strategic  asse t  or  not.  Here,   there  is  a  low  rate  of  cumula tive-strategic  logics.   Ther e  are  no  high   barrier   entries   to  AI  while    also  possessing   high   infrastructur al  logics   through   its  potential  to  modernise   socie ty.  From   there,  a  potential  emer ging   dependency -logic   between  the  US  and  China   could   begin   to  surface,   with   time   only   telling   whe ther   the  US’s  computing   power  can  be  restricted  to  China.   If  so,  a   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 5 7   
 dependency -logic   can  be  taken  advantage  of,  and  if  not,  China   can  continue   to  surge  in  the  AI   power rankings.    Between the lines    AI  can  certainly   be  classed   as  a  strategic  good  in  my  book,   but  I  though t  it  would   be  classified    more  strongly   according   to  the  formula   at  hand.   At  times,   the  lower  barrier   to  entry  to  gain  a   foothold   in  the  AI  arena  is  often  overlook ed.  This  sobering   realiz ation  can  contribut e  to  wha t  I   belie ve in s trongly: seeing AI f or wha t it is.    Corpor ate Go vernance of Artificial In tellig ence in the Public In terest   [  Original paper   by Peter Cihon, Jonas Schue tt, Se th  D. Baum]    [Resear ch Summar y by Jonas Schue tt]   Overview  :  How  can  different  actors  impr ove  the  corpor ate  governance   of  AI  in  the  public    interest?  This  paper   offers  a  broad  introduction   to  the  topic.   It  surveys  opportunities   of  nine    types   of  actors  inside   and  outside   the  corpor ation.   In  man y  cases,   the  best  results   will  accrue    when multiple types of act ors work t ogether.   Introduction    Private  indus try  is  at  the  forefront  of  AI  resear ch  and  developmen t.  AI  is  a  major   focus   of  the   technology   indus try,  which   includes   some   of  the  largest  corpor ations   in  the  world.   As  AI   resear ch  and  developmen t  has  an  increasingly   outsiz ed  impact   on  the  world,   it  is  essen tial  to   ensur e tha t the g overnance of the field’ s leading c ompanies supports the public in terest.   Opportunities t o impr ove the c orpor ate governance of AI    The  opportunities   to  impr ove  AI  corpor ate  governance   are  diverse.  The  paper   surveys   opportunities f or nine dif ferent types of act ors:   Manag emen t  can  establish   policies,   transla te  policies   into  practice,   and  create  structur es  such    as oversight commit tees.   Workers  can  directly   affect  the  design   and  use  of  AI  systems,   and  can  have  indir ect  effects  by   influencing manag emen t.   Investors  can  voice  concerns   to  manag emen t,  vote  in  shar eholder   resolutions,   replace   a   corpor ation’ s  boar d  of  directors,  sell  off  their   investmen ts  to  signal   disappr oval,  and  file  lawsuits    against the c orpor ation.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 5 8   
 Corpor ate  partner s  can  use  their   business-t o-business   mark et  power  and  relations   to  influence    companies,   while   corpor ate  compe titors  can  push   each   other   in  pursuit  of  mark et  shar e  and   reput ation.    Indus try  consortia   can  iden tify  and  promot e  best  practices,   formaliz e  best  practices   as   standar ds, and pool r esour ces t o adv ance indus try interests, such as b y lobb ying g overnmen ts.   Nonpr ofit  organizations   can  conduct   resear ch,  advocate  for  chang e,  organize  coalitions,   and   raise a wareness.    The  public   can  select   which   corpor ate  AI  products   and  services   to  use,  and  also  support   specific    AI public policies.    The  media   can  resear ch,  documen t,  analy ze,  and  gener ate  attention  to  corpor ate  governance    activities and r elated ma tters.   Coor dina tion and c ollabor ation   In  man y  cases,   the  best  results   will  accrue   when   multiple   types   of  actors  work  together.  The   paper sho ws this via e xtended discussion of thr ee running e xamples:    First,  workers  and  the  media   collabor ated  to  influence   manag ers  at  Google   to  leave  Project    Maven,  a  drone  video   classific ation  project   of  the  US  Departmen t  of  Defense.   Workers  initially    leaked  informa tion  about   Maven  to  the  media,   and  then   signed   an  open   letter  against  Maven   following media r eports.    Second,   nonpr ofit  resear ch  and  advocacy  on  law  enforcemen t  use  of  facial   recognition    technology   fueled   worker  and  investor  activism   and  public   pressur e  (especially   the  2020    protests  against  racism   and  police   brut ality)   that  ultima tely  pushed   multiple   compe ting  AI   corpor ations t o chang e their pr actices.    Third,  workers,  manag emen t,  and  indus try  consortia   have  interacted  to  develop   and  promot e   best practices c oncerning the public ation of pot entially harm ful resear ch.   Between the lines    The  paper   will  be  of  use  to  resear cher s  looking   for  an  overview  of  corpor ate  governance   at   leading   AI  companies,   levers  of  influence   in  corpor ate  AI  developmen t,  and  opportunities   to   impr ove corpor ate governance with an e ye towards long-t erm AI de velopmen t.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 5 9   
 AI  Certific ation:   Advancing   Ethical  Practice   by  Reducing   Informa tion   Asymme tries    [  Original paper   by Peter Cihon, Moritz J . Kleinalt enkamp,   Jonas Schue tt, Se th D . Baum]    [Resear ch Summar y by Jonas Schue tt]   Overview  :  How  can  we  incen tivize  the  adop tion  of  AI  ethics   principles?   This  paper   explor es  the   role  of  certific ation.   Based   on  a  review  of  the  manag emen t  literature  on  certific ation,   it  shows   how  AI  certific ation  can  reduce   informa tion  asymme tries   and  incen tivize  chang e.  It  also  surveys   the  current  landsc ape  of  AI  certific ation  schemes   and  briefly  discusses   implic ations   for  the   futur e of AI r esear ch and de velopmen t.   Introduction    Certific ation  is  widely   used   to  convey  that  an  entity  has  met  some   sort  of  performance    standar d.  It  includes   everything   from  the  certific ate  that  people   receiv e  for  comple ting  a   univ ersity  degr ee  to  certific ates  for  ener gy  efficiency   in  consumer   appliances   and  quality    manag emen t  in  organizations.   As  AI  technology   becomes   increasingly   impactful   across  socie ty,   there  can  be  a  role  for  certific ation  to  impr ove  AI  governance.   This  paper   presen ts  an  overview   of  AI  certific ation,   applying   insigh ts  from  prior   resear ch  and  experience   with   certific ation  in   other domains t o the r elatively ne w domain of AI certific ation.    Certific ation c an reduce in forma tion as ymme tries    A  primar y  role  of  certific ation  is  to  reduce   informa tion  asymme tries.   Informa tion  asymme tries    are  acut e  in  AI  systems  because   the  systems  are  often  comple x  and  opaque   and  user s  typic ally   lack  the  data  and  expertise   necessar y  to  under stand  them.   For  example,   it  is  difficult   or   impossible   to  evalua te  from  the  outside   how  biased   or  explainable   a  model   is,  or  whe ther   it  was   developed acc ording t o cert ain e thics principles.    Certific ation c an incen tivize chang e   In  reducing   the  asymme try  of  informa tion  between  insider s  and  outsider s,  certific ation  can   further   serve  to  incen tivize  good  beha vior  by  the  insider s.  For  example,   corpor ations   may  be   more  motiv ated  to  achie ve  ethics   standar ds  if  they  can  use  certific ation  to  demons trate  their    achie vemen ts to cus tomer s who v alue these achie vemen ts.   The curr ent landsc ape of AI certific ation   The  paper   surveys  the  landsc ape  of  AI  certific ation  from  2020,   iden tifying  seven  activ e  and   proposed pr ograms:    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 6 0   
 ●  the  European   Commission   Whit e  Paper   on  Artificial   Intelligence   (this   is  outdated,  see  the   proposed Artificial In telligence Act),    ●  the IEEE E thics Certific ation Pr ogram f or Aut onomous and In telligence S ystems,    ●  the Malt a AI Inno vative Technology Arr angemen t,   ●  the T uring Certific ation pr oposed b y Aus tralia’s Chie f Scien tist,   ●  the Queen’ s Univ ersity e xecutiv e educ ation pr ogram Principles of AI Implemen tation,    ●  the Finland civics c ourse Elemen ts of AI, and    ●  a Danish pr ogram in de velopmen t for labeling IT -security and r esponsible use of da ta.   These   programs  demons trate  the  variety  of  forms   AI  certific ation  can  take,  including   both   public    and priv ate, certif ying both individuals and gr oups, and c overing a r ange of AI-r elated activities.    The v alue of certific ation f or futur e AI r esear ch and de velopmen t   Finally ,  the  paper   addr esses   the  potential  value   of  certific ation  for  futur e  AI  technology .  Some    aspects   of  certific ation  will  likely  remain   relevant  even  as  the  technology   chang es.  For  example,    the  various   roles  of  corpor ations,   their   emplo yees  and  manag emen t,  governmen ts,  and  other    actors  tend  to  stay  the  same.   Likewise,   certific ation  programs  can  remain   relevant  over  time   by   emphasizing   human   and  institutional   factors.  Programs  can  also  build   in  mechanisms   to  upda te   their   certific ation  criteria  as  AI  technology   chang es.  Looking   further   into  the  futur e,  certific ation   may  play  a  constructiv e  role  in  governance   of  the  processes   that  lead   to  the  developmen t  of   advanced   systems.   Certific ation  could   be  especially   valuable   for  building   trust  among   rival  AI   developmen t  groups  and  ensuring   that  advanced   AI  systems  are  built   to  high   standar ds  of  safety   and e thics.    Between the lines    In  summar y,  certific ation  can  be  a  valuable   tool  for  AI  governance.   It  is  not  a  panacea   for   ensuring   ethical  AI,  but  it  can  help   especially   for  reducing   informa tion  asymme tries   and   incen tivizing   ethical  AI  developmen t  and  use.  The  paper   presen ts  the  first-ever  resear ch  study    of  AI  certific ation  and  therefore  serves  to  establish   essen tial  fundamen tals  of  the  topic,    including k ey terms and c oncep ts.   Collectiv e Action on Artificial In tellig ence: A Primer and R eview   [  Original paper   by Robert de Neuf ville and Se th D .  Baum]    [Resear ch Summar y by Robert de Neuf ville]    Overview  :  The  developmen t  of  safe  and  socially   bene ficial   AI  will  requir e  collectiv e  action,   in   the  sense   that  outcomes   will  depend   on  the  efforts  of  man y  different  actors.  This  paper   is  a   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 6 1   
 primer   on  the  fundamen tal  concep ts  of  collectiv e  action   in  social   science   and  a  review  of  the   collectiv e  action   literature  as  it  pertains  to  AI.  The  paper   consider s  different  types   of  AI   collectiv e  action   situa tions,   different  types   of  AI  race  scenarios,   and  different  types   proposed    solutions t o AI c ollectiv e action pr oblems.    Introduction    The  developmen t  of  safe  and  socially   bene ficial   AI  will  requir e  man y  different  people   working    together.  Social   scien tists  have  extensiv ely  studied   different  types   of  “collectiv e  action”    situa tions   that  requir e  actors  to  cooper ate  in  some   way  to  achie ve  the  best  outcomes   for  the   group  as  a  whole.   How  difficult   it  will  be  to  achie ve  the  best  outcomes   may  depend   on   structur al  factors,  like  the  extent  to  which   the  interests  of  individuals   diverge  from  the  interests   of  the  group  as  a  whole,   the  nature  of  the  goods   involved,  and  the  degr ee  to  which   they  hing e   on the e fforts of a single act or or on some c ombina tion of dif ferent act ors.   In  this  paper ,  we  first  presen t  a  primer   on  the  theor y  of  collection   action   and  relate  it  to  the   different  types   of  AI  collectiv e  action   situa tions.   The  paper   look s  in  particular   at  AI  race   scenarios,   which   have  been   a  major   focus   of  the  literature  on  AI  collectiv e  action   literature.  AI   races   could   hasten  the  arriv al  of  bene ficial   forms   of  AI,  but  could   be  dang erous  if  individual    actors  rush   developmen t  in  order  to  be  the  first  to  develop   a  particular   AI  technology .  Second,    we  review  the  three  primar y  types   of  potential  solutions   to  AI  collectiv e  action   problems:    governmen t regula tion, priv ate mark ets, and c ommunity self -organization.    Collectiv e Action and AI issues    The  impact   of  AI  on  socie ty  will  ultima tely  depend   on  the  actions   of  man y  different  people   and   groups.  In  some   cases,   the  interests  of  individual   actors  will  align   with   the  interests  of  socie ty  as   a  whole,   so  that  good  outcomes   will  result   from  individual   actors  pursuing   their   own  interest.  In   other   cases,   some   actors  will  be  able   to  bene fit  individually   from  acting   against  the  interest  of   socie ty.  In  these   cases,   AI  outcomes   may  depend   on  the  extent  to  which   the  interests  of   individuals and socie ty as a whole c an be r econciled.    In  public   choice   theor y,  collectiv e  action   is  requir ed  wher e  outcomes   depend   on  the  actions   of   different  people   with   different  interests.  Man y  aspects   and  applic ations   of  AI  will  requir e   collectiv e  action.   In  particular ,  collectiv e  action   will  be  needed   (1)  to  reach   agreemen t  on  rules    and  standar ds,  (2)  to  develop   AI  that  is  broadly   bene ficial   rather   than   merely  profitable   or   other wise   advantageous   for  particular   developer s,  and  (3)  to  avoid  compe tition   or  conflict  that   could lead t o AI be de veloped or used in a w ay tha t is unsa fe.   In  recen t  years,  a  large  but  dispar ate  literature  has  look ed  at  the  challeng es  of  collectiv e  action    with   respect   to  AI.  One  import ant  distinction   is  between  coordina tion  problems   like  the   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 6 2   
 developmen t  of  common   AI  platforms,   in  which   individual   and  collectiv e  interests  mos tly  align,    and  compe titive  situa tions   like  compe titive  AI  races,   in  which   individual   and  collectiv e  interests   diverge.  In  gener al,  collectiv e  action   is  easier   to  achie ve  when   the  interests  of  individuals   align    with   the  interests  of  the  group.   The  type   of  collectiv e  action   problem   can  in  turn   depend   on   whe ther   the  goods   involved  are  “excludable”   (that  is,  can  be  restricted  to  particular   consumer s)   or  “rivalrous”   (that  is,  is  used   up  when   its  bene fits  are  enjo yed).  Typically,  the  interests  of   individuals   and  the  group  are  easy  to  align   when   goods   are  excludable—bec ause   their   use  can   be  limit ed  to  those   who   have  paid   for  them   in  some   sense—and   non-riv alrous—bec ause   their    supply   is  not  limit ed.  Another   import ant  issue   is  the  degr ee  to  which   addr essing   a  collectiv e   action   situa tion  depends   primarily   on  the  effort  of  a  single   actor  or  requir es  man y  actors  to   contribut e some thing.    One  type   of  collectiv e  action   situa tion  that  has  receiv ed  a  lot  of  attention  in  the  literature  is  AI   race  scenarios.   AI  races   could   be  dang erous  if  individual   actors’  interest  in  winning   the  race  is  at   odds   with   the  gener al  interest  in  developing   AI  that  is  safe  and  socially   bene ficial.   The  paper    look s  at  both   near -term  and  long-t erm  AI  races.   The  literature  iden tified   in  this  paper   focuses   in   particular   on  near -term  races   to  develop   milit ary  AI  applic ations   and  long-t erm  AI  races   to   develop   advanced   forms   of  AI  like  artificial   gener al  intelligence   and  artificial   superin telligence.    The  two  types   of  races   are  potentially   related  since   near -term  races   could   affect  the  long-t erm   developmen t of AI.    Finally ,  the  paper   evalua tes  three  different  types   of  potential  solutions   to  collectiv e  action    problems:   governmen t  regula tion,   private  mark ets,  and  community   self-organization.   All  three   types   of  solution   can  addr ess  collectiv e  action   problems,   but  no  single   appr oach   is  a  silver-bulle t   solution   to  the  entire  range  of  collectiv e  action   problems.   It  may  be  better  to  pursue  a  mix  of   different  types   of  solutions   to  addr ess  AI  collectiv e  action   in  different  ways  and  at  different   scales.   Governance   regimes   will  also  need   to  account  for  other   factors,  like  the  extent  to  which    AI de veloper s are transpar ent about their t echnology .   Between the lines    The  collectiv e  action   issues   raised   by  AI  are  increasingly   pressing.   Collectiv e  action   will  be   necessar y  to  ensur e  that  AI  serves  the  public   interest  rather   than   simply   serving  the  narrow   interests  of  those   who   develop   it.  Collectiv e  action   will  also  be  necessar y  to  ensur e  that  AI  is   developed   with   appr opria te  risk  manag emen t  protocols  and  adequa te  safety  measur es.  The   institutions   we  develop   now  to  help   resolv e  the  AI  collectiv e  action   problems   that  arise   today   could   have  long-las ting  and  far-reaching   consequences.   The  literature  on  AI  collectiv e  action    situa tions   is  still  young;   a  great  deal   more  work  on  designing   systems  to  govern  specific   AI   collectiv e action pr oblems s till remains t o be done.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 6 3   
 AI Ethics Ma turity Model    [  Original paper   by Kathy Ba xter]   [Resear ch Summar y by Connor W right]   Overview  :  With   ethical  AI  certainly   being   a  hot  topic   in  the  business   world,   how  can  this  be   achie ved?  The  Ethical  AI  Practice   Maturity   Model   sets  out  4-steps  towards  achie ving  the  end   goal  of  an  “end-t o-end-e thics-b y-design”   model.   With   that  in  sight,  the  need   for  compan y-wide    participa tion and the passion f or building e thical AI ar e a mus t.   Introduction    Does   your  compan y  engage  in  AI  products?   Does   it  have  an  ethical  AI  team?   If  not,  how  would    such   a  team   be  established?   The  Ethical  AI  Practice   Maturity   Model   aims   to  answer  the  latter.   Stretching   from  the  incep tion  of  an  ad  hoc  review  from  a  group  of  emplo yees,  to  having  ethical   AI  awareness   coursing  through   a  compan y’s  veins,   it  offers  us  a  roadmap.   Calling   on   compan y-wide   engagemen t  alongside   a  passion   for  ethical  AI,  the  end  goal  includes   having   ethical  thresholds   necessar y  for  the  AI  product   to  pass   in  order  to  be  launched.   The  best  way  to   illustrate ho w this c an be achie ved is t o go thr ough the model itself .   Ad Hoc    Ques tioning   of  the  AI  process   at  hand   begins   to  take  hold.   Cert ain  issues   arise   and  are  then    brough t  into  ques tion  on  an  ad  hoc  basis.   The  ques tion  no  long er  becomes   “can  we  do  this? ”,   but  instead  “should   we  do  this? ”.  The  result ant  conversations   can  prove  good  fuel  for  informal    talks  about   the  technology ,  helping   to  clarif y  the  import ance   of  these   problems.   Once   these    issues   are  known  and  emplo yees  can  see  them   being   dealt   with,   trust  can  start  to  be  developed    between those designing the AI and the wider c ompan y.   However,  the  desir ed  confidence   takes  time   to  develop.   So,  building   an  ethical  AI  team   that   accumula tes  “small   wins”   can  help   consolida te  their   position   in  the  AI  process.   Churning   out   results,   big  or  small,   will  help   create  more  advocates  throughout   the  business   and  cultiv ate   pivotal involvemen t from those a t the t op.   Organized and r epea table    Arriving   at  this  stage  means   executiv es  are  now  on  boar d,  and  responsible   AI  practices   are  now   being   rewarded.   As  a  result,   the  next  step  lies  in  convincing   internal   stakeholder s  to  join  the   process   as  well.  Demons trating  why  getting  involved  is  crucial   by  explaining   the  risks  involved   with   AI  is  a  sure  way  to  get  more  emplo yees  to  sign  up.  Mor eover,  contextualizing   AI  in  the   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 6 4   
 compan y  context  and  within   its  ethical  principles   to  explain   its  import ance   could   prove  even   more gripping.    Wha t  executiv es  mus t  not  do  is  simply   “ethics   washing ”  compan y  emplo yees.  This  entails   placing   broad  ethical  principles,   such   as  ‘AI  mus t  always  do  the  right  thing ’,  and  sticking   it  on  the   compan y’s  website.  Instead,   how  these   compan y  principles   that  apply   to  AI  will  be  achie ved  is   paramoun t for forming a success ful e thical AI t eam.    Hence,   the  stage  also  includes   the  forma tion  of  the  team   itself .  Given  the  different  situa tions    the  team   will  face,  different  expertise   will  be  requir ed.  Accordingly ,  the  team   should   be   composed   of  diverse  skill  sets,  backgr ounds,   and  under standings.   Furthermor e,  the  metrics   for   evalua tion  should   not  be  classic   “revenue   gener ation”   and  the  like,  but  rather   making   sure  the   AI systems ar e safe and not being penaliz ed when the y iden tify ethical risk s.   Given  the  need   to  iden tify  these   risks,  consider ations   on  ques tions   of  scale  can  be  help ful.  To   join  the  team,   wha t  is  the  base   knowledg e  all  emplo yees  should   have  of  AI?  How  would   you   design   formal   training   to  convey  this  knowledg e?  Would   teams   working   on  AI  be  able   to  loop   in   the  ethical  AI  team?   Wha tever  the  answers  to  these   ques tions   are,  it  needs   to  be  sustained   and   manag ed in the long run.    Manag ed and Sus tainable    The  training   requir ed  for  the  desir ed  base   level  of  knowledg e  mus t  only   include   manda tory   elemen ts  for  all  emplo yees  if  it’s  necessar y.  The  compan y’s  ethical  principles   ough t  to  be   common   knowledg e,  but  knowing   how  to  mitig ate  AI  system  bias  is  only   relevant  for  data   scien tists. Managing wha t the tr aining allo ws emplo yees t o find is the ne xt import ant step.   Coming   across  an  AI  ethical  risk  is  not  to  be  frowned   upon   comple tely.  No  AI  system  can  be   100%   bias- free,  so  saying  wha t  bias  there  is,  how  it’s  being   mitig ated,  and  the  potential  harms   it   could   cause   is  the  best  way  to  deal   with   the  problem.   Any  damag es  that  are  then   caused   (which    can  vary  depending   on  the  person)   need   to  have  appr opria te  channels   to  be  brough t  up.  Should    your  business   stretch  across  different  countries,   the  ethical  review  mus t  ensur e  the  AI  system   includes   other   languag es  and  cultur es.  Dealing   with   bias  for  your  Americ an  clien ts  will  not  be   the same when appr oaching y our T aiwanese partner s.   Optimiz ed and inno vative   The  final   stage  is  the  one  to  be  mos t  desir ed  to  achie ve.  The  ethical  AI  team   is  no  long er  a   central  hub  but  rather   disper sed  throughout   the  whole   compan y.  Products   and  resour ces   requir e  that  ethical  debt  is  resolv ed  to  be  realiz ed,  ensuring   an  “end-t o-end-e thics-b y-design”    model.   However,  this  does   not  mean   that  striving   for  perfection   is  halted.  With   “practice”   being    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 6 5   
 the  keyword,  ethical  AI  practice   never  reaches   its  conclusion.   New  inno vations   bring   new   techno-e thical  issues,   requiring   even  more  elabor ation  from  the  diverse  backgr ounds   of  the   ethical  AI  team.   This  stage  may  be  the  end  goal,  but  the  end  goal  is  a  refined   process,   not  a   product.    Between the lines    In  my  view,  ethical  AI  practice   is  both   necessar y  and  sufficien t  to  oper ationalise   principles   like   transpar ency ,  fairness   and  equality .  Subsequen tly,  any  ethical  review  needs   to  happen   early   in   the  design   process;   other wise,   there’s  no  time   to  mak e  signific ant  chang es.  Should   this  not  be   the  case,  the  rise  of  “ethical  debt”  from  unethical  AI  models,   though   almos t  invisible   during   the   AI  design,   will  become   very  tangible   in  the  form  of  harm   to  the  public.   The  Ethical  AI  Practice    Maturity   Model   gives  a  compan y  a  roadmap   to  follow  and  harbor s  the  vital  poin t  that  chang e   mus t come fr om all. Br avery is r equir ed, and it all s tarts with tha t first small win.    Mapping v alue sensitiv e design on to AI f or social g ood principles    [  Original paper   by Steven Umbr ello, Ibo v an de P oel]   [Resear ch Summar y by Marianna Ganapini]    Overview  :  Value   sensitiv e  design   (VSD)  is  a  method   for  shaping   technology   in  accordance   with    our  values.   In  this  paper ,  the  author s  argue  that,  when   applied   to  AI,  VSD  faces   some   specific    challeng es  (connect ed  to  machine   learning ,  in  particular).   To  addr ess  these   challeng es,  they   propose   modif ying  VSD,  integrating  it  with   a  set  of  AI-specific   principles,   and  ensuring   that  the   unin tended uses and c onsequences of AI t echnologies ar e monit ored and addr essed.    Introduction    How  do  we  bridg e  theor y  and  practice   when   it  comes   to  following   ethical  principles   in  AI?  This   paper   aims   at  answering   that  very  ques tion  by  adop ting  Value   sensitiv e  design:   a  set  of  steps  to   implemen t  values   in  technologic al  inno vation.   Value   sensitiv e  design   potentially   applies   to  a   vast  range  of  technologies,   but  when   used   in  AI  and  machine   learning ,  it  inevitably  faces   some    specific   challeng es.  The  author s  propose   a  way  to  fix  these   problems   by  integrating  Value    sensitiv e design with other actionable fr ame works.   Value sensitiv e design (V SD)   Value   sensitiv e  design   (VSD)  is  a  method   originally   developed   by  resear cher s  at  the  Univ ersity  of   Washing ton  and  it  lays  out  actional   steps  for  designing   technology   in  accordance   with   our   values.   These   steps  are  grouped   in  three  main   categories:   concep tual,   empiric al  and  technic al   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 6 6   
 investigations.   Concep tual  analy sis  determines   the  appr opria te  set  of  values   (coming   from  the   philosophic al  literature  and/ or  from  the  stakeholder s’  expect ations),   wher eas  empiric al   investigations   may  survey  direct  and  indir ect  stakeholder s  to  under stand  their   values   and  needs.    The  third  set  of  steps  look s  into  potential  technic al  limit ations   and  resour ces  to  design   a   technology f ollowing the appr opria te set of v alues.    Unfortuna tely,  the  self-learning   capabilities   of  AI  pose   some   specific   challeng es  for  VSD.   Notoriously ,  models   developed   through   machine   learning   can  have  features  that  were  not   initially   designed   or  foreseen,   and  some   of  these   features  may  be  opaque   and  thus   not  easily    detectable.   This  could   mean   that  AI  systems,   originally   designed   following   VSD,  “may  have   unin tended   value   consequences,   [ …]  or  unin tentionally   ‘disembody ’  values   embedded   in  their    original   design. ”  As  the  author s  explain,   this  means   that  we  need   design   principles   specific   for   this  kind   of  technology   and  expand   VSD  to  addr ess  those   challeng es.  The  ques tion  is  how  to  do   that.   Solutions    The  author s  propose   to  modif y  VSD  in  the  following   three  ways:  (1)  VSD  should   include   a  set  of   AI-specific   principles   (AI4SG);   (2)  for  VSD,  the  goal  should   be  not  only   to  promot e  outcomes   that   avoid  harming   but  also  to  contribut e  to  social   good  overall;  (3)  VSD  should   look   at  the   downstream   consequences   of  adop ting  a  certain  AI  system  to  mak e  sure  the  designed   values    are in f act r espect ed.   2.1 V SD & AI4SG    Let's  start  with t  the  first  poin t.  The  author s  propose   to  adop t  AI-specific   principles   in  VSD.  In   particular ,  they  look   at  AI4SG   (AI  for  social   good)   principles,   which   are  actionable   guidelines,    inspir ed  by  the  more  high   level  values   of  “respect   for  human   autonom y,  prevention  of  harm,    fairness, and e xplic ability ”. These ar e the principles:    “(i)  falsifiability   and  incremen tal  deplo ymen t;  (ii)  safeguar ds  against  the  manipula tion  of   predict ors;  (iii)  receiv er-contextualiz ed  intervention;   (iv)  receiv er-contextualiz ed  explana tion  and   transpar ent  purposes;   (v)  privacy  protection   and  data  subject   consen t;  (vi)  situa tional   fairness;    and (vii) human- friendly seman ticization. ”   The  author s  of  the  paper   poin t  out  that  applying   these   specific   principles   in  the  design   of  AI   systems  would   addr ess  some   of  the  concerns   men tioned   above.  This  is  because   these   steps  are   not  only   more  practic al  than   the  high-le vel  values   but  they  are  also  specific   to  AI  and  so  are  the   right  tools  to  avoid  the  challeng es  raised   by  this  kind   of  technology .  These   principles   are,  in   other   words,  a  more  concr ete  applic ation  of  the  key  values   (e.g.  bene ficence)   we  want  to  see  as   part of the design of AI g oing f orward.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 6 7   
 2.2 V SD & the social g ood   Here’s  the  second  issue:   VSD  should   be  not  only   to  promot e  outcomes   that  avoid  doing   harm    but  also  to  contribut e  to  social   good  and  so  “ther e  mus t  be  an  explicit   orien tation  toward   socially   desir able   ends. ”  To  promot e  this,   the  author s  recommend   that  VSD  adop ts  “the    Sustainable   Developmen t  Goals   (SDGs),   proposed   by  the  Unit ed  Nations,   as  the  best   appr oxima tion  of  wha t  we  collectiv ely  belie ve  to  be  valuable   socie tal  ends” .  Again,  this  is  a   matter  of  complemen ting  and  enriching   VSD  with   a  set  of  principles   that  activ ely  try  to  promot e   social g ood, and as such, the y should be part of the design of AI s ystems.    2.3 V SD and do wnstream c onsequences    Finally ,  ongoing   monit oring   is  needed   to  addr ess  possible   unin tended   consequences   of  adop ting   AI  systems.   Indeed,   when   emplo yed,  AI  systems  may  not  respect   the  original   design   values   (see   here  for  more).  This  is  why  there  is  the  need   to  apply   VSD  to  the  entire  “life  cycle  of  an  AI   technology ”,  monit oring   systems,   and  adop t  the  necessar y  design   chang es  when   needed.   The   author s  poin t  out  that  prototyping   and  small   scale  testing  could   really   help   addr ess  unforeseen    consequences.    By  combining   these   principles   and  ideas,   the  author s  embr ace  a  frame work  that  encompasses    the f ollowing r ecur sive loop:    Context  Analysis   (e.g.   socie tal  challenges,   values   for  stakeholders)   →  Value   Iden tification   (e.g.    bene ficence,  autonom y,  SDGs,   case  specific   values)   →  Design   Requiremen ts  (e.g.   AI4SG),   →   Prototyping (e.g. small-sc ale t esting)    This  proposed   frame work  is  mean t  to  be  taking   into  account  the  various   aspects   of  VSD  while    also addr essing some of its short comings.    Between the lines    It  is  import ant  to  find  a  way  to  bridg e  theor y  and  practice   when   it  comes   to  building   ethical  AI   systems.   This  paper   is  charting   a  way  forward  to  addr ess  this  need.   It  brings   together   different   methods   and  appr oaches   by  explaining   how  to  integrate  action   steps  within   the  VSD  frame work   while   also  making   sure  social   good  is  taken  into  account.  Now  that  we  have  a  fairly    compr ehensiv e  set  of  high-le vel  values,   futur e  resear ch  will  need   to  establish   more  precise,    actionable   and  concr ete  steps  to  embody   those   values   within   AI  systems,   and  it  will  need   to  find   new  ways  to  determine   the  ethically  relevant,  downstream   consequences   of  the  use  of  those    systems.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 6 8   
 Embedding V alues in Artificial In tellig ence (AI) S ystems    [  Original paper   by Ibo v an de P oel]   [Resear ch Summar y by Andr ea P edeferri]    Overview  :  Though   there  are  numer ous  high-le vel  norma tive  frame works,  it  is  still  quite  unclear    how  or  whe ther   values   can  be  implemen ted  in  AI  systems.   Van  de  Poel  and  Kroes’s  (2014)   have   recen tly  provided   an  account  of  how  to  embed   values   in  technology .  The  current  article    proposes   to  expand   that  view  to  comple x  AI  systems  and  explain   how  values   can  be  embedded    in technologic al systems tha t are “autonomous, in teractiv e, and adap tive”.   Introduction    Though   there  are  numer ous  high-le vel  norma tive  frame works,  it  is  still  quite  unclear   how  or   whe ther   those   frame works  can  be  implemen ted  in  AI  systems.   Van  de  Poel  and  Kroes’s  (2014)    have  recen tly  provided   an  account  of  how  to  embed   values   in  technology   in  gener al.  The   current  article   proposes   to  expand   that  view  to  AI  systems  which,   according   to  the  author ,  have   five  building   block s:  “technic al  artifacts,   institutions,   human   agents,  artificial   agents,  and   technic al  norms” .  This  paper   is  a  very  useful  guide   to  under standing   how  values   can  be   embedded in a c omple x system c omposed of multiple parts tha t interact in dif ferent ways.   Embedding V alues    Organizations   such   as  the  EU  High-Le vel  Expert   Group  on  AI  and  the  IEEE   have  provided   a  list  of   high-le vel  ethical  values   and  principles   to  implemen t  in  AI  systems.   Wha tever  your  views  on   values   migh t  be,  the  paper   poin ts  out  that  we  need   an  account  of  wha t  it  means   for  those    values   to  be  embedded.   To  start,  a  set  of  values   is  said  to  be  ‘embedded’   only   if  it  is  integrated   into  the  system  by  design.   That  is,  those   who   design   the  system  should   intentionally   build   that   system  with   a  specific   set  of  values   in  mind.   Mor e  is  needed,   though,   because   even  if  a  system  is   designed t o comply with cert ain v alues, tha t does not mean it will r eally r ealiz e those v alues.    So  the  paper   proposes   the  following   definition   of  “embodied   values”:   “The  embodied   value   is   the  value   that  is  both   intended   (by  the  designer s)  and  realiz ed  if  the  artifact  or  system  is   properly used. ”   Drawing   both   from  the  current  paper   and  Van  de  Poel  and  Kroes’s  (2014),   we  have  the  following    set of use ful de finitions:    Designed v alue   : any value tha t is in tentionally part   of the design of a t echnologic al system   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 6 9   
 Realiz ed v alue   : any value tha t the (appr opria te) use of the s ystem is pr one t o bring about    Embedded   value   :  any  value   that  is  both   designed   and  realiz ed.  Thus,   a  value-embedded   system   is  a  system  that,  because   of  the  way  it  was  designed,   will  bring   about   certain  values   (when   it  is   properly used).    As  the  paper   explains,   this  opens   the  door   to  the  idea   of  a  feedback   loop:   when   an  intended    value   is  not  realiz ed,  there  has  to  be  some   chang e  in  the  way  it  is  used   and/ or  designed.    Similarly ,  if  a  system  is  used   in  a  way  that  is  contrary  to  intended   values,   a  re-design   migh t  be  in   order.  As  the  author   poin ts  out,  the  practice   of  re-design   systems  to  avoid  unin tended    consequences   “is  particularly   import ant  in  the  case  of  AI  systems,   which   due  to  the  adap tive   abilities   of  AI,  may  acquir e  system  properties   that  were  never  intended   or  foreseen   by  the   original designer s.”   Embedding V alues in AI s ystems    This  account  provides   a  way  to  under stand  how  values   can  be  embedded   in  AI  by  looking   both    at  the  componen ts  and  the  system  level.  Mor e  specific ally,  the  paper   under stands   AI  systems  as   socio-t echnic al  systems  composed   not  only   of  “technic al  artifacts,   human   agents,  and   institutions”   but  also  “artificial   agents  and  certain  technic al  norms   that  regula te  interactions    between  artificial   agents  and  other   elemen ts  of  the  system.”  To  clarif y,  a  socio-t echnic al  system   is  a  system  that  depends   “on  not  only   technic al  hardware  but  also  human   beha vior  and  social    institutions f or their pr oper functioning (cf . Kroes e t al. 2006). ”   To  start,  the  paper   clarifies   that  an  AI  system  will  be  the  result   of  both   social   institutions   and   human   agents  interacting   to  design   technologic al  artifacts  in  accordance   with   certain  values.    Import antly,  the  paper   poin ts  out  that  those   social   institutions   will  also  be  embedded   with    values.   As  such,   the  role  of  humans   is  key:  they  need   to  monit or  and  evalua te  the  outcomes   and   use  of  both   the  technologic al  artifacts  and  the  social   institutions   that  influence   the  production    and  design   of  those   technologic al  artifacts.   In  addition,   because   of  how  AI  systems  work,  there   will  also  be  technic al  norms   that  regula te  how  artificial   agents  interact  with   humans   and  social    institutions. As such, these norms will embed and pr omot e cert ain v alues.    Ther efore,  in  conclusion,   an  AI  system  promot es  a  set  of  values   if  and  only   if  all  five  of  its  main    componen ts  (i.e.  technic al  artifacts,   institutions,   human   agents,  artificial   agents,  and  technic al   norms)   will  either   embody   or  intentionally   promot e  V.  As  the  author   rightly  poin ts  out  then,   “AI   systems  offer  unique   value-embedding   opportunities   and  constraints  because   they  contain   additional   building   block s  compar ed  to  traditional   sociot echnic al  systems.   While   these   allow   new  possibilities   for  value   embedding ,  they  also  impose   constraints  and  risks,  e.g.,  the  risk  that   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 7 0   
 an  AI  system  disembodies   certain  values   due  to  how  it  evolves.  This  means   that  for  AI  systems,    it is crucial t o monit or their r ealiz ed values and t o undert ake continuous r edesign activities. ”   Between the lines    The  paper   is  a  very  useful  guide   to  under standing   how  values   can  be  embedded   in  a  comple x   system  composed   of  multiple   parts   that  interact  in  different  ways.  The  next  step  is  to  figur e  out   how  this  analy sis  connects   to  the  deba te  on  trust  and  trustworth y  AI:  given  the  current  way  we   under stand v alue-embedded AI, is it possible t o build an AI w e can actually trus t?   Mor al consider ation of nonhumans in the e thics of artificial in tellig ence    [  Original paper   by Andr ea Ow e and Se th D . Baum]    [Resear ch Summar y by Andr ea Ow e]   Overview  :  As  AI  becomes   increasingly   impactful   to  the  world,   the  extent  to  which   AI  ethics    includes   the  nonhuman   world  will  be  import ant.  This  paper   calls  for  the  field   of  AI  ethics   to  give   more  attention  to  the  values   and  interests  of  nonhumans.   The  paper   examines   the  extent  to   which   non-humans   are  given  moral  consider ation  across  AI  ethics,   finds   that  attention  to   nonhumans   is  limit ed  and  inconsis tent,  argues   that  nonhumans   merit   moral  consider ation,   and   outlines fiv e sug gestions f or ho w this c an be tter be inc orpor ated acr oss AI e thics.    Introduction    Is  the  field   of  AI  ethics   adequa tely  accounting  for  nonhumans?   Recen t  work  on  AI  ethics   has   often  been   human-cen tered,  such   as  on  “AI  for  people” ,  “AI  for  humanity ”,  “human-c ompa tible    AI”,  and  “human-cen tered  AI”.  This  work  has  value   by  shifting   emphasis   away  from  the  narrow   interests  of  developer s,  but  it  does   not  include   explicit   consider ation  of  nonhumans.   How  do  AI   systems’   resour ces  and  ener gy  use  impact   nonhumans?   Wha t  is  the  potential  of  AI  for   environmen tal  protection   or  animal   welfare?  Social   algorithmic   bias  is  currently  a  major   topic    but  are  there  import ant  nonhuman   algorithmic   biases?   How  may  we  incorpor ate  nonhuman    interests and v alues in to AI s ystem design? Wha t migh t be the risk s of not doing so?    This  paper   documen ts  the  state  of  attention  to  nonhumans   in  AI  ethics   and  argues   that  the  field    can  and  should   do  more.  The  paper   finds   that  the  field   gener ally  fails  to  give  moral   consider ation  to  nonhumans,   such   as  nonhuman   animals   and  the  natural  environmen t,  aside    from  some   consider ation  of  the  AI  itself .  The  paper   calls  on  the  field   to  give  more  attention  to   nonhumans, sug gesting fiv e specific w ays AI r esear cher s and de veloper s can acc omplish this.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 7 1   
 Wha t it means t o giv e mor al consider ation t o nonhumans    Mor al  consider ation  of  nonhumans   means   activ ely  valuing   nonhumans   for  their   own  sake.  In   moral  philosoph y  terminology ,  to  “intrinsic ally  value”   nonhumans.   One  can  fail  to  give  moral   consider ation  to  nonhumans   by  activ ely  denying  their   intrinsic   value   or  by  neglecting   to  activ ely   recogniz e  their   intrinsic   value.   Ther e  are  man y  concep tions   of  which   nonhumans   merit   moral   consider ation,   such   as  the  welfare  of  nonhuman   animals   or  sentient  AI  systems,   or  the   flourishing   of  ecosystems.   Mor al  consider ation  of  nonhumans   does   not  requir e  any  one  specific    concep tion  of  which   nonhumans   merit   moral  consider ation.   It  also  does   not  requir e  a  specific    type of mor al frame work, such as c onsequen tialism, deon tology , or virtue e thics.    Why it ma tters tha t AI e thics mor ally c onsider nonhumans    Mor al  consider ation  of  nonhumans   is  a  practic al  issue   for  real-w orld  AI  systems,   with   several   matters  at  stake.  For  example,   AI  can  be  applied   for  the  advancemen t  of  nonhuman   entities,    such   as  for  environmen tal  protection.   On  the  other   hand,   AI  can  inadv ertently  harm   the   nonhuman   world,   such   as  via  its  consider able   ener gy  consump tion.   Cert ain  algorithmic   biases    could   additionally   affect  nonhumans   in  a  variety  of  ways.  Further ,  the  long-t erm  prospect   of   strong  AI  or  artificial   gener al  intelligence   may  radically  transform  the  world  for  humans   and   nonhumans   alike.  The  extent  to  which   non-humans   are  morally  consider ed  can  play  an   import ant role in assessing ho w AI s ystems should be designed, built, and used.    Empiric al findings: Limit ed a ttention t o nonhumans    The  paper   surveys  a  variety  of  prior   work  in  AI  ethics   in  terms   of  the  extent  to  which   it  gives   moral  consider ation  to  nonhumans.   Overall,  the  paper   finds   that  the  field   gener ally  fails  to  give   moral  consider ation  to  nonhumans.   The  primar y  exception  is  the  line  of  resear ch  on  the  moral   status  of  AI.  The  paper   finds   no  attention  to  nonhumans   in  76  of  84  sets  of  AI  ethics   principles    surveyed  by  Jobin   et  al.,  40  of  45  artificial   gener al  intelligence   R&D   projects   surveyed  by  Baum,    38  of  44  chap ters  in  the  Oxford  Handbook   of  Ethics   of  AI,  and  13  of  17  chap ters  in  the  anthology    Ethics   of  Artificial   Intelligence.   In  the  two  latter  examples,   any  dedic ated  attention  is  on  the   moral status of AI itself . No other types of non-humans ar e giv en dedic ated a ttention.    The c ase f or mor al consider ation of nonhumans    Modern   science   is  unambiguous   in  documen ting  that  humans   are  member s  of  the  animal    kingdom   and  part  of  nature.  Attribut es  of  humans   that  are  commonly   intrinsic ally  valued,   such    as  human   life  or  human   welfare,  are  also  found   in  man y  nonhuman   entities.   It  would   very   arguably   be  an  unfair  bias  to  intrinsic ally  value   some thing   in  humans   but  not  intrinsic ally  value    the  same   thing   in  nonhumans.   Additionally ,  compelling   argumen ts  can  be  made   for  intrinsic ally   valuing   things   that  inher ently  transcend   the  human   realm,   such   as  biodiv ersity.  To  insis t  on  only    giving   moral  consider ation  to  humans   requir es  rejecting   all  of  these   argumen ts.  The  paper    posits tha t this is un tenable, meaning tha t nonhumans merit mor al consider ation.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 7 2   
 Wha t can be done? Fiv e sug gestions f or futur e work   AI  ethics   resear ch  needs   a  robus t  study   of  the  moral  consider ation  of  nonhumans,   focusing   on   issues   such   as  how  to  balance   between  humans   and  nonhumans,   the  handling   of  the  natural   nonhuman   world,   and  the  role  of  nonhumans   in  major   AI  issues.   For  example,   resear ch  in   ecolinguis tics  shows  that  English—the   primar y  languag e  for  AI  system  design—c ontains  biases    in  favor  of  humans   over  nonhumans.   This  insigh t  could   be  applied   to  the  study   of  nonhuman    algorithmic bias in, f or example, na tural languag e processing.    Statemen ts  of  AI  ethics   principles   should   give  explicit   attention  to  the  intrinsic   value   of   nonhumans.   The  Mon tréal  Declar ation  for  the  Responsible   Developmen t  of  Artificial    Intelligence   is  one  example,   with   the  principle   stating:   “The  developmen t  and  use  of  artificial    intelligence   systems  (AIS)   mus t  permit   the  growth  of  the  well-being   of  all  sentient  beings. ”  For   illustration,   an  even  stronger  statemen t  would   be:  “The  main   objectiv e  of  developmen t  and  use   of  AIS  mus t  be  to  enhance   the  wellbeing   and  flourishing   of  all  sentient  life  and  the  natural   environmen t, no w and in the futur e.”   AI  projects   that  advance   the  interests  and  values   of  nonhumans   should   be  among   the  projects    consider ed  when   selecting   which   AI  projects   to  pursue.  The  Micr osoft   AI  for  Earth  program  is  a   good  example   of  AI  used   in  ways  that  bene fit  nonhumans,   and  further   serves  as  an  example   of   how  to  oper ationaliz e  moral  consider ation  for  nonhumans   in  AI  project   selection.   The  program   supports   several  projects   for  environmen tal  protection   and  biodiv ersity  conser vation  that  give   explicit   moral  consider ation  to  nonhumans,   including   Wild   Me,  eMammal,   NatureSer ve,  and   Zamba Cloud.    The  inadv ertent  implic ations   for  nonhumans   should   be  accounted  for  in  decisions   about   which    AI  systems  to  develop   and  use,  such   as  the  material   resour ce  consump tion  and  ener gy  use  of  AI   systems.   AI  groups  should   ackno wledg e  that  if  an  AI  system  will/ could   cause   sufficien t  harm   to   nonhumans, it w ould be be tter to not use it in the fir st place.    AI  resear ch  should   investigate  how  to  incorpor ate  nonhuman   interests  and  values   into  AI  system   designs.   How  to  incorpor ate  human   values   is  currently  a  major   subject   of  study   in  AI,  but  some    of  the  proposed   techniques   do  not  apply   to  nonhumans.   AI  ethics   design   is  of  particular    import ance   for  certain  long-t erm  AI  scenarios   in  which   an  AGI  takes  a  major   or  dominan t   position within human socie ty, the w orld a t large, and e ven br oader portions of out er space.    Even  the  mos t  well-designed   AGI  could   be  catastrophic   for  some   nonhumans   if  it  is  designed   to   advance the in terests of humans or other nonhumans.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 7 3   
 Between the lines    In  summar y,  accounting  for  nonhumans   in  AI  R&D   is  critic al  to  ensur e  that  AI  bene fits  more  than    just  humans.   This  can  prevent  further   harm   to  nonhuman   entities   already   under   immense    pressur e  from  human   activities.   Furthermor e,  this  will  enable   the  field   to  better  handle   futur e   moral  issues,   such   as  the  potential  of  artificial   entities   like  AI  to  merit   moral  consider ation   themselv es.  In  addition,   there  are  plen ty  of  opportunities   for  AI  to  mitig ate  existing  harm   to   nonhumans   and  enable   bene fits  to  also  nonhumans.   As  documen ted  by  this  paper ,  the  AI  ethics    field   has  given  little  attention  to  nonhumans   thus   far.  Ther efore,  there  exist  manif old   opportunities f or w ork addr essing the implic ations of nonhumans acr oss AI design and use.    Governance of artificial in tellig ence    [  Original paper   by Araz Taeihagh]    [Resear ch Summar y by Angshuman K aushik]    Overview  :  The  various   applic ations   of  AI  not  only   offer  opportunities   for  increasing   economic    efficiency   and  cutting  costs,  but  they  also  presen t  new  forms   of  risks.  Ther efore,  in  order  to   maximiz e  the  bene fits  deriv ed  from  AI  while   minimizing   its  threats,  governmen ts’  worldwide    need   to  under stand  the  scope  and  the  depth  of  the  hazards  posed   by  it,  and  develop   regula tory   processes   to  addr ess  these   challeng es.  This  paper   describes   why  the  governance   of  AI  should    receiv e mor e attention, c onsidering the m yriad challeng es it pr esen ts.   Introduction    The  interne t  is  full  of  a  plethor a  of  websites  catering   to  the  diverse  needs   of  its  user s.  Mos t  of   these   websites  use  comple x  machine   learning   algorithms   to  mak e  the  browsing  experience   of  a   surfer  ‘seamless’   (as  the  mark eters  would   love  to  call  it).  For  example,   there  are  content   recommenda tion  algorithms   powering   certain  websites,  which   play  a  consider able   role  in   shaping   the  ‘though t  processes’   of  its  user s.  These   algorithms   apart   from  being   used   for   predicting   and  evalua ting  human   beha vior  are  also  used   for  profiling   and  ranking   people.    However,  there  have  been   instances,   when   these   content  recommenda tion  algorithms   have   been   criticiz ed  for  leading   and  exposing   user s  to  extreme   content.  Since   the  modus   oper andi   of   these   algorithms   is  built   to  engage  user s  and  keep  them   on  the  platform  (‘dollar s  for  eyeballs    men tality ’)  it  creates  a  ‘feedback   loop’ ,  by  suggesting  content  that  user s  have  expressed   interest   in.  The  consequence   is  that  the  user s  migr ate  from  milder   to  more  extreme   content.  The   situa tion  becomes   grave,  when   say,  for  example,   it  becomes   a  fertile   ground   for  any   insurr ectionis t  group  to  broadc ast  propag anda   upon   young   and  impr essionable   minds,   thereby,   attracting   devastating  consequences.   Hence,   in  such   scenarios,   the  governmen ts  need   to  step  in   and  keep  the  system  within   bounds,   by  formula ting  effectiv e  policies   and  regula tions.   This   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 7 4   
 paper   starts  off  with   an  introduction   to  the  all-per vading   and  omnipr esen t  AI,  reple te  with   its   various   value-laden   decisions   for  the  socie ty,  be  it,  in  clinic al  decision   support   systems,   policing    systems,   provision   of  personaliz ed  content  etc.  It  then   enters  into  the  awfully   difficult   territ ory   of  unexpect ed  consequences   and  risks  (in  the  form  of  bias,   discrimina tion  etc.),  associa ted  with    the  use  of  AI  systems  and  then,   proceeds   to  addr ess  the  challeng es  encountered  during   its   governance, and s teps forward.   AI - Gener al   Concep tions   of  AI  date  back   to  the  earlier   efforts  in  developing   artificial   neur al  networks  to   replic ate  human   intelligence,   which   can  be  referred  to  as  the  ability   to  interpr et  and  learn   from   the  informa tion.   The  presen t  AI  capabilities   have  expanded   to  include   comput er  programs  that   can  learn   from  massiv e  amoun ts  of  data  and  mak e  decisions   without   human   guidance,    commonly   referred  to  as  Machine   Learning   algorithms   (ML).   Although   these   algorithms   are   quite  fast  and  efficien t,  there  is  a  broad  consensus   that  it  still  falls  short   of  human   cognitiv e   abilities,   and  mos t  of  the  AI  systems  that  have  been   success ful  till  now,  belong   to  the  category   of  ‘narr ow  or  weak  AI.’  As  per  the  resear cher ,  some   of  the  incen tives  for  deplo ying  AI  include    increasing   economic   efficiency   and  quality   of  life,  mee ting  labor   short ages,  tackling   aging    popula tions e tc.   Under standing the risk s of AI    One  of  the  biggest  challeng es  faced   by  mos t  of  the  AI  systems  is  wha t  is  widely   referred  to  as   ‘corner   cases’   i.e.,  unexpect ed  situa tions,   that  the  system  had  not  been   trained   to  handle.    Further ,  the  decision-making   autonom y  of  AI  signific antly  reduces   human   control  over  their    decisions,   creating  new  challeng es  for  ascribing   liability   for  the  harms   imposed   by  it.  Mor eover,   given  the  value-laden   nature  of  the  outcomes   reached   by  the  algorithms,   AI  systems  can   potentially   exhibit   beha viour s  that  conflict  with   socie tal  norms   and  values,   promp ting  concerns    regarding   the  ethical  issues   that  can  crop  up  from  its  adop tion.   The  paper   also  highligh ts  the   hazards  of  data  privacy,  surveillance,   unemplo ymen t  and  social   instability   arising   from  the   deplo ymen t of AI applic ations.    Challeng es to AI Go vernance    According   to  the  paper ,  the  reason   why  the  governmen ts  face  innumer able   difficulties   in   designing   and  implemen ting  effectiv e  policies   to  govern  AI,  is  due  to  its  high   degr ee  of  inher ent   opacity ,  uncert ainty  and  comple xity,  which   mak es  it  challenging   to  ensur e  its  accountability ,   interpr etability ,  transpar ency   and  explainability .  Another   key  issue   surrounding   the  deba te  on   AI  governance   is  data  governance,   as  multiple   organizational   and  technic al  challeng es  exist  that   impede   effectiv e  control  over  data  and  attribution   of  responsibility   to  data-driv en  decisions    made   by  AI  systems.   To  add  to  the  above,  the  existing  regula tory  and  governance   frame works   are  ill-equipped   to  manag e  the  unique   and  novel  socie tal  problems   introduced   by  the  AI   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 7 5   
 systems.   The  Regula tors  being   gener alists,  struggle  enormously   when   it  comes   to   compr ehending   the  subtle  nuances   of  the  ever  evolving   AI  landsc ape.   Hence,   an  informa tion   asymme try  and  a  chasm   is  created  between  tech  companies   and  regula tors  which   prove  to  be  a   major   hindr ance   for  the  latter  in  formula ting  policies   and  regula tions   that  are  specific   to  the   issue   in  hand.   Further ,  considering   the  issues   associa ted  with   ‘hard’  regula tory  frame works,  the   discussion   in  the  paper   veers  towards  the  adop tion  of  self-regula tory  or  ‘soft  law’  appr oaches,    espoused   by  the  various   indus try  bodies   and  governmen ts  to  govern  AI.  ‘Soft   law’  appr oaches    refer  to  non-binding   norms   that  create  substantive  expect ations   that  are  not  directly    enforceable.   For  example,   indus try  bodies   like  IEEE   and  the  High-Le vel  Expert   Group  on  AI   formed   by  the  European   Commission   have  released   their   own  Ethics   Guidelines   for  Trustworth y   AI.  The  paper   also  raises   ques tion  mark s  at  the  efficacy  of  such   self-regula tory  initia tives  and   standar ds,  considering   their   volun tary  nature.  Another   challeng e  faced   by  the  governmen ts  is   the  signific ant  influence   exerted  by  the  big  technology   companies   in  the  formula tion  and   implemen tation  of  efficacious   AI  Policies,   through   their   lobb ying  efforts,   and  their   inclusion   in   the  AI  expert   groups  formed   by  the  governmen ts.  Studies   have  highligh ted  the  risks  of   regula tory  capture  by  AI  developer s  due  to  their   substantial  informa tional   advantages,  which    mak es  their   technologic al  expertise   particularly   valuable   to  the  regula tors.  The  paper   also  calls   for mor e resear ch in the field t o ensur e greater inclusivity and div ersity in AI g overnance.    Steps forward for AI Go vernance    According   to  the  author ,  as  AI  is  still  developing   with   the  potential  to  grow  more  salien t  and   diverse,  the  comple xity  of  its  challeng es  suggests  that  its  decision-making   needs   to  be  carefully    concep tualiz ed  according   to  their   context  of  applic ation,   and  these   framing   processes   should    be  subject   to  public   deba te.  In  fact,  there  are  increasing   calls  for  the  adop tion  of  inno vative   governance   appr oaches,   such   as,  adap tive  governance   and  hybrid   or  ‘de-cen tered’  governance    to  addr ess  the  governance   challeng es  posed   by  the  comple xity  and  the  uncert ainty  of  the  AI   systems.   The  char acteristic  of  adap tive  and  hybrid   governance   is  the  diminished   role  of  the   governmen t  in  controlling   the  distribution   of  resour ces  in  the  socie ty.  Another   area  of  emphasis    poin ted  out  is  the  presence   of  flexibility ,  which   is  imper ative  to  enable   diverse  groups  of   stakeholder s  to  build   consensus   around   the  norms   and  trade-of fs  in  designing   AI  systems,   as   well  as  for  global   AI  governance   to  be  applic able   across  different  geogr aphic al,  cultur al  and  legal   contexts,  and  aligned   with   existing  standar ds  of  democr acy  and  human   rights.  Further ,  the   paper   calls  for  learning   from  the  experiences   of  governing   previous   emer ging   technologies,   such    as,  the  interne t,  nanot echnology ,  aviation  safety  and  space   law.  Reference   is  also  made   towards   an  emer ging   body   of  literature  that  has  proposed   governing   AI  systems  through   their   design,    wher e  social,   legal  and  ethical  rules   can  be  enforced  through   code  to  regula te  the  beha viour   of   AI  systems.   According   to  the  author ,  the  trend  common   to  recen t  studies   in  their   proposed    frame works  for  AI  governance   is  the  emphasis   on  building   broad  socie tal  consensus   around   AI   ethical  principles   and  ensuring   accountability ,  but  there  is  a  need   for  studies   examining   how   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 7 6   
 these   frame works  can  be  implemen ted  in  practice.   He  goes  on  to  refer  to  different  frame works,   such   as,  the  socie ty-in-the-loop   frame work,  wher e  socie ty  is  first  responsible   for  finding    consensus   on  the  values   that  should   shape   AI  and  the  distribution   of  bene fits  and  costs  among    different  stakeholder s.  Another   appr oach   includes   the  centralization  and  cross-cultur al   cooper ation  to  impr ove  coordina tion  among   national   appr oaches.   However,  the  various   AI   governance   frame works  call  for  producing   more  concr ete  specific ations   on  implemen ting  these    governance   frame works  in  practice,   and  iden tifying  the  parties   in  governmen t  that  are   responsible f or leading dif ferent aspects of AI g overnance.    Between the lines    The  paper   quite  methodic ally,  decr ypts  the  trials   and  tribula tions   faced   in  governing   AI.  Even   the  solutions   envisioned   in  order  to  tackle   the  perils   associa ted  with   AI  systems  by  the   governmen t,  seem   workable,   to  a  large  extent.  In  fact,  the  paper   lays  out  a  very  feasible   and   pragma tic  path  for  the  governmen ts  to  follow,  while   formula ting  their   various   policies   and   regula tions,   concerning   AI.  Mor e  import antly,  the  findings   are  extremely   crucial,   considering   the   situa tion cr eated b y cert ain unbridled AI s ystems.    Avoiding   an  Oppr essiv e  Futur e  of  Machine   Learning:   A  Design   Theor y  for   Emancipa tory Assis tants   [  Original paper   by Ger ald C. K ane, Amber Y oung , Ann   Majchrz ak, and Sam Ransbotham]    [Resear ch Summar y by Sar ah P. Grant]   Overview  :  Broad  adop tion  of  machine   learning   systems  could   usher   in  an  era  of  ubiquit ous  data   collection   and  beha vior  control.  However,  this  is  only   one  potential  path  for  the  technology ,   argue  Gerald  C.  Kane  et  al.  Drawing   on  emancipa tory  pedag ogy,  this  paper   presen ts  design    principles   for  a  new  type   of  machine   learning   system  that  acts  on  behalf   of  individuals   within   an   oppr essiv e en vironmen t.   Introduction    “It  is  capitalism   that   assigns   the  price  tag  of  subjugation   and  helplessness,   not  the   technology ,”  asserts   Shoshana   Zubof f  in  her  bestselling   book,   The  Age  of  Surveillance    Capit alism: The Figh t for a Human Futur e at the Ne w Fr ontier of P ower.   In  contrast,  some   academics   argue  that  technology   itself   can  be  inher ently  oppr essiv e.  In  their    paper   about   emancipa tory  assis tants,  Kane  et  al.  demons trate  that  machine   learning   systems   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 7 7   
 have  several  oppr essiv e  features,  including   the  tendency   to  optimiz e  “on  outcomes   for  large   samples a t the e xpense of [individual user s].”   These   oppr essiv e  char acteristics  mean   that  as  systems  fuelled   by  machine   learning   seep   into   every  aspect   of  life–from  job  sear ching   to  reading   the  news–individuals   face  more  limits   on   their   freedoms.   Addr essing   this  problem,   argue  Kane  et  al.,  requir es  inno vative  appr oaches   to   machine learning s ystem design.    Using   the  emancipa tory  pedag ogy  of  Brazilian   educ ator  and  philosopher   Paulo   Freire  as  a   founda tion,   the  paper   presen ts  design   principles   for  a  new  type   of  machine   learning   system   called   the  “emancipa tory  assis tant”–an   agent  that  “would   help   individuals   express  and  enact    their pr eferences” in a w orld of per vasive da ta extraction and beha vior manipula tion.    The rise of in formania    To  illustrate  how  an  emancipa tory  machine   learning   system  would   work,  the  author s  pain t  a   pictur e  of  a  dystopian   futur e  called   Informania.   In  Informania,   machine   learning   systems   “optimiz e  on  outcomes   for  millions   (or  billions)   of  user s,  with   little  regard  for  individual   rights   within the c ollectiv e.”   Such   a  futur e  is  becoming   more  likely,  the  author s  state,  poin ting  to  China’ s  social   credit  system   and  the  US  Justice  Departmen t’s  COMP AS  algorithm.   The  author s  also  describe   how,  in  a  system   of  uncheck ed  free-mark et  capitalism,   “multiple   organizations   could   develop   [machine   learning]    infrastructur es … .resulting in a massiv e [beha vior] c ontrol in frastructur e.”   While   the  author s  note  that  such   an  outcome   represen ts  “the   logic al  conclusion   of  our  current   traject ory,”  they  also  emphasiz e  that  Informania’ s  oppr ession   “need   not  necessarily   arise   from   malicious   intent.”  Acting   on  behalf   of  the  individual,   an  emancipa tory  assis tant  would   help    redress po wer imbalances within In formania.    Machine Learning S ystems: Oppr essiv e Features   Before  describing   in  detail  wha t  an  emancipa tory  system  would   look   like,  the  author s   demons trate  how  machine   learning   systems  are  “inher ently  oppr essiv e”  by  applying   theor etical   constructs   of  emancipa tion  and  oppr ession.   For  example,   man y  algorithms   use  past  beha viors   to  filter  the  informa tion  that  appear s  on  newsfeeds   and  product   recommenda tions.   This   impacts   a  person’s  “freedom   to  think”   by  controlling   the  amoun t  and  type   of  informa tion   available f or making decisions.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 7 8   
 A  signific ant  shar e  of  the  paper   is  devoted  to  specif ying  how  the  basic   machine   learning   model    is  oppr essiv e  by  nature  as  compar ed  to  code-based   systems.   The  author s  state  that  machine    learning s ystems ar e oppr essiv e bec ause the y:   ●  optimiz e  on  outcome   variables,   which   typic ally  bene fit  the  platform  above  individual    user s;   ●  are based on tr aining da ta tha t ma y reflect his torical biases;    ●  are opaque and dif ficult t o under stand;    ●  typic ally don’t inc orpor ate user f eedback.    Later  on  in  the  paper ,  the  author s  describe   four  modific ations   to  the  basic   machine   learning    model tha t will “yield dis tinctiv e design f eatures” of an emancipa tory assis tant.   The r ole of emancipa tory assis tants   Referring   to  past  resear ch,  the  paper   describes   emancipa tion  as  “a  theor etical  state  in  which    power  dynamics   between  agents  are  neutr al  or  equal. ”  Within   an  oppr essiv e  machine   learning    environmen t,  an  emancipa tory  assis tant  could   act  as  an  intermediar y  that  help s  individual   user s   achie ve mor e po wer.   The  author s  argue  that  critic al  social   theor y  is  well-suit ed  for  the  developmen t  of  new  machine    learning   design   principles.   Freire’s  emancipa tory  pedag ogy  in  particular   “provides   ready -made    pedag ogical steps to foster concr ete gains of emancipa tion. ”   For  example,   Freire  did  not  push   for  the  oppr essed   to  overthr ow  the  oppr essor s,  but  rather   that   they  work  together   in  a  new  type   of  co-educ ation.   In  a  similar   way,  the  emancipa tory  assis tant   could   facilit ate  a  process   of  mutual   inquir y,  “first  by  helping   an  individual   uncover  his  or  her   authen tic  preferences   and  desir es  and  then   by  providing   Informania   with   a  mechanism   to  factor   those desir es in to its op timiz ation function. ”   Key Design Principles    The author s iden tify key design principles f or emancipa tory assis tants, which op timiz e for:   #1. Richness of Pr eferences    Emancipa tory  assis tants  can  help   user s  provide   Informania   with   more  details  about   the   individual’ s  interest.  For  example,   the  assis tant  could   help   an  individual   who   wants  to  chang e   careers overcome In formania’ s assump tion tha t job his tory indic ates futur e job pr eferences.    #2. R ecognizing Con flict   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 7 9   
 An  emancipa tory  assis tant  can  help   user s  recogniz e  when   their   goals  conflict  with   the  goals  of   Informania.   For  example,   the  assis tant  could   help   user s  presen t  attribut es  to  Informania   that   lead t o be tter pricing when finding the bes t loan op tions f or pur chasing a home.    #3. P ersonaliz ed St orytelling    Emancipa tory  assis tants  can  help   user s  manag e  informa tion  sharing   based   on  different   contexts.  According   to  the  author s,  “user s  migh t  be  more  comfortable   with   comple te   informa tion  to  a  spouse   but  migh t  restrict  sligh tly  to  childr en  and  restrict  even  further   to   potential emplo yers.”   #4. Alt erna tive Perspectiv es   Rather   than   encouraging   people   to  click   on  the  same   types   of  news  articles   they  have  read  in   the  past,  the  assis tant  “can  provide   a  richer   article   landsc ape  and  indir ectly   encourage  critic al   consciousness. ”  Emancipa tory  assis tants  can  help   individuals   “develop   the  robus t  rationality    needed t o think critic ally about the w orld ar ound them. ”   While   the  author s  predict   that  Informania   will  domina te  in  the  short er  term,   they  envision   a   long er  term  futur e  wher e  there  is  a  more  balanced   power  dynamic   between  emancipa tory   assis tants  and  Informania.   This  would   necessit ate  the  establishmen t  of  a  certific ation  body   as   well  as  audit   commit tees  to  promot e  compliance   to  standar ds  for  the  newer  types   of  machine    learning s ystems.    Between the lines    This  is  an  import ant  paper   because   it  encourages  more  expansiv e  thinking   within   the  field   of   machine   learning.   By  drawing   on  established   theories   from  multiple   domains,   it  could   also   foster mor e interdisciplinar y collabor ation.    While   the  author s  do  touch   on  the  subject   of  algorithmic   literacy  in  this  paper ,  further   resear ch   could   investigate  the  implic ations   of  divisions   in  algorithmic   awareness.   For  example,   one  survey   of  interne t  user s  in  Norway  (wher e  98%   of  the  popula tion  has  interne t  access),   found   that   educ ation  is  strongly   linked  to  algorithm   awareness,   with   low  awareness   highes t  among   the   least  educ ated  group.   Groups  with   low  algorithm   awareness   were  more  likely  to  hold   neutr al   attitudes t owards alg orithms.    It  could   be  argued,   then,   that  man y  individuals   who   would   bene fit  from  emancipa tory  assis tants   may  not  be  motiv ated  or  may  not  have  the  resour ces  to  use  such   systems.   Futur e  resear ch   could   addr ess  how  new  types   of  machine   learning   systems  could   yield   emancipa tory  outcomes    for all user s of In terne t-based pla tforms–and not jus t a privileg ed few.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 8 0   
 Against Interpr etability: a Critic al Ex amina tion   [  Original paper   by Ma ya Krishnan]    [Resear ch Summar y by Andr ea P ederf erri]    Overview  :  “Explainability ”,  “transpar ency ,  “interpr etability ”  …  these   are  all  terms   that  are   emplo yed  in  different  ways  in  the  AI  ecosystem.  Indeed,   we  often  hear   that  we  should   mak e  AI   more  “interpr etable”   and/ or  more  “explainable” .  In  contrast,  the  author   of  this  paper    challeng es  the  idea   that  “interpr etability ”  and  the  like  should   be  values   or  requir emen ts  for  AI.   First,  it  seems   that  these   concep ts  are  not  really   intuitiv ely  clear   and  technic ally  implemen table.    Second,   they  are  mos tly  proposed   not  as  values   in  themselv es  but  as  means   to  reach   some    other   valuable   goals  (e.g.  fairness,   respect   for  user s’  privacy).   And  so  the  author   argues   that   rather   than   directing   our  attention  to  “interpr etability ”  or  “explainability ”  per  se,  we  should    focus   on  the  ethical  and  epistemic   goals  we  set  for  AI  while   also  making   sure  we  can  adop t  a   variety of solutions and t ools t o reach those g oals.    Introduction    As  we  men tioned   in  a  previous   summar y,  back   box’s  opaqueness   poses   both   epistemic   (are  the   algorithms   in  fact  reliable?)   and  ethical  (are  the  algorithms   ethical?)  challeng es.  Relatedly,  they   also  seem   to  viola te  people’ s  claim-righ t  to  know  why  a  certain  algorithm   has  produced   some    predictions   or  automa ted  decisions   that  concern   them.   For  man y  the  epistemic   and  ethical  risks   back   box’s  opaqueness   poses   could   be  mitig ated  by  making   sure  that  AI  is  someho w   interpr etable,   explainable   and/ or  (as  some   say)  transpar ent.  Contrary  to  the  receiv ed  view  on   this  issue,   the  author   of  this  paper   challeng es  the  idea   that  there  is  a  black   box  problem   and   denies   that  “interpr etability ”,  “explainability ”  or  “transpar ency ”  should   be  values   or   requir emen ts for AI (see also her e).   Key Insigh ts   The  author   of  this  paper   challeng es  the  idea   that  there  is  a  black   box  problem   and  that   “interpr etability ”,  “explainability ”  or  “transpar ency ”  should   be  criteria  for  evalua ting  an  AI   system.   The  first  problem   the  author   poin ts  out  is  that  the  terms   above  (“explainability ”,  “transpar ency ,   “interpr etability ”)  are  often  unclear   and  poorly   defined.   Let’s  take  “interpr etability ”:  AI  is   interpr etable   when,   roughly ,  it  is  under standable   to  consumer s.  The  author   notices   that  this   definition   is  not  really   help ful:  it  does   not  clarif y  wha t  “under standable”   means   and  does   not   offer  any  insigh t  on  wha t  the  term  could   mean   when   applied   to  algorithms.   Also ,  there  is   confusion   on  wha t  should   be  under standable.   Here  are  a  few  candida tes:  the  prediction   of  the   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 8 1   
 algorithm   itself ,  the  inner   workings   of  the  AI  that  produced   its  prediction,   or  the  reasons   why  /   the  justification  for  the  algorithm   making   that  prediction.   Which   one  is  key  for  human    under standing?    Man y  seem   to  belie ve  that  explaining   how  an  algorithm   reached   a  certain  outcome   is   tantamoun t  to  making   AI  under standable.   However,  it  should   be  noted  that  causal   explana tions    are  not  the  same   as  justifications.   That  is,  the  reason   or  justification  for  a  given  outcome   migh t   not  clearly   map   into  the  causal   path  that  brough t  the  algorithm   to  that  conclusion.   As  the   author   puts   it,  “[t]his   poin t  is  particularly   appar ent  in  the  case  of  neur al  networks.  The  causal    process   by  which   some   input   triggers  a  certain  pathway  within   the  network  does   not   straightforwardly  map   on  to  justificatory  consider ations. ”  Indeed   it  would   be  like  asking   “a   person  why  they  have  given  an  answer  to  a  particular   ques tion  and  they  respond   with   an   account  of  how  their   neur ons  are  firing ”.  The  causal   story  is  not  a  rational   explana tion  per  se.   Thus,   if  the  explana tion  we  look   for  tells  us  only   about   the  causal   path  that  gets  the  algorithm   to   a  certain  conclusion,   this  story  would   not  provide   the  right  level  of  explana tion  needed   to   rationally under stand tha t very out come.    Finally ,  the  author   notices   that  interpr etability ,  explainability   and  the  like  are  a  means   to  an  end,    i.e.  ensuring   that  AI  is  ethical  and  trustworth y.  The  author   recommends   that  we  focus   on  those    goals  instead  of  treating  interpr etability   and  the  like  as  they  were  ends   in  themselv es.  Since    there  migh t  be  other   ways  to  reach   those   goals,   it  seems   unhelp ful  to  focus   just  on  one  set  of   solutions.    Between the lines    The  paper   rightly  poin ts  out  that  we  need   a  more  coher ent  and  precise   analy sis  of  concep ts   such   as  interpr etability   and  explainability .  And  the  author   also  clarifies   that  “[w]hile   this  paper    ques tions   both   the  import ance   and  the  coher ence   of  interpr etability   and  cogna tes,  it  does   not   mak e  a  decisiv e  case  for  the  abandonmen t  of  the  concep ts.”  We  agree  with   this  too,  as  we   appr eciate  the  import ance   of  ensuring   explainability   in  AI  as  a  way  to  both   assess   whe ther    algorithms   are  ethical,  robus t  and  reliable,   and  to  protect  people’ s  right  to  know  and   under stand  how  assessmen ts  are  made.   To  do  so,  however,  we  first  need   to  mak e  sure  that  we   agree  on  wha t  we  mean   by  ‘explana tion’   and  on  wha t  kind   of  explana tion  is  needed   for  real   human under standing.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 8 2   
 Transpar ency   as  design   publicity:   explaining   and  justifying   inscrut able    algorithms    [  Original paper   by Michele Loi, Andr ea Ferrario, and   Eleonor a Vig anòngsma]    [Resear ch Summar y by Marianna Ganapini]    Overview  :  It  is  often  said  that  trustworth y  AI  requir es  systems  to  be  transpar ent  and/ or   explainable.   The  goal  is  to  mak e  sure  that  these   systems  are  epistemic ally  and  ethically  reliable,    while   also  giving   people   the  chance   to  under stand  the  outcomes   of  those   systems  and  the   decisions   made   based   on  those   outcomes.   In  this  paper ,  the  solution   proposed   stems  from  the   relationship   between  “design   explana tions”   and  transpar ency:   if  we  have  access   to  the  goals,    the  values   and  the  built -in  priorities   of  an  algorithm   system,  we  will  be  in  a  better  position   to   evalua te its out comes.    Introduction    How  can  we  mak e  AI  more  under standable?   According   to  the  author s  of  the  paper ,  we  care   about   making   AI  more  intelligible   mos tly  because   we  want  to  under stand  the  norma tive  reasons    behind   a  certain  AI  prediction   or  outcome.   In  other   words,  we  want  to  know:  wha t  justifies   the   outcome   of  a  certain  algorithmic   assessmen t,  why  should   I  trust  that  outcome   to  act  and  form   belie fs  based   on  it?  In  the  paper ,  the  solution   proposed   stems  from  the  relationship   between   “design   explana tions”   and  transpar ency:   if  we  have  access   to  the  goals,   the  values   and  the   built -in  priorities   of  an  algorithm-s ystem,  we  will  be  in  a  better  position   to  evalua te  its   outcomes.    Key Insigh ts   The  starting   poin t  for  talking   about   transpar ency   and  explainability   in  AI  is  Lipton’s  (2018)   claim    that  interpr etations   of  ML  models   are  divided   in  two  categories:   model-tr anspar ency   and   post-hoc   explana tions.   Post-hoc   explana tions   look   at  the  prediction   of  a  model   and  include,    mos t  prominen tly,  counterfactual   explana tions   (Wachter  et  al.  2017).   These   are  based   on   certain  “model   features”  which,   if  altered,  chang e  the  outcome   of  the  model,   other   things   being    equal.   By  looking   at  the  features  that  impact ed  a  certain  outcome,   one  can  in  theor y  determine    the  (counterfactual)   causes   that  produced   that  outcome.   Though   these   tools  are  often  used   in   explainable-AI,   the  author s  of  the  paper   are  skeptical:  they  belie ve  counterfactual   explana tions    do not pr ovide the necessar y insigh ts to under stand the norma tive aspects of the model.    Transpar ency   should   someho w  tell  us  how  the  model   works,  at  least  in  Lipton’s  definition.    However,  the  author s  of  the  paper   have  some thing   sligh tly  different  in  mind:   they  belie ve   transpar ency   is  really   the  result   of  making   “design   explana tions”   explicit.   That  is,  we  need   to   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 8 3   
 know  wha t  the  system’s  function   is  and  how  the  system  was  designed   to  achie ve  that  function.    As  the  author s  put  it,  “explaining   the  purpose   of  an  algorithm   requir es  giving   informa tion  on   various   elemen ts:  the  goal  that  the  algorithm   pursues,   the  mathema tical  constructs   into  which    the  goal  is  transla ted  in  order  to  be  implemen ted  in  the  algorithm,   and  the  tests  and  the  data   with which the perf ormance of the alg orithm w as verified. ”   Parallely ,  they  see  “design   transpar ency   of  an  algorithmic   system  to  be  the  adequa te   communic ation  of  the  essen tial  informa tion  necessar y  to  provide   a  satisfactory  design    explana tion  of  such   a  system.”  The  mos t  prominen t  type   of  transpar ency   in  this  context  is  value    transpar ency:   we  need   an  accessible   account  of  wha t  values   were  designed   in  the  system,  how   they  were  implemen ted  and  to  wha t  extent  (wha t  tradeof fs  were  made).   Embedded   values   are   values   that  are  designed   as  part  of  an  algorithmic   system  and  that  the  system  is  also  able   to   show  in  its  output.   As  the  author s  explain,   “[o]nce   the  criteria  to  measur e  the  degr ee  of  goal   achie vemen t  are  specified”   the  “a  design   explana tion  of  an  algorithm   should   provide    informa tion  on  the  effectiv e  achie vemen t  of  such   objectiv es  in  the  environmen t  for  which   the   system w as built. ” Tha t is c alled “perf ormance tr anspar ency ” in the paper .   This  appr oach   is  mean t  to  shed   light  on  the  goals  algorithmic   systems  are  designed   to  achie ve,   the  values   and  tradeof fs  built   into  the  systems,   the  set  of  priorities   the  system  is  designed   to   have  and  the  benchmark s  for  evalua ting  success   and  failur e  of  this  design.   The  goal  of   transpar ency   is  ultima tely  to  provide   “the   public   with   the  essen tial  elemen ts  that  are  needed   in   order  to  assess   the  justification  [ …]  of  the  decisions”   that  are  based   on  automa ted  evalua tions.    If  the  decisions   are  based   on  a  system  not  designed   –  either   intentionally   or  at  the  level  of  how   the  values   are  transla ted  –  to  foster  some   ethical  values,   then   one  migh t  reasonably   suspect   the   decisions   made   won’t  match  some   ethical  requir emen ts.  Mor e  import antly,  these   decisions    cannot   be  morally  accep table   since   they  are  not  motiv ated  by  the  right  set  of  priorities.    Under standing   all  this  is  a  key  requir emen t  for  evalua ting  AI  and  the  decisions   made   based   on   its recommenda tions.    Between The Lines    In  this  very  interesting  paper ,  the  author s  offer  some   actionable   recommenda tions   for  how  to   mak e  AI  more  under standable   which   seem   fully   in  line  with   the  idea   of  achie ving  an  “ethics   by   design”   appr oach   to  AI.  Yet,  we  also  belie ve  that  counterfactual   and  post-hoc   explana tions   could    be  part  of  this  appr oach   with   the  goal,  for  instance,   of  checking   for  things   that  migh t  have  gone   wrong.   Ther efore,  we  would   not  exclude   them   from  an  account  of  explainability   in  AI  and  we   recommend a c ompr ehensiv e appr oach t o mak e AI under standable t o humans.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 8 4   
 Ethics-based   auditing   of  automa ted  decision-making   systems:   intervention   poin ts and policy implic ations    [  Original paper   by Jak ob Mök ander and Maria Ax ente]   [Resear ch Summar y by Angshuman K aushik]    Overview  :  The  governmen t  mechanisms   currently  used   to  oversee  human   decision-making    often  fail  when   applied   to  automa ted  decision-making   systems  (“ADMS ”).  In  this  paper ,  the   resear cher s  propose   the  feasibility   and  effectiv eness   of  ethics-based   auditing   (“EB A”)  as  a  ‘soft’   yet  ‘formal’   governance   mechanism   to  regula te  ADMS   and  also  discuss   the  policy   implic ations   of   their findings.    Introduction    We  are  aware  of  the  ethical  hazards  associa ted  with   ADMS,   which   are  in  fact,  well-documen ted.   In  such   a  scenario ,  the  capacity   to  addr ess  and  mitig ate  these   ethical  risks  posed   by  ADMS   is   essen tial  for  good  governance.   This  paper ,  keeping   aside   the  underlying   technologies   powering    ADMS,   focuses   on  its  features,  for  e.g.,  autonom y,  adap tability   and  scalability   that  underpin    both   its  socially   bene ficial   and  ethically  challenging   uses.   In  fact,  it  narrows  down  its  focus   on   how  organizations   can  develop   and  implemen t  effectiv e  EBA  procedur es  in  practice.   While   the   analy sis  suggests  that  EBA  is  subject   to  a  range  of  concep tual,   technic al,  economic,   legal  and   institutional   constraints,  the  resear cher s  nevertheless   conclude   that,  EBA  should   be  consider ed   as  an  integral  componen t  of  multi- faced   appr oaches   to  managing   the  ethical  risks  posed   by   ADMS.    EBA: Wha t is it?    The  emphasis   of  this  paper   is  entirely  on  EBA,  which   is  functionally   under stood  as  a  governance    mechanism   that  help s  organizations   oper ationaliz e  their   ethical  commitmen ts.  It  concerns   wha t   ough t  and  ough t  not  to  be  done   over  and  above  existing  regula tion.   Oper ationally ,  EBA  is   char acterized  by  a  structur ed  process   wher eby  an  entity’s  presen t  or  past  beha vior  is  assessed    for  consis tency   with   a  predefined   set  of  principles.   Throughout   this  process,   various   tools  and   methods   such   as  softw are  programmes,   stakeholder   consult ation  etc.  are  emplo yed  to  verify   claims   and  create  documen tation.   In  fact,  different  EBA  procedur es  emplo y  different  tools  and   contain  different  steps.  However,  an  EBA  differs  from  simply   publishing   a  code  of  conduct   since    its  main   activity   consis ts  of  demons trating  adher ence   to  a  predefined   standar d.  The  paper   also   emphasiz es  on  how  organizations   can  develop   and  implemen t  effectiv e  EBA  procedur es  in   practice   instead  of  concen trating  only   on  wha t  EBA  is  and  why  it  is  needed.   The  objectiv e  is   twofold.  First,  the  resear cher s  seek   to  iden tify  the  intervention  poin ts,  both   in  organizational    governance   as  well  as  in  the  softw are  developmen t  lifecycle,  at  which   EBA  can  help   inform   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 8 5   
 ethical  deliber ation  and  thereby  mak e  a  positiv e  difference   to  the  ways  in  which   ADMS   are   designed   and  deplo yed.  Second,   they  seek   to  contribut e  to  an  under standing   of  how   policymak ers  and  regula tors  can  facilit ate  and  support   the  implemen tation  of  EBA  procedur es  in   organizations tha t develop ADMS.    EBA: Diff erent appr oaches    The  paper   distinguishes   between  different  appr oaches   for  EBA,  such   as  functionality   audits,    which   for  example,   focuses   on  the  rationale   behind   decisions.   In  contrast,  code  audits   entail   reviewing   the  sour ce  code  of  an  algorithm.   Finally ,  impact   auditing   investigates  the  types,    severity ,  and  prevalence   of  effects  of  an  algorithm’ s  outputs.   These   appr oaches   are   complemen tary  and  can  be  combined   into  holis tic  EBA  procedur es.  According   to  the   resear cher s,  since   autonomous   and  self-learning   ADMS   may  evolve  and  adap t  over  time   as  they   interact  with   their   environmen ts,  EBA  needs   to  include   at  least  the  elemen ts  of  continuous,    real-time monit oring i.e. impact auditing.    Governing S TS and iden tifying in tervention poin ts for EB A   The  paper   then   dwells  upon   Socio-T echnic al  Systems  (STS),  which   comprises   both   social   entities,    like  people   and  organizations,   and  technic al  entities,   like  tools,   infrastructur es,  and  processes.    ADMS,   then,   refers  to  technic al  systems  that  encompass   decision-making   models,   algorithms    that  transla te  models   into  comput able   code,   as  well  as  methods   to  acquir e  and  process   input    data.  Further ,  ADMS   interact  with   the  entire  politic al  and  economic   environmen t  surrounding    their   use.  The  paper   then   goes  on  to  analy ze  how  comple x  STS  are  governed   today  and   discusses   how  EBA  procedur es  can  be  designed   to  complemen t  and  enhance   existing   governance   structur es.  Governance   consis ts  of  both   hard  and  soft  aspects.   Hard  governance    mechanisms   are  systems  of  rules   elabor ated  and  enforced  through   institutions   to  govern  the   beha vior  of  agents.  When   considering   ADMS,   examples   of  hard  governance   mechanisms   range   from  legal  restrictions   on  system  outputs   to  outrigh t  prohibition   of  the  use  of  ADMS   for  specific    applic ations.   Soft  governance,   on  the  other   hand,   embodies   mechanisms   that  abide   by  the   prescrip tions   of  hard  governance   while   exhibiting   some   degr ee  of  contextual  flexibility .  A   further   distinction   is  also  made   between  formal   and  informal   governance   mechanisms,   wher e   formal   governance   mechanisms   refer  to  official   communic ations.   The  resear cher s  go  on  to   advocate  EBA  as  a  soft  yet  formal   governance   mechanism   to  complemen t  and  strengthen   the   congruence   of  existing  governance   structur es  within   organizations   that  develop   and  use  ADMS.    Further ,  the  paper   look s  at  some   of  the  potential  intervention  poin ts  (poin ts  at  which   decisions,    actions,   or  activities   are  likely  to  shape   the  design   and  beha vior  of  ADMS)   at  which   EBA  can  help    shape   the  design   and  deplo ymen t  of  ethical  ADMS   by  informing   ethical  deliber ation.   They  are  as   follows:   ●  value and vision s tatemen t ;   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 8 6   
 ●  principles and c odes of c onduct;    ●  ethics boar ds and r eview commit tees;   ●  stakeholder c onsult ation;    ●  emplo yee educ ation and tr aining;    ●  performance crit eria and incen tives;   ●  reporting channels;    ●  product de velopmen t;   ●  product deplo ymen t and r edesign;    ●  periodic audits; and    ●  monit oring of outputs    Recommenda tions t o policymak ers   The  paper   not  only   iden tifies   limit ations   and  risks  associa ted  with   EBA  but  also  discusses   how   policymak ers  and  regula tors  can  facilit ate  the  adop tion  of  EBA  by  organizations   that  design   and   deplo y  ADMS.   According   to  the  resear cher s,  the  organizations   that  design   and  deplo y  ADMS    have  good  reasons   to  subject   themselv es  and  the  systems  they  oper ate  to  EBA.  For  example,    ensuring   the  ethical  alignmen t  of  ADMS   would   help   organizations   manag e  financial   and  legal   risks,  help   them   gain  compe titive  advantage  etc.  In  fact,  the  documen tation  and  communic ation   of  the  steps  taken  to  ensur e  that  ADMS   are  ethical  can  play  a  positiv e  role  in  both   mark eting   and public r elations.    The  paper   also  highligh ts  eigh t  policy   recommenda tions   for  policymak ers  and  regula tors  to   follow:   ●  Help   provide   working   definitions   for  ADMS   –  regula tors  shall   define  for  organizations   the   material   scope  for  EBA  by  providing   working   definitions   or  risk  classific ations   of  ADMS    that enable pr oportiona te and pr ogressiv e governance;    ●  Provide   guidance   on  how  to  resolv e  tensions   –  when   designing   and  oper ating  ADMS,    conflicts   may  arise   between  different  ethical  principles   such   as  fairness,   privacy  etc.,  for   which   there  are  no  fixed  solutions.   In  such   a  scenario ,  regula tors  shall   provide   guidance    on ho w to resolv e tensions be tween such c onflicting v alues in dif ferent situa tions;    ●  Support   the  creation  of  standar dized  evalua tion  matrices   and  reporting   forma ts  –  while    organizations   should   be  free  to  adop t  different  EBA  procedur es,  regula tors  can  also   support the cr eation of s tandar dized e valua tion me trics and r eporting f orma ts;   ●  Facilit ate  knowledg e  sharing   and  communic ation  of  best  practices   –  regula tors  can  not   only   provide   digit al  platforms   wher e  softw are  code  and  data  could   be  shar ed  but  also   create  forums   wher e  stakeholder s  could   discuss   and  shar e  best  practices   for  EBA  of   ADMS;    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 8 7   
 ●  Create  an  independen t  body   to  oversee  EBA  of  ADMS   –  create  an  independen t  body   that   authoriz es  organizations   who ,  in  turn,   conduct   EBA  of,  or  issue   ethics-based    certific ations f or, ADMS;    ●  Create  incen tives  for  volun tary  adop tion  of  EBA  –  implemen ting  EBA  across   organizations   would   involve  costs.  Ther efore,  to  incen tivize  the  volun tary  adop tion  of   EBA, regula tors should enc ourage and r eward demons trable achie vemen ts;   ●  Promot e  trust  through   transpar ency   and  accountability   –  regula tors  can  strengthen   trust   in  emer ging   EBA  procedur es  by  ensuring   accountability ,  e.g.,  by  imposing   sanctions    wher e trus t is br eached; and    Provide   governmen tal  leader ship  –  politic al  leader s  can  help   strengthen   the  feasibility   and   effectiv eness   of  EBA  as  a  governance   mechanism   by  explaining   and  endor sing  it.Ther efore,  in   order  to  demons trate  their   commitmen t  to  officially   stated  policies,   governmen ts  can  consider    conducting   EBA  of  ADMS   emplo yed  in  the  public   sector  and  include   ethics-based   criteria  in  the   public pr ocur emen t of ADMS.    Between the lines    This  paper   provides   a  very  holis tic  and  process-orien ted  appr oach   to  EBA.  In  fact,  man y  of  the   intervention  poin ts  listed  in  the  paper   already   exist  within   organizations   that  design   and  deplo y   ADMS.   Hence,   implemen ting  EBA  would   not  entail  imposition   of  any  additional   layers  of   governance   upon   them.   It  is  pertinen t  to  men tion  here  that  the  key  to  developing   feasible   and   effectiv e  EBA  procedur es  is  to  combine   existing  concep tual  frame works  into  structur ed   processes   that  monit or  each   phase   of  the  ADMS   lifecycle  to  iden tify  and  correct  the  poin ts  at   which   ethical  failur es  may  occur .  To  sum   up,  the  recommenda tions   delinea ted  in  this  paper    would de finitely g o a long w ay in mitig ating some of the e thical haz ards posed b y ADMS.    Trustworthiness of Artificial In tellig ence    [  Original paper   by Sonali Jain, Shagun Sharma, Manan   Luthr a, Meh tab Fatima]    [Resear ch Summar y by Connor W right]   Overview  :  If  you  are  new  to  the  space   of  AI  Ethics,   this  is  the  paper   for  you.  Offering   a  wide    coverage  of  the  issues   that  enter  into  the  deba te,  AI  governance   and  how  we  build   trustworth y   AI ar e explor ed b y the author s.   Introduction    One  of  the  strengths  of  this  paper   is  how  it  proves  a  productiv e  introduction   for  those   who   are   new  to  the  AI  Ethics   space.   Touching   upon   governance   (as  we  have  done),   how  we  create   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 8 8   
 trustworth y  AI  is  explor ed.  Wha t  we  mean   by  ‘trus tworth y’  is  open   for  review,  but  some   aspects    mus t enter the deba te. Thr ee of these ar e highligh ted belo w.   Key Insigh ts   The author s appeal t o ho w AI should be c omplian t in the f ollowing 3 w ays:   1. La wful: The AI s ystem should be c omplian t with v arious rules and la ws.   2. Ethical: It should c ontain mor als and e thics and adher e to mor al values and principles.    3. Robus t: AI should be s turdy in both social and t echnic al sense.    How AI c an be made la wful: A righ ts appr oach t o AI   The  bene fit  of  such   an  appr oach   is  its  ability   to  put  humanity   at  the  center  of  AI  consider ations    while   main taining   respect   for  human   dignity .  One  example   of  how  this  works  is  the  right  to   freedom   from  coercion.   Focused   on  preventing  manipula tion,   laws  such   as  the  Calif ornia   Law   try  to  mak e  sure  that  “AI  systems  mus t  not  in  any  case  domina te,  force,  deceiv e  or  manipula te   human beings” (p. g. 908).    The  appr oach   becomes   even  more  intriguing   when   applied   to  harm.   Often,  AI  systems  are  said   to  be  designed   not  to  harm   humans.   While   being   an  intuitiv e  claim,   such   an  appr oach   does    requir e the AI t o be a ware of humans alongside the c ontext in which it finds itself .   Furthermor e,  the  depth  of  awareness   requir ed  depends   on  which   AI  system  you’re  talking    about.   You  can  imagine   that  the  AI  used   in  CV  screening   does   not  need   to  have  an  acut e  sense    of other humans c ompar ed to facial r ecognition (especially a t Facebook).    However, a righ ts-based appr oach c an’t do it all on its o wn.   Ethical principles in the AI space    The  import ance   of  privacy,  explainability   and  transpar ency   were  rightly  explor ed  here,  staple    products   in  building   trustworth y  AI.  However,  wha t  jumped   out  at  me  was  how  the  author s  did   not  advocate  for  comple te  transpar ency .  Instead,   transpar ency   is  to  be  pursued   in  the  name   of   fueling   explainability ,  but  some   informa tion  should   only   be  accessible   to  those   in  the   appr opria te positions.    Nevertheless, those in these positions should be both in terdisciplinar y and div erse.   The import ance of univ ersal design    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 8 9   
 Given  AI’s  wide-r eaching   effects,   the  design   should   be  accessible   to  all  gender s,  ages  and   ethnicities.   This  comes   from  designing   the  AI  with   diversity  already   in  the  team,   a  token  of  its   all-enc ompassing   nature.  Furthermor e,  the  ‘common   AI  fight’  is  shown  in  the  paper ’s  methods    for  trustworth y  AI  involving   cross-business   and  cross-sect or  collabor ation.   With   AI’s  impact    being both men tal and ph ysical, the AI space needs all the c ollabor ation it c an get.   Between the lines    While   a  good  introduction   into  the  AI  space,   I  would’ ve  liked  a  deeper   explor ation  into  the   practic al  side  of  these   appr oaches.   For  example,   how  human   intervention  in  AI  processes   can  be   bene ficial,   rather   than   having  it  assumed   to  be  so.  Nevertheless,   should   any  human   intervention   have  a  chance   of  success,   the  correct  educ ation  would   be  requir ed.  Here,  I  liked  how  the  paper    men tioned   AI’s  potential  call  for  the  educ ational   system  to  be  more  job  orien tated  and  reflect    the  state  of  the  world  it  will  be  creating.   While   this  may  not  be  the  actuality ,  it  will  soon   convert   into a necessity .   Getting  from  Commitmen t  to  Content  in  AI  and  Data  Ethics:   Justice  and   Explainability    [  Original paper   by John Basl, R onald Sandler and St even  Tiell]    [Resear ch Summar y by Angshuman K aushik]    Overview  :  AI  or  data  ethics   principles   or  frame works  mean t  to  demons trate  a  commitmen t  to   addr essing   the  challeng es  posed   by  AI  are  ubiquit ous  and  are  an  ‘easy  first  step’.  However,  the   harder  task  is  to  oper ationaliz e  them.   This  report,   inter  alia,  stipula tes  strategies   for  putting   those principles in to practice.    Introduction    Amids t  the  chaotic   AI  Ethics   principles   landsc ape,   this  report   emer ges  as  a  much-needed   guide    in  under standing   the  entire  gamut   of  issues   related  with   the  applic ation  of  those   principles   in   governance   scenarios.   It  emphasiz es  the  comple xities   associa ted  with   moving  from  gener al   commitmen ts  to  substantive  specific ations   in  AI  and  data  ethics.   According   to  it,  much   of  this   comple xity arises fr om thr ee key factors:   ●  ethical  concep ts  such   as  justice  and  transpar ency   that  often  have  man y  senses   and   meaning;    ●  which senses of e thical concep ts ar e oper ative or appr opria te is oft en contextual; and    ●  ethical  concep ts  are  multidimensional   e.g.,  in  terms   of  wha t  needs   to  be  transpar ent,  to   whom, and in wha t form.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 9 0   
 Further , the objectiv es of the r eport ar e to:   ●  demons trate  the  import ance   and  comple xity  of  moving  from  gener al  ethical  concep ts   and principles t o action-guiding sub stantive content, i.e., norma tive content;   ●  provide   detailed   analy sis  of  two  widely   discussed   and  interconnect ed  ethical  concep ts,   justice and tr anspar ency; and    ●  indic ate  strategies   for  moving  from  gener al  ethical  concep ts  and  principles   to  more   specific norma tive content and ultima tely t o oper ationalizing tha t content.   AI Ethics – Under standing the challeng es   The  report   talks  about   consider able   convergence   among   the  man y  AI  ethics   frame works  that   have  been   developed.   They  coalesce   around   core  concep ts,  some   of  which   are   individual-orien ted,  other s  socie ty-orien ted  and  still  other s  system-orien ted.  However,   according   to  the  resear cher s,  enuncia ting  ethical  values   and  principles   is  only   the  first  step  in   addr essing   AI  and  data  ethics   challeng es  and  it  is  in  man y  ways  the  easies t.  The  much   harder   work is the f ollowing:    ●  substantively specif ying the c ontent of the c oncep ts, principles and c ommitmen ts; and    ●  building pr ofessional, social and or ganizational c apacity t o realiz e these in pr actice.    An e xample fr om the field of bioe thics    In  order  to  better  compr ehend   the  obstacles   encountered  in  moving  from  gener al  ethical   concep ts  to  a  functioning   AI  frame work  (norma tive  content),  the  paper   takes  the  case  of   informed   consen t  in  bioe thics,   which   is  widely   recogniz ed  as  a  crucial   componen t  of  ethical   clinic al pr actice. In formed c onsen t oper ationaliz es the principle of individual aut onom y.   Practic ally, it requir es the fulfillmen t of thr ee conditions namely:    ●  disclosur e  – provision of clear , accur ate and r elevant  informa tion t o the subjects;    ●  compr ehension   –  informa tion  is  provided   to  the  subjects   in  a  way  that  they  can   under stand; and    ●  volun tariness   – the subjects mak e the decision without   undue in fluence or c oercion.    The  enforcemen t  of  these   three  conditions   is  the  task  of  bioe thicis ts,  hospit al  ethics   commit tees   and  institutional   review  boar ds.  They  prepar e  guidelines,   best  practices,   procedur es  etc.,  for   mee ting the abo ve informed c onsen t conditions.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 9 1   
 According   to  the  resear cher s,  while   informed   consen t  is  mean t  to  protect  the  value   of   autonom y  and  express  respect   for  persons,   a  gener al  commitmen t  to  the  principle   of  informed    consen t  is  just  the  beginning.   The  principle   mus t  be  explic ated  and  oper ationaliz ed  before  it  is   meaningful   and  useful  in  practice.   The  same   is  true   for  principles   of  AI  and  data  ethics.   The   resear cher s  then   narrow  down  their   focus   on  the  comple xities   involved  in  moving  from  core   concep ts  and  principles   to  oper ationaliz ation  of  the  norma tive  content  for  two  prominen tly   discussed and in terconnect ed AI and da ta ethics c oncep ts: jus tice and tr anspar ency .   Meaning of jus tice in AI    The  report   men tions   that  the  concep t  of  justice  is  a  comple x  one,   and  can  mean   different  things    in  different  contexts.  To  determine   wha t  justice  in  AI  and  data  use  requir es  in  a  particular    context,  it  is  imper ative  to  clarif y  the  norma tive  content  and  underlying   values.   Only   then   it  is   possible   to  specif y  wha t  is  requir ed  in  specific   cases,   and  in  turn   how  or  to  wha t  extent  justice   can  be  oper ationaliz ed  in  technic al  systems.   According   to  the  report,   the  gener al  principle   of   justice  is  that  all  people   should   be  equally   respect ed  and  valued   in  social,   economic   and  politic al   systems  and  processes.   However,  there  are  man y  ways  this  very  gener al  principle   of  justice   intersects   with   social   structur es  and  systems.   As  a  result,   there  is  a  diverse  set  of  more  specific    justice-orien ted principles such as pr ocedur al, dis tributiv e and r ecognition jus tice.    Wha t does c ommit ting t o jus tice mean?    The  resear cher s  consider   context  to  be  critic ally  import ant  in  determining   which    justice-orien ted  principles   take  precedence.   Ther efore,  the  first  step  in  specif ying  the  norma tive   content  is  to  iden tify  the  justice-orien ted  principles   that  are  crucial   to  the  work  that  the  AI   system  does.   Only   then   can  a  commitmen t  to  justice  be  effectiv ely  put  into  practice.   Articula ting   the  relevant  justice-orien ted  principles   will  also  requir e  considering   organizational   missions,   the   types   of  products   and  services   involved,  how  those   products   and  services   could   impact    communities   and  individuals   etc.  In  iden tifying  these,   it  will  be  help ful  to  reflect   on  similar   cases    and  carefully   consider   the  sorts   of  concerns   that  people   have  raised   about   AI  systems.   The   resear cher s  have  cited  two  hypothe tical  cases   to  illustrate  this.   Further ,  the  report   states  that   the  diversity  of  the  justice-orien ted  principles   and  the  need   to  mak e  context-specific    determina tions   about   which   are  relevant  and  which   to  prioritiz e  expose   the  limits   of  a  strictly    algorithmic   manner   in  incorpor ating  justice  in  AI  systems.   The  reason   being ,  firstly,  there  is  no   singular ,  gener al  justice-orien ted  constraint,  optimiz ation  or  utility   function   and  secondly ,  there   will  not  be  a  strictly   algorithmic   way  to  fully   incorpor ate  justice  into  decision-making ,  even  once    the  relevant  justice  consider ations   have  been   iden tified.   The  report   then   goes  on  to  ask  the   ques tion  as  to  how  and  to  wha t  extent  can  the  salien t  aspects   of  justice  be  achie ved   algorithmic ally.  According   to  the  resear cher s,  accomplishing   justice  in  AI  will  requir e  developing    justice-in formed,   techno-social   or  human-alg orithm   systems.   AI  systems  can  support   social    workers  in  service  determina tions,   admissions   officer s  in  colleg e  admissions   determina tions,   or   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 9 2   
 healthc are  professionals   in  diagnos tic  determina tions   and  they  migh t  even  be  able   to  help    reduce   biases   in  those   processes.   According   to  the  resear cher s,  a  commitmen t  to  justice  in  AI   involves  remaining   open   to  the  possibility   that  some times   an  AI-orien ted  appr oach   migh t  not  be   a  just  one.   They  stress  on  the  fact  that,  organizations   that  are  commit ted  to  justice  in  AI  will   requir e  signific ant  organizational   capacity   and  processes   to  oper ationaliz e  and  implemen t  their    commitmen t,  in  addition   to  technic al  capacity   and  expertise.   Reliance   upon   techno   solutionism    or on s tandar ds de veloped in other c ontexts is not desir able.    Transpar ency in AI    In  the  view  of  the  resear cher s,  in  spite  of  the  role  that  transpar ency   plays  in  helping   to  achie ve   justice,   it  can  also  play  an  import ant  role  in  realizing   other   concep ts  and  values.   They  also  lay   down  the  man y  ways  in  which   a  decision   system  could   be  made   transpar ent.  The  forms   that   commitmen ts to transpar ency ma y take are as f ollows:   ●  Interpr etability   – requiring AI s ystems t o be in terpr etable;    ●  Explainability   –  a  decision-making   system  is  explainable   when   it  is  possible   to  offer   stakeholder s an e xplana tion tha t can be under stood as jus tifying a giv en decision;    ●  Justified   Opacity   –  transpar ency   about   the  reasons   for  adop ting  opaque   systems  can   serve to jus tify other f orms of opacity; and    ●  Audit ability   –  a  carefully   construct ed  audit   can  provide   assur ance   that  decision-making    systems br oadly ar e trus tworth y, reliable and c omplian t.   Way forward   The  resear cher s  poin t  out  that  for  organizations   to  be  success ful  in  realizing   their   ethical   commitmen ts  and  accomplishing   responsible   AI,  they  mus t  think   broadly   about   how  to  build    ethical capacity within their or ganizations. Some of the initia tives cit ed ar e as f ollows:   ●  creating  AI  and  data  ethics   commit tees  that  can  aid  in  developing   policies   and  other    governance measur es;   ●  meaningfully   engaging   with   impact ed  communities   to  better  compr ehend   ethical  issues    and other w ays to broaden per spectiv es and c ollabor ations;    ●  training and educ ation;    ●  integrating e thics in to practice; and    ●  building an AI and da ta ethics c ommunity .   Between the lines    The  plethor a  of  vaguely   formula ted  AI  Ethics   principles,   guidelines,   standar ds  etc.,  that  have   come   to  domina te  the  AI  Ethics   space   in  the  last  few  years  have  hardly  aided   in  oper ationalizing    ethical  AI  in  practice.   With   the  passag e  of  time   such   principles   have  begun   to  sound   banal   and   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 9 3   
 an  appendag e  to  an  organization’ s  other   ‘signific ant  documen ts’.  In  such   a  scenario ,  this  report    serves  as  a  guidepos t  by  laying  down  strategies   for  moving  from  gener al  ethical  concep ts  and   principles   to  more  specific   norma tive  content  and  ultima tely  to  oper ationalizing   that  content.   Further ,  the  report   simplifies   compr ehension   of  the  comple xities   involved  in  such   a  transition,    by  the  use  of  illustrations.   The  report   can  prove  handy   and  a  ‘go-to  guide’   not  only   for  those    entities   that  are  struggling   to  formula te  ethical  principles   but  also  to  those   that  are  trying  to  get   an AI E thics fr ame work up and running.    Founda tions   for  the  futur e:  institution   building   for  the  purpose   of  artificial    intellig ence g overnance    [  Original paper   by Charlot te Stix]    [Resear ch Summar y by Angshuman K aushik]    Overview  :  To  implemen t  governance   efforts  for  artificial   intelligence   (AI),  new  institutions    requir e  to  be  established,   both   at  a  national   and  an  interna tional   level.  This  paper   outlines   a   scheme   of  such   institutions   and  conducts   an  in-dep th  investigation  of  three  key  componen ts  of   any  futur e  AI  governance   institution,   exploring   bene fits  and  associa ted  drawback s.  Ther eafter,   the  paper   highligh ts  signific ant  aspects   of  various   institutional   roles  specific ally  around    ques tions   of  institutional   purpose,   and  frames   wha t  these   could   look   like  in  practice,   by  placing    these   deba tes  in  a  European   context  and  proposing   different  iterations   of  a  European   AI  Agency .   Finally , conclusions and futur e resear ch dir ections ar e proposed.    Introduction    The  paper   begins   by  drawing   the  attention  of  the  reader s  to  the  fact  that  the  governmen ts   around   the  world  have  begun   to  appr oach   the  governance   of  AI  through   multiple   controls.  One   example   being   the  European   Union’ s  recen t  Proposal   for  a  Regula tion  of  the  European    Parliamen t  and  of  the  Council   Laying  Down  Harmoniz ed  Rules   on  Artificial   Intelligence   and   Amending   Cert ain  Union   Legisla tive  Acts   (“Artificial   Intelligence   Act”)  which   puts   forward  a   regula tory  frame work  for  high-risk   AI  systems  and  the  other   being   the  Trade  and  Technology    Council   co-es tablished   by  the  US  and  the  EU  with   the  manda te  to  cooper ate  on  the   developmen t  of  suitable   standar ds  for  AI.  Further ,  as  the  field   of  AI  governance   is  relatively  new,   as  such,   there  exist  only   a  few  specialis t  governmen tal  institutions   exclusiv ely  dedic ated  in  the   area.  According   to  the  author ,  to  properly   develop,   support   and  implemen t  new  AI  governance    efforts,   it  is  likely  that  a  number   of  new  institutions   will  need   to  be  established   in  the  futur e.   Ther e  are  broadly   two  types   of  institutions   that  one  could   investigate:  those   that  exist  and  may   be  adap ted  and  those   that  do  not  exist  yet  but  will  eventually   come   into  existence   to  fill  the   void  created  by  new  governance   initia tives.  This  paper   puts   emphasis   on  the  latter  type   of   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 9 4   
 institutions,   with   particular   focus   on  institutions   set  up  by  governmen ts.  In  order  to  proceed    with   its  objectiv e,  the  paper   builds   on  recen t  academic   calls  for  an  interna tional   governance    coordina ting  commit tee  for  AI,  for  an  interna tional   regula tory  agency   for  AI  etc.,  and  draws  on   existing  scholar ship  in  the  area,  and  addr esses   itself   to  those   individuals   who   will  be  involved  in   setting  up  new  institutions   and  those   who   are  interested  in  conducting   further   resear ch  on   pragma tic ins titution building f or AI g overnance.    Building ne w AI g overnance ins titutions    The  paper   states  that  AI-specific   governance   institutions   working   on  soft  governance    mechanisms   with   non-binding   rules   have  already   come   into  existence.   A  few  examples   are   OECD,  G7,  and  the  Global   Partner ship  on  AI  etc.  However,  there  has  been   moun ting  pressur e  to   develop   and  implemen t  stronger  and  more  binding   AI  governance   mechanisms   than   those    covered  by  ethical  principles.   As  countries   move  towards  harder  governance   efforts,   they  are   likely  to  requir e  increasingly   specializ ed  institutions   to  oversee  their   implemen tation.   Mor eover,   as  AI  governance   efforts  soar   and  more  coordina tion,   action   and  policy   proposals   become    necessar y  within   a  nation  as  well  as  at  an  interna tional   level,  it  is  likely  that  there  will  be  a  need    for  more  specializ ed  governmen tal  agencies   to  handle   an  increasingly   diverse  set  of  tasks  on  top   of  the  existing  work.  It  migh t  be  overall  quick er,  cheaper   and  more  effectiv e  to  build   a  new   institution   from  scratch  that  is  ‘fit  for  purpose’   rather   than   exert  time,   effort  and  politic al   goodwill   to  chang e  the  structur e  of  an  existing  institution.   The  author   then   puts   forward  a   selection   of  axes  that  need   to  be  consider ed  in  building   new  AI  governance   institutions,   namely ,   purpose, g eogr aphy and c apacity , with particular emphasis on purpose.    Purpose    The  first  ques tion  that  needs   to  be  answered  is  the  purposes   of  the  new  institution   i.e.,  wha t  is   it  mean t  to  do?  Under   the  broad  heading   of  purpose,   the  paper   introduces   the  outline   of  four   different  roles  an  institution   for  AI  governance   could   take.  The  roles  are  namely ,  coordina tor,   analy zer, developer and in vestigator.   The c oordina tor ins titution    The  task  of  a  coordina tor  institution   could,   for  instance,   include   working   with   the  rising   number    of  ethical  guidelines   and  attemp ting  to  oper ationaliz e  them   more  clearly .  It  could   also  serve  as   an  umbr ella  organization  and  coordina te  activities   amongs t  different  groups.  Some   examples   of   coordina tor  institutions   are  the  UN,  the  G20,   and  NATO  etc.  The  paper   goes  on  to  highligh t  the   fact  that  the  actions   of  the  coordina tor  institution   shall   be  timely   and  appr opria te  and  proposes    that a futur e AI Ag ency in the EU migh t take up the r ole of a c oordina tor ins titution.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 9 5   
 The analy zer ins titution    The  duties   of  an  analy zer  institution   could   be  varied,   such   as  mapping   existing  efforts  and   iden tifying  gaps  across  various   governmen ts  (for  instance   the  European   Commission),   compiling    data  sets  and  informa tion  on  the  technic al  landsc ape  and  sketching   technologic al  traject ories    (for  instance   the  AI  Inde x)  etc.  The  role  of  an  analy zer  institution   is  more  activ e  than   that  of  a   coordina tor  institution,   in  that  it  interferes  more  directly   with   the  governance   or  policy   making    process   by  way  of  providing   crucial   informa tion  that  can  inform  and  shape   those    decision-making pr ocesses.    The de veloper ins titution    A  developer   institution   shall   provide   either   directly   actionable   measur e  or  formula te  new  policy    solutions   to  existing  issues.   It  may  take  up  the  role  of  examining   blind   spots   and  proposing    solutions   for  those   by  way  of  its  own  initia tive,  in  addition   to  work  it  migh t  be  asked  to   undert ake by various g overnmen t agencies.    The in vestigator ins titution    It  is  envisioned   as  a  ‘watchdog ’  assigned   with   the  task  to  investigate  whe ther   or  not  actors  such    as  governmen ts,  companies   or  specific   organizations   are  adhering   to  the  relevant  standar ds,   procedur es,  laws  or  not.  One  example   of  an  investigator  institution   is  the  Human   Righ ts  Council.    The  mos t  import ant  requir emen t  of  such   an  institution   would   be  its  independence   and   impartiality .   Geogr aphy   The  effects  of  AI  systems  transcend   geogr aphies   and  are  not  confined   within   national   borders.   Ther efore,  man y  AI  governance   issues   could   be  seen   as  multi-c ountry  concerns.   The  couple   of   broad  consider ations   with   respect   to  geogr aphy  that  the  paper   delves  into  are:  wha t  is  the   bene fit  or  downside   of  a  new  multi-c ountry  institution   and  how  does   it  fare  in  comparison   to   nationally   ‘restricted’  institutions?   A  multi-c ountry  institution   mus t  consider   ques tions   of   access,   inclusion   and  participa tion.   One  model   proposed   is  if  several  nations   expect   that  their    position   towards  AI  governance   is  broadly   more  bene ficial   than   that  of  other   nations,   it  may  be   reasonable   for  them   to  cooper ate  and  coordina te  to  establish   a  dedic ated  institution.    Conversely,  if  nations   choose   not  to  form  a  new  institution,   a  proliferation  of  similar   but  distinct    institutions c ould a ffect fr agmen tation of global AI g overnance r egimes.    Capacity    The  third  axis  is  capacity   which   relates  to  the  previous   two  axes  i.e.,  purpose   and  geogr aphy.  It   concerns   wha t  the  institution   needs   in  terms   of  capacities   for  it  to  thriv e,  both   on  the  technic al   and  non-t echnic al  side.   The  paper   proposes   that  access   to  technic al  infrastructur e  could   play  an   import ant  role  for  futur e  AI  governance   institutions.   The  said  technic al  infrastructur e  may   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 9 6   
 include   access   to  comput e,  available   datasets,  testing  and  experimen tation  facilities   etc.  It  can   minimiz e  bottleneck s  in  terms   of  informa tion  exchang e  and  increase   speed   between  wha t  is  to   be  governed   and  the  associa ted  governance   actions   and  decisions,   thereby  contributing   to   more  agility ,  specificity   and  foresigh t  in  policy   making   for  AI.  On  the  non-t echnic al  side,   the   paper   under scores  the  need   to  build   up  human   capacity   which   could   broadly   take  two  forms:   (a)   out  of  the  house   capacity   with   either   (1)  a  network  of  individual   experts   to  draw  upon   when    needed,   or  (2)  expert   groups  and  external   panels   and  (b)  in-house   capacity   with   a  team   having  a   diverse backgr ound with r elevant experience in t echnic al, leg al and e thical ar eas.   Between the lines    The  rapid   and  unbridled   increase   in  the  use  of  AI  systems  has  necessit ated  its  effectiv e   governance.   In  fact  to  govern  efficien tly,  the  need   of  the  hour   is  institutions   that  can  deliv er.   This  paper ,  instead  of  making   norma tive  assessmen ts  of  the  various   institutional   setups,  charts    out  a  pragma tic  appr oach   in  building   up  institutions   for  AI  governance   at  a  time   when   proposals    for  setting  up  such   institutions   are  gathering   steam.   Mor e  import antly,  it  provides   a  frame work   to  start  with.   Another   highligh t  of  the  paper   is  that  it  shows  the  way  for  futur e  resear ch   direction on the t opic.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 9 7   
 Go Wide: Article Summaries  (summariz ed b y Abhishek   Gup ta)   Five Recommenda tions F or Cr eating Mor e Ethical AI    [Original article b y  Forbes   ]   Wha t  happened   :  The  article   proposes   some   founda tional   steps  that  will  be  import ant  in   creating  more  ethical  AI  systems  and  mak e  them   a  normaliz ed  practice.   Hiding   behind    technic alities   and  shirking   moral  oblig ations   as  not  part  of  the  leader ship  role  should   be   avoided.   Instead,   the  author   asks  companies   to  mak e  long-t erm  resear ch  commitmen ts  and   working   with   monied   partner s  who   under stand  this  appr oach   rather   than   asking   them   to  take   short cuts.   Empo wering   emplo yees  not  only   so  that  they  can  raise  issues   as  they  arise   but  also   so  that  they  can  propose   inno vative  solutions   and  see  them   implemen ted  is  another   crucial    step.  Finally ,  being   transpar ent  about   one’ s  appr oach   to  AI  ethics   and  holding   oneself    accountable f or following tha t appr oach will also help build public trus t in one’ s work.   Why  it  matters  :  Having  more  actionable   appr oaches   to  AI  ethics,   especially   guidance   for   leader ship,   will  be  essen tial  for  the  actual   implemen tation  of  these   ideas   in  practice.   The   shortlis t  provided   here  serves  as  a  reminder   to  practitioner s  and  resear cher s  in  AI  ethics   that   the  organizational   challeng es  are  just  as  signific ant  as  the  technic al  and  socio-t echnic al   challeng es in building mor e ethical, sa fe, and inclusiv e AI s ystems.    Between  the  lines   :  I’ve  found   the  appr oach   undert aken  at  Micr osoft   as  outlined   in  this  WEF    Case   Study   to  effectiv ely  marr y  the  organizational,   technic al,  and  socio-t echnic al  methods   to   achie ve  Responsible   AI  objectiv es.  We  need   more  examples   wher e  Responsible   AI  methods   and   recommenda tions   such   as  the  ones   highligh ted  in  this  article   are  trialed   and  analy ze  those    results t o learn wha t works and wha t doesn’t.    Police Ar e Telling ShotSpot ter to Alt er Evidence Fr om Gunshot -Detecting AI    [Original article b y  Vice   ]   Wha t  happened   :  An  AI-po wered  tool  that  is  used   to  detect  whe ther   shots   were  fired  in  a   neighborhood   was  used   as  evidence   in  a  case  in  Chic ago  but  the  accused   was  acquit ted  when   it   was  discovered  through   cross-e xamina tion  and  deeper   investigation  that  the  alerts   from  the   system  were  modified   to  better  align   with   the  narrative  that  was  being   presen ted  by  the   prosecution.   As  the  article   goes  on  to  show,  this  wasn’t   the  first  time   that  this  happened,   and   that  trust  in  the  system  has  been   declining   over  time.   In  particular ,  there  are  man y  false  alerts    that  are  issued   by  the  system,  but  more  so  that  the  “humans-in-the-loop”   that  work  for  the   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 9 8   
 compan y  receiv e  reques ts  from  law  enforcemen t  to  dive  deeper   and  have  modified   the  actual    alerts t o bols ter the c ase being pr esen ted ag ainst the de fendan t.   Why  it  matters  :  In  matters  of  someone’ s  life,  relying   on  flims y  evidence,   especially   one  that  is   not  subject   to  more  rigorous  tests  and  support ed  by  studies   that  have  been   funded   by   unrestricted  funding   from  the  very  compan y  selling   the  tool  to  attest  to  its  efficacy  should   be   taken  with   more  than   just  a  grain  of  salt.  Just  as  we  wouldn’t   trust  a  thor oughly   tested  DNA   test   as  evidence   in  a  court,   digit al  forensics   tools  should   face  similar   scrutin y.  As  the  article    men tions,   the  city  of  Chic ago  is  the  second-lar gest  clien t  for  the  compan y  and  with   their    contract  coming   up  for  renewal,  a  more  unbiased,   and  scien tifically  grounded   analy sis  should   be   conduct ed be fore eng aging their ser vices ag ain.   Between  the  lines   :  Some thing   that  really   jumped   out  in  the  article   was  the  men tion  of  how   unevenly   such   systems  are  deplo yed  across  different  neighborhoods   in  the  city  with   Latinx,   Black,   and  Brown  neighborhoods   facing   the  brun t  of  this  form  of  policing   while   being   notably   absent  from  more  affluen t  and  Whit e  neighborhoods.   Mor e  so,  residen ts  of  these   policed    neighborhoods   raise  a  very  pertinen t  poin t:  if  law  enforcemen t  just  asked  them   if  a  shot   was   fired,  as  responsible   neighbor s,  they  would   shar e  that  with   them   rather   than   having  to  rely  on   flims y technology .   Optimizing P eople Y ou Ma y Know (P YMK) f or equity in ne twork cr eation   [Original article b y  LinkedIn Engineering   ]   Wha t  happened   :  LinkedIn   has  applied   two  fairness   measur es  of  equality   of  opportunity   and   equaliz ed  odds   to  mak e  the  recommenda tions   for  potential  connections   more  equit able   across   the  user s  of  the  platform,   especially   for  those   who   don’t   have  as  “influen tial”  profiles   as  some    of  the  more  frequen t  member s  (FM)   of  the  platform.   In  applying   these   fairness   measur es  on  top   of  their   ranking   algorithms,   they’ve  found   that  engagemen t  on  the  platform  didn”t   go  down,   showing tha t fairness objectiv es don’t necessarily ha ve to stand ag ainst the business objectiv es.   Why  it  matters  :  All  social   media   platforms   are  prone  to  having  bias  in  terms   of  the  bene fits  of   the  platform  skewing   towards  those   who   have  amassed   influence   both   on  and  off  the  platform.    Creating  opportunities   for  newer  entrants  is  a  way  of  more  fairly   distributing   the  opportunities    on  the  social   media   platform,   as  is  shown  in  this  article   for  LinkedIn   wher e  recommenda tions    are  now  being   made   of  those   profiles   that  have  fewer  pending   reques ts  amongs t  some   other    balancing f actors.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   1 9 9   
 Between  the  lines   :  As  social   media   platforms   take  on  a  more  pervasive  presence   in  our  lives,   those   that  seek   to  aid  the  less  influen tial  participan ts  of  that  ecosystem  have  the  potential  to   become   more  prominen t.  In  a  sense,   we  have  seen   this  happen   with   TikTok  wher e  even  new   entrants  on  the  platforms   have  a  chance   to  quickly   amass   huge  followings   compar ed  to  other    platforms   wher e  having  a  large  following   predisposes   your  presence   and  dominance   in  the   newsfeed of tha t pla tform.    AI da tasets ar e prone t o mismanag emen t, study finds    [Original article b y  VentureBea t  ]   Wha t  happened   :  Resear cher s  from  Prince ton  found   that  popular   datasets  containing   imag es   that  are  used   to  train  comput er  vision   models   contain  data  for  which   they  migh t  not  have  had   consen t.  In  addition,   they  found   misuses   of  the  datasets  through   modific ations   made   to  it   wher e  it  wasn’t   clear   if  that  was  allowed  under   the  licenses   and  even  when   they  were  not   clearly   allowed  by  the  licenses   on  the  original   datasets.  While   two  out  of  the  three  datasets   have  been   taken  down  from  their   original   sour ces,  one  continues   to  exist  with   a  disclaimer   that   the  data  shouldn’t   be  used   for  commer cial  purposes.   The  other   two  datasets  though   are  still   accessible thr ough non-of ficial means via t orrents and other places tha t have archived it.    Why  it  matters  :  Ethically  dubious   applic ations   are  powered  using   the  training   data  offered  by   these   datasets,  often  going   beyond  the  original   intentions   and  purposes   for  which   they  were   created.  The  author s  of  the  paper   recommend   stewardship   of  datasets  throughout   their    existence,   and  being   more  proactiv e  about   potential  misuses.   They  also  recommend   being   more   clear   in  the  licenses   associa ted  with   these   datasets.  This  will  hope fully   reduce   consen t  viola tions    and do wnstream misuses.    Between  the  lines   :  Wha t  is  still  missing   from  the  conversation  is  how  the  solutions   men tioned    in  the  article   are  non-binding ,  volun tary,  and  won’t  actually   lead   to  any  chang e  as  long   as  the   bene fits  to  be  deriv ed  from  using   the  training   datasets  outw eigh   the  (non-e xistent)  costs   associa ted  with   their   misuses.   If  this  problem   is  to  be  tackled   effectiv ely,  the  suggestions   need    to  be  a  lot  more  rigorous  and  have  elemen ts  of  enforceability   and  legal  migh t  that  will  deter   misuse and s trongly manda te consen t for an y data used t o compose the da taset.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 0 0   
 Apple sa ys collision in child-abuse hashing s ystem is not a c oncern    [Original article b y  The V erge  ]   Wha t  happened   :  Apple   recen tly  unveiled   the  Neur alHash   perceptual  hash   algorithm   that  will  be   applied   to  iCloud   back ed  content  on  Apple   devices   to  detect  child   sexual  abuse   material    (CSAM).   This  has  been   met  with   backlash   from  privacy-minded   organizations   and  activis ts  who    have  called   it  out  for  setting  a  preceden t  that  migh t  allow  for  more  invasive  monit oring   of   people’ s  private  content  on  their   devices.   As  the  details  of  the  system  have  come   to  light,   resear cher s  have  reverse  engineer ed  the  algorithm   and  have  demons trated  hash   collisions    (when   two  imag es  that  are  different  produce   the  same   hash   -  a  represen tation  code  for  that   imag e)  that  will  befuddle   the  system  into  giving   out  false  positiv es.  Apple   has  men tioned   that   there  are  secondar y  check s  in  place   that  will  minimiz e  the  impact   from  such   false  positiv es   through   the  use  of  an  additional   server-side   algorithm   different  from  Neur alHash   and  more   than   30  imag es  need   to  be  flagged  before  they  are  passed   on  as  an  alert   for  human    intervention.    Why  it  matters  :  Robus tness   in  systems  that  detect  and  automa tically  flag  content  is  import ant,   especially   if  there  is  analy sis  being   performed   on  private  content.  Yet,  it  would   appear   that   there  are  some   flaws  in  the  system  as  demons trated  by  resear cher s.  Mor e  import antly,  a  lack  of   comple te  transpar ency   on  the  secondar y  systems  and  wha t  the  real-w orld  probabilities   of  these    collisions   is  going   to  be  like  further   exacerba te  the  doub ts  that  people   have  about   the   effectiv eness of such a s ystem.   Between  the  lines   :  While   the  intention  behind   the  deplo ymen t  of  such   a  system  stands   to  mak e   the  informa tion  ecosystem  safer,  especially   as  it  relates  to  CSAM,  without   trust  from  the  user s   who   form  that  ecosystem,  there  is  bound   to  be  pushback   and  hesit ation  in  full  participa tion.    Apple   can  of  course  railroad  ahead   since   they  own  the  softw are  and  hardware  stack  but  that   will  be  harakiri   in  a  compe titive  mark etplace   wher e  they  have  always  prided   themselv es  on   keeping   the  privacy  of  their   user s  above  all  else,   even  in  the  face  of  moun ting  pressur e  from  law   enforcemen t agencies in the pas t.   Dele ting une thical da ta sets isn’t g ood enough    [Original article b y  MIT T echnology R eview  ]   Wha t  happened   :  The  rapid   progress  of  AI  systems’   capabilities   has  been   fueled   by  the   availability   of  large-scale,  benchmark   datasets.  A  lot  of  those   were  collect ed  by  scraping   data   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 0 1   
 from  the  interne t,  often  without   the  consen t  of  subjects   whose   data  was  scooped   up  in  that   scraping   process.   The  paper   highligh ted  in  this  article   talks  about   DukeMTMC ,  MS-Celeb-1M,    and  other   datasets  which   have  been   retracted  after  concerns   were  raised   by  the  community .   But,  they  continue   to  linger  online   in  various   forms,   often  morphed   to  serve  new  purposes:    adding   mask s  to  faces   in  these   datasets  to  impr ove  the  capability   of  facial   recognition    technology during the pandemic.    Why  it  matters  :  Some   datasets  do  come   with   warnings   and  documen tation  on  their   limit ations    but  these   end  up  being   ignor ed  and  deriv ed  datasets  even  lose  those   pieces   of  documen tation.    This  means   that  we  have  problema tic  datasets,  reple te  with   biases   and  privacy  viola tions,    continuing   to  exist  in  the  wild,   with   hundr eds  of  paper s  being   written  and  published   at   conferences based on the r esults fr om tr aining AI s ystems on tha t data.   Between  the  lines   :  A  potential  solution   men tioned   in  the  article   talks  about   data  stewardship    wher e  a  potentially   independen t  organization  can  take  on  the  role  of  stewarding   the  proper   use   of  that  data  throughout   the  lifecycle  of  its  existence.   While   noble,   there  are  tremendous    challeng es  in  sour cing  funding   for  such   organizations   and  alloc ating  sufficien t  recognition   to   such   work  wher e  the  emphasis   in  the  academic   domain   continues   to  center  on  publishing    state-of-the-art   results   and  work  such   as  stewardship   would   face  an  uphill   battle.  I’d  be   deligh ted  if  I’m  proven  wrong  on  this  front  and  hope   that  we  start  to  recogniz e  the  hard  work   that goes in to preparing , main taining and r etiring da tasets.   Six Essen tial Elemen ts Of A R esponsible AI Model    [Original article b y  Forbes   ]   Wha t  happened   :  The  article   presen ts  a  simple   model   with   6  items  to  think   about   in  Responsible    AI:  accountable,   impartial,   transpar ent,  resilien t,  secur e,  and  governed.   This  is  a  combina tion   and  perhap s  rehash   of  man y  existing  frame works,  guidelines,   and  sets  of  principles   already   out   there.  The  author   admits   to  as  much.   Wha t  is  interesting  in  the  article   is  the  list  of  ques tions    that  are  provided   when   thinking   about   whe ther   or  not  to  have  an  AI  ethics   boar d  and  those   are   the  biggest  takeaways  from  the  article.   In  particular ,  split ting  up  who   should   be  held    accountable   when   some thing   goes  wrong  and  who   is  responsible   for  making   chang es  to  addr ess   undesir able out comes is some thing tha t is import ant.   Why  it  matters  :  A  lot  of  frame works  in  Responsible   AI  can  end  up  being   overly  complic ated  or   overly  simplis tic.  This  model   perhap s  has  the  right  level  of  granularity   but  more  than   that  wha t   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 0 2   
 is  useful  is  that  it  provides   a  great  starting   poin t  for  someone   who   is  early   in  their   Responsible    AI journe y to get started with a c ore set of priorities.    Between  the  lines   :  Wha t  we  really   need   next  is  a  compr ehensiv e  evalua tion  of  the  effectiv eness    of  this  frame work  and  all  the  other   frame works  that  are  out  there  in  terms   of  mee ting  the   stated  goals  of  actually   achie ving  responsible   AI  in  practice   at  their   organization.   Unless   that   happens,   we  can’t  meaningfully   pick  one  frame work  over  another   because   all  evalua tions   as  of   yet would be theor etical in na ture.   Why you should hir e a chie f AI e thics officer    [Original article b y  World E conomic F orum   ]   Wha t  happened   :  As  we  have  more  organizations   moving  from  principles   to  practice,   this  article    gives  a  quick   overview  of  the  Chie f  AI  Ethics   Officer   role  including   wha t  it  should   include   in  its   purview  and  how  it  can  acceler ate  adop tion  of  AI  ethics   within   organizations.   The  role  should    drive  the  broad  definition   of  ethics   principles   for  the  organization,   define  suitable   properties   for   the  AI  systems,   and  drive  tooling   and  processes   within   the  organization  for  practitioner s.  A  wide    range  of  expertise   is  also  requir ed  to  take  on  this  role  including   a  multi-disciplinar y  backgr ound,    the  ability   to  effectiv ely  communic ate  with   a  diversity  of  stakeholder s,  driving   compan y-wide    engagemen t, and helping t o mak e the business c ase f or AI e thics as a c ore consider ation.    Why  it  matters  :  In  a  recen tly  published   article,   we  highligh ted  wha t  it  would   take  for  a  Chie f  AI   Ethics   Officer   to  succeed   within   the  organization  and  why  it  is  an  import ant  role.  In  particular ,   the  current  problem   with   the  move  from  principles   to  practice   is  that  such   efforts  are  often   ad-hoc   or  don’t   have  enough   executiv e  support   to  really   drive  meaningful   chang e  within   the   organization. The appoin tmen t of such a position help s to overcome some of these challeng es.   Between  the  lines   :  But,  the  mere  appoin tmen t  of  such   a  position   to  mee t  public   appear ance    requir emen ts  will  only   cause   more  harm   in  the  long   run.  Wha t  needs   to  be  carefully   consider ed   is  how  much   actual   power  is  alloc ated  to  this  person  and  whe ther   they  have  the  necessar y   backgr ound   and  skills   to  be  able   to  drive  chang e  across  the  organization.   One  of  the  mos t   problema tic  issues   at  the  momen t  is  a  lack  of  sufficien t  technic al  and  oper ational   expertise   in   such   roles  that  leads   to  a  breakdown  of  strategy  when   it  comes   time   to  actually   put  these   ideas    into practice.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 0 3   
 Thinking Thr ough the E thics of Ne w Tech … Before Ther e’s a Pr oblem    [Original article b y  Harvard Business R eview  ]   Wha t  happened   :  The  article   poin ts  out  how  socie ty  has  typic ally  stopped   to  addr ess  ethical   issues   with   the  use  of  new  technology   after  it  rapidly   permea tes  our  life;  the  author   asks  us  to   imagine   wha t  would   happen   if  that  is  not  the  case?   Taking   the  examples   of  automobile   safety   features  like  the  seatbelt   that  appear ed  man y  years  after  which   if  implemen ted  earlier   would    have  saved  man y  lives.  We  are  on  a  similar   cusp   with   AI  rapidly   permea ting  man y  parts   of  our   lives.  By  bringing   in  specialis ts  who   are  domain   experts,   esche wing   haste  in  the  deplo ymen t  of  a   new  piece   of  technology   because   it  seems   to  offer  immedia te  gains,   but  one  that  migh t  have   dela yed,  severe  downstream   consequences   and  assigning   accountability   to  stakeholder s  in   different  parts   of  the  lifecycle  and  having  someone   in  a  leader ship  position   take  this  on  as  a  core   responsibility ar e some pr oposed w ays tha t we migh t be able t o mitig ate these issues.    Why  it  matters  :  As  organizations   struggle  to  move  from  principles   to  practices,   the  advice    offered  in  this  article   is  a  great  starting   poin t  for  those   who   want  to  realiz e  some   early   wins   in   the  ethical,  safe,  and  inclusiv e  deplo ymen t  of  AI  systems.   Reframing   the  challeng es  as   opportunities   to  do  better  and  guide   other s  along   the  way  migh t  be  yet  another   bene fit   emer ging fr om adop ting these pr actices.    Between  the  lines   :  I  think   another   layer  of  nuance   needs   to  be  added   to  the  advice   of  “pausing    and  thinking ,”  which   is  to  under stand  the  incen tives  that  guide   emplo yee  and  stakeholder    beha vior  within   the  organization.   In  particular ,  if  KPIs  are  such   that  a  certain  number   of  user s   need   to  be  secur ed  or  a  certain  sales   quot a  needs   to  be  met  to  secur e  a  bonus   at  the  end  of  the   year,  then   we  need   to  mak e  sure  that  these   ideas   are  discussed   keeping   this  in  mind,   other wise    implemen tations of t ech e thics ar e doomed t o fail.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 0 4   
 7. La ws and R egula tions    Introduction  by  Abhishek   Gup ta,  Founder   and  Principal   Resear cher ,  Mon treal  AI  Ethics    Institut e   The  legal  landsc ape  is  getting  busy  with   man y  regula tions   coming   forth   to  regula te  the  use  of  AI.   This  chap ter  opens   with   an  analy sis  of  the  EU  AI  Act  and  wha t  the  introduction   of  the  Act  means    for  cooper ation  between  different  parts   of  the  world,   such   as  the  US  and  EU.  The  next  piece   on   “Algorithmic   accountability   for  the  public   sector”  resona ted  with   me  a  lot  given  our  focus   on  the   rising   import ance   of  context  and  public   participa tion  as  a  pivotal  factor  in  the  success   of  new   policy   mechanisms.   Going   further   with   those   ideas,   this  chap ter  also  covers  the  Responsible   AI   principles   from  NATO  that  applies   to  the  milit ary  use  of  AI.  Ther e  remain   several  ambiguous    terms   in  the  strategy  documen t  (or  at  least  the  summar y  version  of  it  which   is  the  publicly    available   version)   which   water  down  the  ability   for  people   to  implemen t  those   ideas   in  practice.    None theless,   given  the  prominence   of  NATO,  this  is  a  great  start  to  move  countries   towards   coalescing   around   a  shar ed  set  of  principles   for  the  responsible   developmen t  and  deplo ymen t   of AI s ystems, especially in the milit ary context.   As  we  enter  the  era  of  co-creation  and  co-invention  with   machines,   it  is  import ant  to  examine,    with   a  legal  lens,   how  we  grant  patents  and  to  whom   for  wha t  kinds   of  invention.   In  fact,  in  my   view,  it  even  raises   ques tions   about   wha t  the  usefulness   of  patents  is,  at  least  purely  in  the   domain   of  algorithmic   advances   within   the  domain   of  AI.  “Summoning   a  New  Artificial    Intelligence   Patent  Model:   In  the  Age  of  Pandemic”   proposes   creative  solutions   to  the  hurdles  of   patenting AI t echnology b y establishing a ne w pa tent track model f or AI in ventions.    So  far,  regula tions   have  had  a  mixed  track  record  on  curbing   the  spread  of  harm   from   technologic al  systems.   Uneven  enforcemen t  and  ambiguity   seem   to  be  the  culprits   for  this   problem.   Wha t  we  see  from  the  rest  of  the  chap ter  are  a  few  examples   such   as  missing    geofence   warrants  in  the  Calif ornia   DoJ  Transpar ency   Database   that  showcases   how  vulner able    we  are  still  to  the  actual   following   of  practice   even  if  there  are  manda tes  in  place   to  ensur e   accountability   and  citizen  welfare.  Other   examples   include   the  chang es  that  we  need   in   Congr essional   practices   today  so  that  regula tion  for  Big  Tech  happens   in  a  more  timely   and   meaningful   fashion.   With   the  launch   of  CoPilot   from  GitHub,   code  gener ation  and  verba tim   replic ation  of  code  samples   has  started  posing   legal  challeng es,  especially   since   there  are   sligh tly  varying  interpr etations   of  how  licenses   are  placed   in  each   code  reposit ory  and  the   overall license and f air use policies tha t govern the en tire GitHub pla tform.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 0 5   
 It  suffices   to  say  that  there  is  a  lot  of  movemen t  in  the  legal  domain   and  it  will  be  interesting  to   see  wha t  other   developmen ts  take  place   in  the  rest  of  2022   as  we  seek   to  concr etize  regula tions    and  sear ch  for  effectiv e  mechanisms   to  enforce  them   so  that  it  doesn’t   just  happen   in  letter  but   also in spirit.    MAIEI   is  also  happ y  to  endor se  the  upcoming   Algorithmic   Accountability   Act  (2022)   from   Sena tor  Wyden’ s  office  in  the  US  and  is  happ y  to  engage  with   other   policymak ers  from  around    the w orld. Please don’t hesit ate in r eaching out t o us t o collabor ate!   Abhishek Gupta (  @atg_abhishek  )   Founder, Director, & Principal Researcher   Montreal AI Ethics Institute   Abhishek  Gupta  is  the  Founder,  Director,  and  Principal  Researcher  at  the   Montreal  AI  Ethics  Institute.  He  is  a  Machine  Learning  Engineer  at  Microsoft,   where  he  serves  on  the  CSE  Responsible  AI  Board.  He  also  serves  as  the  Chair   of the Standards Working Group at the Green Software Foundation.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 0 6   
 Go Deep: Research Summaries   The  European   Commission’ s  Artificial   Intellig ence   Act  (Stanford  HAI  Policy    Brief)   [  Original paper   by Marie tje Schaak e]   [Resear ch Summar y by Abhishek Gup ta]   Overview  :  With   the  recen tly  released   Artificial   Intelligence   Act  in  the  EU,  a  lively  deba te  has   erup ted  around   wha t  this  means   for  different  AI  applic ations,   companies   building   these    systems,   and  more  broadly ,  the  futur e  of  inno vation  and  regula tion.   Schaak e  provides   an   excellen t  overview  of  the  Act  with   an  analy sis  of  the  implic ations   and  sentimen ts  around   this   Act, including global c ooper ation be tween dif ferent regions of the w orld lik e the US and EU .   Introduction    Just  as  we  had  a  scramble   in  the  wake  of  the  GDPR   in  2018   as  companies   rushed   to  become    complian t,  the  announcemen t  of  the  AI  Act  has  triggered  a  frenzy  amongs t  organizations   to  find   ways  to  become   complian t  while   main taining   their   ability   to  inno vate.  The  current  paradigm   of   AI  applic ations   incen tivizes  more  invasive  data  collection   to  power  these   applic ations   while    providing   recommenda tions,   decisions,   and  influencing   people’ s  lives  in  more  and  more   signific ant ways.   The  policy   brief  provides   a  quick   overview  of  the  definition   of  AI  used   in  the  AI  Act,  which   kinds    of  applic ations   it  applies   to  (high-risk),   wha t  high-risk   means,   some   banned   use  cases,   some    exceptions   to  those   banned   use  cases,   wha t  conformity   assessmen ts  are,  the  implic ations   of  the   AI  Act  on  the  rest  of  the  world,   and  how  civil  socie ty  and  other   organizations   have  react ed  to   the  Act.  Ther e  are  mixed  reactions,   but  Schaak e  concludes   on  an  optimis tic  note  that  the  Act   can  become   a  rallying   poin t  to  achie ve  more  consis tency   in  cyber security   and  other   practices   in   addition   to  AI  developmen t  across  the  world.   We  shouldn’t   treat  the  harms   from  AI  systems  as   inevitable.    The  definition   of  AI  utiliz ed  in  the  Act  follows  an  interesting  path  of  using   a  broad,   overarching    definition   with   some   specific ally  defined   categories   and  use  cases.   This  hybrid   appr oach   is   supplemen ted  by  the  power  to  amend   these   definitions   as  we  go  along   to  mak e  them   more   compa tible   with   technic al  and  sociologic al  developmen ts  in  the  futur e.  This  will  be  critic al  for   the  continued   applic ability   of  the  Act,  which   is  lacking   in  man y  other   proposed   regula tions   that   are either t oo v ague or t oo specific.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 0 7   
 Risk and unaccep table uses    The  central  oper ating  mechanism   of  the  Act  is  to  look   at  high-risk   AI  uses-c ases   which   include    biome tric  iden tification,   critic al  infrastructur e  that  can  signific antly  impact   human   lives,   determining   access   to  educ ation  and  emplo ymen t,  worker  manag emen t,  access   to  private  and   public   services   (e.g.,  finance),   law  enforcemen t,  migr ation  and  immigr ation,   and  adminis tration   of  justice  and  democr atic  processes.   Article   7(2)  gives  more  details  on  how  to  mak e  these    assessmen ts.  For  such   high-risk   systems,   they  cannot   be  released   to  the  public   before   under going   a  conformity   assessmen t  which   determines   whe ther   all  the  needs   of  the  AIA  risk   frame work ha ve been me t.   Distorting   human   beha vior,  exploiting   vulner abilities   of  marginaliz ed  groups,  social   scoring ,  and   real-time   biome tric  iden tification  in  public   spaces   (except  in  certain  circums tances   like  those    manda ted  by  national   law,  or  for  tracking   terrorist  activities,   sear ching   for  missing   persons,   etc.)   are prohibit ed use c ases.    Complying with the AIA    Articles   9  through   15  of  the  AIA  provide   guidance   on  how  to  comply   with   the  Act  and  include    practices   like  main taining   a  risk  manag emen t  system,  data  governance   and  manag emen t,   transpar ency   via  constantly  upda ted  documen tation  of  the  high-risk   AI  system,  logging   and   traceability   through   the  AI  system,  appr opria te  human   oversight,  and  balancing   accur acy  of  the   system  with   other   desir ed  properties   like  robus tness   and  explainability   of  the  system.  Some   of   these   requir emen ts  will  sound   familiar   to  those   who   had  worked  in  compliance   before  and   helped   their   organizations   transition   into  the  GDPR   era.  Other s  emer ge  from  best  practices   in   the  MLOps  domain   as  well.  A  combined   policy   and  technic al  appr oach   is  the  way  forward  to   build   AIA-complian t  systems.   This  will  help   in  mee ting  the  post-mark et  monit oring   requir emen ts   as pr oposed in the AIA .   We  can  expect   there  to  be  some   intense   lobb ying  from  different  corpor ations   and  other    organizations   to  tailor   the  AIA  to  align   better  with   their   needs.   Standar d-se tting  organizations    will  become   more  potent  through   economic,   legal,  and  politic al  levers,  and  we  mus t  account  for   the  potential  power  imbalances   that  occur   through   this  channel.   Finally ,  through   the  Brussels    effect,  we  will  potentially   see  a  more  positiv e  chang e  in  the  attitude   towards  building   more   ethical, sa fe, and inclusiv e AI s ystems w orldwide.    Between the lines    In  line  with   the  work  done   at  the  Mon treal  AI  Ethics   Institut e  in  creating  resear ch  summaries,    such   policy   briefs  provide   a  great  avenue   to  catch  up  on  pertinen t  issues   without   diving   into  all   the  details  until  needed.   These   are  especially   valuable   for  those   impact ed  by  policy   and   technic al  chang es  in  the  field   but  migh t  lack  the  time   and  resour ces  to  parse  through   the   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 0 8   
 fast-moving  field.   The  next  step  in  making   such   pieces   more  actionable   is  to  analy ze  case   studies.   In  the  case  of  the  AI  Act,  it  would   be  great  to  see  how  this  impacts   currently  deplo yed   high-risk   AI  systems,   wha t  that  means   for  the  process,   and  technic al  chang es  requir ed  to  mak e   these   systems  conform  with   the  requir emen ts  to  be  allowed  deplo ymen t  in  the  field.    Companies   that  are  fast  to  act  on  these   compliance   requir emen ts  will  surely  gain  a  compe titive   mark et edg e, essen tially mimicking the chang es during the tr ansition t o the GDPR er a.   Algorithmic acc ountability f or the public sect or   [  Original   paper   by  Ada  Lovelace   Institut e,  AI  Now  Institut e  and  Open   Governmen t   Partner ship]    Overview  :  The  developmen ts  in  AI  are  never-ending ,  and  so  is  the  need   for  policy   regula tion.    The  report   exposes   wha t  has  been   implemen ted,  their   successes   and  failings   while   also   presen ting  the  emer gence   of  two  pivotal  factors  in  any  policy   context.  These   are  the  import ance    of context and public participa tion.    Introduction    With   AI  developing   at  warp  speed,   wha t  is  the  current  situa tion  in  the  algorithmic   space?   Do  we   know  wha t  works  in  terms   of  regula tion?   Due  to  the  lack  of  policy   and  data  about   algorithmic    regula tion  in  the  Global   South,   the  paper   adop ts  a  European   and  North   Americ an  focus.    Nevertheless,   this  report   aims   to  under stand  the  success   of  algorithmic   accountability   policies    from  different  actors’  perspectiv es.  While   exposing   wha t  has  been   attemp ted  (alongside   its   successes   and  failur es),  two  crucial   factors  emer ged:  public   participa tion  and  context.  The  latter   is wher e we are going t o begin.    The import ance of c ontext when implemen ting policy    The  literature  review  conduct ed  in  the  report   showed  that  people   under stand  algorithmic    accountability   but  not  so  much   about   implemen ting  it.  Nevertheless,   one  key  elemen t  in   realizing   policy   is  the  context  in  which   it  is  deplo yed.  The  Canadian   ADM   directiv e  requir es  any   custom  sour ce  code  owned   by  the  Governmen t  to  be  made   public.   Yet,  the  New  Zealand    Aotearoa  NZ  Algorithm   Chart er  asks  how  the  data  was  collect ed  and  stored  to  be  made    available.    With   this  in  mind,   the  effectiv eness   of  the  same   policy   can  be  drastically  different  in  two   different  contexts.  Hence,   wha t  has  been   implemen ted  and  wha t  are  the  gener al  problems   with    these appr oaches?    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 0 9   
 Wha t has been a ttemp ted, and wha t are their f aults?    In  this  section,   I  will  list  a  broad  overview  of  the  policy   methods   carried   out  by  different  actors   in the r eport and their associa ted pr oblems.    High-le vel ethical policies   : provide a help ful fr ame   of reference t o appr oach alg orithmic issues.    Problem   : doesn’t pr ovide an y form of oblig ation t o  specific actions.    Prohibitions   and  moratoria  :  prevent  harm ful  technologies   from  being   used   entirely,  or  gives   regula tors  time   to  catch  up  to  their   developmen t.  have  also  been   attemp ted  to  be   implemen ted.   Problem   :  they  rest  on  the  assump tions   that  either   the  technology   should   never  be  used   and   that the policy and r egula tion e fforts will be adequa te in a c ouple of y ears.   Impact   assessmen ts  :  aim  to  expose   how  the  agents  have  subjectiv ely  defined   wha t  harms,   and   risks are.   Problem   : the y struggle t o provide clear a venues f or  public participa tion.    Audits   :  standar dize  and  scrutiniz e  the  efforts  being   made   to  gener ate  an  environmen t  of   algorithmic acc ountability .   Problem   :  the  compan y  mus t  provide   adequa te  data  to  be  audit ed,  and  the  performance   during    auditing is the same as a fterwards.   Oversight bodies   : possibility of in fluencing the beha vior  of pr ominen t act ors.   Problem   : the in fluence ma y only be minut e.   Appeals   to  human   intervention  :  involving   humans   in  the  process   to  better  ensur e  fairness   and   establish some f orm of r esponsibility .   Problem   :  assumes   that  having  a  human   in  the  process   does   help   to  ensur e  fairness   and  doesn’t    ackno wledg e ho w alg orithmic da ta can in fluence human decision making.    The r ole of the public    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 1 0   
 Given  the  last  poin t  of  human   intervention,   the  role  of  public   intervention  should   not  be   under estima ted.  The  intervention  help s  to  match  better  governmen tal  actions   with   the  needs   of   the people.    Wha t’s  still  noteworth y  is  how  different  people   have  varying  resour ces  that  allow  them   to  get   involved. Her e, access t o the media c an help le vel this pla ying field.    The r ole of the media    Legal  frame works  don’t   just  rely  on  the  law  to  be  effectiv e,  but  also  on  other   factors  such   as   “politic al  will  and  cultur al  norms” .  Pressur e  from  media   outle ts  can  help   to  reinforce  the  need   to   implemen t  and  main tain  policies   beyond  just  their   legally  binding   status.  Such   intervention  can   mak e  the  policies   ‘socie tally  binding ’,  fixing   the  need   for  communic ation  between  policymak ers   and the public.    Between the lines    For  me,  the  key  findings   are  the  import ance   of  the  public   and  the  context  within   policymaking.    No  long er  can  a  ‘one  size  fits  all’  attitude   be  adop ted  in  the  algorithmic   space,   bringing   in  the   need   for  an  appr opria te  scope.   Regula ting  individual   actors  too  closely   can  ignor e  the  systemic    and  social   pressur es  presen t.  Adop ting  too  broad  a  viewpoin t  can  then   gener alize  import ant   peculiarities   that  need   attention  in  different  contexts.  Wha t’s  for  sure,  in  my  eyes,  is  that  while    policy aims t o ser ve the public, it mus t first learn fr om the public.    Summoning   a  New  Artificial   Intellig ence   Patent  Model:   In  the  Age  of   Pandemic    [  Original paper   by Shlomit Y anisky -Ravid and R egina   Jin]   [Resear ch Summar y by Avantika Bhandari]    Overview  :  The  article   analy zes  the  challeng es  posed   by  the  current  patent  law  regime   when    applied   in  the  context  of  Artificial   Intelligence   (AI)  in  gener al  and  especially   at  the  time   of  covid   pandemic.   The  article   also  proposes   creative  solutions   to  the  hurdles  of  patenting  AI  technology    by establishing a ne w pa tent track model f or AI in ventions.    Introduction    Covid-19   has  created  a  worldwide   pandemic,   causing   millions   of  deaths  within   mon ths.  Lack   of   vaccines   or  the  FDA  appr oved  drugs   in  the  initial   days  have  all  aggravated  the  global   health    crisis.   On  the  forefront  against  Covid-19,   AI  technology   is  proving  to  be  an  effectiv e  and   powerful   tool  in  developing   new  drugs,   vaccines,   and  diagnos tic  methods.   Also ,  AI  mechanisms    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 1 1   
 are  built   for  the  purposes   of  tracking   and  forecasting  the  outbr eaks,  processing   health   claims,    managing   drones   to  deliv er  supplies,   and  iden tifying  high-risk   individuals.   For  instance,   the   Korean  biotech  compan y  Seeg ene  utiliz ed  an  AI  system  to  create  ‘a  novel  coronavirus   testing   method—an   unpr eceden ted  short   period   of  time   as  it  usually   takes  several  mon ths  with   a  large   group  of  scien tists  to  develop   such   testing  protocol.’  Alibaba   developed   an  AI-based   platform  to   detect  complic ations   in  CT  scans  of  patients’  ches ts  with   96%   accur acy.  Furthermor e,  the  AI   system  created  by  the  Canadian   startup   Bluedot   success fully   predict ed  the  virus   outbr eak  even   before  the  World  Health   Organization  (WHO)   officially   declar ed  the  discovery  of  a  novel   coronavirus.   These   examples   bring   to  light  man y  crucial   features  of  AI  tools-   they  are  efficien t,   creative,  accur ate,  evolving ,  and  rapid.   Ackno wledging   its  bene fits  the  Whit e  House   has  urged   resear cher s  to  emplo y  AI  to  analy ze  tens  and  thousands   of  documen ts  to  decipher   the  origins   of   coronavirus.    Key Insigh ts   The  resear cher s  of  the  article   argue  that  AI-made   inventions   mus t  be  patentable.   However  as   per  the  patent  law  only   ‘human   inventors’  are  eligible   for  patent  owner ship,   therefore,  a  new   model   is  needed.   When   talking   about   AI  inventions   there  are  gener ally  two  (2)  types   of   inno vative  AI  applic ations.   First,  when   AI  inventions   are  creative  AI  systems  themselv es,  referred   to  as  ‘creativity   machines’   that  are  capable   of  gener ating  new  inventions   themselv es.  Second,    when   AI  inventions   are  AI-made   inventions,   in  other   words,  the  resulting   inventions   gener ated   by  the  AI  systems.   The  AI-made   inventions   have  posed   challeng es  for  the  current  patent  law   regime,   which   was  drafted  in  an  era  when   AI  technology   was  absent.  Realizing   the  absence   of   AI-made   inventions   in  patent  laws,  the  Unit ed  States  Patent  and  Trademark   Office  (USP TO)   published   a  Reques t  for  Commen ts  on  Patenting  AI  Inventions   on  the  Feder al  Register  in  Augus t   2019.   However,  USPTO  has  not  issued   any  guidelines   regarding   patent  rights  in  respect   of  AI   inventions.    The  resear cher s  suggest  a  comple te  novel  model   to  incorpor ate  AI-made   inventions.   They  argue   that  the  current  patent  model   is  inapplic able   and  propose   a  new  legal  paradigm   for  examining    AI  inventions.   They  assert   that  a  revolution   is  needed   to  establish   a  ‘distinct   AI  patent  track   model   separ ating  from  the  current  patent  regime   applied   to  human-made   inventions. ’  A  new   track  model   is  crucial   as  man y  factors  of  the  current  patent  law  regime   are  inapplic able   in  the  AI   context  and  amending   would   not  addr ess  all  the  existing  concerns.   The  resear cher s  belie ve  that   the  new  AI  patent  track  model   would   provide   a  distinct   scope  of  protection   for  creative  AI   systems  and  AI-made   inventions-   all  of  which   migh t  not  be  patentable   under   the  current  patent   regime.    AI’s  creativity   can  be  found   in  the  pharmaceutic al  indus try,  wher e  AI  tools  are  emplo yed  in  the   process   of  drug   discovery  to  disease   target  iden tification.   Man y  new  drug   targets  based   on  RNA    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 1 2   
 binding   proteins  were  discovered  by  IBM  Watson   to  cure  a  neur odeg ener ative  disease.   A  drug    design   AI  held   by  AstraZenec a,  U.K.  has  formula ted  a  large  number   of  new  drug   structur es   catering   to  the  chemic al  space   that  the  human   may  not  have  though t  of.  These   examples   prove   that  AI  systems  not  only   facilit ate  creativity   but  also  help   in  speeding   the  drug-disc overy  process    in an accur ate and e fficien t manner .   Protection of Cr eative AI S ystems and AI-Made In ventions    The  resear cher s  argue  that  such   AI-made   inventions   should   be  patentable   to  incen tivize   inno vation  and  to  reward  the  labor .  Allowing   patenting  of  algorithms,   a  part  of  AI  creative   systems,   would   incen tivize  the  resear ch  on  fundamen tal  AI  building   block s.  This  will  help   boos t   the  advancemen t  of  AI  technology   and  encourage  technologic al  developmen t  in  the  fields   like   medic al,  engineering ,  and  science.   The  resear cher s  also  main tain  that  allowing   the  patenting  of   AI-tr ained   models   would   incen tivize  data  scien tists  and  trainer s  to  gener ate  new  resour ceful  AI   models   in  an  attemp t  to  solve  practic al  problems.   AI-tr ained   models   are  capable   of  finding    answers  by  learning   from  the  data  and  target  attribut es.  DeepMind,   for  instance,   is  a  trained    model   that  learns   how  to  solve  ‘problems   and  advances   discovery  in  various   fields   such   as   science,   medicine,   and  ener gy.’  The  patenting  of  AI-made   inventions   would   boos t  efficiency   in   resear ch  and  developmen t,  leading   to  more  inno vation  in  useful  products   and  processes.   The   investors  are  motiv ated  by  economic   returns   through   licensing   and  sales   from  the  exclusiv e   patent righ ts in AI-made in ventions.    The  resear cher s  in  this  article   propose   a  new  patent  track  model   ‘to  incorpor ate  a  wider   scope   of  patent  protection   for  AI  inventions. ’  They  recommend   the  following   rules   for  the  new  track   models:    ●  Chang e  of  person  of  ordinar y  skill  in  the  art  (POSIT A)  standar ds  :  The  POSIT A  standar d   may  not  be  applic able   in  the  obviousness   assessmen t  for  AI  inventions   under   both   the   motiv ation  test  and  the  ‘obvious   to  try’  analy sis.  This  is  because   the  invention  is   intended   to  addr ess  the  intricate  problems   in  a  seemingly   unforeseen   way.  To  settle  the   implic ation  of  the  ‘obviousness   requir emen t’  in  respect   to  AI  inventions,   the  resear cher s   propose   the  POSIT A  standar d  of  ‘a  skilled   person  using   an  ordinar y  AI  tool  in  the  art.’  This   would   help   a  professional   under stand  the  comple xity  of  the  AI  algorithm,   the  versatility    of the AI s ystem, and the c omplic ation of the pr oblem in a pa tent applic ation.    ●  Expedit ed  Patent  Examina tion  :  The  time   taken  to  acquir e  a  patent  is  crucial   in  the   COVID-19   urgency .  Given  the  constraints  of  the  patent  system  the  long   waiting   period   for   examina tion  may  discourage  organizations   from  investing  in  resear ching   a  cure  for  the   virus.   By  the  time   a  drug   is  granted  a  patent,  a  pharmaceutic al  compan y  may  have   already   missed   the  peak   in  demand   for  the  drug   and  may  not  be  able   to  reap  the  highes t   rewards. This is wh y expedit ed pa tent examina tion f or AI in ventions is needed.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 1 3   
 ●  Use  of  AI  for  Patent  Examina tion  :  The  resear cher s  advise   the  use  of  AI  tools  for  patent   examina tion  to  review  the  difficult   algorithms   and  huge  amoun ts  of  data  which   may  be   overwhelming   for  humans   to  handle,   as  the  AI  tools  would   boos t  efficiency   and  can   acceler ate  the  patent  examina tion  process.   The  USPTO  uses   the  AI  system  Unity   to   increase   the  efficiency   of  patent  examina tion.   However,  the  applic ation  of  Unity   seems    limit ed to sear ching pa tents, public ations, and imag es, rather than e xamining pa tents.   ●  Short ened   Patent  Lifetime   :  “In  the  AI  indus try,  the  invention  process   as  well  as  product    life  cycles  can  some times   be  extremely   short. ”  A  patent  is  granted  for  20  years  however,   resear cher s  belie ve  in  short ening   the  patent  lifetime   for  AI  patents  as  this  would   allow   the  technology   to  come   to  the  public   domain   faster  for  the  bene fit  of  knowledg e   dissemina tion.    Between the lines    The  article   rightly  evalua tes  the  need   to  establish   a  new  AI  patent  track  as  the  current  patent   law  regime   poses   substantial  hurdles  and  uncert ainties  for  patenting  AI-made   inventions.   The   new  track  addr esses   man y  ambiguous   elemen ts  of  the  patent  law  to  be  more  in  sync  with   the   3A  era  digit al  tools,   such   as  the  ‘person  skilled   in  the  art’  standar d,  the  examina tion  of  timing    and me thod, and the pa tent lifetime.    NATO Artificial In tellig ence Str ategy   [  Original documen t  by NA TO]   [Resear ch Summar y by Angshuman K aushik]    Overview  :  On  October   21-22,   2021   during   the  NATO  Defence   Minis ters  Mee ting,  held   in   Brussels,   the  minis ters  agreed  upon   to  adop t  the  NATO  Artificial   Intelligence   Strategy   (“her eina fter  the  strategy”).  The  strategy  is  not  publicly   available   and  wha t  is  accessible   is  a   documen t  titled   ‘Summar y  of  the  NATO  Artificial   Intelligence   Strategy’.  This  write-up   provides    an o verview of the said summar y.   Introduction    “We  see  authorit arian   regimes   racing   to  develop   new  technologies,   from  artificial   intelligence   to   autonomous   systems, ”  NATO  Secr etary  Gener al  Jens   Stoltenber g  said  during   a  media   conference    at  NATO  headquart ers  in  Brussels   on  October   20,  2021,   a  day  prior   to  the  aforesaid   Defence    Minis ters  Mee ting.   No  prizes  for  guessing   as  to  who   he  was  referring   to  by  the  use  of  the  phrase   ‘authorit arian   regimes’ .  Although,   putting  out  a  strategy  on  AI  is  a  step  in  the  right  direction,    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 1 4   
 how  far  the  same   would   be  implemen ted  in  practice   is  the  sixty-four  thousand   dollar   ques tion.    None theless, the f ourfold aim of the s trategy is as f ollows:   ●  to  provide   a  founda tion  for  NATO  and  Allies   to  lead   by  example   and  encourage  the   developmen t  and  use  of  AI  in  a  responsible   manner   for  Allied   defence   and  security    purposes;    ●  to  acceler ate  and  mains tream   AI  adop tion  in  capability   developmen t  and  deliv ery,   enhancing   interoper ability   within   the  Alliance,   including   through   proposals   for  AI  Use   Cases, ne w structur es, and ne w pr ogrammes;    ●  to  protect  and  monit or  our  AI  technologies   and  ability   to  inno vate,  addr essing   security    policy   consider ations   such   as  the  oper ationalisa tion  of  our  Principles   of  Responsible   Use;    and   ●  to  iden tify  and  safeguar d  against  the  threats  from  malicious   use  of  AI  by  state  and   non-s tate act ors.   The Str ategy   The  strategy  talks  about   AI  changing   the  global   defense   and  security   environmen t  and  offering    an  unpr eceden ted  opportunity   to  strengthen   NATO’s  technologic al  edge  and  at  the  same   time,    escalate  the  speed   of  the  threats  it  faces.   It  further   men tions   that  AI  will  likely  affect  the  full   spectrum   of  activities   undert aken  by  the  Alliance   in  support   of  its  three  core  tasks;  collectiv e   defense,   crisis   manag emen t,  and  cooper ative  security .  In  the  futur e,  the  NATO  Alliance   aims   to   integrate  AI  in  an  interoper able   way  to  support   its  three  core  tasks.  The  strategy  recogniz es  the   leading   role  played  by  the  private  sector  and  the  academia   in  the  developmen t  of  AI  and   envisag es  signific ant  cooper ation  between  NATO,  the  private  sector  and  academia;   a  capable    workforce  of  NATO  technic al  and  policy -based   AI  talen t;  a  robus t,  relevant,  secur e  data   infrastructur e;  and  appr opria te  cyber   defenses.   According   to  the  footnot e  in  the  strategy,   ‘priv ate  sector’  includes   Big  Tech,  start-ups,  entrepreneur s  and  SMEs   as  well  as  risk  capital  (such    as  venture  and  private  equity   funds).   It  is  obvious   that  the  AI  revolution   is  being   spearheaded   by   the  private  sector  and  the  academia   and  NATO  plans   attracting   the  best  talen t  to  join  its   workforce.  Under   the  forthc oming   Defence   Inno vation  Acceler ator  for  the  North   Atlantic   (DIANA),   NATO  aims   to  support   its  AI  ambition   through   the  national   AI  test  centers  and  also   intends   to  conduct   regular   high-le vel  dialogues,   engaging   technology   companies   at  a  strategic   politic al  level.  At  the  forefront  of  the  strategy  lie  the  NATO  Principles   of  Responsible   Use  for  AI   in  Defence,   which   are  based   on  existing  and  widely   accep ted  ethical,  legal,  and  policy    commitmen ts.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 1 5   
 NATO Principles of R esponsible Use of AI in De fense    NATO  and  the  Allies   commit   to  ensuring   that  the  AI  applic ations   they  develop   and  consider   for   deplo ymen t  will  be  at  the  various   stages  of  their   life  cycles,   in  accordance   with   the  following   six   principles:    A.  Lawfulness   :  AI  applic ations   will  be  developed   and  used   in  accordance   with   national   and   interna tional la w, including in terna tional humanit arian la w and human righ ts law, as applic able;    B.  Responsibility   and  Accountability   :  AI  applic ations   will  be  developed   and  used   with    appr opria te  levels  of  judgmen t  and  care;  clear   human   responsibility   shall   apply   in  order  to   ensur e acc ountability;    C.  Explainability   and  Traceability   :  AI  applic ations   will  be  appr opria tely  under standable   and   transpar ent,  including   through   the  use  of  review  methodologies,   sour ces,  and  procedur es.  This   includes   verific ation,   assessmen t  and  valida tion  mechanisms   at  either   a  NATO  and/ or  national    level;   D.  Reliability   :  AI  applic ations   will  have  explicit,   well-de fined   use  cases.   The  safety,  security ,  and   robus tness   of  such   capabilities   will  be  subject   to  testing  and  assur ance   within   those   use  cases    across  their   entire  life  cycle,  including   through   established   NATO  and/ or  national   certific ation   procedur es;   E.  Governability   :  AI  applic ations   will  be  developed   and  used   according   to  their   intended    functions   and  will  allow  for:  appr opria te  human-machine   interaction;   the  ability   to  detect  and   avoid  unin tended   consequences;   and  the  ability   to  take  steps,  such   as  diseng agemen t  or   deactiv ation of s ystems, when such s ystems demons trate unin tended beha vior; and    F.  Bias   Mitig ation  :  Proactiv e  steps  will  be  taken  to  minimiz e  any  unin tended   bias  in  the   developmen t and use of AI applic ations and in da ta sets.   The  commitmen t  to  abide   by  the  principles   at  the  various   stages  of  a  lifestyle  of  AI  systems  is  a   substantial  one,   and  only   time   will  tell  as  to  the  oper ationaliz ation  of  the  same.   Mor eover,   terms   like  ‘appr opria te  levels’,  ‘judgmen t  and  care’,  and  ‘appr opria tely  under standable’   etc.   need   exposition.   Further ,  the  strategy  also  talks  about   NATO  oper ationalizing   its  Principles   of   Responsible   Use  to  ensur e  the  safe  and  responsible   use  of  AI.  It  lays  emphasis   on  consciously    putting  bias  mitig ation  efforts  into  practice,   which   will  seek   to  minimiz e  biases   such   as  gender ,   ethnicity   or  personal   attribut es.  Ther e  is  a  further   commitmen t  to  conduct   appr opria te  risk   and/ or  impact   assessmen ts  prior   to  deplo ying  AI  capabilities.   The  strategy  also  takes  note  of  the   fact  that  some   state  and  non-s tate  actors  will  likely  seek   to  exploit   defects  or  limit ations   within    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 1 6   
 NATO’s  AI  technologies.   Hence,   it  mus t  strive  to  protect  the  AI  systems  from  such   interference,    manipula tion,   or  sabot age,  in  line  with   the  ‘Reliability   Principle   of  Responsible   Use’.  Adequa te   security   certific ation  requir emen ts,  such   as  specific   threat  analy sis  frame works  and  tailor ed   security   audits   for  purposes   of  ‘stress-t esting’,  also  find  men tion  in  the  strategy.  The  strategy   also  refers  to  AI’s  impact   on  critic al  infrastructur e,  capabilities   and  civil  prepar edness,   including    those   covered  by  NATO’s  seven  resilience   Baseline   Requir emen ts,  creating  potential   vulner abilities   that  could   be  exploit ed  by  certain  state  and  non-s tate  actors.  Issues   such   as   disin forma tion  and  public   distrust  of  milit ary  use  of  AI  by  state  and  non-s tate  actors  are  also   stressed.   The  strategy  envisions   further   working   with   relevant  interna tional   AI  standar ds  setting   bodies t o help f oster milit ary-civil s tandar ds coher ence with r egards to AI s tandar ds.   Between the lines    Some   of  the  key  areas  that  need   elucida tion  with   respect   to  the  aim  of  the  strategy  include    firstly,  the  position   of  NATO  with   respect   to  the  use  of  Lethal  Autonomous   Weapon   Systems   (LAWS)  in  a  ‘responsible   manner ’.  In  fact,  the  strategy  does   not  even  men tion  anything   about    LAWS.  Secondly ,  the  aspect   of  ‘interoper ability ’  needs   further   clarity   with   regard  to  its  scope.    Thirdly,  elabor ation  on  how  security   policy   consider ations   come   under   the  ambit   of   ‘oper ationalisa tion  of  Principles   of  Responsible   Use’.  Fourthly ,  whe ther   a  NATO  member   state   will  fall  within   the  meaning   of  a  ‘state  actor’  if  it  is  involved  in  the  malicious   use  of  AI  needs   to   be  clarified?   For  instance,   wha t  happens   in  a  scenario   like  Turkey’s  use  of  AI-controlled   drones    (read LA WS) in the Lib yan skies in the r ecen t pas t?   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 1 7   
 Go Wide: Article Summaries  (summariz ed b y Abhishek   Gup ta)   Judg e Thr ows Out 2 An titrus t Cases Ag ainst Facebook    [Original article b y  NYTimes   ]   Wha t  happened   :  In  a  monumen tal  setback   to  efforts  to  rein  in  BigTech,  a  judg e  dismissed    antitrus t  cases   against  Facebook   because   more  evidence   was  needed   and  that  the  regula tors   filed   their   lawsuits   late,  given  that  the  acquisitions   that  they  refer  to  happened   6  and  8  years   ago  (Wha tsapp   and  Instagram  respectiv ely).   While   the  regula tors  have  30  days  to  file  again,  they   face  a  stiff  challeng e  as  the  courts   have  narrowed  their   interpr etation  of  antitrus t  law  over  the   last  few  years.  The  courts   also  took  the  position   that  if  a  monopoly   emer ged  from  Facebook’ s   acquisitions, then the y should ha ve act ed years ago rather than no w.   Why  it  matters  :  As  principles   for  technology   use  promulg ate,  this  is  a  reminder   that  wha t  is   enshrined   in  regula tions   and  law  is  ultima tely  wha t  holds   a  signific ant  amoun t  of  sway  on   whe ther   we  can  gener ate  the  socially   friendly   outcomes   that  we  desir e.  The  call  from  sena tors   and  lawmak ers  arguing   for  broadening   the  scope  of  antitrus t  regula tions   is  a  step  in  the  right   direction,   especially   as  they  are  applied   to  Interne t  companies   who   migh t  not  have  the  same    hallmark s  of  traditional   monopolies,   for  example,   pricing   wher e  a  lot  of  these   services   are   offered for fr ee to the user s.   Between  the  lines   :  The  call  for  breaking   out  Instagram  and  Wha tsapp   from  Facebook   addr esses    only   a  tiny  part  of  the  more  signific ant  problem.   Such   monopolies   are  bound   to  arise   again  and   again  due  to  the  network  effects  and  structur e  of  social   media   networks  today.  This  will  help   to   stem  the  tide  with   the  current  crop  of  companies   but  it  does   little  to  chang e  wha t  will  inevitably   happen   again  in  the  futur e.  A  more  systema tic  overhaul   of  the  regula tory  ecosystem  is  perhap s   wha t is needed.    To regula te AI, tr y pla ying in a sandbo x   [Original article b y  Morning Br ew  ]   Wha t  happened   :  Sandbo xes  have  been   proposed   as  a  part  of  EU  regula tions   on  getting  AI   systems  to  comply   with   requir emen ts  from  the  GDPR   among   other s.  The  article   details  some   of   the  engagemen ts  with   companies   that  Norway  has  embark ed  on  to  figur e  out  these   challeng es.   In  particular ,  complying   with   requir emen ts  like  privacy-by-design   and  reporting   on  that   compliance   in  an  under standable   manner   requir es  cooper ation  between  legal  and  technology    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 1 8   
 stakeholder s.  The  initia tive  in  Norway  is  helping   to  facilit ate  that.  Similar   efforts  are  also   under way  in  the  US  with   Broussar d  from  NYU   and  O’Neill   from  ORCAA  attemp ting  to  create   sandbo xes tha t can help unearth c oncr ete practices tha t will help addr ess r egula tory needs.    Why  it  matters  :  This  movemen t  towards  trying  to  figur e  out  tangible   solutions   through    trial-and-err or  is  a  welcome   chang e  compar ed  to  the  incessan t  merr y-go-round   that  we  have   today  talking   about   problems   and  regula tions   in  the  abstract  with   untested  solutions   being   put   forward without empiric al evidence t o back up ho w the y migh t work and in which c ontexts.   Between  the  lines   :  It  will  be  interesting  to  see  the  lessons   learned   both   from  the  Norway   experimen t  and  some   of  the  ones   being   run  in  the  US.  The  regula tory  ecosystem  is  vastly   different  between  the  two  regions   and  I  foresee   that  the  appr oaches   that  emer ge  from  sandbo x   efforts  in  both   places   will  be  quite  different.  But,  there  should   be  some   common   threads   from   both   experimen ts  that  will  help   practitioner s  put  these   regula tory  requir emen ts  into  practice    rather than jus t running in cir cles tr ying t o mak e their s ystems c omplian t.   Wha t Is Congr ess’s Plan t o Crack Do wn on Big T ech?    [Original article b y  The Mark up  ]   Wha t  happened   :  Six  bills  are  being   introduced   in  the  US  Congr ess  that  come   from  a  16-mon th   House   Judiciar y  Commit tee  investigation  into  the  antitrus t  beha viour   of  tech  giants.  Two  of  the   proposed   bills  have  very  little  controversy  and  are  expect ed  to  pass   without   much   furore:  one   that  increases   merger  filing   fees  and  another   that  limits   the  moving  around   of  antitrus t  cases    from  one  state  to  another ,  some thing   that  has  been   misused   in  the  past  by  tech  companies   to   obtain  more  favorable   jurisdictions   and  judiciaries   along   with   a  dela y  in  the  process   and   increase   in  cost  for  the  case.  The  other   4  bills  are  expect ed  to  raise  quite  a  bit  of  fuss  since   they   target  antitrus t  and  anti-compe titive  beha viour   of  tech  giants  including   things   like  more   string ent  limits   on  the  favoritism   of  companies   to  feature  their   own  products   and  services   on   their   platforms,   limits   on  using   insigh ts  from  compe titor  beha viour   on  their   platform  to  develop    and  promot e  their   own  offerings,   limits   on  mergers  and  acquisitions   that  reduce   mark et   compe tition   and  a  push   for  increasing   interoper ability   and  data  port ability   between  different   services in the mark et, thus incr easing c onsumer choice.    Why  it  matters  :  Each  of  these   bills  presen t  solid   cases   for  wha t  can  be  achie ved  through   the   legisla tive  process   in  reining   in  Big  Tech  and  making   sure  that  consumer   welfare  is  kept  top  of   mind   in  a  world  wher e  monopolies   abound   and  unethical  beha viour   is  hard  to  control,   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 1 9   
 especially   when   there  are  strong  network  effects  and  platform  lock-in  for  almos t  all  of  the   products and ser vices tha t we use.    Between  the  lines   :  While   there  is  bipartisan   support   for  the  bills,   wha t  will  be  interesting  to  see   is  how  success ful  these   bills  end  up  being   in  creating  an  ecosystem  wher e  ethical  and   compe titive  practices   become   the  norm   rather   than   the  exception.   Adequa te  enforcemen t   mechanisms also need t o be en visioned if these ar e going t o bec ome a success.    Analy zing the Leg al Implic ations of GitHub Copilot    [Original article b y  FOSSA  ]   Wha t  happened   :  With   the  recen t  release   of  the  AI-po wered  pair  programming   /   code-c omple tion  tool  from  GitHub,   man y  have  raised   concerns   about   whe ther   the  outputs   from   the  system  have  copyrigh t  infring emen ts.  This  article   provides   some   nuance   stating  that  despit e   the  licensing   on  specific   reposit ories,   under   the  terms   of  service  of  hosting  code  on  GitHub,    there  wouldn’t   be  a  strict  viola tion  per  se.  Furthermor e,  given  the  leng th  of  code  outputs   from   the  system,  for  smaller   snippe ts,  there  migh t  not  be  copyrigh table   material,   as  the  interviewee   describes   them   as  Lego  block s  that  are  common   everywher e  in  the  programming   ecosystem.   Finally ,  from  a  legal  standpoin t,  there  are  argumen ts  to  be  made   similar   to  how  Google   Book s   used   copyrigh ted  book   material   as  a  part  of  its  service  allowing   people   to  sear ch  book s.  This   was  accep table   because   it  was  a  “transforma tive”  use  and  created  new  value   that  was  different   from the original t ext in the book s themselv es.   Why  it  matters  :  As  tools  like  this  become   more  common   and  more  powerful,   especially   being    able   to  produce   long er  segmen ts  of  working   and  coher ent  code,   the  legal  implic ations   of  such    code  gener ation  will  become   more  relevant.  Preceden ts  like  Google’ s  use  of  copyrigh ted  book    material   serves  as  a  very  loose   analogue   and  we’ll  need   more  scholar ship  and  legal  preceden ts   before we are able t o be tter under stand the implic ations of t ools lik e Copilot.    Between  the  lines   :  A  lot  of  the  incensed   discussions   on  Twitter  and  elsewher e  have  focused   on   the  surface-le vel  argumen t  that  they  feel  a  lot  of  the  long er  code  snippe ts  are  just   reproductions   of  code  snippe ts  from  the  training   corpus.   The  paper   published   by  OpenAI    explains   that  the  probability   of  that  happening   is  appr oxima tely  0.1%.   Mor e  so,  a  lot  of  the   long er  code  snippe ts  that  have  been   gener ated  are  wha t  is  called   boilerpla te  code  (code  that   isn’t   a  direct  copy-paste  but  it  requir es  little  cognitiv e  effort)  meaning   that  there  is  a  diminished    risk  of  copyrigh t  infring emen t  since   such   boilerpla te  code  is  made   freely  available   on  tutorial    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 2 0   
 pages  for  pack ages  and  libraries.   We  need   a  lot  more  nuance   in  the  discussion   before  we  are   able t o say anything de finitiv ely about the leg al and other implic ations of t ools lik e Copilot.    Should Do xing Be Illeg al?   [Original article b y  The Mark up  ]   Wha t  happened   :  Doc-dr opping ,  short ened   to  doxxing ,  is  the  process   of  releasing   private   informa tion  about   an  individual   such   as  their   addr ess,  phone   number ,  iden tifier s,  and  other    informa tion  with   the  intent  of  targeting  the  interne t  mob   to  harass  the  individual   through    threats  and  unwanted  contact.  The  article   documen ts  the  case  of  an  individual   in  Mon tana  who    suffered  tremendously   at  the  hands   of  such   an  attack  and  success fully   sued   the  individual   that   instigated this a ttack, though she is y et to receiv e the c ourt -awarded c ompensa tion.    Why  it  matters  :  As  outlined   in  the  article,   the  law  implemen ted  in  several  states  in  the  US   mak es  doxxing   a  civil  offense   in  some,   criminal   in  other s.  In  some   cases,   the  law  is  also  geared   towards  protecting   specific   kinds   of  people   from  doxxing   attacks  like  reproductiv e  healthc are   workers,  police   officer s,  etc.  Each  of  the  appr oaches   come   with   their   own  pros  and  cons,  in  the   case  of  civil  offenses,   the  burden  of  proof  remains   lighter  making   it  perhap s  easier   to  obtain   justice but criminal of fenses c arry a higher punitiv e bur den of fering a s tronger de terrent.   Between  the  lines   :  In  the  case  of  the  person  men tioned   in  the  article,   she  belie ves  that  such    laws  would   have  stemmed   the  hateful  outpour   against  her  by  making   it  clear   that  perpe trators   cannot   hide   behind   a  screen  and  keyboar d.  These   virtual   attacks  have  very  real  consequences    for  the  victims   and  stronger  legisla tion  that  offers  protections   against  such   beha vior  to  all   citizens has the pot ential t o mak e our in teractions in the virtual w orld much sa fer.   We  need   concr ete  protections   from  artificial   intellig ence   threatening    human righ ts   [Original article b y  The Con versation  ]   Wha t  happened   :  The  article   mak es  a  succinct   case  for  how  human   rights  based   appr oaches    migh t  be  better  in  getting  more  robus t  adop tion  of  responsible   AI  rather   than   relying   on  ethics    principles   alone.   First,  it  argues   that  since   ethics   are  grounded   in  values,   there  is  an  indir ect   path  to  their   enforcemen t.  Second,   since   ethics   depend   on  values   and  values   can  differ   signific antly,  the  enforcemen t  becomes   even  harder  as  there  is  a  lack  of  consensus   and  unified    frame work.  Finally ,  given  that  human   rights  have  preceden ts  established   in  law  already   and   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 2 1   
 there  is  some   degr ee  of  univ ersal  agreemen t  on  them,   designing   more  just  AI  systems  migh t   bene fit mor e from f ollowing this pa th ins tead.    Why  it  matters  :  The  friction   between  wanting  responsible   AI  systems  and  actually   having  them    in  practice   has  been   an  ongoing   theme.   Wha t  we  lack  currently  is  the  absence   of  concr ete   enough   regula tion  that  can  enforce  all  these   ideas   of  privacy-by-design,   ethics-b y-design,   and   man y other X -by-design fr amings tha t are presen t in the domain of AI e thics.    Between  the  lines   :  A  human-righ ts  based   appr oach   has  been   proposed   man y  times   before  but   wher e  it  runs   into  trouble   as  well  is  lacking   the  connection   with   more  concr ete  practices   which    can  transla te  those   ideas   into  better  designed   technologies.   Wha tever  appr oach   we  choose   to   take,  we  need   to  mak e  sure  that  practitioner s  are  consult ed  and  made   an  integral  part  of  the   process, other wise the solutions pr oposed will f all fla t when it c omes time t o implemen t them.    Americ ans Need a Bill of Righ ts for an AI-P owered W orld   [Original article b y  Wired  ]   Wha t  happened   :  From  the  Office  of  Science   and  Technology   Policy   in  the  US,  this  article   mak es   a  strong  case  for  including   in  the  Bill  of  Righ ts  consider ations   for  how  technology ,  especially   AI,   impacts   the  ability   of  people   to  enjo y  their   freedoms   and  exercise  their   rights.  They  mak e  the   case  that  codifying  that  technology   respects   fundamen tal  democr atic  values   will  help   us  adher e   to  the  rights  and  freedoms   that  people   are  entitled   to  without   leaving  it  up  to  mark et  forces  and   private  interests  doing   so  out  of  goodwill.   Ther e  are  preceden ts  when   the  Bill  of  Righ ts  has  been    reinterpr eted,  reaffirmed,   and  expanded   to  keep  up  with   the  times   as  chang es  happened   in   socie ty po wered b y technology and other wise.    Why  it  matters  :  Wha t  is  different  with   the  current  wave  of  technology ,  in  particular   AI,  is  the   scale  and  pace   of  its  impact.   Hithert o  it  took  a  while   before  technology   moved  from  labs  to   products,   but  that  timeline   has  now  been   short ened   down  to  a  few  mon ths  with   integrated   resear ch  labs  within   indus try  firms.   The  interne t  and  smartphones   with   ample   comput e  and   storage  become   ready   vectors  for  the  dissemina tion  of  these   technologic al  advances;   far  more   rapidly   than   ever  before,  not  allowing   us  a  chance   to  grasp  their   impact   before  they  embed    themselv es in to all f acets of our liv es.   Between  the  lines   :  It  is  great  to  see  the  leader s  of  governmen t  institutions   at  the  highes t  levels   taking   a  deep   interest  in  how  technology   is  shaping   our  socie ty  and  seeking   to  mak e  some    fundamen tal  chang es  to  the  oper ating  system  of  our  democr acies   so  that  we  take  a  more  activ e   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 2 2   
 role  in  addr essing   the  impacts   that  this  technology   has  on  us  rather   than   succumbing   to  fatalism    and tr eating the mar ch of AI in to all parts of our liv es as an ine vitability .   Applying arms-c ontrol frame works to aut onomous w eapons    [Original article b y  Brookings   ]   Wha t  happened   :  With   the  recen t  utiliz ation  of  an  autonomous   weapon   by  Israel  to  assassina te   the  top  nuclear   scien tist  in  Iran  and  last  year  the  use  of  an  automa ted  drone  by  Turkey  to  target   member s  of  the  Libyan  National   Arm y,  discussions   on  autonomous   weapons   and  their    limit ations   and  capabilities   are  gaining   steam.   The  article   mak es  the  case  that  leaning   on   existing  arms   controls  treaties  as  models   can  help   regula te  this  field.   Namely ,  it  poin ts  out  how   the  Ottawa  Convention  on  Anti-per sonnel   landmines   provided   a  good  starting   poin t  to  bring    together   actors  in  the  space   for  more  fruitful   discussions   later.  Building   consensus   and  gaining    momen tum  through   targeted  treaties  that  can  help   separ ate  the  concerns   that  milit aries   have  in   giving   up  on  weapons   vs.  those   that  we  want  to  absolut ely  stop  the  proliferation  of  will  be  a   meaningful out come fr om such an appr oach.    Why  it  matters  :  Autonom y  in  weapons   systems  can  be  some thing   as  simple   as  a  sensor   that  is   able   to  detect  chang es  in  the  environmen t,  some   computing   capability   to  act  on  chang es   signaled   by  the  sensor ,  and  then   dispensing   the  payload   of  the  weapon   based   on  that   comput ation.   This  spans   the  gamut   from  simple   pressur e-trig gered  landmines   to  the  more   sophis ticated  swarm  drones   that  are  being   created  by  national   milit aries   in  their   pursuit  of   dominance   on  the  battlefield.   The  big  concern   raised   by  anyone  participa ting  in  the  domain    comes   down  to  how  much   autonom y  and  wha t  meaningful   human   control  look s  like  in  these    scenarios, and w e don’t y et have an y concr ete ans wers to these ques tions.    Between  the  lines   :  The  problem   with   such   appr oaches   to  regula tion  always  come   down  to  how   strictly   they  can  be  enforced,  and  whe ther   all  countries   who   sign  onto  this  will  uphold   the  same    high   standar ds  of  robus tness   and  verific ation  that  are  requir ed  for  safe  oper ations.   Ther e  are   calls  to  comple tely  ban  such   weapons   but  resistance   emer ges  from  some   countries   who   claim    that  while   they  migh t  halt  such   work,  there  are  those   who   won’t  and  their   lackadaisic al   appr oach   migh t  cause   more  net  harm.   And  this  ultima tely  fuels   the  arms   race  wher e  each    pushes   to  develop   the  technology   defensiv ely  but  in  the  process   further s  the  state-of-the-art.    Hope fully,  those   efforts  while   still  being   pursued   are  aimed   towards  making   these   systems  safer   rather than mor e lethal and une thical.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 2 3   
 Europe  wants  to  champion   human   rights.  So  why  doesn’t   it  police   biased    AI in r ecruiting?    [Original article b y  Sifted  ]   Wha t  happened   :  Making   the  case  for  how  Europe  is  in  dire  need   of  inno vation  and  growth,   some thing   that  diversity  in  hiring   can  enable,   the  author   mak es  the  case  that  at  all  levels  of  the   regula tions   and  legisla tions,   the  impact   of  biased   hiring   algorithms   is  being   ignor ed,  leaving  job   seek ers  at  the  mercy  of  systems  that  are  highly   problema tic.  For  those   unfamiliar   with   hiring    practices   in  Europe,   the  CVs  typic ally  include   pictur es  along   with   the  name   which   can  lead   to   implicit bias on r ace. The Digit al Ser vices Act in Eur ope is curr ently ill-equipped t o handle this.    Why  it  matters  :  This  goes  against  laws  in  several  countries   in  Europe  that  prohibit   the  use  of   race  in  hiring   decisions.   Given  that  a  lot  of  companies   use  automa ted  systems  to  process    incoming   applic ations   and  fast-track  the  process   that  is  time-   and  resour ce-in tensiv e,  illegality    migh t  be  getting  buried   behind   an  opaque   wall  of  black -box  systems  wher e  it  is  difficult   to  poin t   out wha t factors have been used t o mak e a hiring decision.    Between  the  lines   :  Man y  examples   have  already   demons trated  that  hiring   decisions   made   on   the  basis   of  algorithmic   filtering   tend  to  reproduce   strong  biases,   especially   along   gender   and   race  lines.   This  happens   even  when   data  related  to  these   protected  attribut es  is  not  collect ed   and  this  manif ests  itself   in  the  form  of  proxy  variables   that  capture  correlations   between  the   protected  attribut es  and  non-pr otected  attribut es,  negating  the  effectiv eness   of  no  data   collection   related  to  the  protected  attribut es.  Without   stronger  manda tes  in  the  form  of  law,   firms   may  continue   to  exercise  such   biased   systems  severely  impacting   the  livelihoods   of  people    who bec ome the subjects of alg orithmic discrimina tion.    Thousands   of  Geof ence   Warrants  Appear   to  Be  Missing   from  a  Calif ornia    DOJ Transpar ency Da tabase    [Original article b y  The Mark up  ]   Wha t  happened   :  Investigation  by  the  public ation  The  Mark up  found   discr epancies   in  the   number   of  geofence   warrants  that  were  report ed  in  the  public   database   from  the  Calif ornia   DOJ   and  the  number   of  reques ts  that  Google   receiv ed  for  geofences.   The  article   reports   that  such    discr epancies   migh t  arise   because   of  reques ts  being   revised   during   the  warrant  granting   process,   the  lack  of  standar ds  in  filing ,  lack  of  entering   this  informa tion  in  the  database,   sealed    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 2 4   
 warrants  wher e  the  informa tion  is  not  filed   in  the  public   database,   and  how  the  informa tion  is   captured and r eport ed when the r eques ts ar e made b y agencies tha t are outside of the s tate.   Why  it  matters  :  But,  all  of  these   pose   signific ant  challeng es  to  those   who   migh t  want  to   challeng e  unla wful  warrants,  especially   civil  socie ty  agencies   that  keep  track  of  these   from   public   databases.   These   challeng es  also  defeat  the  efficacy  of  transpar ency   requir emen ts  and   laws  like  Calif ornia   Electr onic   Communic ations   Privacy  Act.  Geof ences   by  their   very  nature  don’t    have  a  specific   target  individual,   and  hence   can  be  quite  invasive,  especially   scooping   up  data   about   a  bunch   of  unrelated  individuals   who   happen   to  be  in  the  area  that  the  geofence   targets.   This s tands in c ontrast to wir etaps wher e the w arrants ar e highly t argeted.   Between  the  lines   :  This  is  a  great  demons tration  of  how  even  when   we  have  laws  and   transpar ency   requir emen ts,  the  way  they  are  enforced  and  the  reporting   standar ds  can  mak e  or   break  whe ther   we  actually   get  the  results   that  they  set  out  to  achie ve.  Standar dization  in   reporting   and  more  string ent  requir emen ts  placed   on  the  agencies   seeking   these   geofence    warrants can help alle viate some of the challeng es iden tified in the article.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 2 5   
 8. Trends    Introduction  by  Abhishek   Gup ta,  Founder   and  Principal   Resear cher ,  Mon treal  AI  Ethics    Institut e   Great  to  see  that  you’ve  chosen   to  open   up  this  chap ter!  It  definitely  is  one  of  my  favorites  in   this  report   because   it  is  indic ative  of  things   that  you  can  expect   to  see  in  the  field   based   on   current  resear ch.  Before  going   ahead,   I  would   caveat  this  by  saying  that  in  an  acceler ating  field    like  AI,  a  crystal  ball  is  nothing   but  an  illusion.   Chang e  comes   fast  and  it  comes   big.  But,  from   observing  the  field   since   the  public ation  of  the  last  report   ,  I  can  say  that  the  ideas   men tioned   in   this  chap ter  and  the  sub-domains   within   which   they  are  presen t  will  come   to  play  some wha t  of   a  signific ant  role  in  2022.   If  you’re  reading   in  the  futur e,  you  can  send   us  an  email   and  let  us   know ho w we did!    In  the  first  piece,   “  Machines   as  teamma tes:  A  resear ch  agenda   on  AI  in  team   collabor ation  ”,   we  get  an  opportunity   to  explor e  wha t  it  would   mean   for  us  to  oper ate  within   human-machine    ensembles.   My  take  has  always  been   that  AI  is  better  as  a  tool  for  augmen tation  rather   than    replacemen t  of  human   beings.   It  is  complemen tary  in  man y  ways  helping   us  bridg e  the  gaps  in   our  cognitiv e  capabilities   to  ultima tely  achie ve  better  results.   But,  this  also  means   that  we  need    to  think   about   how  we  migh t  need   to  adap t  to  a  workspace   wher e  we  have  machines   as   constant  collabor ators.  Building   on  this  idea,   in  a  world  that  will  increasingly   have  AI  systems  all   around   us  playing  various   roles,   wha t  skills   will  be  mos t  necessar y  in  that  space?   In  “  Digit al   transforma tion  and  the  renewal  of  social   theor y:  Unpacking   the  new  fraudulen t  myths  and   misplaced me taphor s  ”, we get a peek in to some w ays  that we can cope with this.    In  “  AI  Ethics:   Enter  the  Dragon!  ”,  we  look   at  how  China   has  taken  on  a  more  serious   interest  in   integrating  ethics   into  the  entire  lifecycle  of  AI.  In  my  view,  this  will  certainly   be  interesting  as   we  get  to  explor e  AI  ethics   from  the  lens  of  a  different  social   system  compar ed  to  the  typic al   Western-cen tric  formula tions   of  AI  ethics   principles.   The  next  piece   on  “  Balancing   Data  Utility    and  Confiden tiality   in  the  2020   US  Census   ”  covers  a  developmen t  that  I  had  been   watching    with   bated  breath  because   it  was  one  of  the  first  mass   implemen tations   of  the  idea   of   differential  privacy  in  practice.   The  piece   details  how  the  US  Census   Bureau  rolled   this  out  and   wha t  it  could   have  done   better  to  build   public   trust  using   the  privacy-preser ving  concep t  of   differential  privacy.  In  an  era  wher e  even  small   actors  can  get  their   hands   on  massiv e   comput ational   power,  it  is  not  surprising   that  we  have  state-level  data  collection   authorities    trying out ne wer mechanisms t o uphold their manda tes of pr otecting critic al citiz en da ta.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 2 6   
 The  rest  of  the  chap ter  covers  a  plethor a  of  interesting  notions,   notable   amongs t  them   how   even  experts   are  too  quick   to  rely  on  outputs   from  AI  systems,   an  explor ation  of  deep fakes  from   an  ethics   standpoin t  and  how  in  the  futur e  we  migh t  have  more  and  more  people   “brough t  back    from  the  dead”   by  creating  deep fake  likeness   of  their   voice  and  visuals.   The  rise  of  founda tion   models   and  pre-trained   models   will  pose   even  greater  ethical  challeng es  for  the  field   as  the   provenance   of  the  models   and  their   data  remains   inscrut able   at  times   to  those   using   them    downstream.   It  will  also  widen   the  chasm   between  those   who   have  resour ces  to  create  such    systems  and  those   who   don’t.   Finally ,  a  quick   peek   into  the  impacts   that  AI  has  on  labor   and   how  it  will  continue   to  impact   labor ,  especially   through   “manag emen t  by  algorithm” ,  we  see   how  Alibaba   tracks  and  domina tes  their   deliv ery  agents  through   the  use  of  algorithmic    monit oring   and  manag emen t.  The  final   piece   in  this  chap ter  pain ts  a  more  optimis tic  note   explaining   how  workers  migh t  still  play  a  key  role  in  warehouses   and  robots   can  only   provide    partial solutions (a t leas t the w ay the y oper ate curr ently).   I  hope   that  you  enjo y  the  eclectic   mix  of  topics   in  this  chap ter  and  if  you  have  other   trends   that   you  see  coming   down  the  pipe   in  2022,   please   feel  free  to  reach   out  to  me  at   abhishek@mon treale thics.ai   and le t’s cha t about them!    Abhishek Gupta (  @atg_abhishek  )   Founder, Director, & Principal Researcher   Montreal AI Ethics Institute   Abhishek  Gupta  is  the  Founder,  Director,  and  Principal  Researcher  at  the   Montreal  AI  Ethics  Institute.  He  is  a  Machine  Learning  Engineer  at  Microsoft,   where  he  serves  on  the  CSE  Responsible  AI  Board.  He  also  serves  as  the  Chair   of the Standards Working Group at the Green Software Foundation.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 2 7   
 Go Deep: Research Summaries   Machines as t eamma tes: A r esear ch ag enda on AI in t eam c ollabor ation   [  Original   paper   by  Isabella   Seeber a  ,  Eva  Bittnerb   ,  Robert   O.  Briggsc  ,  Triparna   de  Vreeded   ,   Gert -Jan  de  Vreeded,   *,  Aaron  Elkinsc   ,  Ronald   Maier a  ,  Alexander   B.  Merz a  ,  Sarah   Oeste-Reiße , Nils Randrup f , Gerhar d Sch wabeg , Ma tthias Söllner]    [Resear ch Summar y by Connor W right]   Overview  :  The  import ance   of  collabor ation  in  the  AI  space   is  not  only   between  humans   but  also   between  humanity   and  AI.  Imagining   working   with   an  AI  teamma te  may  no  long er  be  imaginar y   in  the  futur e,  and  under standing   how  this  will  affect  collabor ation  will  be  essen tial.  For,   under standing this will highligh t the import ance of the human c og to the human-AI machine.    Introduction    Have  you  ever  imagined   consulting   an  AI  co-worker?  Wha t  would   you  like  them   to  be  like?  The   implic ations   of  AI  as  a  teamma te  are  consider ed  within   this  piece,   stretching   from  how  they   look   to  how  they  could   upset  human   team   dynamics.   While   we  mus t  consider   the  bene fits  of   this  collabor ation,   the  human   elemen t  to  the  process   mus t  remain,   especially   in  terms   of   human de velopmen t itself .   A diff erent kind of t eamma te   Whe ther   the  AI  is  in  a  physical  robot   or  an  algorithm,   it  cannot   be  compar ed  to  a  regular   human    teamma te.  One  key  difference   is  its  ability   to  assess   millions   of  different  alterna tives  and   situa tions   at  a  time,   proving  impossible   for  humans.   While   useful,  the  form  in  which   the   communic ation  of  this  assessmen t  arriv es  would   need   to  be  determined.   It  could   be  in  speech    or  text,  with   or  without   facial   expressions   for  visual   feedback.   Ques tions   like  these   lead   us  to   ques tion wha t we prefer in an AI t eamma te over a human.    Wha t do w e want in an AI t eam member?    The  paper   holds   the  classic   Alan   Turing   definition   that  “AI  refers  to  the  capability   of  a  machine    or  comput er  to  imitate  intelligent  human   beha vior  or  though t”.  In  this  sense,   should   our   thinking   about   AI  collabor ators  be  centered  in  human   terms?   Like  with   chatbots,   similar    consider ations   are  brough t  into  play,  such   as  whe ther   the  AI  should   have  a  gender ,  can  it   differentiate  between  serious   and  social   chatter  etc.  Our  decisions   on  these   ques tions   will  then    certainly a ffect ho w the t eam dynamic pla ys out.    The e ffect on c ollabor ation   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 2 8   
 In  this  regard,  it’s  essen tial  to  differentiate  between  AI  as  a  teamma te  and  AI  as  an  assis tant.   Collabor ation  with   AI  as  a  tool  is  not  as  though t-provoking   as  holding   it  in  the  ‘higher ’  regard  as   a counterpart.    In  this  way,  collabor ating  with   such   an  entity  could   enhance   or  negatively  impact   the  team    dynamic.   The  AI  could   become   a  leader   in  the  group  on  specific   issues   it  can  handle   best,  yet   depending   too  much   on  the  machine   could   lose  human   compe tencies.   Furthermor e,  the  AI   teamma te  could   prove  excellen t  at  drawing   insigh ts  from  data,  but  the  lack  of  out-the-bo x   thinking   could   reinforce  already   presen t  views.  Hence,   while   collabor ation  is  undoub tedly    affected b y introducing AI, the righ t balance s till needs t o be s truck t o mak e the mos t of it.    Consider ations when c ollabor ating   Given  the  novelty  of  this  practice   and  AI  in  gener al,  why  an  AI  would   suggest  a  particular   course   of  action   becomes   a  critic al  ques tion.   In  addition,   the  extent  to  which   we  recognise   the  AI’s   involvemen t  can  also  have  far-reaching   impacts.   Should   the  AI  become   a  leader   on  a  topic,    should   it  be  credited  with   its  work?   Much   of  this  stems  from  whe ther   AI  can  be  creative  or  not,   which c an be f ound in poe try, fashion and music.    Between the lines    While   collabor ation  with   AI  teamma tes  may  be  essen tial  practice   in  the  futur e,  I  would   be   cautious   against  throwing   such   collabor ation  into  every  problem   possible.   Sure,  using   AI’s   analy tical  capabilities   will  nearly   always  be  help ful,  but  that  pertains  more  to  AI  as  an  assis tant   rather   than   a  counterpart.   Hence,   problems   such   as  trying  to  solve  world  hung er,  I  belie ve,   would   not  bene fit  from  an  AI  as  a  teamma te  intervention,   mainly   due  to  how  AI  can  never   actually   feel  or  under stand  wha t  being   hungr y  feels  like.  Wha t’s  for  sure  is  that  while   AI   collabor ation c an reap bene fits, human in volvemen t remains par amoun t.   Digit al  transforma tion  and  the  renewal  of  social   theor y:  Unpacking   the   new fraudulen t myths and misplaced me taphor s   [  Original paper   by Marinus Osse waarde]   [Resear ch Summar y by Connor W right]   Overview  :  With   the  emer gence   of  technology ,  socie ty  has  chang ed  immeasur ably.  Ques tioning    the  status  quo  has  become   less  of  a  pressing   issue   in  favor  of  continuing   to  use  a  digit al  service.    However,  reflection   is  one  of  the  mos t  critic al  skills   in  preventing  a  digit al  futur e  guided   and   domina ted b y the f ew.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 2 9   
 Introduction    The  lives  of  man y  have  become   densely   linked  with   technology .  The  digit al  transforma tion  is   being   led  and  developed   by  certain  parties   (labeled   as  the  “googliz ation”   of  everything).   Hence,    social   theor y  mus t  adap t  to  the  dominan t  economic   and  digit al  spher es,  promot ed  and   sustained   through   different  technologic al  “myths”.  To  do  so,  ackno wledging   the  status  quo  and   advocating  the  import ance   of  ques tioning   will  prove  essen tial  in  both   under standing   and   comba ting digit al domina tion b y the f ew. Up fir st is ackno wledg emen t.   Digit al and ph ysical lif e ha ve bec ome insepar able    The  reality   we  live  in  is  becoming   more  and  more  recognised   as  inextricably  linked  with   the   digit al  space.   The  influence   technology   possesses   often  goes  unnoticed   until  it  is  briefly  taken   away.  Such   influence   is  so  strong  that  user s  readily   accep t  the  missions   of  businesses   to   continue   using   their   services   (especially   when   accep ting  the  role  that  technology   plays  in  our   lives has led t o the sub sequen t domina tion of digit alisa tion.    The domina tion of digit alisa tion   The  informa tion  available   through   tech  has  ended   up  in  its  economisa tion.   Less   thinking   and   more  simply   accep ting  is  the  easies t  way  to  drive  profit,  reducing   the  value   of  men tal  activities    such   as  reflection.   Wher eas  previously ,  socie ty  has  been   driven  by  the  politic al  and  religious    spher es  of  life,  the  economic   spher e  has  overtaken  them   thank s  to  digit al  transforma tion.    Through   this,   the  few  driving   the  transforma tion  can  dictate  the  game’ s  rules   as  to  wha t  this   digit al  transforma tion  will  look   like.  The  chang es  under gone  do  not  bode   well  for  the  academic    spher e.   The c onsequences f or in tellectual pr actices    With   digit alisa tion  as  the  driver  behind   the  dominan t  economic   spher e,  academic   work   becomes   valued   when   it  adop ts  the  norms   and  languag e  of  the  prevailing   economic   spher e.   Such   dominance   then   mak es  it  difficult   to  imagine   alterna tive  scenarios   to  the  reality   in  which    we  find  ourselves.  Instead  of  being   encouraged  to  think,   the  mind   is  being   used   as  an   instrumen t  of  power  rather   than   being   critic al.  The  ability   of  art  and  science   to  influence   the   mind   gets  weaker  as  this  conditioning   goes  on  while   increasing   the  passiv e  accep tance   of  the   status quo. It is thr ough this lack of ques tioning tha t “fake” m yths c an be de veloped.    “Genuine” v s “fake” m yths   In  contrast  to  a  “fake”  myth,  a  “genuine”   myth  leads   to  enligh tenmen t  through   insigh ts  and  a   deeper   under standing   of  the  current  state  of  affairs.  The  author s  use  Homer ’s  myths  in  the   Odyssey  as  some   examples.   On  the  other   hand,   a  “fake”  myth  doesn’t   lead   to  enligh tenmen t   and  is  instead  manuf actur ed  to  reinforce  the  status  quo.   The  fabric ation  is  designed   to  blindf old   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 3 0   
 the  public   so  that  they  adher e  to  the  status  quo  without   ques tion.   One  such   example   presen ted   by the author s is Silic on V alley being lauded as the digit al revolution her oes.   The digit al m yth   A  reason   Silicon  Valley  is  seen   in  this  way  owes  to  how  digit alisa tion  is  seen   as  bringing   new   enligh tenmen t  to  the  fore,  making   everyone  more  ready   to  accep t  wha tever  form  it  comes   in.   Promises   of  a  new  technologic al  reality   are  made   to  condition   how  the  public   sees   technology    casting  aside   other   potential  realities   to  preser ve  the  status  quo.   How  this  is  done   can  be  seen    in the use of me taphor s.   The use of me taphor s   The  digit al  reality   desir ed  by  those   at  the  helm   of  the  technologic al  reality   is  argued   to  utiliz e   languag e  to  main tain  the  current  state  of  affairs.  Metaphor s  such   as  “data  mining ”  and  “the    cloud”   are  emplo yed  but  are  inappr opria te  as  they  mak e  data  sound   like  a  natural  resour ce.   Even  metaphor s  such   as  “digit al  community ”  distracts  from  how  communities   are  built   on   face-t o-face  interactions.   Hence,   again,  the  deep   interaction   between  digit al  and  physical  reality    comes thr ough, sub tly adjus ting ho w we express our selves and vie w technology itself .   Between the lines    I  find  how  our  use  of  languag e  is  also  influenced   by  the  technology   we  use  very  intriguing    indeed.   Similar   occurr ences   can  be  seen   in  the  anthropomorphic   languag e  used   to  describe   AI   at  times,   especially   with   self-driving   cars  being   described   as  ‘making   decisions’ .  I  also  see  how   big  corpor ations   spin  their   own  take  on  reality   often  unde tected.  As  a  result,   the  import ance   of   building   up  civic   compe tence   shines   even  brigh ter.  Should   we  choose   to  stop  asking   ques tions,    those   who   dictate  the  space   will  stop  giving   answers  and  there  remain   man y  unans wered   ques tions y et.   AI Ethics: En ter the Dr agon!   [  Original documen t  by Minis try of Science and T echnology   of the P eople’ s Republic of China]    [Resear ch Summar y by Angshuman K aushik]    Overview  :  On  September   25,  2021,   the  National   New  Gener ation  Artificial   Intelligence    Governance   Professional   Commit tee  issued   the  “New  Gener ation  of  Artificial   Intelligence   Code    of  Ethics”   (hereina fter  “the   Code”).   According   to  the  Code,   its  aim  is  to  “integrate  ethics   into  the   entire life cycle of artificial in telligence, and t o eng age in artificial in telligence r elated activities” .   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 3 1   
 Introduction    It’s  quite  mystifying  to  see  a  country  as  infamous   as  China   globally   for  its  AI  ethics   viola tions,    come   up  with   an  Ethics   Code   for  the  world  to  sit  up  and  take  notice.   Its  viola tions   list  is  endless,    ranging   from  the  use  of  Uighur -tracking   facial   recognition   technology   and  the  use  of  emotion    detection   softw are  against  them   in  its  Xinjiang   province,   to  its  flouting   of  human   rights  norms    and  draconian   manner   of  applic ation  of  the  social   credit  system.  In  fact,  China’ s  reput ation  as  a   country  with   an  appalling   human   rights  track  record  has  gone  from  bad  to  worse  in  the  past   couple   of  years  or  so.  To  come   up  with   an  ethics   code  in  such   a  setting  and  at  this  poin t  in  time,    is  quite  surprising ,  to  say  the  least.  Travel  back   in  time   to  2017,   and  you  have  the  “New   Gener ation  Artificial   Intelligence   Developmen t  Plan”   which   outlines   China’ s  policy   to  become    the  leading   AI  power  by  2030.   It  is  interesting  to  men tion  here  that,  according   to  the  object   part   of  the  Code,   a  couple   of  its  objectiv es  includes,   to  thor oughly   implemen t  the  “New  Gener ation   Artificial   Intelligence   Developmen t  Plan” ,  and  detailed   implemen tation  of  the  “New  Gener ation   Artificial   Intelligence   Governance   Principles” .  Coming   back   to  the  Code,   it  contains  25  Articles    divided in to 6 chap ters.   High-le vel overview   Belo w is a high-le vel overview of the Code:    Chap ter One (“ One” as giv en in the Code) (Gener al Pr ovisions) (Articles 1- 4)    This  chap ter  talks  about   integrating  ethics   and  morals  into  the  full  life  cycle  of  AI,  promoting    fairness   and  avoiding   problems   such   as  discrimina tion,   privacy  etc.  Wha t  is  interesting  to  note   here  is  that  the  chap ter  not  only   talks  about   incorpor ating  ethics,   but  also  morals.  Ther efore,   clarity   on  the  definition   of  morals  for  the  purpose   of  this  Code   and  how  the  integration  will  take   place   becomes   imper ative.  Further ,  the  chap ter  states  that,  apart   from  applying   to  natural  and   legal  persons,   the  Code,   also  applies   to  ‘other   related  institutions’   engaged  in  related  activities    such   as  artificial   intelligence   manag emen t,  resear ch  and  developmen t,  supply ,  and  use.  Ther e  is   ambiguity   surrounding   the  meaning   of  the  term  ‘other   related  institutions’ ,  and  without   any   elucida tion,   the  same   can  have  disas trous  consequences   to  the  entities   concerned   in  today’s   globaliz ed  world.   Article   3  is  one  of  the  mos t  import ant  provisions   of  the  Code,   as  it  lays  down   some basic e thical norms t o be f ollowed b y various AI r elated bodies, under six dis tinct heads.    They are as f ollows:   ●  Enhance   human   well-being   –  This  first  heading   lists  out  several  high-sounding   ethical   guidelines   to  be  followed.  Some   of  them   include,   follow  the  common   values   of  mankind,    respect   human   rights  and  fundamen tal  interests  of  mankind,   impr ove  people’ s  livelihood    etc.  One  interesting  norm   is  to  promot e  harmon y  and  friendship   between  man   and   machine.   Wha tever  that  means,   it  would   be  some   task  for  the  people   associa ted  with    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 3 2   
 the  field   of  AI  to  accomplish.   Another   ethical  norm   men tioned   is  “adher e  to   people-orien ted”, which is e xtremely ar duous t o compr ehend.    ●  Promot e  fairness   and  justice  –  It  talks  about   adher ence   to  inclusiv eness   and   inclusiv eness,   which   again  does   not  convey  any  meaning ,  wha tsoe ver.  The  other   ethical   norms   clubbed   under   this  broad  heading   effectiv ely  protect  the  legitima te  rights  and   interests  of  all  relevant  subjects,   promot e  social   fairness   and  justice  and  equal    opportunities.    ●  Protect  Privacy  and  safety  –  This  head   includes   inter  alia,  fully   respect   the  rights  of   personal   informa tion  to  know  and  consen t,  protect  personal   privacy  and  data  security ,   informa tion  mus t  not  infring e  on  personal   privacy  etc.  The  above  sounds   more  like   clauses   from  a  data  protection   statute.  Although,   incorpor ation  of  oblig ations    concerning   privacy  and  data  protection   seems   like  another   layer  of  fortific ation  for  the   rights-holder s,  but  how  far  it  will  stay  clear   of  not  involving   in  an  interpr etation   imbr oglio   with   the  recen tly  passed   Personal   Informa tion  Protection   Law  (PIPL)   will  be   one riv eting duel t o watch out f or.   ●  Ensur e  controllability   and  credibility   –  It  comprises   ensuring   that  humans   have  full   autonomous   decision-making   power,  the  right  to  choose   whe ther   to  accep t  the  services    provided   by  artificial   intelligence,   the  right  to  withdr aw  from  the  interaction   with    artificial   intelligence   at  any  time,   and  the  right  to  suspend   the  oper ation  of  artificial    intelligence   systems  at  any  time   to  ensur e  that  artificial   intelligence   is  always  under    human   control.  It  is  quite  obvious   that  the  above  requir emen ts  (which   entails  some    explana tion), w ould pr ove extremely bur densome f or the c ompanies t o follow.   ●  Strengthen   Responsibility   –  Insis t  that  human   beings   are  the  ultima te  responsible    subject,   clarif y  the  responsibilities   of  stakeholder s,  introspect   and  self-discipline   in  all   links  of  the  artificial   intelligence   life  cycle  etc.  are  some   of  the  ethical  norms   included    under   this  head.   Explic ations   requir ed  include   ‘ultima te  responsible   subject ’,  ‘introspect    and self -discipline’ e tc.   ●  Impr ove  ethical  literacy  –  Activ ely  learn   and  populariz e  artificial   intelligence   ethics    knowledg e,  deeply   promot e  the  practice   of  artificial   intelligence   ethical  governance   etc.,   are some of the e thical norms c ontained under this head.    Other chap ters   Without   going   into  the  compr ehensibility   and  the  interpr etability   issues,   the  other   chap ters   containing the v arious articles ar e as f ollows:   Chap ter II (Manag emen t Standar ds) (Articles 5- 9)    The  manag emen t  standar ds  are  contained   in  this  chap ter.  Some   of  them   include   “stay  true   to   reality   and  rush   for  quick   success   in  the  process   of  strategic  decision-making ,  correctly   exercise   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 3 3   
 power  and  use  power”,  etc.As  it  is  obvious,   the  above  requir emen ts  are  extremely   hard  to   compr ehend and ther efore, cannot be giv en e ffect t o, in a meaningful manner .   Chap ter III (R&D Specific ations) (Articles 10- 13)    Strengthen   the  awareness   of  self-discipline,   impr ove  data  quality ,  strengthen   security   and   transpar ency , and a void pr ejudice and discrimina tion e tc.   Chap ter IV (Supply Specific ations) (Articles 14- 17)    Respect   mark et  rules,   strengthen   quality   control,  protect  the  rights  and  interests  of  user s,  and   strengthen emer gency pr otection e tc.   Chap ter 5 (“5” as giv en in the Code) (Specific ation) (Articles 18- 22)    Promot e  good  faith  use,  avoid  misuse   and  abuse,   forbid   illegal  use  of  artificial   intelligence    products and ser vices e tc.   Chap ter VI (Or ganization and Implemen tation) (Articles 23- 25)    This  chap ter  deals   with   the  implemen tation  aspect   of  the  Code.   It  states  that  the  specific ation  is   issued   by  the  National   New  Gener ation  Artificial   Intelligence   Governance   Professional    Commit tee,  and  is  responsible   for  explaining   and  guiding   its  implemen tation.   It  further   states   that  the  manag emen t  departmen ts  at  all  levels  may  formula te  more  specific   ethical  codes   and   related  measur es  based   on  this  code  and  combined   with   actual   needs.   Article   25  talks  about    coming   into  force  of  the  specific ation  on  the  date  of  promulg ation,   and  its  revision   in  due  course   according   to  the  needs   of  economic   and  social   developmen t  and  the  developmen t  of  artificial    intelligence.    Between the lines    Prima   facie  a  proper   drafting   of  the  Code   is  conspicuous   by  its  absence.   In  fact,  it  is  very  loosely    drafted  and  seems   not  to  have  under gone  any  revision   wha tsoe ver,  before  public ation.   Further ,   it  appear s  to  have  been   passed   in  a  hurr y,  the  reper cussions   of  which   can  be  devastating.   Apart    from  the  syntactic   and  other   gramma tical  gaffes,  the  Code   brims   with   lofty   ethical  ideals,   which    are  easy  to  prescribe   but  extremely   difficult   to  implemen t  in  practice.   Nevertheless,   the  burden   now  rests  on  the  shoulder s  of  the  concerned   authorities   to  provide   more  clarity ,  not  only   on  the   interpr etation  issues   but  also  on  the  implemen tation  aspects   of  the  Code.   Only   time   will  tell  as   to  whe ther   the  Chinese   Governmen t  is  able   to  deliv er  on  the  principles   and  standar ds  enshrined    in  the  Code.   To  sum   up,  China   can  draw  inspir ation  from  Robert   Frost  and  his  immort al  lines,    “And miles t o go be fore I sleep” , as f ar as implemen ting the Code is c oncerned.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 3 4   
 Balancing Da ta Utility and Con fiden tiality in the 2020 US Census    [  Original paper   by danah bo yd]   [Resear ch Summar y by Lair d Gallaghar]    Overview  :  Due  to  advancemen ts  in  comput ational   power  and  the  increased   availability   of   commer cial  data,  the  traditional   privacy  protections   used   by  the  U.S.  Census   Bureau  are  no   long er  effectiv e  in  preventing  the  mass   reconstruction   and  reiden tification  of  confiden tial  data.   In  this  paper ,  danah   boyd  explor es  the  bureau’s  response   for  the  2020   Census:   a  new  disclosur e   avoidance   system  called   “differential  privacy,”  which   creates  a  mathema tical  trade-of f  between   data  utility   and  privacy.  But  the  opaque   manner   in  which   the  bureau  has  rolled   out  the  chang es   has  risked  undermining   trust  between  the  bureau  and  the  diverse  stakeholder s  who   use  Census    data in policymaking , resear ch, and adv ocacy.   Introduction    Even  before  COVID-19   had  taken  hold   in  the  Unit ed  States,  the  2020   Census   was  off  to  a  rocky    start,  with   a  majority   of  U.S.  adults   mistakenly   belie ving  the  form  contained   a  citizenship    ques tion.   Then,   the  pandemic   upended   the  Census   Bureau’s  normal   oper ations   and   complic ated e fforts t o ensur e an accur ate count.   However,  barrier s  to  enumer ation  are  not  the  only   challeng es  faced   by  the  Census   this  year.   Chang es  in  the  data  and  computing   landsc ape  over  the  past  decade  have  made   it  much   easier    to  reconstruct   and  reiden tify  confiden tial  informa tion  out  of  Census   data  products.   To  respond    to  those   threats,  the  Census   has  implemen ted  an  entirely  new  “disclosur e  avoidance   system”    (DAS).  The  system  works  by  introducing   noise–ma thema tical  randomness–in to  the  calcula tions    used t o gener ate da ta products.    But  wher e  and  how  much   noise   you  inject   matters.  As  danah   boyd  documen ts  in  Balancing   Data   Utility   and  Confiden tiality   in  the  2020   US  Census,   the  DAS  requir es  a  system-wide   balance   of   privacy  risk,  which   means   that  making   certain  statistical  tables   more  accur ate  in  turn   requir es   other s  to  include   more  noise.   These   trade-of fs  have  widespr ead  implic ations   for  the  utility   of   data  that  stakeholder s  in  governmen t,  academia,   and  the  nonpr ofit  and  business   sectors  have   come t o rely on.    How the Census c onstructs da ta products    Since   1790,   every  ten  years  the  U.S.  governmen t  has  conduct ed  a  census   of  all  people   living   in   the  country.  This  decennial   count  determines   the  apportionmen t  of  legisla tive  represen tation   and  the  fair  alloc ation  of  feder al  funding   and  resour ces.  But  the  process   also  gener ates  powerful    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 3 5   
 data  products   used   by  policymak ers,  social   science   resear cher s,  and  other s.  In  order  to  protect   individual   privacy,  the  Census   does   not  release   the  full  underlying   data  for  72  years.  Instead,   it   releases   aggregated  and  anon ymiz ed  data  products   that  ensur e  confiden tiality   while   still   providing v aluable demogr aphic in forma tion.    Through   self-response   and  follow-up  oper ations,   the  Census   collects   basic   data  about    households:   the  type   of  housing   unit;   its  owner ship  status;  and  the  name,   date  of  birth,   sex,   race,  and  Hispanic   origin   of  everyone  living   there.  After  resolving   addr esses,   this  data  becomes    the  “Census   Unedit ed  File”   (CUF)   which   is  used   to  calcula te  the  popula tion  of  each   state  and   thus   determine   apportionmen t  in  the  U.S.  House   of  Represen tatives.  Afterward,  the  Census    resolv es  missing   and  conflicting   demogr aphic   data,  using   statistical  models   to  fill  every  cell  with    a value and pr oduce the “Census E dited File” (CEF).    Then   come   the  measur es  to  avoid  disclosur e.  Before  this  year,  the  Census   would   swap   households   from  one  location  to  another   in  an  effort  to  scramble   whe ther   a  record  matches   its   real  location.   In  addition,   the  Census   would   simply   suppr ess  certain  informa tion  about    subpopula tions   that  would   disclose   too  much   detail.  After  recoding   and  quality   assur ance,   these    privacy-protected  tabula tions   (the  “Hundr ed-per cent  Detail  File”)   would   get  released   to  the   public as a series of da ta products. But s wapping and suppr essing is no long er enough.    Why a ne w system t o protect priv acy?    Due  to  increases   in  computing   power,  it  is  now  much   easier   for  attackers  to  rebuild   individual    records  out  of  aggregate  data.  They  do  this  by  triangula ting  across  statistical  tables   to  determine    which   individuals   likely  contain  which   attribut es,  yielding   a  reconstruct ed  list  of  individuals    matched   to  attribut es  like  race,  sex,  and  Census   block.   From  here,  an  attacker  can  then   use   external   data  sour ces,  including   widely -available   commer cial  data,  to  link  these   anon ymiz ed  yet   reconstruct ed individual r ecords and r e-iden tify individuals b y name and other char acteristics.   boyd  explains   that  while   reconstruction,   linkage,  and  reiden tification  attacks  were  once    theor etical,  they  are  no  long er.  “Using   the  published   available   statistical  tables   from  only   the   2010   decennial   census,   resear cher s  at  the  bureau  reconstruct ed  a  comple te  set  of  individual    records  that  could   effectiv ely  serve  as  a  comple te  micr odata  file  down  to  the  block   level,”  she   writes.  Due  to  swapping   and  other   measur es,  the  comple te  set  did  not  fully   match  the   unpr otected,  edited  files–but   fully   46  percent  of  individual   records  were  perfect  matches.   And   just  by  allowing   the  age  variable   to  be  +/-  one  year,  fully   71  percent  of  individual   records   matched.   From  this  reconstruct ed  data,  census   resear cher s  were  able   to  re-iden tify  (and    confirm) 17 per cent of individual r ecords–t ens of millions of U .S. residen ts.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 3 6   
 And  that  was  with   2010   data.  Since   then,   commer cial  data  availability   has  increased   nearly    exponen tially .  It  became   clear   to  census   resear cher s  that  far  more  than   17  percent  of  records   could   be  exposed   if  they  stuck   to  their   standar d  practices   of  swapping   and  suppr essing.   They’d   either ha ve to release f ar less da ta products or implemen t a ne w system of priv acy pr otection.    Balancing c onfiden tiality and accur acy   For  the  2020   census,   the  bureau  has  decided   to  implemen t  a  new  “Disclosur e  Avoidance    System” built on the principles of dif ferential priv acy. Acc ording t o bo yd:   “Differen tial  privacy   work s  to  prevent  accurat e  reconstruction   attacks  while   making   certain  that    the  data  are  still  useful  for  statistical  analyses.   It  does   this  by  injecting   a  controlled   amoun t  of   uncertainty  in  the  form  of  mathematic al  randomness,   also  called   noise,   into  the  calculations    that   are  used   to  produc e  data  products.   The  range   of  noise   can  be  shared   publicly   because   an   attacker  cannot   know  exactly   how  much   noise   was  introduc ed  into  any  particular   table.   With    differen tial  privacy   it  is  still  possible   to  reconstruct   a  database,   but  the  database   that   is   reconstruct ed  will  include   privacy-ensuring   noise.   In  other   words,   the  individual   records   become    synthetic b yproducts of the s tatistical system.”   The  problem   is  that  this  system  involves  choices   about   wher e  to  introduce   noise,   and  how  much    noise.   In  order  to  main tain  a  certain  privacy-loss   budg et,  designer s  mus t  alloc ate  noise   levels   throughout   the  data,  prioritizing   the  accur acy  of  certain  statistical  tables   over  other s.  This  is   wha t  mak es  the  privacy  differential.  But  as  a  consequence   of  how  this  top-do wn  algorithmic    appr oach   works,  it  would   create  undesir able   outcomes   like  geogr aphic   inconsis tencies,   partial    people,   and  negative  people,   without   additional   processing.   The  need   to  perform  a   post-processing   cleanup   is  primarily   politic al,  according   to  boyd.  Laws  around   redistricting    requir e  the  Census   to  prioritiz e  making   block -level  data  consis tent  and  ensur e  the  data  consis ts   only   of  non-neg ative  integers  (no  negative  or  fractional   counts  of  people).   But  this   post-processing g ener ates all sorts of s tatistical oddities en tirely unr elated to priv acy.   Communic ation br eakdown   The  Census’   announcemen t  of  a  new  disclosur e  avoidance   system  in  late  2018   caugh t  man y   data  user s  and  advocates  by  surprise.   The  lack  of  educ ation  on  how  differential  privacy  works   and  why  it  is  necessar y  left  man y  stakeholder s  confused   and  frustrated.  This  new  appr oach   to   protecting   confiden tiality   requir ed  all  data  uses   to  be  determined   in  advance   so  that  the  noise    could   be  best  alloc ated  throughout   the  statistical  tables,   but  mos t  Census   data  user s  had  never   appr oached   their   work  in  this  way.  In  addition,   user s  didn’t   always  under stand  why  a  new   appr oach   to  privacy  was  even  needed.   And  unlik e  the  comput er  scien tists  who   devised   the   disclosur e  avoidance   system,  they  often  lacked  the  skill  set  to  analy ze  and  commen t  on  it.  Data   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 3 7   
 user s  became   increasingly   worried   as  they  explor ed  how  the  injection   of  noise   could   affect  the   reliability of their o wn scien tific w ork.   Possible solutions    How  can  the  bureau  best  maximiz e  data  utility   while   minimizing   privacy  loss?   boyd   recommends se veral ways to relax the c onstraints on the da ta.   First  is  to  reduce   geogr aphic   precision.   If  the  Census   stopped   publishing   block -level  data,  more   of  the  privacy  budg et  could   be  spen t  elsewher e.  Unfortuna tely,  feder al  law  dictates  that  the   Census mus t produce r edistricting files with block -level counts.   That  fix  is  out  of  the  ques tion  without   an  unlik ely  congr essional   intervention,   so  boyd  suggests   publishing   “pre-pos t-processed   data”  so  that  user s  can  get  acclima ted  to  negative  counts,   fractional people, and mor e. Doing so w ouldn’t jeopar dize priv acy.   In  addition,   the  Census   migh t  also  look   to  reduce   the  dimensions   of  certain  variables   and   withhold publishing block -level da ta belo w a cert ain popula tion thr eshold.    Between the lines    People   like  me–r esear cher s  for  whom   analy zing  trends   in  Census   data  is  a  secondar y  aspect   of   our  work–ha ve  by  and  large  not  even  consider ed  the  effects  of  this  sea-chang e  in  the  Census    appr oach   to  privacy  protection.   We  didn’t   see  the  2018   notice,   didn’t   attend  any  mee tings,   and   didn’t   look   at  the  demons tration  data.  We  haven’t  had  the  time.   And  now,  we  migh t  not  be  able    to  use  the  Census   like  we  did  before.  Luckily ,  differential  privacy  won’t  be  applied   to  the   Americ an  Community   Survey  until  2025,   which   buys  us  some   time   to  under stand  this  new   reality .  But  the  Census   is  in  a  challenging   place.   Ther e  is  a  major   threat  to  public   trust  in  Census    data  collection   that  requir es  these   new  privacy  measur es.  If  data  collection   suffers,  the  data   products   will  suffer,  too.  But  there’s  also  a  threat  to  the  utility   of  the  data,  data  which   is   import ant  not  just  for  advancing   knowledg e  but  also  for  public   policy   advocacy  and  more.   Indeed,   boyd  is  right  in  her  premonition:   “Wha t’s  at  stake  is  not  simply   the  availability   of  the   data; it is the legitimacy of the census. ”   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 3 8   
 Go Wide: Article Summaries  (summariz ed b y Abhishek   Gup ta)   DeepMind A GI paper adds ur gency t o ethical AI    [Original article b y  VentureBea t  ]   Wha t  happened   :  Reinforcemen t  learning   (RL)  is  often  much   less  discussed   than   other   forms   of   machine   learning.   Usually ,  discussions   around   it  in  terms   of  its  achie vemen ts  in  beating  humans    in  games   like  Go.  Adop ting  a  different  appr oach   by  creating  a  reward  feedback   loop   with   the   environmen t,  RL  could   be  a  pathway  to  achie ving  artificial   gener al  intelligence   (AGI)  with   an   acceler ated  timeline.   The  recen t  work  from  DeepMind   has  made   some   resear cher s  revise  their    estima tes of when A GI migh t bec ome a r eality , if a t all.    Why  it  matters  :  The  mos t  frequen tly  discussed   ethical  aspects   in  the  context  of  RL  include   value    alignmen t,  reward  hacking ,  safe  explor ation,   and  avoiding   adverse  side  effects.   In  the  current   ecosystem  of  AI  ethics   resear ch,  these   are  severely  under -discussed   aspects,   with   mos t  of  the   focus on issues lik e fairness and priv acy.   Between  the  lines   :  Deplo yed  ML  systems  will  be  a  mixture  of  different  appr oaches,   and  keeping    an  eye  on  developmen ts  like  these   and  the  implic ations   they  will  have  on  ethics,   safety,  and   inclusion   is  an  integral  part  of  working   in  the  field.   We  need   to  broaden   the  scope  of  the   discussion   of  concerns   as  they  arise   and  relate  to  different  ML  methodologies   so  that  our   proposed appr oaches don’t ignor e essen tial f acets of deplo yed ML s ystems.    The E thics of a Deep fake An thon y Bour dain V oice    [Original article b y  The Ne w Yorker  ]   Wha t  happened   :  In  the  documen tary  titled   “Roadrunner ”  about   the  life  of  Anthon y  Bour dain,    there  were  segmen ts  of  audio   that  were  synthesiz ed  using   previous   audio   data  from  his  real   voice.   The  words  that  were  uttered  in  this  synthetic  voice  were  words  he  had  actually   written   down.  The  use  of  synthetic  media   is  rife  with   ethical  troubles,   as  it  became   eviden t  with   the   backlash   that  the  producer s  of  the  documen tary  have  faced   since   the  release   of  their   film.    Notably,  people   have  expressed   concerns   also  in  terms   of  disclosur e  that  synthetic  voice  was   used   and  the  flippance   with   which   the  people   involved  in  the  making   of  the  film  dismissed   some    of the c oncerns when the y were brough t up the fir st time.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 3 9   
 Why  it  matters  :  Synthetic  media,   notably  deep fakes,  is  notorious   for  all  the  harm   that  it  can   cause.   In  some   cases,   there  are  positiv e  uses   for  deep fakes  as  have  outlined   in  a  previous    edition   of  this  newsletter.  But,  when   consen t  and  intent  is  not  clear ,  ethical  qualms   arise,    especially   for  someone   who   passed   recen tly  and  heavily  emphasiz ed  authen ticity   as  some thing    he valued in his w ork.   Between  the  lines   :  We  will  see  a  rise  in  the  use  of  synthetic  media   over  time,   especially   as  the   technology   becomes   easier   to  use  and  the  data  available   to  train  these   systems  becomes   more   widespr ead,   as  is  the  case  with   our  increasing   digit al  footprin t.  Building   awareness   around   wha t   constitut es  appr opria te  and  inappr opria te  use  of  synthetic  media   will  mak e  us  more  informed    and  nuanced   in  our  discussions   rather   than   lionizing   or  demonizing   its  use  with   a  careful  study    of  the  underlying   ideas   of  disclosur e,  consen t,  and  context  which   are  essen tial  to  discussing   the   ethics in the fir st place.    Founda tion models risk e xacerba ting ML ’s ethical challeng es   [Original article b y  VentureBea t  ]   Wha t  happened   :  A  massiv e  report   released   recen tly  from  Stanford  AI  resear cher s  titled   “On  the   Opportunities   and  Risks  of  Founda tion  Models”   has  brough t  forth   fervent  discussion   on  the  role   that  large-scale  pretrained   and  other   models   are  going   to  play  in  AI  applic ations   downstream    that  rely  on  them   to  build   out  their   systems.   An  example   of  this  is  GPT-3  that  now  powers   hundr eds  of  apps  processing   billions   of  words  every  single   day.  Any  bias  in  it  gets  amplified    hundr eds  of  times   over  in  all  its  downstream   uses.   Such   models   also  create  risks  of   centralization  of  power  in  the  hands   of  those   who   have  the  comput e  and  data  infrastructur e  to   build such models.    Why  it  matters  :  Our  penchan t  for  larger  AI  systems  has  man y  impacts   that  exacerba te  the   problems   in  the  domain   of  Responsible   AI  including   bias  and  fairness,   privacy,  inclusion,    accountability ,  and  increasingly   an  environmen tal  impact   as  well.  Careful  analy sis  needs   to  be   performed   and  more  resear ch  funded   so  that  we  can  construct   an  in-dep th  under standing   of   the  risks  that  such   systems  pose.   The  opportunities   are  quite  clear   in  terms   of  being   able   to   potentially   democr atize  access   to  advanced   AI  capabilities   and  applying   such   advances   to  better   humanity   but  as  we’ve  seen   with   mos t  AI  systems,   there  is  always  a  cost  that  can  have  sinis ter   consequences.    Between  the  lines   :  The  newly  formed   Center  for  Resear ch  on  Founda tion  Models   at  Stanford   can  become   an  example   of  encouraging   cross-domain   collabor ation  trying  to  answer   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 4 0   
 fundamen tal  ques tions   about   how  we  use  AI  and  wha t  the  futur e  holds.   It  would   be  interesting   also  to  see  how  they  choose   to  interact  with   groups  like  EleutherAI   and  HuggingF ace  which   are   more  community -driv en  and  are  building   such   founda tional   models   that  will  have  an  impact   on   our futur e.   Now Tha t Machines Can Learn, Can The y Unlearn?    [Original article b y  Wired  ]   Wha t  happened   :  The  article   covers  the  nascen t  area  of  “machine   unlearning ”  which   has  the   goal  of  effectiv ely  erasing   personal   informa tion  that  is  captured  in  parts   by  AI  systems  when    they  are  trained   and  later  on  the  user   withdr aws  consen t  or  wants  to  have  their   informa tion   erased.   This  is  no  easy  task  since   millions   of  dollar s  migh t  be  spen t  in  training   up  an  AI  system   and  asking   to  remo ve  certain  parts   of  the  data  from  the  training   set  means,   at  the  momen t,   retraining   the  entire  system  and  hence   spending   all  that  mone y  again.  This  disincen tivizes   organizations fr om mee ting these demands, especially when the financial bur den is so high.    Why  it  matters  :  While   there  is  a  “righ t  to  be  forgotten”  in  the  EU,  mos t  of  the  current   legisla tions   focus   on  data  erasur e  and  consen t  withdr awal  for  data,  but  few  talk  about   the  need    to  also  erase  traces   of  the  snippe ts  of  personal   informa tion  that  are  incorpor ated  in  the  learned    represen tations   in  AI  models.   This  will  become   a  more  essen tial  consider ation  with   more   signific ant  legisla tion  coming   up  in  the  US  and  EU  and  will  also  be  more  meaningful   as  AI   systems per vade mor e parts of our liv es.   Between  the  lines   :  As  poin ted  out  in  the  article,   the  techniques   of  machine   unlearning   are  still   in  the  early   days  wher e  their   efficacy  is  quite  limit ed.  It’s  on  the  same   journe y  as  differential   privacy  wher e  the  technique   is  incredibly   promising ,  tooling   is  being   developed   around   it,  and   hope fully   we  will  have  more  widespr ead  utiliz ation  of  the  technique   over  time.   Wha t  remains   is   for  the  efficacy  to  be  proven  along   with   it  being   practic ally  viable,   as  we  get  more  resear cher s   and  practitioner s  focussing   on  it,  we  will  build   up  the  tooling   and  related  processes   that  will   mak e this a mor e common pr actice.    Even e xperts ar e too quick t o rely on AI e xplana tions, s tudy finds    [Original article b y  VentureBea t  ]   Wha t  happened   :  The  article   covers  a  recen tly  published   resear ch  study   that  found    discr epancies   in  the  intention  of  features  of  AI  systems  as  put  together   by  designer s  and   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 4 1   
 developer s  and  those   who   use  and  interact  with   the  systems  in  how  they  perceive  them.    Building   on  prior   work  from  the  domain   of  human-c omput er  interaction   (HCI),   the  resear cher s   found   that  people   both   over-relied   on  the  outputs   from  an  AI  system  and  misin terpr eted  wha t   those   outputs   mean t,  even  when   they  had  knowledg e  about   how  AI  systems  work,  when   one   migh t  expect   that  to  be  the  case  only   with   those   who   don’t   know  how  such   systems  oper ate.   The  study   evalua ted  these   discr epancies   through   a  game   wher e  a  robot   had  to  gather   supplies    for  stranded   humans   in  space   and  explain   its  actions   as  it  navigated  the  terrain  to  get  those    supplies.   Humans   who   were  recruit ed  to  be  a  part  of  the  experimen t  judg ed  those   robots   more   who   provided   numeric al  descrip tions   of  their   actions   compar ed  to  those   who   provided   natural   languag e explana tions.    Why  it  matters  :  This  has  direct  implic ations   for  how  we  design   explainability   requir emen ts,   especially   as  those   put  forth   by  the  EU,  NIST,  etc.  in  the  sense   that  we  need   to  know  whe ther    the  perception  of  the  provided   explana tions   is  the  same   as  the  ones   that  we  intend.   In   particular ,  a  misma tch  between  the  two  can  lead   to  disas trous  results   and  over-  or   under confidence in situa tions wher e mor e human a ttention is w arranted.   Between  the  lines   :  The  results   from  the  resear ch  study   are  not  all  that  surprising.   Perhap s  the   only   novel  elemen t  is  that  even  those   with   a  backgr ound   in  AI  tended   to  fall  for  this  trap  and   this  only   serves  to  under score  the  problem   more:  we  need   to  be  more  deliber ate  in  how  we   design   explana tions   for  AI  systems  so  that  the  gap  between  intended   meaning   and  perceived   meaning is minimiz ed.   Stopping Deep fake Voices    [Original article b y  USC Vit erbi School of Engineering   ]   Wha t  happened   :  Resear cher s  have  discovered  that  voice  assis tants  that  use  automa tic  speech    recognition   can  be  attacked  using   adversarial   examples   that  can  drop  their   performance    accur acy  from  94%   to  0%  in  some   cases.   This  work  has  also  revealed   strategies   on  how  to  add   noise   imper ceptible   to  the  human   ear  to  surreptitiously   attack  such   systems  so  that  they  beha ve   in  an  unin tended   fashion   aiding   the  malicious   actor’s  goals.   The  paper   also  shar es  some    resear ch dir ections on de fense s trategies tha t can be used t o protect ag ainst such a ttacks.   Why  it  matters  :  Given  the  rising   proliferation  of  listening   devices   all  around   us,  arguably   waking    up  only   on  specific   promp ts,  such   vulner abilities   are  import ant  to  analy ze  and  defend  against  if   they  control  import ant  facets  of  our  lives.  Examples   of  this  include   things   like  the  home’ s   heating and c ooling s ystems and security s ystems lik e door lock s.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 4 2   
 Between  the  lines   :  The  field   of  adversarial   machine   learning   and  machine   learning   security   are   going   to  be  founda tional   for  the  responsible   deplo ymen t  of  AI  technologies,   and  such   resear ch   work  help s  to  raise  import ant  ques tions   and  provide   more  resear ch  directions   so  that  we  can   build   more  robus t  systems  over  time.   While   it  is  great  to  continue   deplo ying  AI  systems  and   incorpor ating  them   into  various   products   and  services,   without   giving   due  consider ation  to  how   these   systems  migh t  break  down,  we  risk  opening   up  new  attack  surfaces   for  malicious   actors   beyond the alr eady v ast vulner abilities w e face in the digit al infrastructur e tha t surr ounds us.    The Thir d Revolution in W arfare   [Original article b y  The A tlantic  ]   Wha t  happened   :  On  the  20th   anniv ersary  of  the  9/11   attacks,  there  has  been   a  lot  of  reflection    on  warfare  and  terrorism.   With   AI  pushing   into  all  facets  of  our  lives,  it  is  natural  to  examine    wher e  we  will  end  up  with   AI-enabled   weapons   systems.   In  this  article,   author   and  VC  Kai-Fu    Lee  talks  about   some   of  the  challeng es,  technic al  and  moral,  in  the  use  of  autonomous   weapons    systems.   In  particular ,  he  highligh ts  the  clear   moral  dilemmas   that  arise   when   we  don’t   have   clear   chains   of  accountability   and  a  lack  of  transpar ency   in  terms   of  how  the  systems  oper ate.   He  also  poin ts  to  potential  solutions   ranging   from  protocols  of  engagemen t  to  outrigh t  bans    each   of  which   have  a  different  likelihood   of  success.   Ther e  are  some   potential  bene fits  in  the   use  of  AI  in  warfare,  notably  the  potential  to  save  lives  and  reduce   collateral  damag e,  but  that   comes a t a cost.   Why  it  matters  :  The  current  state  of  the  ecosystem  is  that  we  have  an  arms-r ace  atmospher e   wher e  it  appear s  that  AI-enabled   weapons   are  inevitable   and  countries   are  rushing   to  try  out   the  technology   to  ensur e  that  they  don’t   get  left  behind.   The  article   men tions   the  Harp y  drone   from  Israel  as  an  example.   Of  course,  some   hypothe tical  scenarios,   like  the  Slaugh terbots   from  a   fictional   short -film,   poin t  to  a  possible   futur e  wher e  such   capabilities   are  in  the  hands   of   malicious   actors  who   don’t   need   a  lot  of  resour ces  to  execut e  fairly   sophis ticated  and  damaging    attacks.   Between  the  lines   :  Ultima tely,  the  biggest  disrup tion  that  will  arise   from  the  use  of  AI  is  the   degr ee  of  leverage  it  will  create  for  non-s tate  and  small   actors  to  utiliz e  this  technology ,  often   using   open-sour ce  designs   and  softw are,  with   cheap   off-the-shelf   hardware  to  assemble   and   deplo y  weapons   that  can  wreak  havoc,  at  least  at  a  moder ate  scale,  harming   people   and  making    it  difficult   to  deter  such   attacks  because   of  the  nimbleness   of  such   systems.   At  the  momen t,  I   don’t   belie ve  that  non-s tate  and  low-resour ced  actors  will  be  able   to  use  such   systems  to  rival   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 4 3   
 large  milit aries   but  it  definitely  gives  them   a  leg  up  in  small-sc ale  comba t  due  to  lowering   the   costs and c ollateral tha t the y have to put up t o eng age.   Everyone will be able t o clone their v oice in the futur e   [Original article b y  The V erge  ]   Wha t  happened   :  The  ability   to  clone   voices   has  existed  for  some   time   now  but  the  new  crop  of   tools  are  faster,  easier ,  and  more  realis tic  to  boot.   A  simple   web  sear ch  yields   results   poin ting  to   companies   like  Respeecher ,  Resemble.ai,   Veritone,   and  Descrip t  all  of  whom   have  product    offerings   that  can  create  a  clone   of  your  voice  that  can  be  used   for  various   purposes.   One  of  the   promising   avenues   advertised   by  firms   like  Veritone  is  that  it  allows  creative  talen t,  like   influencer s,  to  scale  their   impact   by  “loaning ”  out  their   likeness   to  advertiser s  without   them    needing   to  be  presen t.  The  article   poin ts  out  though   that  the  results   still  have  a  weird  warble    and  lack  the  ability   to  char ge  the  gener ated  voice  with   emotion   and  intonation  that  a  real  actor   can bring but the r esults ar e de finitely r ealis tic enough t o be spooky .   Why  it  matters  :  The  recen t  debacle   with   cloning   Anthon y  Bour dain’ s  voice  showed  that  even   potentially   positiv e  uses   of  such   technologies   can  have  an  uncanny  valley  effect.  In  other   cases,    like  the  one  wher e  this  technology   was  used   to  revive  the  voice  of  Val  Kilmer   who   suffered  from   voice  loss  due  to  a  tracheot omy,  the  results   were  perceived  in  a  much   more  positiv e  light.  The   technology   can  definitely  be  put  to  a  positiv e  use  but  this  requir es  a  careful  consider ation  of   pros and c ons, as is the c ase with all dual-use t echnology .   Between  the  lines   :  Some   interesting  applic ations   men tioned   in  the  article   include   how  voice   clones   could   be  used   to  mak e  games   more  personaliz ed  by  adding   in  the  player’s  voice  clone   to   deliv er  all  the  dialogues   in-game   from  the  protagonist  making   the  game   more  immer sive.   Another   one  utilizing   parents’  voice  clones   to  read  bedtime   stories   to  childr en  when   parents  are   away.  As  long   as  we  can  prevent  stealing   the  likeness   of  our  voices   which   can  be  used   for   automa ting  fraud,   such   applic ations   definitely  have  the  potential  to  bring   about   some   useful   capabilities.    The pandemic is t esting the limits of f ace r ecognition    [Original article b y  MIT T echnology R eview  ]   Wha t  happened   :  The  article   dives  into  wha t  happens   when   we  have  larger  portions   of  our   socie ty’s  core  oper ating  infrastructur e  become   automa ted,  often  run  by  private  companies.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 4 4   
 Facial   recognition   technology   has  already   been   shown  to  be  deeply   flawed,  but  the  article    documen ts  the  case  of  a  transg ender   person  who   was  put  in  a  precarious   financial   position   as   the  Calif ornia   governmen t  put  in  place   facial   recognition   technology   to  verify  iden tities   and  the   chang e  in  appear ance   for  this  person  wasn’t   correctly   picked  up  by  the  system.  In  mos t  places,    automa tion  like  facial   recognition   technology   is  deplo yed  as  a  force  multiplier   allowing    low-resour ced  governmen ts  to  provide   more  personaliz ed  services   to  a  larger  number   of   people.   In  other   places,   they  are  pitched   as  a  health   and  safety  option  by  providing   contactless    alterna tives.   Why  it  matters  :  But,  without   an  underlying   supporting   infrastructur e  composed   of  humans,    such   technology ,  when   imperf ect,  and  layered  on  top  of  an  unjus t  socie ty,  can  exacerba te   injus tices   in  socie ty,  making   it  particularly   difficult   for  those   who   are  already   marginaliz ed.   When   such   technology   for  example   works  in  95%   cases,   the  5%  who   are  left  out  need   human    intervention  to  still  be  able   to  access   services.   But,  the  current  wave  of  automa tion  often   reduces   human   support   down  to  the  poin t  wher e  the  5%  get  permanen tly  locked  out  of  being    able t o access ser vices and the help the y need.    Between  the  lines   :  When   thinking   about   deplo ying  automa tion,   design   consider ations   are   essen tial  if  they’re  going   to  achie ve  lofty   goals  of  increasing   access   for  everyone  and  impr oving   the  quality   of  service.   While   the  technology   migh t  work  in  a  large  percentage  of  cases,   those    who   are  unable   to  be  served  by  the  technology ,  often  those   who   were  previously   marginaliz ed   too,  need   to  be  provided   alterna tives  that  still  mee t  their   needs.   Without   that,  we  only   risk   making   socie ty  worse  than   it  is  by  promoting   automa tion  as  a  way  forward  when   it  migh t  be   one s tep forward and tw o steps backw ards in r eality .   Three pr edictions f or the futur e of r esponsible t echnology    [Original article b y  World E conomic F orum   ]   Wha t  happened   :  Providing   a  quick   overview  of  the  work  taking   place   at  WEF   on  responsible    technology ,  the  article   lays  out  three  trends   that  they  belie ve  will  come   to  pass   in  the  field    including   investmen t  efforts  taking   on  responsible   developmen t  as  a  pillar   in  assessing   the   quality   of  investmen ts  just  as  ESG  became   a  criterion   for  assessmen ts.  They  also  belie ve  that  we   are  just  at  the  beginning   of  targeted  regula tions   and  will  only   see  them   adop ted  in  more   countries   in  the  world.   Finally ,  they  also  see  higher   educ ation  making   tech  ethics   a  manda tory   part of v arious curricula.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 4 5   
 Why  it  matters  :  Broad  adop tion  of  responsible   practices   in  technology   design,   developmen t,   and  deplo ymen t  is  wha t  is  missing   today  and  the  three  pillar s  iden tified   by  WEF   do  provide   a   good  high-le vel  roadmap   for  attacking   this  problem   in  a  multi-pr onged  appr oach.   Wha t  we  need    to  think   about   in  addition   to  this  is  wha t  are  the  incen tive  structur es  that  will  actually   enable    these   trends   to  come   to  pass,   rather   than   requiring   constant  push   to  mak e  them   a  reality ,  but   ones tha t will e vapor ate without support.    Between  the  lines   :  Diving   into  just  teaching   AI  ethics   in  higher   educ ation,   there  are  man y   courses  that  are  being   developed   and  provided   at  univ ersities   with   each   pursuing   this  in  their    own  manner .  We  have  done   a  deep   dive  into  this  area  through   our  series   “Office  Hour s”  that   highligh ts  how  some   educ ators  are  going   about   this.   As  for  making   this  a  criterion   within    investmen t  assessmen ts,  I  think   we  are  still  a  long   way  away  from  that  because   there  aren’t  yet   enough   mark et  forces  that  call  for  such   evalua tions   and  the  mak e-up   of  mos t  investmen t  shop s   continues   to  lean   away  from  being   diverse  enough   to  ackno wledg e  these   problems   in  the  first   place.   But,  just  as  was  the  case  with   ESG  criteria,   I  firmly   belie ve  that  utilizing   mone tary   incen tives  through   investmen ts  will  push   the  indus try  towards  responsible   technology   practices    faster than without it.    A  tiny  tweak  to  Zoma to’s  algorithm   led  to  lost  deliv ery  rider s,  stolen   bikes   and missed w ages   [Original article b y  Rest of W orld  ]   Wha t  happened   :  Zoma to,  a  food  deliv ery  app  popular   in  India,   increased   the  deliv ery  radius   for   workers  from  10  km  to  40  km  which   had  an  immedia te  impact   on  the  number   of  deliv eries   they   are  able   to  comple te  in  a  day.  The  workers  are  forced  to  take  on  deliv eries   that  push   them    progressiv ely  further   from  their   “home   zones. ”  They  tried   things   like  switching   off  their   GPS  so   that  they  would   not  receiv e  orders  taking   them   far  away  but  that  mean t  time   off  the  app  which    reduced   their   earning   potential.  After  signific ant  protests  by  workers  in  Beng aluru,   Zoma to   rolled   the  chang e  back   for  workers  who   have  been   with   the  platform  for  more  than   3  years,  but   not f or the ones who ar e ne w.   Why  it  matters  :  The  agents  are  incen tivized  based   on  the  number   of  deliv eries   they  are  able   to   comple te  in  a  day  and  having  to  travel  further   diminishes   the  number   of  deliv eries   they  are  able    to  comple te.  Rejecting   orders  is  also  not  an  option  since   that  directly   affects  their   rating  within    the  platform  and  the  number   of  deliv eries   they  get  alloc ated  based   on  that  status.  Finally ,   redressal   mechanisms   are  mos tly  automa ted,  fixed  menu   options   that  don’t   give  them   much    agency with the c ompan y.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 4 6   
 Between  the  lines   :  Workers  have  tried   to  organize  and  raise  concerns   with   the  Supr eme   Court    of  India   to  be  classified   as  wage  workers  rather   than   contractors  so  that  they  have  more  rights   and  labor   law  protections   but  that  has  been   unsuccess ful  so  far.  Similar   to  the  gig  econom y   issues   elsewher e  in  the  world,   workers  are  disempo wered  and  helpless,   especially   in  a  country   like  India   wher e  the  wages  they  do  receiv e  are  very  low,  barely  helping   them   mee t  basic   needs    on a daily basis.    Small Da ta Are Also Crucial f or Machine Learning    [Original article b y  Scien tific Americ an  ]   Wha t  happened   :  The  article   mak es  the  case  that  the  technique   of  transfer  learning   wher eby  a   small,   highly   domain-specific   dataset  can  be  used   to  leverage  a  pre-trained   model   to  fine-tune    performance   on  a  task  has  a  lot  of  promise   and  remains   under -explor ed  at  the  momen t.  It   poin ts  out  that  there  has  been   a  lot  of  success   in  applying   this  to  tasks  in  comput er  vision   (CV)    and  natural  languag e  processing   (NLP)   such   as  the  use  of  models   pre-trained   on  Imag eNet.  But,   it  also  poin ts  out  if  there  is  limit ed  overlap   in  the  domains   of  the  new  task  and  the  dataset  on   which the model w as pr e-trained, perf ormance c an suf fer.   Why  it  matters  :  None theless,   transfer  learning   is  a  promising   area  of  resear ch  that  deser ves   attention  given  that  it  can  elevate  the  power  of  small   data.  Especially   when   there  is  a  high    financial   cost  to  training   large  models,   the  ability   to  use  pre-trained   models   fine-tuned   using    transfer  learning   can  provide   an  avenue   to  resour ce-constrained   resear cher s  to  harness   the   power  of  AI.  This  can  also  help   us  mitig ate  the  environmen tal  impact   of  AI  systems  by   preventing  the  need   to  train  really   large  models   from  scratch  and  oper ate  well  in  small   data   regimes.    Between  the  lines   :  Ther e  are  man y  bene fits  to  having  large,  gener alized  models   which   can  be   taken  off-the-shelf   and  fine-tuned   for  new  tasks  because   they  demons trate  the   “gener alizability ”  of  such   powerful   models,   one  of  the  key  things   that  any  AI  practitioner   would    love  to  have  when   developing   AI  systems.   The  more  we  are  able   to  harness   existing  models    wher e  investmen ts  have  already   been   made   to  bring   them   up  to  a  baseline   level  of   performance,   the  more  we’ll  be  able   to  democr atize  access   to  performan t  AI  systems  in  novel   domains t o people who w ere previously limit ed in their ability t o build and access such s ystems.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 4 7   
 How Big T ech Is Pit ching Digit al Elder Car e to Families    [Original article b y  The Mark up  ]   Wha t  happened   :  As  the  pandemic   rolled   on  through   all  parts   of  the  world,   elder   care  facilities    felt  a  particular   twing e  of  isola tion.   In  a  woefully   underpr epar ed  ecosystem,  with   constant   under staffing,  the  elder   popula tion  remained   isola ted  and  family   member s  turned   to  consumer    devices   like  Apple   Watches   and  Alexas  to  step  in  partially   in  place   of  caregiver  responsibilities,    especially   around   monit oring   and  alerting   in  case  of  acciden ts.  But,  this  comes   with   a  slew  of   privacy  and  consen t  problems,   given  that  elder s  are  less  likely  to  under stand  implic ations   of  the   use of such t echnologies.    Why  it  matters  :  As  an  example,   for  elder s  with   demen tia,  consen t  becomes   problema tic  as  their    state  of  mind   may  not  be  such   that  they  are  fully   able   to  grasp  wha t  it  means   to  be  monit ored   via  an  audio   or  visual   device.   In  addition,   their   ability   to  withdr aw  consen t  also  becomes   limit ed   if  circums tances   chang e.  Then,   the  deplo ymen t  of  such   technologies   also  have  second-or der   effects,   for  example,   the  conversations   of  those   around   with   such   monit oring   devices   are  also   captured, not necessarily with their c onsen t.   Between  the  lines   :  It  is  not  surprising   that  such   technology   has  taken  off.  Ther e  is  an  untapped    mark et  for  technology   in  elder   care  and  companies   are  trying  to  dive  into  this  sector  (as  also   covered  in  AI  Ethics   Brief  #43).   Also ,  caregivers  tend  to  have  a  fair  bit  of  power  over  elder s  and   even  through   “bene volen t  coercion”   nudg e  them   into  using   technology   that  they  migh t  not   other wise   be  comfortable   with.   Finally ,  and  mos t  import antly,  technology   cannot   serve  as  a   replacemen t  for  human   warmth  and  care.  The  rapid   deplo ymen t  of  technology   as  a   replacemen t  for  functions   that  are  provided   by  human   caregivers  will  only   shift   ecosystem   investing  away  from  wha t  actually   needs   to  be  done   (training ,  hiring ,  and  paying  well  for  human    caregivers) towards technologic al solutions.    How Alibaba tr acks China’ s deliv ery driv ers   [Original article b y  MIT T echnology R eview  ]   Wha t  happened   :  Getting  meals   deliv ered  on  time   requir es  a  coordina ted  effort  across   restaurants,  service  provider s,  and  deliv ery  drivers.  With   moun ting  pressur e  from  consumer s  to   get  their   deliv eries   on  time,   and  a  highly   compe titive  landsc ape  with   man y  service  provider s   trying  to  snatch  up  mark et  shar e,  inno vation  in  tracking   and  estima ting  deliv ery  times   can  offer   an  edge.  In  China,   companies   like  Eleme,   owned   by  Alibaba,   with   over  83  million   mon thly  activ e   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 4 8   
 user s,  have  deplo yed  more  than   12,000   Blue tooth   beac ons  to  enable   indoor   tracking   and  figur e   out ho w long the driv er w aits f or or ders and when the y enter and lea ve a r estaurant.   Why  it  matters  :  The  stated  reason   behind   these   deplo ymen ts  is  that  they  will  mak e  the  job  of   deliv ery  drivers  more  efficien t,  since   they  won’t  have  to  pull  out  their   phones   every  few  minut es   to  “check -in”  with   the  system  on  their   status.  Automa tically  doing   so  through   Blue tooth   beac ons   and  proximity   will  alleviate  this.   But,  more  accur ate  location-da ta  and  using   that  to  tighten  up   deliv ery  times   will  also  increase   pressur e  on  an  already   under -compensa ted  and  strenuous   job.   Gig w orkers with f ew righ ts will be f orced t o oper ate under e ven mor e draconian cir cums tances.    Between  the  lines   :  In  addition   to  the  man y  labor   rights  problems   with   such   a  technology ,   including   the  well-being   of  workers  and  stress  concerns,   having  so  man y  Blue tooth   beac ons,   both   virtual   and  physical,  pose   unexplor ed  challeng es  when   it  comes   to  exchanging   so  much    location-based   informa tion  constantly  throughout   the  day.  Perhap s,  tempering   our  expect ations    as  consumer s  on  deliv ery  times   and  aiding   workers  in  getting  better  rights  is  a  more  fruitful    investmen t  of  resour ces  than   enabling   more  string ent  technology   from  micr omanaging   every   aspect of their job.    Curr ent  AI  Practices   Could   Be  Enabling   a  New  Gener ation  of  Copyrigh t   Trolls   [Original article b y  Unit e  ]   Wha t  happened   :  In  a  study   conduct ed  by  resear cher s  from  Huawei,  they  discovered  that  for  the   6  mos t  common   datasets  used   by  them   in  training   their   AI  models,   mos t  of  them   posed    signific ant  legal  challeng es  when   it  comes   to  commer cial  use.  Specific ally,  challeng es  included    things   like  wha t  kind   of  licenses   the  models   needed   to  be  released   under   since   they  constitut ed   deriv ed  work,  whe ther   commer cializ ation  was  even  possible,   and  the  legal  liabilities   in  case   claims   were  made   by  anyone  affected  by  adverse  outcomes   from  the  use  of  those   models.   In   several  of  those   datasets,  there  were  challeng es  in  tracing   the  lineag es  of  the  licenses   that   would   be  applic able   given  that  they  were  curated  and  scraped   datasets  rather   than   original   data   gathering.   In  addition,   mos t  also  come   with   auto-indemnific ation  for  the  original   author s  of   those da tasets, placing the liability on to those who use them in building their models.    Why  it  matters  :  Given  the  push   towards  large  models,   which   in  the  current  paradigm   of   super vised   learning   mean   the  consump tion  of  large  datasets  for  training ,  the  use  of  such    datasets  and  their   legal  implic ations   pose   challeng es  if  the  current  legal  landsc ape  evolves   towards  some thing   stricter  wher eby  such   viola tions   are  pursued   more  string ently.  The  reasons    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 4 9   
 iden tified   in  the  article   and  paper   that  allow  for  such   viola tions   to  pass   unaddr essed   is  that   there  is  a  laisse z-faire  and  caveat-emp tor  appr oach,   at  least  in  the  US.  But,  should   that  chang e   or  as  is  the  case  in  other   jurisdictions,   wher e  such   activities   are  firmly   disallo wed,  these    viola tions will ha ve to be t ackled head-on r ather than lea ving them nebulously unaddr essed.    Between  the  lines   :  Large  public   datasets  have  been   the  bedr ock  upon   which   powerful   models    have  been   built   in  the  modern   era  of  AI.  But,  as  has  been   showcased   over  the  past  18-24    mon ths,  a  lot  of  these   datasets  come   with   challeng es  in  terms   of  not  only   biases,   but  also  how   that  data  was  collect ed,  often  without   consen t.  A  deep-div e  into  the  licensing   lineag e,  as  done    by  the  paper   cited  in  this  article   showcases   that  there  are  numer ous  challeng es  that  are  yet  to   be  solved,  especially   as  the  regula tory  regime   stiffens  with   respect   to  the  use  of  data  in  AI   systems.   This  migh t  also  have  implic ations   for  how  AI  systems  are  import ed  and  export ed  if   there are dif ferences in the r egula tory requir emen ts acr oss dif ferent jurisdictions.    The Futur e of Digit al Assis tants is Queer    [Original article b y  Wired  ]   Wha t  happened   :  The  article   dives  into  building   upon   the  case  that  was  laid  out  in  the  UN  report    “I’d  blush   if  I  could”   that  highligh ted  how  a  lot  of  smart   voice  assis tants  have  a  feminiz ed  voice   and  are  made   to  take  on  archaic,   stereotypic al  feminine   char acteristics  of  obeisance   emer ging    from  the  lack  of  diversity  and  other   problems   in  the  domain   of  technology .  In  particular ,  it   showcases   how  the  futur e  for  these   assis tants  migh t  be  queer ,  not  just  in  the  formula tion  of  the   actual   timbr e  of  the  voice,   but  more  so  in  wha t  being   outside   of  traditional   binaries   mean   when    it comes t o whe ther such an assis tant should mimic humans in the fir st place.    Why  it  matters  :  Not  only   does   such   an  appr oach   esche w  the  problema tic  formula tion  of  digit al   assis tants  today,  it  also  enriches   the  discussion   by  providing   alterna te  formula tions   for  wha t   digit al  assis tants  can  look   like.  It  help s  us  imagine   an  alterna te  futur e.  One  of  the  examples   that   they  men tion  include   an  explor ation  of  having  multiple   personalities   that  more  accur ately   reflect   the  man y  versions   of  femininity ,  but  even  more  on  the  poin t  that  such   bots   are  not   human.   The  example   of  Eno,  the  bot  from  Capit al  One  stands   out  as  an  example   wher e  it  talks   about binar y as 1s and 0s r ather than g ender when ask ed about its g ender .   Between  the  lines   :  Calif ornia   in  2019   created  the  first  legal  preceden t  asking   bots   to  iden tify   themselv es,  some thing   that  is  increasingly   import ant  as  we  have  capabilities   like  Duple x  from   Google   being   capable   of  making   appoin tmen ts  on  our  behalf   sounding   human.   While   the  legal   preceden t  is  far  from  perfect,  it  lays  down  an  imper ative  for  us  to  start  thinking   differently   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 5 0   
 about   such   technologies,   especially   as  they  inch   into  mimicking   human   assis tants  more  and   more. It will also shape the in teractions be tween humans and machines much mor e.   Robots W on’t Close the W arehouse W orker Gap An ytime Soon    [Original article b y  Wired  ]   Wha t  happened   :  Anytime   there  is  a  conversation  about   the  labor   impacts   of  AI,  the  first  thing    that  we  hear   about   are  the  impacts   that  will  take  place   on  the  factory  floor .  This  article   dives   deeper   into  how  that  is  actually   manif esting  and  wha t  it  means   for  the  futur e  of  work.  Mos t  of   the  robot   deplo ymen ts  on  the  factory  floor   today  are  things   that  requir e  limit ed  intelligence   and   still  rely  heavily  on  human   co-workers  to  comple te  jobs,  wher e  they  only   play  a  small   part  by   taking o ver some t asks.   Why  it  matters  :  As  we  look   for  more  nuance   on  the  direct  impacts   from  automa tion  on  factory   floor s  and  elsewher e,  it  help s  to  gain  an  under standing   of  which   indus tries   are  deplo ying   automa tion  in  wha t  manner s  and  to  wha t  extent.  For  example,   when   we  look   at  Amaz on  putting   out  number s  saying  they’re  hiring   150,000   more  seasonal   workers  to  mee t  the  holida y  demand,    it  help s  to  under stand  how  they  co-work  in  the  warehouse   environmen t,  and  given  the   capabilities   of  wher e  robotics   are  headed,   wha t  can  we  reasonably   expect   to  chang e  in  the   futur e.   Between  the  lines   :  As  is  men tioned   in  the  article   by  a  lot  of  the  robotics   companies   who   supply    places   like  FedEx   and  Amaz on,  there  are  a  lot  of  unsolv ed  and  unan ticipa ted  edge  cases   which    we  can’t  design   for  just  yet.  Wha t  that  means   is  that,  at  least  in  the  near -futur e,  we  will   continue   to  have  both   humans   and  machines   working   side-b y-side.   Or  at  least  through   isola ted   environmen ts,  given  the  current  safety  concerns   wher e  machines   are  housed   in  separ ate  cages   to  prevent  any  acciden ts  from  taking   place.   The  takeaway  for  me  from  this  article   is  that  as  we   think   about   upskilling   and  redeplo ying  human   labor   capacities,   keeping   a  keen  eye  on  the  edge   cases   that  are  still  unsolv ed,  and  speaking   with   technic al  experts   to  gain  an  under standing   of   the  timeline   to  solve  them   will  be  critic al  to  better  prepar e  for  labor   transitions   as  the  need   for   those arise.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 5 1   
 9. Outside the bo xes   Introduction  by Kathy Ba xter, Principal Ar chitect,  Ethical AI Pr actice, Sales force   Recen t  US  Congr essional   testimon y  is  another   indic ation  that  the  US  Governmen t  continues   to   explor e  regula ting  the  tech  indus try  .  The  deba te  itself   touches   on  traditional   issues   such   as   antitrus t,  but  also  includes   more  novel  issues   like  the  impacts   of  artificial   intelligence   (AI).  In   particular ,  deba tes  are  focusing   on  AI’s  carbon   footprin t  ,  the  need   for  AI  regula tions   ,  if   regula tions actually impede inno vation  , and ho w to  iden tify and manag e bias   .   Since   2019,   at  least  17  US  states  have  proposed   121  bills  or  resolutions   with   the  goal  of   regula ting  AI  applic ations.   We  saw  the  mos t  regula tion  in  2019,   with   10  out  of  47  bills  being    adop ted  or  enact ed,  and  2020   as  the  least  prolific,   with   only   one  bill  passing ,  five  pending   (all  in   New  Jersey  and  all  still  pending   at  the  end  of  2021),   and  36  failing   to  pass.   In  2021,   32  bills  or   resolutions   were  proposed   and  six  were  enact ed.  For  more  informa tion  about   the  US  AI  bills  or   resolutions in each s tate, check out this   site  .   The  deba te  about   regula ting  AI,  though,   is  global.   In  Februar y,  India   published   their   appr oach    for  responsible   AI  ,  followed  by  the  EU’s  draft  AI  regula tion  in  April.   In  September ,  China    published   Ethical  Norms   for  the  New  Gener ation  Artificial   Intelligence   ,  which   “aims   to  integrate   ethics   into  the  entire  life  cycle  of  AI  and  provide   ethical  guidelines   for  natural  persons,    enterprises,   and  other   related  institutions   engaged  in  AI-related  activities. ”  And,   all  of  these    discussions   are  happening   as  governmen ts  themselv es  increasingly   emplo y  AI  in  public    governance and decision-making   .   However,  some   countries   are  still  developing   their   appr oach   to  the  global   AI  race  (e.g.,   Vietnam).   A  MAIEI   op-ed   concluded   that  to  be  success ful,  these   countries   would   have  to  secur e   signific ant  investmen t  in  AI  developmen t,  develop   and  retain  technic al  talen t,  and  cultiv ate  a   willingness t o addr ess e thical risk s so AI bene fits e veryone equally in socie ty.   With   an  increasing   interna tional   ambition   of  achie ving  safe  and  responsible   AI,  organizations    like  the  MAIEI   have  ques tioned   whe ther   the  world  can  unite  under   a  global   AI  regula tory   frame work  .  Participan ts  concluded   that  “global   convergence   could   indeed   help   us  overcome    problems   such   as  gaps  in  datasets.”  However,  given  the  diversity  of  cultur es,  values,   and  AI   capabilities   around   the  world,   “fragmen tation  of  AI  regula tion  is  guar anteed,   and  the   import ance of loc al regula tory efforts is an essen tial c onsequence of tha t.”   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 5 2   
 In ligh t of this   focus on AI de velopmen t and r egula tion  ,  it is not surprising tha t we ha ve seen:    1.  Man y  paper s  and  presen tations   summarizing   the  various   proposals   (e.g.  Stanford  HAI’s   Policy   Brief  on  the  draft  EU  AI  Act  ,  visual   guide   to  the  EU  AI  Act  by  the  founder   of   OpenE thics);    2.  Wide-r anging   analy ses  of  their   implic ations   (e.g.  Centre  for  European   Policy   Studies    analy sis of the c osts of the EU AI Act   ); and    3.  Remaining   open   ques tions   about   their   implemen tation  (e.g.  IDC’s  list  of  unans wered   ques tions in the EU AI Act   ).   Concerns   have  also  been   raised   about   “the   dozens  of  separ ate  AI  ethics,   policy ,  and  technic al   working   groups  across  the  [US]   feder al  governmen t”  and  that  “resulting   policies   may  be   incomple te,  inconsis tent  or  incompa tible   with   each   other .”  Man y  of  those   paper s  are  featured  in   the  current  State  of  AI  Ethics   Report.   I  encourage  you  to  take  a  few  minut es  to  review  them   as  it   is  critic al  to  a  deeper   under standing   of  wha t  is  being   proposed,   the  implic ations,   and  how  we   can impr ove these policy pr oposals.    Policymak ers  have  varying  levels  of  AI-lit eracy  including   knowledg e  of  how  t  he  man y  different   types   of  AI  systems  work,  and  how  biases   and  harms   emer ge,  as  well  as  how  to  best  mitig ate   them.   In  terms   of  bias  mitig ation,   we  as  a  field   of  AI  ethics   researchers   and  practitioners   still   don’t   always   know  the  optimal   ways   to  do  this,   depending   on  the  type   of  AI  applic ation  and  the   context  of  use.  It  is  my  hope   that  after  reading   this  introduction   and  the  paper s  linked  here,  you   will  be  motiv ated  to  engage  in  discussions   with   policymak ers  on  methodologies   to  iden tify  bias   and  harms   (e.g.  balancing   different  measur es  of  fairness   ),  the  downsides   for  each,   realis tic   thresholds   for  bias  (e.g.  no  dataset  or  model   can  ever  be  “bias- free”),   and  realis tic  mechanisms    to monit or for and mitig ate harms (e. g.  human o versight  ,  sandbo xes  ,  debiasing   ).   Key issues t o consider    Man y  proposed   regula tions   like  the  draft  EU  AI  Act  requir e  developer s  of  high-risk   AI  systems  to   perform  both   pre-deplo ymen t  conformity   assessmen ts  and  post-mark et  monit oring   analy ses  to   demons trate  that  their   systems  are  in  compliance.   Governmen ts  and  companies   alike  need   to   invest  more  in  their   capacity   to  systema tically  measur e  and  monit or  the  capabilities   and  impacts    of AI s ystems  .   Although   AI  governance   tooling   is  one  necessar y  componen t  for  creating  and  implemen ting  AI   responsibly ,  it  is  not  sufficien t.  You  can’t  know  if  your  datasets  or  models   are  biased   for  or   against  some   groups  if  you  are  unable   to  analy ze  measur es  like  dispar ate  impact   or  individual    versus  group  fairness.   However,  iden tifying  bias,   analy zing  by  different  definitions   of  fairness,    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 5 3   
 and  mitig ating  bias  in  datasets  and  models   is  as  much   an  art  as  a  science   .  Even  small   miss teps   can  decr ease   the  accur acy  of  your  models   and/ or  increase   the  risk  for  harm ful  outcomes    without one r ealizing it.    This  amazing   paper   by  EDRi   discusses   the  need   to  go  beyond  debiasing   as  a  solution   to  ensur e   AI  is  safe  and  fair.  Different  aspects   of  fairness   and  founda tional   assump tions   about   those   that   will  be  impact ed  are  often  left  out  of  debiasing ,  providing   creators  the  ability   to  game   an  audit    and appear t o comply with r egula tions when the y do not.    Since   a  lack  of  represen tativeness   in  training   and  evalua tion  datasets  is  only   one  way  that  bias   enters  a  system,  automa ted  debiasing   won’t  addr ess  the  root  cause.   Only   by  engaging   with   the   popula tions   underr epresen ted  in  the  data  can  you  impr ove  your  datasets,  under stand  biased    founda tional assump tions, and kno w if y our AI ma y result in unin tended harm.    Additionally ,  AI  can  be  unevenly   applied   to  different  groups  (e.g.  predictiv e  policing   or  facial    recognition   surveillance   applied   only   in  predominan tly  black   and  brown  neighborhoods).    Debiasing   mechanisms   will  do  nothing   to  addr ess  those   harms.   The  EDRi   author s  end  their    report with actionable r ecommenda tions f or policymak ers. Specific ally, the y recommend tha t:   ●  Policymak ers  adop ting  technocen tric  appr oaches   to  addr ess  the  discrimina tory  impact    of  AI  mus t  define  problems   clearly ,  set  criteria  for  solutions,   develop   guidance   on  known   limit ations, and support further in terdisciplinar y resear ch.   ●  AI  policies   mus t  limit   the  discr etion  of  AI  service  provider s  in  addr essing   discrimina tion   and inequalities.    ●  AI  regula tion  needs   to  go  beyond  ADMS   [automa ted  decision-making   systems],   data,   and  algorithms   to  include   the  spectrum   of  AI  applic ations   and  the  broader   harms    associa ted with the pr oduction and deplo ymen t of these s ystems.    ●  AI  policies   should   empo wer  individuals,   communities,   and  organizations   to  contest   AI-based s ystems and t o demand r edress.   ●  AI  regula tion  cannot   be  divorced  from  the  power  of  big  tech  companies   to  control   comput ational in frastructur es.   ●  AI  regula tion  should   protect,  empo wer  and  hold   accountable   organizations   and  public    institutions as the y adop t AI-based s ystems.    A  key  componen t  of  responsible   AI  and  often  included   in  AI  regula tion  is  the  requir emen t  for   explainability   or  interpr etability   --  making   clear   how  a  model   works  or  why  it  is  making   a  certain   recommenda tion  or  prediction.   It,  too,  is  necessar y  but  not  sufficien t  for  creating  and   implemen ting  AI  responsibly .  This  paper   by  resear cher s  at  GA  Tech,  Cornell,   and  IBM  found   that   “people   both   over-relied   on  the  outputs   from  an  AI  system  and  misin terpr eted  wha t  those    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 5 4   
 outputs   mean t,  even  when   they  had  knowledg e  about   how  AI  systems  work.”  Misunder standing    how  AI  systems  work  “can  lead   to  disas trous  results   and  over-or  under confidence   in  situa tions    wher e  more  human   attention  is  warranted.”  They  conclude   that  “we  need   to  be  more   deliber ate  in  how  we  design   explana tions   for  AI  systems  so  that  the  gap  between  intended    meaning and per ceived meaning is minimiz ed.  “   AI  governance   tools  typic ally  requir e  well-structur ed  data  that  is  labeled   by  demogr aphic   or   sensitiv e  variables   (e.g.  loan   appr oval  by  race  or  gender).   Some   in  the  tech  indus try  have  relied    on  “ghos t  workers”  --  low-paid,   third-party   workers  primarily   in  the  Global   South   or  even   refugee  camp s  --  to  label   data  to  train  imag e  recognition   systems,   large  languag e  models,   and   even  self-driving   cars  .  Increasingly ,  ghos t  workers  are  demanding   better  conditions   .  Given  the   critic al  nature  of  their   work  in  ensuring   cars  can  accur ately  iden tify  objects   on  the  road  or   moder ating  toxic  content  and  disin forma tion,   regula tions   are  needed   to  provide   robus t   protection f or these w orkers and socie ty as a whole.    Ther e  are  no  silver  bulle ts  --  a  multipr onged,  multis takeholder   effort  will  be  requir ed  that   involves  collabor ation  among   governmen ts,  indus try,  academia,   civil  socie ty,  and  consumer s –   especially   the  mos t  underr epresen ted,  historically  marginaliz ed,  and  vulner able   groups.  If  you   are reading this r eport, y ou ar e a much-needed v oice in this discussion!    Kathy Baxter   Principal Architect, Ethical AI Practice   Salesforce   As  a  Principal  Architect  of  Ethical  AI  Practice  at  Salesforce,  Baxter  develops   research-informed  best  practices  to  educate  Salesforce  employees,  customers,   and  the  industry  on  the  development  of  responsible  AI.  She  collaborates  and   partners  with  external  AI  and  ethics  experts  to  continuously  evolve  Salesforce   policies,  practices,  and  products.  Prior  to  Salesforce,  she  worked  at  Google,  eBay,  and  Oracle  in  User   Experience  Research.  She  received  her  MS  in  Engineering  Psychology  and  BS  in  Applied  Psychology  from   the  Georgia  Institute  of  Technology.  She  is  the  coauthor  of  "Understanding  Your  Users:  A  Practical  Guide   to User Research Methodologies."   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 5 5   
 From our events   Top  10  Takeaways  from  our  Conversation  with   Sales force  about    Conversational AI    [Original article b y Connor W right]   Overview  :  Would   you  relate  to  a  chatbot   or  voice  assis tant  more  if  they  were  female?   Would    such   conversational   AI  help   you  feel  less  lonely?   Our  event  summar y  of  our  collabor ation  with    Sales force se ts out t o discuss jus t tha t.   Introduction    Would   you  feel  less  lonely   if  you  had  access   to  some   conversational   AI?  Does   the  naming   and   gender   of  the  chatbot   matter?  Some   belie ve  no,  some   belie ve  yes,  and  some   belie ve  yes  too   much.   Facilit ated  by  Kathy  Baxter,  Yoav  Schlesing er,  Greg  Benne tt,  Connor   Wright  and  Abhishek    Gupta,  conversational   AI  as  chatbots   and  voice  assis tants  was  deeply   explor ed  in  our  event  with    Sales force.  With   so  much   potential  for  both   positiv e  and  negative  outcomes,   it  mak es  you  start   to wonder: c an I ha ve a g ood c onversation with a cha tbot?    The k ey takeaways   With   our  ques tion  promp ts  centering   on  the  gender   and  name   of  different  chatbots,   the   technology ’s  effect  on  the  vulner able   and  the  potential  for  bias  that  it  brings   with   it,  immedia te   reflection   on  chatbots   itself   is  called   into  action.   Specific ally,  how  does   it  affect  the  basic   notion    of conversation itself?    Wha t mak es a g ood c onversation?    When   thinking   of  programming   a  chatbot,   you  may  find  yourself  thinking   about   wha t  actually    mak es  a  good  conversation.   Is  it  the  speed   at  which   you  obtain  an  answer  you  were  looking   for?   How  did  you  feel  afterwards?  The  informa tion  you  learn t  along   the  way?  One  thing ’s  for  sure,   the  context  in  which   your  chatbot   is  deplo yed  plays  a  consider able   role  in  determining   wha t  a   ‘good’ c onversation is.    Context ma tters   If  your  chatbot   is  to  help   customer s  with   their   banking ,  you’re  not  going   to  prioritise   making   the   customer   feel  good  about   themselv es  but  rather   achie ve  wha t  they  set  out  to  do.  From  here,   the  distinction   between  ‘narr ow’  and  ‘wide’   chatbots   comes   to  the  fore.  ‘Narr ow’  chatbots   are   geared  towards  achie ving  a  particular   outcome   within   a  very  focussed   context,  such   as  a   chatbot   for  a  fashion   brand  helping   you  find  the  item  of  clothing   you  want.  A  ‘wide’   chatbot   can   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 5 6   
 be  found   in  Alexa  and  Siri,  tasked  with   a  varied   list  of  activities   to  do  and  accomplish   in  a  wide    range  of  contexts.  For  example,   asking   Alexa  to  both   order  some thing   from  Amaz on  and  wha t   the  weather   will  be  like  tomorr ow.  However,  whe ther   ‘narr ow’  or  ‘wide’ ,  does   a  chatbot ’s  name    and g ender c ontribut e to its o verall success?    Does the bot need a name and g ender?    It’s  curious   as  to  why  a  majority   of  chatbots   have  been   attribut ed  a  name   and  gender .  Some   say   how,  perhap s,  the  chatbot   shouldn’t   have  either   as  it’s  just  a  machine   comple ting  a  task.  For   example,   the  streaming   service  Hulu’ s  Hulubot   in  its  help   centre  is  an  excellen t  example   of  a   chatbot functioning without ha ving a human name and a g ender .   However,  the  norm   is  to  assign   a  fixed  gender   and  name.   Doing   so  has  a  lot  to  do  with   the   audience   that  the  chatbot   is  being   mark eted  towards.  It  is  found   that  people   are  more  likely  to   welcome   into  their   home   a  female   chatbot   by  finding   the  female   voice  more  relatable   and   trustworth y.  One  problem   this  does   cause   is  potentially   reinforcing  the  gender   stereotype   of   ‘women   assis tants’,  so  should   you  be  allowed  to  choose   whe ther   you  want  your  chatbot   or   voice assis tant to be a particular g ender?    Should y ou be allo wed to choose?    ALongside   avoiding   any  potential  gender   stereotypes,   it  may  be  that  I  feel  like  talking   to   different  ‘people’   about   various   things,   so  deciding   on  gender   and  name   should   be  left  open.    For  example,   having  controls  on  the  chatbot   and  voice  assis tant  wher e  I  can  play  around   with    the pit ch rather than f eeling lik e I’m talking t o the same per son all the time.    However,  if  the  choice   is  left  open,   you  may  run  the  risk  of  someone   wanting  to  read  in  a   potentially   problema tic  persona   (like  a  timid   tone  of  voice  to  feel  dominan t  over  the  chatbot).    Furthermor e,  such   customisa tion  possibilities   could   lead   to  a  severe  attachmen t  to  the  bot   itself , making the line be tween humanity and machine e ven mor e blurr ed.   Potentially g etting t oo a ttached    Although   the  human   knowing   whom   they  are  talking   to  is  a  chatbot,   it  may  still  not  be  enough    to  prevent  humans   from  getting  attached   and  deceiv ed  about   their   other   interlocut or,  especially    given  how  people   still  love  anime   char acters  despit e  knowing   wha t  they  are.  Such   attachmen t   could   then   be  exploit ed  by  actors  taking   advantage  of  any  vulner ability   to  use  the  human    involved.  A  non-human   name   could   potentially   serve  to  comba t  this,   but  not  all  manipula tion  in   itself c ould be a bad thing.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 5 7   
 Manipula tion c an ha ve tw o sides t o it   Manipula tion  can  be  used   to  achie ve  more  positiv e  ends,   such   as  a  voice  assis tant  reminding    your  sick  Mother   to  take  her  medic ations   and  using   persuasiv e  languag e  in  doing   so.   Alterna tively,  the  voice  assis tant  could   requir e  a  parent’s  voice  authen tication  for  certain   products   to  be  ordered  off  Alexa  in  order  to  dissuade   childr en  from  abusing   the  service.   In  this   sense, while cha tbots c an ser ve to manipula te, the y can also ser ve to bene fit human e xistence.    Chatbots f or good   The  chatbot   evolution   from  just  a  machine   to  being   a  companion   can  substantially   impact   the   dark est  corner s  of  some   human   lives.  Chatbots   can  provide   a  24/7   communic ation  outle t  to   help   comba t  loneliness   and  depr ession   and  serve  as  a  digit al  companion   in  the  dark   depths  of   the  pandemic   last  year.  Fortuna tely,  with   such   experiences   not  being   shar ed  by  all,  the   import ance of the cha tbot pr ocess being inclusionar y cannot be under estima ted.   Designing cha tbots and v oice assis tants with all and not jus t for all    A  clear   example   can  be  found   in  differing   opinions   on  voice  recordings   being   done   by  voice   assis tants.  Here,  some   are  against  voice  assis tants  taking   recordings   of  the  daily   happenings   in   the  house.   However,  other s  belie ve  that  this  can  be  a  crucial   step  to  comba ting  gender   violence,    with v oice r ecordings pot entially pr oving k ey evidence of dif ferent inciden ts.   Making   this  kind   of  potential  service  accessible   then   proves  paramoun t  as  well.  Incorpor ating   local  dialects   and  different  accen ts  for  optimum   bene fit  to  be  guar anteed  to  all  is  one  aspect   of   judging   how  good  these   conversational   AI  are.  However,  do  we  get  too  carried   away  with   such    technology?    Seeing a cha tbot f or wha t it is    Some times,   without   having  any  benchmark s,  we  may  get  over-excited  about   conversational   AI   in  itself .  This  is  not  helped   by  any  personal   relationship   developed   through   the  voice  assis tant  or   chatbot   having  its  own  name   and  gender ,  which   lead   us  to  attribut e  more  humanity   to  these   AI   than   we  actually   should.   For  example,   gender   for  voice  assis tants  is  instead  just  a  pitch  value   to   which   we  attribut e  our  human   interpr etation,   rather   than   a  voice  assis tant  or  chatbot   actually    being on the g ender spectrum.    It’s  import ant  to  note  how  chatbots   and  voice  assis tants  are  programmed   to  say  things   to  you   rather   than   to  under stand  you.  For  example,   Sirir  may,  at  some   poin t,  be  able   to  book   you  on  a   fligh t  in  your  preferred  windo w  seat,  but  it  would   not  know  the  reasoning   behind   it.  Maybe  this   is, in f act, f or the bes t, giv en the priv acy c oncerns associa ted with cha tbots themselv es.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 5 8   
 Privacy issues    Despit e  the  Calif ornia   Chatbot   Law  and  new  EU  AI  law  requir emen ts  regula ting  conversational    AI,  some   though t-provoking   ques tions   still  arise.   For  example,   is  it  a  privacy  viola tion  if  we  look    at  the  chatbot   conversations   involving   other   people   in  our  family?   If  we  are  caugh t  on  our   neighbour ’s  Amaz on  Ring   commit ting  a  crime,   does   my  neighbour   have  the  right  to  shar e  such    informa tion  with   the  police?   The  more  integrated  into  our  lives  conversational   AI  becomes,   the   more of these ques tions will sur ely surf ace.   Between the lines    Our  event  proved  both   inspiring   and  stimula ting  for  me.  The  import ance   of  involving   all  in  the   conversational   AI  design   process   is  now  abundan tly  clear ,  especially   with   the  field   of  naming    and  assigning   gender   to  your  chatbot   proving  extremely   rich  with   ques tions.   I  find  attributing    such   aspects   to  the  chatbot   import ant  given  how  it  can  affect  how  a  conversation  is  conduct ed   (such   as  feeling   more  trustworth y  of  a  female   voice  assis tant  or  chatbot).   Although,   wha t  I   caution   against  is  attributing   too  much   personality   and  humanity   to  such   AI,  which   can  only    increase the lik elihood of neg ative manipula tion and harm ful emotional a ttachmen t.   Top  5  takeaways  from  our  conversation  with   I2AI  on  AI  in  different  national    contexts   [Original article b y Connor W right]   Overview  :  Can  the  world  unite  under   a  global   AI  regula tory  frame work?   Are  different  cultur al   interpr etations   of  key  terms   a  sticking   poin t?  These   ques tions   and  more  formed   the  basis   of  our   top  5  takeaways  from  our  mee tup  with   I2AI.   With   such   a  variety  of  nations   presen t,  it  shows   that while w e ha ve dif ferent vie ws on v arious issues, this is not a bad thing a t all.    Introduction    Can  the  world  unite  under   a  global   AI  regula tory  frame work?   Can  problems   with   AI  join   together   other   nations   in  a  common   cause?   These   ques tions   form  the  basis   of  the  top  5   takeaways  from  our  mee tup  with   I2AI.   Spanning   topics   like  centralisa tion  and  the  import ance   of   localised   AI  regula tions,   our  mee tup  showed  how  AI  governance   mus t  be  seen   as  a   context-dependen t phenomenon, s tarting with po wer relations.    Ther e are po wer relations a t pla y   Any  talk  about   enacting   localised   regula tions   on  AI  mus t  consider   how  uneven  the  playing  field    is  in  terms   of  decision-making   and  economic   power.  The  extent  to  which   local  governmen ts  can   instantiate  local  laws  depends   heavily  on  the  resour ces  available   to  each   country.  How  this  is   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 5 9   
 conduct ed  is  then   affected  by  the  power  relations   in  the  interna tional   arena.   For  example,   the   attitude   adop ted  to  privacy  laws  could   depend   on  the  relationship   a  nation  has  with   either    China or the US A (tw o opposing vie ws on priv acy).    Centralisa tion ma y be a moot poin t   Given  such   global   diversity,  it  may  be  difficult   for  all  countries   to  follow  one  system  to  which    there  are  even  differences   of  interpr etation  within   countries.   Jamaic a  has  dismissed   digit al  ID   cards  in  the  Caribbean   as  unconstitutional,   but  Barbados   is  still  trying  to  implemen t  them.    Furthermor e,  the  interpr etation  of  compan y  data  storage  laws  in  India   has  a  lot  to  do  with    cultur al  under standing.   As  a  result,   fragmen tation  of  AI  regula tion  may  be  inevitable,   but  is  this   a bad thing?    Fragmen tation isn’t inher ently undesir able    Fragmen tation  doesn’t   mean   that  you  have  incoher ent  pieces.   Peace ful  coexistence   between   the  AI  regula tion  fragmen ts  can  be  moulded,   primarily   through   a  common   thread.   Setting  a   global   target  for  all  to  reach   can  help   direct  all  the  different  appr oaches   towards  the  problem.    Sure,  there  will  be  some   inconsis tencies   in  the  appr oach,   but  arriving   at  the  same   poin t  through    different pathways is undoub tedly a viable op tion.    The import ance of loc al regula tions    To  arriv e  at  the  same   destination,   local  regula tions   and  interpr etations   of  the  issues   in  AI  are   very  import ant.  They  will  serve  to  define  wha t  is  mean t  by  terms   such   as  ‘fair’  and   ‘represen tative’,  as  well  as  proving  the  mos t  accur ate  expression   of  a  country’s  views  on  issues    within   AI.  If  these   were  not  in  place,   individual   countries’   values   and  concerns   would   be  lost  in   the  big-sc ale  legisla tion  conceiv ed  elsewher e.  Without   localised   efforts,   someone   else  ends   up   designing y our AI f or you.   The languag e we speak and the languag e we use    The  import ance   of  these   regula tions   is  mos t  clearly   seen   in  their   relationship   with   languag e.   Even  reading   the  law  in  one  languag e  (say,  German)   can  produce   a  comple tely  different   interpr etation  than   reading   in  English.   With   our  mee tup  spanning   from  South   Americ a  to   Europe,   we  found   that  some   participan ts  harbour ed  different  interpr etations   of  the  same    legisla tion  depending   on  the  languag e  used.   The  subtle  meanings   and  context  of  each   word   chang es thr oughout each languag e, emphasising the vit al role of loc alisa tion e ven mor e.   It  is  not  just  the  languag e  in  which   we  speak   about   AI  that  matters,  but  also  how  we  talk  about    AI.  At  times,   the  AI  vernacular   tends   to  anthropomorphise   the  technology   by  saying  “the   AI   decided”   or  “the   AI  is  thinking ”.  Furthermor e,  such   ways  of  expressing   AI  render s  countries   like   Brazil  (with   barely  any  initia tives  towards  AI)  at  risk  of  the  buzz word  effect  that  AI  gener ates.  For   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 6 0   
 example,   reading   phrases   like  “the   AI  determined   its  course  of  action”   and  immedia tely  thinking    about t ermina tors.   Between the lines    Technology   is  an  excellen t  way  of  demons trating  how  different  countries   treat  their   citizens  and   how  difficult   it  is  to  find  a  common   thread.   Global   convergence   could   indeed   help   us  overcome    problems   such   as  gaps  in  datasets.  With   enough   data  in  all  the  right  places,   resear cher s  would    no  long er  need   to  construct   ‘represen tative  AI’  wher e  the  data  is  mos t  available   but  instead   select   the  mos t  relevant  data.  However,  I  belie ve  that  fragmen tation  of  AI  regula tion  is   guar anteed, and the import ance of loc al regula tory efforts is an essen tial c onsequence of tha t.   Our  Top-5   takeaways  from  our  mee tup  “Protecting   the  Ecosystem:  AI,  Data   and Alg orithms”    [Original article b y Connor W right]   Overview  :  In  our  mee tup  with   AI  Policy   Labs,  we  discussed   AI’s  involvemen t  with   clima te   chang e.  From  the  need   for  corpor ate  buy-in  to  data  centers,  AI’s  role  in  the  fight  can  often  be   confused. Ho wever, it starts with wha t is factual tha t will giv e us the bes t chance of using it.    Introduction    In  partner ship  with   AI  Policy   Labs,  we  discussed   how  AI  is  interconnect ed  with   the  fight  against   clima te  chang e.  The  group  quickly   iden tified   the  role  of  misin forma tion;   the  group  soon   realiz ed   the  need   for  a  collectiv e  and  not  just  individual   effort.  How  this  would   be  achie ved  then   brough t   up  ques tions   regarding   governance   while   the  ever-presen t  problem   of  tangibility   continued   to   plague   efforts  to  fight  the  crisis   potentially .  Wha t  is  import ant  to  note  is  that  knowing   wha t’s   factual is the fir st step of man y in c onfronting this challeng e.   Knowing wha t’s factual    Part  of  the  problem   of  fighting  clima te  chang e  is  comba ting  those   who   deny  there  is  any  fight  at   all,  with   a  worrying  amoun t  of  counter-informa tion  on  clima te  chang e  in  circulation.   The  role  AI   plays  in  this  fight  is  result antly  confused,   for  example,   AI  being   used   to  iden tify  pollution    hotspots and spr ead misin forma tion.    Ther efore,  part  of  the  fight  is  under standing   how  to  detect  misin forma tion  and  how  to  know   when   some thing ’s  factual.   Dem ystifying  clima te  chang e  and  knowing   wha t  is  factual   can  help    iden tify  the  actual   problems,   allowing   us  to  focus   on  each   issue   one  by  one.   The  fight  can  seem    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 6 1   
 overwhelming   at  the  best  of  times,   so  different  people   concen trating  their   efforts  can  help   to   mak e great strides in the ar eas the y choose.    However, this c an’t be done alone.    Efforts a t the individual le vel alone w on’t cut it    Despit e  it  being   a  global   fight,  only   specific   popula tions   and  sectors  are  buying   in.  Corpor ations    are  gener ally  the  mos t  signific ant  contribut ors  to  pollution.   So,  without   their   involvemen t  in   altering   their   habits,   individual   actions   will  become   meaningless.   The  combina tion  of  personal    and  corpor ate  action   (whe ther   a  tech  compan y  or  restaurant)  will  prove  a  potentially   winning    formula.    However, while the c orporat e side has its challenges, so t oo does the individual.    The pr oblem of da ta collection    It  mus t  be  ackno wledg ed  that  even  altering   actions   at  the  individual   level  is  troublesome.   Take,   for  example,   the  Side walk  Labs’  Smart   City  project   in  Toronto.  Striving   to  try  and  create  a   revolutioniz ed  city,  the  data  requir ed  to  do  so  is  deep   and  personal.   Concerns   about   wha t  this   data would in volve and ho w it w ould be s tored w ere key in e ventually s talling the pr oject.    The  kind   of  infrastructur e  needed   for  this  project   in  the  first  place   is  also  noteworth y,  whe ther    physical or r egula tory. Data cen ters ma y provide the ans wer.   Data cen ters   Data  centers  could   be  a  way  to  store  and  shar e  data  to  facilit ate  a  cooper ative  effort  on  the   crisis,   but  this  brings   up  governance   problems.   Any  data  that  leaves  a  country’s  soil  will  involve   relinquishing   at  least  some   control  over  wha t  data  is  accessed   and  used.   Different  countries    have  different  privacy  laws,  and  the  type   of  data  that  one  country  migh t  want  to  collect   may  not   be  possible   in  another .  Even  then,   100%   wifi  reliability   in  both   countries   is  needed   to  keep  the   data collect ed aliv e.   A  theor etical  appr oach   and  futuris tic  consider ations   are  strongly   presen t  in  discussing   clima te   chang e. Yet, this some times g ener ates a pr oblem of t angibility .   The t angibility pr oblem    At  times,   individuals   tend  to  see  clima te  chang e  as  a  theor etical  issue   rather   than   seeing   it  for   its  effects  on  us.  Here,  men tioned   in  the  mee tup  from  a  developer ’s  view,  the  impacts   of  any   non-clima te-chang e-friendly   policies   are  far  remo ved.  Helping   to  solve  this  could   mak e  carbon    footprin ts of particular t echnologies, lik e washing machines, visible.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 6 2   
 Although,   the  next  ques tion  surrounds   whe ther   this  would   influence   a  consumer ’s  decision.    With so man y choices in lif e to mak e, would a c onsumer w ant to be disposed t o mak e another?    Between the lines    In  answer  to  the  previous   ques tion,   individuals   making   choices   are  an  essen tial  componen t  of   the  clima te  chang e  fight.  It  provides   an  opportunity   not  to  allow  clima te  chang e  compliance   to   be  put  on  the  back   burner ,  especially   when   influencing   wha t  products   companies   are  to   produce.   To  facilit ate  this  choice,   AI  needs   to  be  seen   as  the  right  solution,   not  just  another    technologic al  solution   utiliz ed  just  because.   From  my  view,  AI  is  still  early   enough   to  emplo y   these   kinds   of  consider ations   and  with   the  correct  factual   informa tion  shar ed,  these    consider ations c an take a cen tral role in the figh t against clima te chang e.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 6 3   
 Go Deep: Research Summaries   Building Bridg es: Gener ative Artw orks to Explor e AI E thics    [  Original paper   by Ram ya Sriniv asan and De vi Parikh]    [Resear ch Summar y by Ram ya Sriniv asan]    Overview:  The  paper   outlines   some   ways  in  which   gener ative  artw orks  could   aid  in  narrowing    the  communic ation  gaps  between  different  stakeholder s  in  the  AI  pipeline.   In  particular ,  the   author s  argue  that  gener ative  artw orks  could   help   surface  different  ethical  perspectiv es,   highligh t  misma tches   in  the  AI  pipeline,   and  aid  in  the  visualiz ation  of  counterfactual   scenarios,    and non-w estern e thical per spectiv es.   Introduction    A picture is worth a thousand words!    Indeed,   visuals   are  extremely   effectiv e  in  conveying  comple x  concep ts  in  an  accessible    manner—the y  transcend   languag e  barrier s,  simula te  engagemen t,  trigger  critic al  thinking ,  and   leave  lasting  imprin ts  in  the  minds   of  the  observer.  Back ed  by  this  under standing ,  the  author s   posit   that  gener ative  artw orks  (i.e.,   artw orks  created  by  AI  systems)   could   come   handy   in   educ ating  AI  scien tists  with   regards  to  potential  pitfalls  in  the  design,   developmen t,  and   deplo ymen t  of  the  AI  systems.   To  substantiate  their   argumen t,  the  author s  lay  out  four  potential   pathways  in  which   gener ative  artw orks  could   be  leveraged  in  educ ating  AI  scien tists  about   AI   ethics,   namely ,—1)   by  visualiz ations   of  different  ethical  viewpoin ts,  2)  by  visualiz ations   of   misma tches   in  the  AI  pipeline,   3)  by  visualiz ations   of  counterfactual   scenarios,   and  4)  by   visualiz ations of non-w estern e thical per spectiv es.   Key Insigh ts   Here,  a  brief  descrip tion  of  each   of  the  four  aforemen tioned   potential  pathways  (through   which    gener ative artw orks could aid in enhancing AI e thics) is pr ovided.    Visualiz ations   of  different  ethical  perspectiv es  :  Different  ethical  theories   emphasiz e  different   principles   in  decision   making ,  and  can  thus   shed   light  on  varying  viewpoin ts  relevant  in  a  given   context.  For  example,   in  utilit arian   ethics,   the  emphasis   is  on  maximizing   the  well-being   of  all   stakeholder s,  which   is  not  necessarily   the  case  in  deon tologic al  ethics,   wher e  the  emphasis   is  on   following   the  laws  and  regula tions.   Thus,   even  within   the  context  of  a  single   problem   setting,   there  can  be  diverse  viewpoin ts  about   wha t  is  right,  fair,  just,  or  appr opria te.  In  order  to   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 6 4   
 enhance   AI  ethics,   it  thus   becomes   import ant  to  educ ate  AI  resear cher s  and  developer s  about    these   diverse  viewpoin ts  and  thereby  aid  in  reflexive  design.   Gener ative  artw orks  could   serve  as   powerful   visualiz ation  tools  to  surface  such   diverse  perspectiv es.  For  example,   through    gener ative  artw orks,  it  may  be  possible   to  visualiz e  the  compounded   adverse  effects  of  an  AI   decision in an individual’ s life, a c onsequen tialism e thical per spectiv e.   Visualiz ations   of  misma tches   in  the  AI  pipeline   :  Comput ational   systems  involve  quan titatively   modeling   abstract  concep ts  or  constructs   which   may  or  may  not  be  observable.   Furthermor e,   there  may  be  unob servable   factors  that  affect  the  constructs   themselv es.  Consider ,  for  example,    a  construct   such   as  “skill”   or  “ability ”,  which   is  relevant  across  man y  applic ations   such   as  hiring    and  admissions.   These   constructs   can  be  influenced   by  both   inna te  potential  specific   to  the   individual   and  other   factors  such   as  socio-ec onomic   status.  Thus,   a  misma tch  can  be  introduced    even  before  measuring   a  construct.   Gener ative  artw orks  could   aid  in  visualizing   such    misma tches.   For  example,   it  may  be  possible   to  highligh t  differences   in  measur emen t  of  similar    constructs,   thereby  aiding   AI  resear cher s  and  developer s  in  under standing   system  beha vior.   Consider   an  AI  based   hiring   use  case.  Suppose   one  of  the  features  in  making   the  decision    concerns   measuring   social   skills   of  the  candida te.  In  this  regard,  one  migh t  expect   the   constructs   “self-esteem”   and  “confidence”   to  be  related.  Visualiz ations   of  AI  system’s  beha vior   under   different  scenarios   could   reveal  whe ther   it  treats  these   constructs   similarly   –  whe ther   it   exhibits   “convergent  validity ”  ,  which   refers  to  the  degr ee  to  which   two  measur es  of  constructs    that theor etically should be r elated, ar e in f act r elated.   Visualiz ations   of  counterfactuals   :  Gener ative  artw orks  could   also  aid  in  visualizing    counterfactual   situa tions   which   in  turn   can  be  bene ficial   in  reflexive  design   via  empa thy   fostering.   Coun terfactual   thinking   can  help   in  engendering   empa thy  by  enabling   one  to  visualiz e   situa tions   through   another   person’s  world.   Thus,   certain  situa tions   that  may  be  irrelevant  in  one   person’s  context,  but  relevant  in  another   person’s  context,  can  be  under stood  via  such    counterfactual   visualiz ations.   Gener ative  artw orks  could   be  used   as  tools  to  visualiz e  the   consequences   of  AI  decisions   so  AI  resear cher s  and  developer s  (for  instance),   who   may  not   necessarily   be  affected  by  the  decision,   can  empa thize  with   the  impact ed  popula tion,   and   thereby redesign their s ystem f or the be tter.   Visualiz ations   of  non-w estern  perspectiv es  :  Gener ative  artw orks  can  serve  as  visualiz ations   of   social,   cultur al,  and  economic   differences   that  exist  across  geogr aphies.   For  example,   through    gener ative  artw orks  it  may  be  possible   to  highligh t  different  viewpoin ts  regarding   fairness   based    on  the  local  context  such   as  social   practices,   religious   belie fs,  economic   status,  etc.  By  training    gener ative  models   on  data  across  cultur es  and  looking   at  the  latent  visualiz ations,   it  migh t  also   be  possible   to  view  how  different  everyday  practices   (e.g.  dress,  food,   etc.)  and  objects   (e.g.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 6 5   
 furnitur e,  houses,   etc.)  can  vary  across  cultur es  thereby  shedding   light  on  local  contexts  which    can be v aluable in AI s ystem design.    Between the lines    The  ideas   postulated  in  the  paper   offer  promise   in  that  they  can  open   up  new  ways  of  reflexive   design   and  facilit ate  introspection.   Gener ative  artw orks  could   be  especially   bene ficial   in   highligh ting  counterfactual   scenarios—   given  that  such   visualiz ations   may  not  exist  in  the  real   world,   and  thereby  could   shed   light  on  new  and  latent  perspectiv es.  That  said,   for  surfacing    non-w estern  perspectiv es  and  viewpoin ts  based   on  various   ethical  theories,   existing  artw orks   could   also  be  used.   Also ,  as  the  author s  ackno wledg e,  gener ative  artw orks  could   themselv es  be   biased,   so  it  is  necessar y  to  emplo y  these   tools  mindfully .  Ecologic al  costs/environmen tal   impacts   of  gener ative  artw orks  are  however  not  discussed   in  the  paper .  Given  that  gener ating   artw orks  requir es  signific ant  comput ational   resour ces,  there  exists  a  tradeof f  between  the   ecologic al cost and educ ational bene fit, which c alls f or further analy sis.   Brave: wha t it means t o be an AI E thicis t   [  Original paper   by Olivia Gambelin]    [Resear ch Summar y by Connor W right]   Overview  :  The  position   of  AI  Ethicis t  is  a  recen t  arriv al  to  the  corpor ate  scene,   with   one  of  its   key  novelties   being   the  import ance   of  bravery.  Whe ther   taken  seriously   or  treated  as  a  PR  stunt,   alongside the need t o decipher righ t or wr ong is the ability t o be br ave.   Introduction    The  position   of  AI  Ethicis t  is  a  recen t  arriv al  to  the  corpor ate  scene.   Tasked  with   ethical   evalua tions   of  AI  systems,   there  may  be  times   that  the  role  feels  lonely .  Potentially   being   the   only   object or  to  the  deplo ymen t  of  an  AI  product   which   could   earn   your  compan y  a  health y   profit,  no  matter  how  sure  you  are,  is  a  scary  though t.  Hence,   it  is  import ant  to  note  that  the  AI   Ethicis t’s  role  requir es  bravery.  Yet,  the  AI  Ethicis t  is  not  the  only   agent  oper ating  in  the  Ethical   AI space.    AI Ethics is not jus t for the AI E thicis t   An  import ant  distinction   is  how  an  AI  Ethicis t  is  not  the  only   one  who   engages  in  AI  Ethics.   With    AI  stretching   into  multiple   walks  of  life  and  business   practices,   a  sole  AI  ethicis t  would   not  be   able   to  capture  the  different  perspectiv es  needed   to  consider .  Hence,   technologis ts,  data   scien tists,  lawyers,  and  the  public   form  part  of  the  field’ s  multidisciplinar y  nature.  Different   backgr ounds   are  more  suited  to  iden tifying  different  types   of  ethical  risks.  Be  it  a  lawyer   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 6 6   
 iden tifying  a  tricky   definition   used   in  describing   an  AI  system,  or  a  public   member   bringing   up   their vie w of ho w it w ould a ffect their liv es.   To  illustrate  more  clearly ,  an  example   involving   autonomous   vehicles   fits.  While   an  Ethicis t  can   commen t  on  the  traditional   Trolley  Problem,   data  engineer s  mus t  also  under stand  how  to   incorpor ate  its  thinking   into  hard  code.   Not  only   that,  but  consult ation  with   the  broader   public    can  help   under stand  the  broader   requir emen ts  these   vehicles   are  mean t  to  fill,  especially   with    the  older   popula tion.   All  in  all,  just  because   the  AI  Ethicis t’s  job  title  is  closes t  seman tically  to  AI   Ethics doesn’t mean it ’s the sole act or in the space.    The r ole of an AI E thicis t   Nevertheless,   an  AI  Ethicis t  still  has  a  role  to  fill  within   the  field.   The  job  includes   potentially    being   the  only   member   of  a  team   to  veto  an  AI  product   that  could   earn   your  compan y  a  health y   profit.  Whils t  other   team   member s  could   be  “silenced   by  a  profit  margin”,  an  AI  Ethicis t  is   expect ed  to  draw  on  moral  principles   to  help   decipher   wha t  is  right  and  wrong  within   an  AI   context  before  applying   their   deduction   to  concr ete  examples.   The  applic ation  then   needs   to  be   presen ted in an empa thetic manner not t o receiv e de fensiv e responses.    It  is  also  the  AI  Ethicis t’s  responsibility   to  main tain  objectivity   in  ethically  char ged  situa tions    within   this  process.   As  a  result,   the  Ethicis t  may  become   the  default  Gener al  of  assigning    responsibility   when   consult ed  on  the  location  of  potential  ethical  faults   in  an  AI  product.   To  do   this  effectiv ely,  proficiency   in  the  design,   developmen t  and  deplo ymen t  of  the  AI  system  at  hand    is  paramoun t.  This  does   not  mean   that  the  ethicis t  mus t  be  fluen t  in  every  ethical  system  in   existence, but ho w the y mus t be fluen t in their indus trial c ontext.   Part  of  under standing   the  context  lies  in  recognising   both   the  logic al  and  illogic al  inputs   presen t   in  making   a  decision.   Ther e  is  no  poin t  in  simply   appealing   to  logic   when   trying  to  explain   an   illogic al  decision   made,   making   the  quality   of  awareness   of  an  AI  Ethicis t  a  vital  tool.  One  such    example   could   be  how  IBM  released   their   facial   recognition   technology   despit e  the  bias   problems   that  result ed.  Here,  it  doesn’t   help   to  ask  ‘why  did  they  release   a  harm ful  product? ’   but  rather   examine   other   factors  in  the  decision.   Ther e  could’ ve  been   a  lack  of  informa tion   about   the  potential  for  bias,   or  internal   compan y  pressur e  to  release   the  product.   It  is  not  the  AI   Ethicis t’s job t o excuse an y form of indus try beha vior, but t o be sensitiv e to non-logic al factors.   All of this r equir es br avery.   Why bravery is needed    An  AI  Ethicis t  is  to  be  prepar ed  to  walk  into  a  room   wher e  they  only   disagr ee  with   an  AI   proposal.   This  also  means   that  the  AI  Ethicis t  becomes   the  focal  poin t  of  responsibility   when    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 6 7   
 discussing   ethical  decisions   and  may  be  used   as  a  scapeg oat  should   the  product   not  be   launched.   Cases   may  arise   wher e  a  moratorium   results,   placing   the  blame   more  on  socie ty  ‘not   being r eady ’ rather than an AI E thicis t being dif ficult.    However,  policies   that  result   from  a  moratorium   aren’t  guar anteed  to  be  water-tigh t.  Some    procedur es  could   potentially   only   command   the  bare  minimum   for  a  complian t  AI  product   yet   still  leave  room   for  an  AI  Ethicis t  to  give  a  red  light.  It  could   be  that  a  compan y  keeps  the  raw   data  for  an  AI  system  private  to  external   parties   in  one  national   context  (as  manda ted  by  the   law)  but  doesn’t   do  so  in  a  different  space.   So,  while   technic ally  being   complian t,  an  AI  Ethicis t   may  still  need   to  step  in  to  encourage  against  damaging   the  compan y’s  reput ation.   To  do  so,   requir es br avery.   Between the lines    With   the  AI  Ethicis t  position   becoming   more  and  more  prominen t,  certain  qualities   are  requir ed   to  prevent  it  from  becoming   a  mark eting  stunt.  The  paper   claims   that  bravery  is  one  of  them,    and  I  wholeheart edly   agree.  One  thing   that  I  belie ve  can  help   is,  as  men tioned   in  my  last   resear ch  summar y,  being   more  than   one  AI  Ethicis t  involved.  Instead,   boas ting  of  AI  Ethicis ts   dissemina ted  throughout   the  compan y  will  allow  ethical  problems   to  be  picked  up  and  talked   about   far  quick er.  Nevertheless,   every  one  of  these   positions,   no  matter  how  man y  there  are,   will r equir e bravery.   You c annot ha ve AI e thics without e thics    [  Original paper   by Da ve Lauer]    [Resear ch Summar y by Connor W right]   Overview  :  AI  systems  are  often  fixed  by  looking   for  the  broken  part,   rather   than   the  system  that   allowed  the  error  to  occur .  The  paper   advocates  for  a  more  systema tic  examina tion  of  the  AI   process, which the mor e you think about it, the mor e sense it mak es.   Introduction    Would   Aristotle  have  bough t  into  AI  ethics?   Or,  does   AI  ethics   sit  as  a  separ ate  entity  to  all  that   has  gone  before  it?  Given  AI  ethics’   rise  in  popularity ,  it  has  often  been   held   in  its  own  regard,   with   special   men tions   of  AI  principles   at  big  corpor ations   like  Facebook   and  Google.    Nevertheless,   the  answer  to  the  ques tion  ‘can  AI  ethics   exist  in  a  vacuum? ’  is  a  resounding   no.   An  examina tion  of  an  ‘une thical  AI’  problem   needs   to  be  systemic   and  aware  of  the  incen tives   involved  in  the  process,   rather   than   just  looking   for  the  ‘broken  part ’.  Thus,   let’s  first  look   at  why   AI ethics does not e xist in a v acuum, with a c omparison t o medic al ethics along the w ay.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 6 8   
 AI Ethics does not e xist in a v acuum    The  key  notion   that  I  found   in  this  piece   was  how  AI  ethics   could   not  come   about   without   an   ethical  environmen t  to  surround   it.  As  seen   in  medic al  ethics,   the  AI  ethics   space   comes   into   contact  with   a  whole   host  of  issues   also  touched   upon   by  other   fields.   Take,  for  example,   the   issues   of  autonom y  and  moral  responsibility   in  AI  ethics   and  for  the  past  500+   years  of   philosoph y.  Hence,   without   an  all-enc ompassing   ethical  appr oach,   the  subfield   of  AI  ethics    quickly bec omes isola ted and ine ffectiv e.   In  this  sense,   given  AI  ethics’   ties  to  an  overall  ethical  environmen t,  we  need   to  examine   the   system  as  a  whole   when   some thing   goes  wrong  with   an  AI  system.  Here,  systems  thinking   is   introduced   to  men tion  the  relationship s  between  parts   of  a  process/pr oduct   as  being   key,  not   just  individual   parts   themselv es.  In  other   words,  if  an  AI  system  fails,  don’t   examine   its  features;   examine its ec osystem.   The br oken part f allacy    Tying  into  this  last  poin t,  the  “broken  part  fallacy ”  is  introduced.   About   how  humans   examine    problems,   the  fallacy   lies  in  seeing   that  a  system/pr oduct   has  malfunctioned   and  looking   for  the   broken  part  with   which   to  fix  and  resolv e  the  issue.   Such   an  appr oach   deems   the  problem   as   some thing   individualis tic,  which   won’t  necessarily   fix  it  if  it’s  systemic.   Looking   for  a  broken  part   treats  a  systemic   problem   as  too  simple,   given  the  comple x  interactional   nature  of  an   ecosystem.   Hence,   looking   for  a  malfunction   in  an  AI  system  will  not  automa tically  fix  its  problem   of  being    unethical.  Instead,   a  thor ough   look   at  how  that  unethical  beha vior  has  surpassed   the  check s   and  balances   is  requir ed,  especially   surrounding   the  product ’s  deplo ymen t  into  social   and   cultur al contexts.   The import ance of social and cultur al sensitivity    When   examining   the  systemic   nature  of  an  AI’s  deplo ymen t,  more  abstract  notions   are   discovered  that  requir e  chang e  than   a  simple   ‘broken  part ’.  Listening   to  those   closes t  to  the   problem   and  avoiding   top-do wn  legisla tion  is  an  excellen t  first  step.  This  offers  a  closer   look   at   the situa tion fr om those who designed the AI pr oduct, cultiv ating a mor e trus ting r elationship.    The ques tion of incen tives   The  next  ques tion  is  whe ther   businesses   can  enact   this  kind   of  appr oach   and  whe ther   they  are   incen tivised   to.  The  incen tives  created  by  law  and  policy   can  be  a  good  starting   poin t,  examining    whe ther ther e is a legisla tive push behind specific actions tha t can be deemed ‘ ethical’.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 6 9   
 Such   examina tions   can  then   expose   the  type   of  owner ship  within   a  business.   To  illustrate,   Facebook   oper ates  on  an  Absentee  Owner ship  model,   wher eby  the  “locus   of  control  and  locus    of  responsibility   are  different”.  In  Facebook’ s  case,  they  control  wha t  is  allowed  on  their    platform  but  do  not  have  legal  responsibility   for  the  content  that’s  eventually   put  on  there.  In   this  case,  an  AI  ethics   programme   coming   out  of  Facebook   would   not  prosper   without   sharing   in   the  center  of  responsibility .  Instead,   ethical  frame works  are  needed   to  be  part  of  the  compan y’s   ethos   and  not  just  some thing   to  be  check ed  off  the  list.  AI  ethics   can  then   be  a  branch   of  central   ethical pr actices and fr ame works ins tead of holding its o wn f ort.   Between the lines    I  very  much   shar e  how  AI  ethics   is  not  born   in  a  vacuum.   I  liken  it  to  conversations   about   bias  in   AI  systems,   wher eby  if  the  humans   programming   the  AI  product   have  their   own  biases,   then   we   cannot   expect   some   of  these   to  turn   up  in  the  AI  system.  The  aim  is  then   to  mitig ate  the  harm    that  is  produced   from  these   biases   taking   hold.   Applied   to  our  presen t  context,  I  would   not  be   surprised   if  a  compan y  with   a  flawed  ethical  appr oach   created  an’  unethical  AI’.  Without    self-reflection   on  the  AI  process   itself ,  the  reason   why  an  AI  is  producing   the  ‘une thical’   beha vior  that  it  does   will  remain   an  even  dark er  black   box.  Hence,   before  looking   for  the  broken   part, w e should ask our selves ho w it g ot ther e.   Implic ations   of  the  use  of  artificial   intellig ence   in  public   governance:   A   systema tic lit erature review and a r esear ch ag enda    [  Original paper   by Annek e Zuider wijk, Y u-Chen Chen   and F adi Salem]    [Resear ch Summar y by Angshuman K aushik]    Overview  :  The  expanding   use  of  Artificial   Intelligence   (AI)  in  public   governance   worldwide,   has   not  only   opened   up  new  opportunities,   but  has  also  created  challeng es.  This  paper   mak es  a   systema tic  review  of  existing  literature  on  the  implic ations   of  the  use  of  AI  in  public   governance,    and ther eafter, develop s a resear ch ag enda.    Introduction    Ther e  is  no  denying  the  fact  that  AI  has  been   used   for  quite  some   time   now,  and  its  use  has   result ed  in  both   positiv e  and  negative  outcomes.   Further ,  considering   its  scope,   AI  is  a   multidisciplinar y  area  of  resear ch,  rich  with   a  vast  number   of  paper s  pertaining   to  its  myriad    applic ations.   Within   that  extensiv e  gamut,   the  emphasis   of  this  paper   is  on  the  literature  that   addr esses   the  effects  of  the  uses   of  AI  in  the  public   governance   setting.   This  paper   narrows   down  its  focus   on  the  articles   that  resear ch  on  the  implic ations   of  AI  in  the  context  of  public    adminis tration,   digit al  governmen t,  manag emen t,  informa tion  science   and  public   affairs.  It  deals    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 7 0   
 with   the  issues   relating  to  fairness,   bias  and  governance   ques tions   pertaining   to  transpar ency ,   and  regula tory  frame works.  For  instance,   how  does   the  implemen tation  of  specific   AI   technologies   affect  accountability   of  governmen t  institutions?   As  far  as  the  resear ch  appr oach    is  concerned,   the  resear cher s  conduct ed  a  systema tic  literature  review  of  the  relevant  material.    After  an  extensiv e  review,  the  resear cher s  found   a  number   of  potential  bene fits  and  challeng es   relating t o the use of AI in public g overnance enumer ated in them.    They  iden tified   the  bene fits  in  nine   categories:   1)  efficiency   and  performance   bene fits,  2)  risk   iden tification  and  monit oring   bene fits,  3)  economic   bene fits,  4)  data  and  informa tion  processing    bene fits,  5)  service  bene fits,  6)  bene fits  for  socie ty  at  large,  7)  decision-making   bene fits,  8)   engagemen t  and  interaction   bene fits,  and  9)  sustainability   bene fits.  In  addition   to  the  potential   bene fits,  they  also  iden tified   eigh t  challeng es  of  the  use  of  AI  in  their   literature  review,  which    are  divided   into  eigh t  categories:   1)  data  challeng es  2)  organizational   and  manag erial   challeng es   3)  skills   challeng es  4)  interpr etation  challeng es  5)  ethical  and  legitimacy   challeng es  6)  politic al,   legal and policy challeng es 7) social and socie tal challeng es, and 8) ec onomic challeng es.   Use of AI in public g overnance    One  example   relating  to  the  applic ation  of  AI  in  the  governance-se tting  is  the  use  of  SyRI   (“System  Risk  Indic ation”)   by  the  Dutch  Governmen t  to  detect  possible   social   welfare  fraud.   It   had  not  only   issues   with   transpar ency   and  a  host  of  other   factors,  but  the  algorithm   also  turned    out  to  be  a  ‘black   box’.  Its  oper ation  was  eventually   brough t  to  an  end  by  the  court   for  viola ting   Article   8  of  the  European   Convention  on  Human   Righ ts  (ECHR),   which   protects  the  right  to   respect   for  private  and  family   life.  The  requir emen t  of  Article   8  is  that,  any  legisla tion  should    strike  a  ‘fair  balance’   between  social   interests  and  viola tion  of  the  private  life  of  the  individuals.    (The   intention  of  citing   this  particular   example   is  not  to  portr ay  the  dele terious   effect  of  AI,  but   to  show  the  applic ation  of  AI  in  governance,   in  gener al.)  Ther e  are  numer ous  such   other   cases,    with bene ficial out comes pert aining t o the use of AI in v arious sect ors of the g overnmen t.   Potential bene fits   The  use  of  AI  in  governance   has  massiv e  implic ations   for  socie ty,  in  gener al  and  individuals,   in   particular .  The  reason   being ,  the  adminis tration,   and  its  various   functionalities   have  to  directly    deal   with   the  masses,   within   their   respectiv e  spher es  of  jurisdiction.   Through   this  paper ,  the   resear cher s  have  uncovered  their   findings   with   respect   to  a  compr ehensiv e  review  of  26  articles    pertaining   to  the  use  of  AI  in  public   governance,   which   were  published   in  the  last  3  years.  After   analy zing  the  content  of  the  articles,   the  resear cher s  found   that  they  contained   a  number   of   potential  bene fits  of  the  use  of  AI  in  public   governance.   It  was  found   that  efficiency   is  impr oved   by  automa ting  processes   and  tasks  or  by  simplif ying  processes   using   machine   learning.   Further ,   AI  aids  in  increasing   monit oring   of  urban   areas,  fraud  detection,   law  enforcemen t  and   enhancing   the  ‘smartness’   of  the  cities.   The  resear cher s  also  noticed   that  AI  for  public    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 7 1   
 governance   leads   to  economic   bene fits,  such   as  making   e-governmen t  services   and  systems   more  economic al.  Mor eover,  data  and  informa tion  processing   bene fits  also  accrue   due  to   processing   of  large  amoun ts  of  data  in  limit ed  time.   Another   area  wher e  AI  has  a  potential   positiv e  impact   with   respect   to  public   governance   is  that  it  leads   to  impr ovemen t  in  the  quality    of  public   services.   It  also  leads   to  the  creation  of  public   value,   decision-making   and   sustainability bene fits.   Challeng es   Alongwith   the  potential  bene fits,  the  resear cher s  also  sear ched   for  the  challeng es  of  AI  use  in   governmen t,  in  the  review  of  the  26  articles   zeroed  in  on  by  them.   They  iden tified   challeng es   relating  to  the  availability   and  acquisition   of  data,  organizational   resistance   to  data  sharing ,   limit ed  in-house   talen t,  comple xity  in  interpr eting  AI  results,   ethical  challeng es,  undermining    the due pr ocess of la w and e ffect on the labor mark et.   Going F orward – The R esear ch Ag enda    After  an  analy sis  of  the  various   potential  bene fits  and  challeng es,  the  resear cher s  put  forward  a   resear ch  agenda   on  the  implic ations   of  the  use  of  AI  for  public   governance.   It  comprises   eigh t   process-r elated  recommenda tions   and  seven  content-related  recommenda tions   for  resear cher s   that examine the implic ations of AI use in public g overnance.    Process-r elated resear ch recommenda tions    ●  Avoid applying AI-r elated terms superficially in public g overnance sour ces   ●  Move be yond the g eneric f ocus on AI in public g overnance sour ces   ●  Move to me thodologic al div ersity ins tead of dominan t qualit ative me thods    ●  Expand c oncep tual and pr actice-driv en resear ch fr om the priv ate to the public sect or   ●  Increase empiric al resear ch on the implic ations of AI use f or public g overnance    ●  Go be yond e xplor atory resear ch and e xpand e xplana tory resear ch   ●  Openly   shar e  the  resear ch  data  used   for  studies   on  the  implic ations   of  the  use  of  AI  for   public g overnance    ●  Learn   from  applic able   pathways  followed  by  digit al  governmen t  scholar ship  in  its  early    phases    Content-related resear ch recommenda tions    ●  Develop   AI  public   governance   scholar ship  from  under -theoriz ation  into  solid,    multidisciplinar y, theor etical founda tions    ●  Investigate  effectiv e  implemen tation  plans   and  metrics   for  governmen t  strategies   on  AI   use in the public sect or   ●  Investigate bes t practices in managing the risk s of AI use in the public sect or   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 7 2   
 ●  Examine   how  governmen ts  can  better  engage  with   and  communic ate  their   AI  strategic   implemen tation plan t o stakeholder s   ●  Investigate a lar ge div ersity of possible g overnance modes f or AI use in the public sect or   ●  Resear ch  how  the  performance   and  impact   of  public   sectors’  AI  solutions   can  be   measur ed   ●  Examine the impact of sc aling up AI usag e in the public sect or.   Between the lines    Although   the  paper   has  dealt   with   the  subject   very  thor oughly ,  this  is  only   the  starting   poin t  of  a   ‘journe y  of  learning ’,  which   necessit ates  an  iterative  appr oach,   involving   relevant  stakeholder s.   The  findings   matter,  as  it  would   enable   the  various   actors  involved  in  the  process   to  have  a   better  under standing   of  the  issue   in  hand,   and  take  appr opria te  steps,  in  the  right  direction,    going   forward.  In  my  view,  further   deep   dives  into  the  applic ation  of  AI  in  public   governance   at   the  grassroots   level,  through   case  studies,   will  yield   specific   insigh ts.  It  deser ves  men tion  here   that  different  cultur es  perceive  AI  and  its  outcomes   in  a  different  manner .  Hence,   more  in-dep th   resear ch,  keeping   in  mind   the  cultur al  sensitivities,   tastes  and  habits   of  different  communities,    would   definitely,  bring   about   a  new  flavor  to  the  ever-growing   field   of  AI,  and  its  applic ation,    particularly , in the field of g overnance.    Animism, Rinri, Moderniz ation; the Base of Japanese R obotics    [  Original paper   by Naho Kit ano]    [Resear ch Summar y by Connor W right]   Overview  :  Technology   is  not  going   anywher e  anytime   soon,   so  why  not  respect   it  for  wha t  it  is?   The  appr oach   adop ted  by  Japanese   cultur e  is  to  recogniz e  how  natural  and  technologic al   phenomena   have  a  soul  that  intertwines   with   ours.  The  result:   a  beautiful   sight  of   human-t echnologic al relations indeed.    Introduction    Have  you  ever  felt  your  soul  harmoniously   intertwined   with   a  technology   you  are  using?   As  part   of  the  Japanese   governmen t’s  aims   for  techno-in tegration,   the  Japanese   tradition   appeals   for   harmonious   integration  for  the  bene fit  of  socie ty.  While   the  mystical  elemen t  of  this  appr oach   is   notable,   it  creates  a  beautiful   sight  of  wha t  human-t echnology   relationship s  can  look   like.   Technology is no w her e to stay, so w e ma y as w ell start of f on a positiv e not e.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 7 3   
 The r obotiz ation of socie ty is pr esen ted in a positiv e ligh t   To  appr opria tely  set  the  scene,   the  robotiz ation  and  technologiz ation  of  socie ty  are  seen   as  a   positiv e  in  Japanese   cultur e.  Interestingly ,  the  chang e  is  seen   as  so  much   of  a  positiv e  that  the   efforts  placed   on  ethical  consider ations   are  not  too  deep.   For,  it  is  a  cultur al  assump tion  that   robots   will  be  designed   to  keep  the  socie ty’s  ethical  values   anyways.  One  way  to  explain   the   Japanese people’ s confidence is thr ough e xploring the r ole of spirit.    The e xistence of spirits    A  strong  Japanese   cultur al  belie f  resides   in  how  natural  phenomena   have  spirits.   Traditionally ,   this  applied   to  the  sun,  moon   and  moun tains,   which   had  their   own  spirits   and  associa ted  Gods.    Subsequen tly,  each   God  had  a  name   and  was  assigned   char acteristics  while   having  perceived   control  over  natural  events.  Thank s  to  the  technologic al  revolution,   the  belie f  was  expanded   to   artificial   objects,   which   are  belie ved  to  have  souls   in  harmon y  with   those   of  humans.   Such   belie f   consequen tly  affects  how  the  Japanese   people   interact  with   these   objects,   pleasan tly  seen    when applied t o tools.    How this applies t o tools    Artificial   tools  made   out  of  natural  and  unna tural  phenomena   possess   anima   (a  soul).   When   in   contact  with   humans,   these   objects   are  seen   to  work  in  tandem.   Both   the  human   and  tool   anima   work  harmoniously   together.  The  relationship   runs   deep,   seeing   as  tools  are  often   companions   for  life,  leading   to  them   bearing   names.   Result antly,  the  tools  were  traditionally    inscribed   with   the  owner ’s  name   and  its  date  of  first  use  when   it  took  on  its  anima   through    coming in to contact with humans.    The  relationship   between  the  human   and  their   tool  is  respect ed  even  after  it  is  no  long er  in  use.   Even  today,  tools  that  break  are  not  thrown  away  but  taken  to  a  temple   to  be  burned   divinely .  A   sign  of  respect   to  how  intertwined   the  instrumen t  had  become   with   its  human.   For  example,   in   2005, T msuk Co. L td. took their r obot cr eation KIY OMORI t o the shrine t o pray for its success.    However, wha t is the bene fit of all of this?    Why talk about spirituality?    It  is  import ant  to  ackno wledg e  that  the  spirituality   men tioned   is  not  to  advocate  for  a  tool’s   subjectivity ,  but  to  show  how  it  relates  to  its  owner .  It  takes  on  and  bear s  a  spiritual   connection    with   its  human   owner   from  the  first  minut e  it’s  used,   forming   the  basis   of  the  Japanese   “Rinri ”   (“Ethics”).    “Rinri ”  is  the  study   of  achie ving  harmon y  in  human   relationship s,  offering   a  guide   on  forming    and  main taining   lasting  human   relationship s  with   the  natural  phenomena   surrounding   us.  Each   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 7 4   
 individual   has  a  responsibility   to  the  social   wellbeing   of  their   community .  For  example,   in  2004,    Mr.  Koda  apologiz ed  for  disturbing   the  social   peace   and  causing   harm   to  the  people   of  Japan    due  to  the  diploma tic  storm  created  with   his  traveling   to  Iraq.  Guided   by  the  attitude   of  “social    harmoniz ation  over  the  individual   subjectivity ”,  the  spirituality   of  Japanese   cultur e  aims   to   foster  a  lasting  relationship   with   technology .  We  see  an  interconnect ed  reliability   on  one   another; the t ool on the human f or its anima and the human on the t ool f or the t ask a t hand.    This is a w onderful sigh t if y ou ask me.    Between the lines    Thank s  to  my  links  with   Ubun tu  philosoph y  (see  previous   resear ch  summar y  and  panel    discussion),   I’m  a  fan  of  the  interconnect edness   and  community -orien tated  appr oach   offered.  I   think   relating  to  technology   in  this  way  readjus ts  how  we  are  to  use  it,  namely   for  the  bene fit  of   the  community   we  live  in.  While   there  are  warranted  concerns   about   wha t  kind   of  relationship    deriv es  from  this  interconnect edness   (such   as  sexual),   grounding   the  action   in  respect   is  the  way   to  go.  By  respecting   the  technology   for  wha t  it  is  and  can  do  for  us,  we  can  better  learn   how  to   develop this r elationship with other s.   Who   is  afraid  of  black   box  algorithms?   On  the  epistemologic al  and  ethical   basis of trus t in medic al AI    [  Original paper   by Juan Manuel Dur án & K arin R olanda   Jongsma]    [Resear ch Summar y by Marianna Ganapini]    Overview  :  The  use  of  AI  in  medicine   promises   to  advance   this  field   and  help   practitioner s  mak e   faster  and  more  accur ate  diagnosis   and  reach   more  effectiv e  decisions   about   patient’s  care.   Unfortuna tely,  this  technology   has  also  come   with   a  specific   set  of  ethical  and  epistemologic al   challeng es.  This  paper   aims   at  shedding   some   light  on  these   issues   and  providing   solutions   to   tackle   the  problems   connect ed  to  using   AI  in  clinic al  practice.   We  ultima tely  concur   with   the   author s  of  the  paper   that  medic al  AI  cannot   and  should   not  replace   physicians.   We  also  add  that   a  trustworth y  AI  will  probably   lead   to  more  trust  among   humans   and  increase   our  reliance   on   experts.   Thus,   we  propose   that  we  start  looking   at  the  ques tion:   under   wha t  conditions   is  an  AI   system c onduciv e to mor e human-t o-human trus t?   Introduction    The  use  of  AI  in  medicine   promises   to  advance   this  field   and  help   practitioner s  mak e  faster  and   more  accur ate  diagnosis   and  reach   more  effectiv e  decisions   about   patient’s  care.  Unfortuna tely,   this  technology   has  also  come   with   a  specific   set  of  ethical  and  epistemologic al  challeng es.  The   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 7 5   
 epistemologic al  challeng es  are  specific ally  connect ed  to  the  opacity   of  the  so-called   “black -box   algorithms”:   “black   boxes  are  algorithms   that  humans   cannot   survey,  that  is,  they  are   epistemic ally  opaque   systems  that  no  human   or  group  of  humans   can  closely   examine   in  order   to  determine   its  inner   states”.  The  problem   is  that  these   algorithms   mak e  assessmen ts  in  a  way   that  is  opaque   to  both   their   designer s  and  the  physicians   using   them   because   it  seems    impossible t o kno w ho w the alg orithms c ame t o their c onclusions.    The  challeng es  that  this  epistemic   opaqueness   poses   are  both   epistemic   (are  these   algorithms    in  fact  reliable?)   and  ethical  (are  these   algorithms   ethical,  e.g.  fair,  respectful   of  human    autonom y?).  Both   of  these   challeng es  touch   on  the  issue   of  warranted  trust  in  AI:  if  I  can’t  check    whe ther an alg orithm is trus tworth y (reliable & e thical), is trus ting it e ver permissible?    Even  though   this  is  not  some thing   the  author s  poin t  out,  it  is  worth   noticing   that  ‘trus t’  is   already   a  loaded   term:   so  let’s  unpack   it  a  little  bit.  Say,  an  agent  A  can  be  said  to  trust  B  on   some   issue   Y  if  A  is  willing   to  do  at  least  one  of  the  following:   (i)  A  comes   to  belie ve  wha t  B  says   about   Y  and  (ii)  A  uses   wha t  B  says  about   Y  as  a  sufficien t  reason   for  reaching   a  specific   decision    (e.g.  making   a  certain  diagnosis).   Though   we  don’t   emplo y  the  same   terminology ,  I  belie ve  the   author s  of  the  paper   would   agree  that  (i)  and  (ii)  are  not  the  same   thing:   (i)  is  wha t  we  can  call   ‘doxastic  trust’  and  (ii)  is  ‘pragma tic  trust’  (note:  the  norma tive  standar ds  for  doxastic  trust   migh t not be the same as f or pr agma tic trus t).   We  are  now  in  a  position   to  reformula te  the  ques tion  of  the  paper:   When   is  it  permissible   for  a   physician   to  pragma tically  trust  a  black   box  algorithm?   The  author s’  answer  is:  even  if  the   algorithm   is  reliable,   wha t  it  says  should   rarely  be  used   as  a  sufficien t  reason   to  mak e  a   diagnosis,   prescribe   a  cure  and  so  on.  The  algorithms’   recommenda tions   need   to  be  interpr eted   by the ph ysician’ s kno wledg e and under standing of the c ontext and situa tion of the pa tient.   Key Insigh ts   To  answer  the  ques tion  above  the  author s  of  the  paper   look   at  the  relationship   between   transpar ency and opacity in black bo x alg orithms.    Transpar ency   “refers  to  algorithmic   procedur es  that  mak e  the  inner   workings   of  a  black   box   algorithm   interpr etable   to  humans.   To  this  end,   an  interpr etable   predict or  is  set  out  in  the  form   of  an  exogenous   algorithm   capable   of  making   visible   the  variables   and  relations   acting   within    the black bo x alg orithm and which ar e responsible f or its out come. ”   Opacity   “focuses   on  the  inher ent  impossibility   of  humans   to  survey  an  algorithm,   both    under stood as a scrip t as w ell as a c omput er pr ocess. ”   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 7 6   
 Relation be tween tr anspar ency and opacity:    “designing   and  programming   interpre table   predict ors  that   offer  some   form  of  insigh t  into  the   inner   workings   of  black   box  algorithms   does   not  entail  that   the  problems   posed   by  opacity   have    been   answered.   To  be  more   precise,   transparency   is  a  methodology   that   does   not  offer  sufficien t   reasons   to  belie ve  that   we  can  reliably   trust  black   box  algorithms.   At  best,  transparency    contribut es  to  building   trust  in  the  algorithms   and  their   outcomes,   but  it  would   be  a  mistake  to   consider it as a solution t o overcome opacity alt ogether.”   The  author s  are  arguing   here  that  transpar ency   is  not  the  solution   to  the  problems   of  an   opaque   AI:  it  migh t  be  part  of  the  solution,   but  it  is  not  enough.   Wha t  is  the  missing   piece?    Ensuring our blackbo x AI is r eliable.    Solution   :  as  part  of  the  solution   the  author s  adop t  comput ational   reliabilism   (CR).   As  the   author s  put  it,  “CR  states  that  resear cher s  are  justified   in  belie ving  the  results   of  AI  systems   because   there  is  a  reliable   process   (ie,  the  algorithm)   that  yields,   mos t  of  the  time,    [correct/accur ate]  results. ”  They  provide   some   insigh ts  on  how  reliability -assessmen ts  should   be   made   in  the  context  of  blackbo x  algorithms   by  discussing   some   reliability -indic ators  (e.g.   verific ation,   expert   knowledg e,  transpar ency).   These   reliability -indic ators  are  still  quite  unclear ,   though.    However,  the  key  poin t  is  that  doxastically  trusting  AI  migh t  not  be  enough   to  justify  acting   on  it,   as  we  men tioned   earlier .  This  is  a  contextual  matter:  wha t  constitut ed  enough   reason   for  acting    may  vary  given  the  context  and  wha t  is  at  stake.  This  could   mean   two  things:   one  has  to  do  with    the  fact  that  epistemic   standar ds  for  pragma tic  trust  may  be  more  string ent  than   for  doxastic   trust.  The  second  has  to  do  with   the  fact  that  reliability   is  just  one  among   the  factors  that  mak e   AI  trustworth y:  we  need   to  mak e  sure  AI  is  also  ethical  (e.g.  fair)  before  acting   on  its   assessmen ts  and  predictions.   The  author s  explain   that  “if  recommenda tions   provided   by  the   medic al  AI  system  are  [doxastically]   trusted  because   the  algorithm   itself   is  reliable,   these   should    not  be  followed  blindly   without   further   assessmen t.  Instead,   we  mus t  keep  humans   in  the  loop    of decision making b y alg orithms. ”   In  other   words,  even  if  consider ed  reliable,   an  algorithm   should   be  rarely  used   as  the  only    reason   for  reaching   a  decision   in  clinic al  practice.   “It  follows  that  it  is  unlik ely  and  undesir able    for alg orithms t o replace ph ysicians alt ogether.”   Between The Lines    The  author s  of  this  paper   rightly  argue  that  given  wha t  is  at  stake,  (pragma tic)  trust  in  medic al   blackbo x  algorithms   is  rarely  justified.   Practitioner s  and  doct ors  still  provide   the  necessar y   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 7 7   
 experience,   reliability   and  commitmen t  for  patients  to  trust  their   decisions   and  diagnosis.   That   is,  patients  should   trust  doct ors  not  algorithms.   Doct ors  may  trust  algorithms   to  form  belie fs  but   should not base their decisions only on wha t those alg orithms sa y.   As  a  result,   I  belie ve  we  need   to  focus   our  attention  on  how  AI  can  be  trust-conduciv e:  experts    that  rely  on  a  robus t,  ethical  and  help ful  AI  are  also  themselv es  more  trustworth y.  Doct ors  that   rely  on  a  trustworth y  AI  system  will  themselv es  be  and  be  perceived  as  more  skillful,    experienced   and  reliable.   Hence,   AI  does   not  replace   physicians:   a  trustworth y  AI  is  conduciv e   to  more  and  better  trust  among   humans   and  will  probably   mak e  us  rely  on  our  experts   even   more.  So  from  now  let’s  ask  the  following   ques tion:   under   wha t  conditions   is  an  AI  system   conduciv e to human-t o-human trus t?   Anthropomorphic in teractions with a r obot and r obot -like agent   [  Original paper   by  Sar a Kiesler , Aar on P owers, Susan   R. Fussell, and Cris ten Torrey]   [Resear ch Summar y by Connor W right]   Overview  :  Would   you  be  more  comfortable   disclosing   personal   health   informa tion  to  a  physical   robot   or  a  chatbot?   In  this  study ,  anthropomorphism   may  be  seen   as  the  way  to  interact  with    humans, but mos t definitely not in acquiring per sonal in forma tion.    Introduction    Would   you  be  more  comfortable   disclosing   personal   health   informa tion  to  a  physical  robot   or  a   chatbot?   Explor ed  in  this  study   is  whe ther   a  humanlik e  robot   solicits   stronger  anthropomorphic    interactions   than   just  a  chatbot.   With   both   physical  presence   and  physical  distance   measur ed,   the  anthropomorphised   robot   wins   the  interaction   race  hands-do wn.  However,  when   it  comes    to acquiring the medic al informa tion, the an thropomorphic s trategy lea ves much t o be desir ed.   Setting the scene    The  main   actors  of  the  study   were  a  physically  embodied   robot,   the  same   robot   project ed  onto   a  screen,   a  softw are  agent  (like  a  chatbot)   on  a  comput er  next  to  the  participan t  and  a  softw are   agent  project ed  onto  a  big  screen  farther   away.  From  there,  four  scenarios   were  set  out  (p.g.   172):    ●  The participan t interacts with a ph ysically pr esen t and embodied r obot.    ●  The participan t communic ates with the same r obot, but it is pr oject ed on a big scr een.    ●  The participan t eng ages with a softw are agent on a nearb y lap top.   ●  The participan t converses with the softw are agent on the further a way  big scr een.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 7 8   
 Two hypotheses w ere proposed    ●  The  participan ts  will  interact  and  thus   anthropomorphise   the  physically  embodied   robot    more  so  than   the  softw are  agent.  However,  they  won’t  disclose   as  much   personal    informa tion t o the embodied ag ent.   ●  The  participan ts  will  interact  and  thus   anthropomorphise   a  softw are  agent  on  a   comput er mor e than a r obot pr oject ed on to a big scr een.    The  instructions   for  the  discussion   men tioned   how  the  goal  was  to  “have  a  discussion   with   this   robot   about   basic   health   habits. ”  (p.g.  173).   Once   carried   out,  the  first  conclusion   drawn  was  on   the import ance of embodimen t.   Robot embodimen t is k ey   The  participan ts  interacted  with   the  embodied   robot   a  lot  more  than   the  social   agent.  Not  only    that,  but  it  ranked  top  of  all  the  robot   trait  ratings   (such   as  trustworthiness   and  compe tency ,  see   the t able on p. g. 178).    In  addition,   the  softw are  agent  was  not  seen   as  a  “real”  robot.   The  participan ts,  of  course,  had   their   own  preconcep tions   about   how  the  robot   was  to  look,   with   some   being   left  disappoin ted   when f aced with a softw are agent.   The embodied ag ent vs. the softw are agent   Alongside   the  superior   level  of  interaction,   the  first  hypothesis   was  confirmed   by  how  the   participan ts  did  disclose   less  to  the  physical  robot   than   the  softw are  agent.  Instead,   the   softw are  agent  was  viewed  more  as  an  adminis trative  process   that  simply   requir ed  personal    informa tion,   which   participan ts  were  more  comfortable   giving.   While   the  softw are  agent  may   have  suffered  in  lacking   human   interaction,   this  proved  bene ficial   in  acquiring   the  desir ed   medic al informa tion.    The dis tance f actor   About   the  physical  distance   between  the  participan t,  the  physical  robot   and  the  softw are  agent   did  not  differ.  The  variation  in  engagemen t  time   between  having  the  robot   and  softw are  agent   project ed  and  not  project ed  was  negligible.   Hence,   the  study ’s  second  hypothesis   was  proved   false.    Between the lines    While   the  physical  robot   was  more  anthropomorphised,   it  was  still  not  seen   as  a  fully   human    interlocut or.  Participan ts  men tioned   how  the  robot,   at  times,   wasn’t   flexible   and  interrup tible    enough   for  an  entirely  natural  conversation  to  flow.  Furthermor e,  the  higher   level  of   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 7 9   
 anthropomorphisa tion  did  not  immedia tely  lead   to  a  sufficien t  level  of  trust  to  disclose   personal    health   informa tion.   Hence,   while   anthropomorphisa tion  does   gener ate  increased   human    interaction, it does not na turally f ollow tha t we trus t the t echnology .   Ubun tu’s Implic ations f or Philosophic al Ethics    [  Original t alk  by Thaddeus Me tz]   [Resear ch Summar y by Connor W right]   Overview  :  Philosopher s  have  been   puzzled   over  sear ching   for  an  underlying   principle    expounded   by  a  moral  theor y  for  over  400  years.  Through   his  talk,  Thaddeus   Metz   demons trates ho w Ubun tu is also w orth c onsidering in the journe y to solving this puzzle.    Introduction    Thaddeus   Metz  aims   to  demons trate  how  Ubun tu  look s  when   construed   as  a  moral  theor y.  The   goal  is  not  to  show  Ubun tu  being   ‘better’  when   compar ed  to  other   moral  theories   but  rather   as   a  perspectiv e  worth y  of  consider ation.   With   the  slogan  of  “a  person  is  a  person  through   other    persons” ,  we  shall   explor e  wha t  Ubun tu  construed   as  such   entails  and  how  this  is  applied   to   different  situa tions.   The  Utilit arian   and  Kantian  views  are  explor ed  as  comparisons,   with   the   path tha t Ubun tu utiliz es to arriv e at similar c onclusions pr oving particularly in teresting.    Key Insigh ts   Ubun tu is fir st represen ted as a mor al theor y. To be the c ase, it mus t offer the f ollowing:    ●  A compr ehensiv e acc ount of righ t and wr ong.    ●  A specific ation of wha t all immor al actions ha ve in c ommon.    ●  A reduction of v arious duties do wn t o jus t one.    Interpr eting Ubun tu as such has the f ollowing bene fits:   ●  Having a fundamen tal principle in philosoph y would be super in teresting.    ●  Having  an  underlying   ethical  principle   can  also  be  used   to  solve  controversial  issues   (like   abortion and the dea th penalty).    ●  Hence,   the  ques tion  becomes   how  we  migh t  draw  on  indig enous   Afric an  though t  to   construct a mor al theor y?   Figur es  such   as  Archbishop   Desmond   Tutu,  Professor   Gessler   Mux e  Nkondo   and  Justice  Yvonne    Mokg oro  have  commen ted  on  Ubun tu.  Here,  they  men tion  Ubun tu’s  emphasis   on  being    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 8 0   
 gener ous,  hospit able,   holding   a  commitmen t  to  the  community   and  towards  sympa thetic  social    relations   as  basic   tenets  of  the  moral  theor y.  As  a  result,   Metz  suggests  the  following   two   guidelines:    A real per son bec omes so thr ough r especting other s’ capacity t o relate harmoniously .   An act is wr ong if and only if it f ails t o honor those tha t commune or be c ommuned with.    However,  wha t  is  a  communal   relationship,   and  how  do  you  relate  communally?   A  communal    (harmonious)   relationship   includes   two  different  strands:   Iden tity  and  Solidarity .  Iden tity  is  a   sense   of  togetherness   and  coordina tion.   Solidarity   (caring   for  someone   else’ s  quality   of  life)   includes s ympa thetic altruism.    Three corollaries of Ubun tu as a mor al principle f ollow to pur sue the harmonious r elationship:    ●  You mus t avoid tr eating people in the opposit e way to harmon y; ther e is no us v s them.    ●  You  mus t  go  out  of  your  way  to  relate  communally   (exhibit   iden tity  and  solidarity)   and   emphasiz e another per son’s dignity b y allo wing them t o iden tify communally .   ●  Prioritiz e  main taining   ties  with   people   you  already   have  a  relation  with,   rather   than    strangers.   Following   these   steps  leads   to  a  communal   relation.   To  fully   manif est  this,   we  can  expect   to  see   actions lik e those lis ted belo w:   ●  Appealing   to  consensus   –  everyone  sits  together   until  a  solution   is  reached   –  necessar y   conditions   for  a  just  way  of  going   forward.  COnsensus   =  no  split   between  majority   and   minority .   ●  Collectiv e  labor   –  everyone  gathers  to  help   one  another   harvest  from  plot  to  plot.    Mutual aid f or one another ’s sak e.   ●  Reconcilia tion  –  rather   than   punishmen t  that  seek s  to  confine,   punishmen t  that  aims   to   reconcile   differences   is  pursued,   like  with   the  South   Afric a  Truth   and  Reconcilia tion   Commission.    ●  Mor al value a ttribut ed to tradition, ritual and cus tom.   To  show  Ubun tu  as  a  moral  theor y  in  action,   Metz  draws  on  examples   from  two  different  forms    of  consider ations.   Here,  a  basic   intuition   is  explor ed  through   the  Utilit arian,   Kantian  and  Ubun tu   views,  allowing   us  to  see  how  each   differs.  In  this  sense,   Ubun tu  entails  the  same   kind   of   intuition   as  other   Western  theories,   but  for  different  reasons.   To  demons trate,  I  have  select ed   the  mos t  personally   interesting  examples   from  each   section   and  listed  how  Ubun tu  differs  from   the other tw o vie ws explor ed.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 8 1   
 The fir st comparison: whe ther t o figh t poverty:    ●  The  intuition   :  it  is  unjus t  for  the  extremely   wealth y  not  to  help   out  those   who   are  poor    due t o cir cums tances out of their c ontrol.   ●  Ubun tu  :  poverty  is  unjus t  because   the  poor   now  have  nothing   to  give  to  other s,  rather    than   the  harm   it  does   to  the  individual   (Utilit arianism)   or  because   the  poor   are  less  free   to choose (K antianism).    The sec ond sort of c omparison: whom t o rescue fr om dea th:   ●  The  Intuition   :  when   having  to  choose   between  a  young   adult   stranger  and  your  mother ,   you should sa ve your mother ins tead of a s tranger.   ●  Ubun tu  :  the  long-s tanding   communal   tie  with   your  Mum   means   you  should   save  her,   rather   than   the  stranger.  A  utilit arian   would   advocate  for  saving  the  stranger  as  they   probably   take  up  less  resour ces  and  the  Kantian  would   advocate  for  randomizing   on  who    to save, seeing as their dignity is equal.    Between the lines    While   it  certainly   proves  controversial  at  times   to  say  that  one  moral  theor y  is  ‘outrigh tly’  better   than   another ,  it  is  certainly   less  so  to  say  one  is  worth y  of  consider ation.   I  think   Metz  does    exceptionally   well  not  to  force  Ubun tu  down  our  throats  but  to  succinctly   demons trate  why  it   ough t  to  be  consider ed.  At  times,   I  find  that  discussions   in  the  West  are  suscep tible   to  being    stuck   in  the  conventional   ways  of  thinking   about   problems,   a  well-w orn  path,  if  you  will.   Ubun tu,  in  this  sense,   provides   a  welcomed   new  perspectiv e  on  the  issues   at  hand.   An  Ubun tu   perspectiv e is not only w orth c onsidering , but it ’s also bene ficial.    Risk  and  Trust  Perceptions   of  the  Public   of  Artificial   Intellig ence    Applic ations    [  Original paper   by Keele y Crockett, Ma tt Garr att,  Annabel La tham, E dwin Coly er, Sean Goltz]    [Resear ch Summar y by Connor W right]   Overview  :  Does   the  gener al  public   trust  AI  more  than   those   studying   a  higher   educ ation   programme   in  comput er  science?   The  report   aims   to  answer  this  very  ques tion,   emphasising    the import ance of civic c ompe tence in AI.    Introduction    Is  the  opinion   of  the  gener al  public   on  AI  different  to  those   studying   comput er  science   in  higher    educ ation?   With   a  survey  titled   “You,  me  and  “AI”:  Wha t’s  the  risk  in  giving   AI  more  control?”   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 8 2   
 The  paper   aims   to  compar e  responses   on  the  level  of  trust  and  risk  that  people   of  the  gener al   public   and  studen ts  studying   comput er  science   in  higher   educ ation  give.  Wha t  is  for  sure  is  that   civic   compe tency   in  AI  is  crucial   in  creating  represen tative  technology ,  some thing   we  hold   dear    to our hearts her e at MAIEI.    Civic c ompe tency in AI    One  of  the  main   slogans  of  this  paper ,  my  TEDxY outh   talk  and  wha t  we  do  at  MAIEI   is  the   import ance   of  civic   compe tency   in  the  AI  field.   By  impr oving  public   under standing   of  AI,  we   better  equip   them   to  fight  any  misin forma tion  on  the  subject.   One  way  to  do  this  is  to  develop    online   courses,  following   in  the  foots teps  of  the  Univ ersity  of  Helsinki.   By  allowing   non-e xperts    to bec ome in volved in the deba te, we enrich and mak e the AI space mor e represen tative.   Nevertheless,   the  paper   poin ts  out  how  some   may  feel  intimida ted  by  courses  offered  by   univ ersities,   for  they  don’t   feel  they  have  the  right  qualific ations.   Hence,   a  futur e  focus   can  be  in   creating c ourses specific ally designed f or the c ommon per son.    One  of  my  core  belie fs  is  that  everyone  can  bring   some thing   to  the  AI  table,   no  matter  the  level   of expertise. Such v alue is clearly demons trated in the da ta collated in the paper ’s sur veys.   The R esults    One  of  the  main   driving   forces  behind   the  survey  is  how  previous   studies   conduct ed  on  the   gener al  public   show  varying  degr ees  of  knowledg e  about   AI,  but  they  all  lack  a  robus t   descrip tion  of  the  gener al  public.   Hence,   the  paper   takes  the  gener al  public   to  be  those   who    have no specific kno wledg e in AI.    The  groups  of  participan ts  were  split   into  Group  1  (the  gener al  public)   and  Group  2  (studen ts  of   a  higher   educ ation  comput er  science   programme).   The  groups  were  then   asked  ques tions   on  3   different  themes:   trust,  risk  and  ques tions   on  a  scale  of  0-10.   A  bird’s  eye  view  of  the  results   are   as follows:   Trust   ●  The  groups  were  found   to  agree  on  ques tions   such   as  not  trusting  an  automa ted   messag e  from  their   boss,   but  differed  on  ques tions   as  to  whe ther   to  trust  a  driverless    car tha t had passed a “ digit al MO T” (p. g. 4).    ●  In this c ase, univ ersity s tuden ts were mor e trus ting of the AI in volved.   Risk   The  studen ts  always  associa ted  the  same   if  not  more  risk  to  different  AI  applic ations,   especially    in terms of f ollowing ins tructions fr om a r ecognisable digitised v oice.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 8 3   
 On a sc ale fr om 0-10    Ther e  was  gener al  parity   between  the  two  groups  on  statemen ts  such   as  “I  belie ve  the  minority    of  AI  systems  are  biased” .  The  only   difference   came   in  how  studen ts  placed   less  emphasis   on  AI   system decisions being e xplainable.    Between the lines    While   the  gener al  public   is  defined   as  being   without   deep   knowledg e  in  the  field,   it  is  crucial    that  they  are  deemed   to  be  a  key  stakeholder .  In  this  way,  their   interactions   with   AI  systems   mus t  be  consider ed  when   evalua ting  an  AI  model’ s  performance.   As  the  paper   rightly  men tions,    risk  can  occur   at  different  poin ts  of  the  AI  lifecycle,  making   system  monit oring   a  vital  aspect   of  a   success ful  AI  system.  I  hold   that  we  cannot   view  AI  systems  as  able   to  gener alise   over  the  whole    popula tion,   meaning   the  practice   is  critic al  in  ensuring   that  the  system  accur ately  tends   to  wha t   it is designed t o do and the pr oblems this c ould bring.    The E thics of Sus tainability f or Artificial In tellig ence    [  Original paper   by Andr ea Ow e and Se th D . Baum]    [Resear ch Summar y by Andr ea Ow e]   Overview  :  AI  can  have  signific ant  effects  on  domains   associa ted  with   sustainability ,  such   as   aspects   of  the  natural  environmen t.  However,  sustainability   work  to  date,  including   work  on  AI   and  sustainability ,  lacks  clarity   on  the  ethical  details,  such   as  wha t  is  to  be  sustained,   why,  and   for  how  long.   Differences   in  these   details  have  import ant  implic ations   for  wha t  should   be  done,    including   for  AI.  This  paper   provides   a  founda tional   ethical  analy sis  of  sustainability   for  AI  and   calls  for  work  on  AI  to  adop t  a  concep t  of  sustainability   that  is  non-an thropocen tric,  long-t erm   orien ted, and mor ally ambitious.    Introduction    Sustainability   is  widely   consider ed  a  good  thing ,  especially   a  good  thing   related  to   environmen t-socie ty  interactions.   It  is  in  this  spirit   that  recen t  initia tives  on  AI  and  sustainability    have  emer ged,  such   as  the  conference   AI  for  People:   Towards  Sustainable   AI,  of  which   this   paper   is  part.   But  wha t  exactly   should   be  sustained,   and  why?  Should,   for  example,   the  natural   environmen t  be  sustained   only   to  the  extent  that  it  supports   the  sustaining   of  human    popula tions,   or  should   natural  ecosystems  and  nonhuman   popula tions   be  sustained   for  their    own  sake?  Is  it  enough   to  sustain  some thing   for  a  few  gener ations   or  should   sustainability    endur e  into  the  distant  futur e?  Is  sustainability   even  enough,   or  should   we  strive  toward  loftier    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 8 4   
 aspir ations?   These   are  import ant  ethical  ques tions   whose   answers  carry  diverging   implic ations    for AI.    This  paper   surveys  existing  work  on  AI  sustainability ,  finding   that  it  lacks  clarity   on  its  ethical   dimensions.   This  is  shown  through   quan titative  analy sis  of  AI  ethics   principles   and  resear ch  on   AI  and  sustainability .  The  paper   then   mak es  a  case  for  a  concep t  of  sustainability   for  AI  that  is   long-t erm  orien ted,  including   time   scales  in  the  astronomic ally  distant  futur e,  and   non-an thropocen tric,  meaning   that  humans   should   not  be  the  only   entities   sustained   for  their    own  sake.  The  paper   additionally   suggests  the  more  ambitious   goal  of  optimiz ation  rather   than    sustainability .   The e thical dimensions of sus tainability    To  under stand  the  ethics   of  sustainability   for  AI,  it  is  essen tial  to  first  under stand  the  ethics   of   sustainability .  In  its  essence,   “sustainability ”  simply   refers  to  the  ability   of  some thing   to   continue   over  time;   the  thing   to  be  sustained   can  be  good,   bad,   or  neutr al.  However,  common    usag e  of  the  term  assumes   that  the  thing   to  be  sustained   is  some   combina tion  of  social   and   ecologic al  systems,   with   the  mos t  prominen t  definition   being   that  of  the  1987   Brundtland    Report,   defining   sustainable   developmen t  as  “mee ting  the  needs   of  the  presen t  without    compr omising   the  ability   of  futur e  gener ations   to  mee t  their   own  needs. ”  Since   then,    “sustainability ”  has  been   widely   applied,   often  in  ways  that  are  impr ecise   or  inconsis tent  with    the  basic   idea   of  the  ability   to  sustain  some thing.   This  paper   argues   that  usag e  of  the  term   should be sharpened, and specific ally tha t it should addr ess thr ee e thics ques tions:    ●  Wha t  should   be  able   to  be  sustained,   and  why?  For  example,   common   concep tions   of   sustainability   are  anthropocen tric  in  that  they  only   aim  to  sustain  humans   for  their   own   sake,  with   the  natural  environmen t  or  other   nonhumans   sustained   only   for  the  bene fit   of  humans.   In  contrast,  a  wide   range  of  moral  philosoph y  calls  for  non-an thropocen tric   ethics tha t value both humans and nonhumans f or their o wn sak e.   ●  For  how  long   should   it  be  able   to  be  sustained?   Ther e  is  a  big  difference   between   sustaining   some thing   for  a  few  days  or  inde finitely  into  the  distant  futur e.  For  example,    the  Brundtland   Report ’s  emphasis   on  futur e  gener ations   implies   a  time   scale  of  at  least   decades,   but  how  man y  futur e  gener ations?   The  limits   of  known  physics  suggest  that  it   may  be  possible   to  sustain  morally  valuable   entities   for  millions,   billions,   or  trillions   of   years into the futur e, or e ven long er.   ●  How  much   effort  should   be  made   for  sustainability?   Should   a  person  or  an  organization   give  “everything   they’ve  got”  to  advance   sustainability   or  is  just  a  little  effort  enough?    How  much   should   sustainability   be  emphasiz ed  relative  to  other   compe ting  values?   The   Brundtland   definition   was  specific ally  crafted  to  ackno wledg e  the  compe ting  values   of   presen t and futur e gener ations.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 8 5   
 The  paper   additionally   compar es  sustainability   to  the  ethics   concep t  of  optimiz ation.    Sustainability   means   enabling   some thing   to  be  sustained   in  at  least  some   minimal   form,    wher eas  optimiz ation  means   making   some thing   be  the  best  that  it  can  be.  For  example,   the   Brundtland   Report   calls  for  the  presen t  gener ation  to  act  “without   compr omising   the  ability   of   futur e  gener ations   to  mee t  their   needs” .  Arguably ,  the  presen t  gener ation  should   act  to  enable    futur e  gener ations   to  do  much   better  than   mee ting  their   basic   needs.   Likewise,   if  human    civiliz ation  has  to  focus   on  sustaining   itself   rather   than   loftier   goals  like  optimiz ation,   then   it  is  in   a very bad situa tion.    Empiric al findings: AI and Sus tainability    Based   on  these   ethical  dimensions,   the  paper   presen ts  a  quan titative  analy sis  of  published   sets   of  AI  ethics   principles   and  academic   resear ch  on  AI  and  sustainability .  The  paper   finds   that  mos t   work  on  AI  and  sustainability   focuses   on  common   concep tions   of  socio-en vironmen tal   sustainability ,  with   smaller   amoun ts  of  work  on  the  sustainability   of  AI  systems  and  other    miscellaneous   things.   Further ,  mos t  work  is  orien ted  toward  sustaining   human   popula tions,    with   AI  and  the  environmen t  having  value   insof ar  as  they  support   human   popula tions.   Mos t   work  does   not  specif y  the  timesc ales  of  sustainability ,  nor  the  degr ee  of  effort  to  be  taken,  and   overall lack clarity on the e thical dimensions pr esen ted abo ve.   The c ase f or long-t erm, non-an thropocen tric sus tainability    Following   these   findings,   the  paper   gives  its  own  answers  on  the  ethical  dimensions.   First,   sustainability   should   be  non-an thropocen tric,  meaning   that  both   humans   and  nonhumans    should   be  sustained   for  their   own  sake.  This  is  motiv ated  by  the  scien tific  observation  that   humans   are  member s  of  the  animal   kingdom   and  part  of  nature,  and  that  nonhumans   often   possess   attribut es  that  are  consider ed  to  be  morally  signific ant,  such   as  the  ability   to  experience    pleasur e  and  pain   or  have  a  life  worth   living.   Second,   sustainability   should   focus   on  long    timesc ales,   including   the  astronomic ally  distant  futur e.  This  is  motiv ated  by  a  principle   of   equality   across  time:   everything   should   be  valued   equally   regardless   of  wha t  time   period   it   exists  in.  Third,  a  large  amoun t  of  effort  should   be  made   toward  sustainability ,  and  optimiz ation   should   be  emphasiz ed  over  sustainability   wher e  the  two  diverge.  Long-t erm  sustainability   of   any  Earth-origina ting  entities   will  eventually   requir e  expansion   into  space,   making   it  necessar y   to  first  handle   any  major   threats  on  Earth,   such   as  global   warming   and  nuclear   warfare.   Additionally ,  the  astronomic ally  distant  futur e  offers  astronomic ally  large  opportunities   for   advancing   moral  value,   making   an  objectiv e  to  optimiz e  moral  value   diverge  signific antly  from   an objectiv e of sus taining mor al value only .   Implic ations f or AI    Finally , the paper pr esen ts implic ations of the abo ve for AI.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 8 6   
 ●  First,  AI  should   be  used   to  impr ove  long-t erm  sustainability   and  optimiz ation.   For  current   and  near -term  forms   of  AI,  this  includes   addr essing   immedia te  threats  to  the   sustainability of global civiliz ation, such as global w arming and pandemics.    ●  Second,   attention  should   be  paid   to  long-t erm  forms   of  AI,  which   could   be  particularly    consequen tial  for  long-t erm  sustainability   and  optimiz ation  due  to  its  potential.   Long-t erm  AI  is  seldom   discussed   in  relation  to  sustainability ,  but  the  paper   argues   that   these   topics   are  a  more  appr opria te  focus   for  work  on  AI  and  sustainability .  Long-t erm  AI   could   bolster  efforts  to  addr ess  threats  such   as  global   warming ,  and  it  could   also  pose    threats  of  its  own,  especially   for  runa way  AI  scenarios.   Furthermor e,  it  could   play  an   import ant  role  in  space   expansion,   which   is  central  to  the  long-t erm  sustainability   and   optimiz ation of mor al value.    Between the lines    In  sum,   this  paper   calls  for  work  on  AI  and  sustainability   to  be  specific   about   its  ethical  basis   and   to  adop t  non-an thropocen tric,  long-t erm  orien ted  concep ts  of  sustainability   or  optimiz ation.   In   practice,   that  entails  focusing   on  applying   AI  to  addr ess  major   global   threats  and  impr oving  the   design   of  long-t erm  AI,  in  order  to  ensur e  the  long-t erm  sustainability   of  civiliz ation  and  to   pursue  opportunities   to  expand   civiliz ation  into  outer  space.   Actions   involving   AI  are  among   the   mos t  signific ant  ways  to  affect  the  distant  futur e.  The  field   of  AI  therefore  has  special    opportunities t o mak e an as tronomic ally lar ge positiv e dif ference.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 8 7   
 Go Wide: Article Summaries  (summariz ed b y Abhishek   Gup ta)   The hack er who spen t a y ear r eclaiming his f ace fr om Clear view AI    [Original article b y  Coda St ory  ]   Wha t  happened   :  A  person  living   in  German y  on  reading   the  Clear view  AI  story  in  the  NYT  in   2020   wanted  to  check   if  Clear view  AI  had  any  data  about   him  given  his  concern   about   his   privacy  and  how  rarely  he  shar ed  pictur es  of  himself   online.   He  was  shock ed  to  discover  that   Clear view  AI  had  found   two  imag es  of  him  that  he  didn’t   even  know  existed.  He  raised   a   complain t  in  the  Hambur g  Data  Protection   Authority   which   after  a  12-mon th  back   and  forth    with   the  compan y  finally   ordered  them   to  remo ve  the  mathema tical  hash   that  char acterized  his   biome tric da ta, his f ace.   Why  it  matters  :  With   the  way  that  Clear view  AI  has  gathered  data  from  public   sour ces  on   people’ s  faces,   the  person  from  German y  rightly  claims   in  the  interview  that  the  compan y  has   made   it  impossible   to  remain   anon ymous   now.  It  is  not  like  a  regular   sear ch  engine   process   in   that  on  input ting  faces   it  digs  up  specific   matches   to  your  face  thus   making   it  perhap s   impossible   to  participa te  in  a  protest  for  the  fear  of  being   iden tified,   even  when   it  is  legal  to  do   so.  Mor e  so,  it  has  implic ations   for  wha t  happens   to  all  the  data  that  is  captured  from  CCTVs  and   other   surveillance   mechanisms   that  capture  our  data  without   our  consen t  all  the  time,   thus    potentially limiting fr eedom of mo vemen t of people in the built en vironmen t.   Between  the  lines   :  Finally ,  the  thing   that  caugh t  my  attention  was  the  fact  that  the  person  from   German y  men tioned   that  there  were  erroneous   matches   that  were  returned   to  him  as  a  part  of   his  data  reques t.  This  is  to  be  expect ed  because   no  algorithm   can  be  perfect  but  there  is  a   severe  consequence:   if  there  are  authorities   that  rely  on  this  data  to  mak e  determina tions    about   the  movemen ts  of  people,   they  migh t  draw  false  conclusions.   Also ,  it  migh t  still  be  OK  in   a  perfectly   functioning   democr acy  (which   rarely  if  ever  exists  or  will  exist)  but  wha t  would    happen   to  this  technology   and  its  capabilities   if  the  regime   chang es  to  some thing   more   authorit arian?    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 8 8   
 An Artificial In tellig ence Helped W rite This Pla y. It Ma y Con tain Racism    [Original article b y  Time   ]   Wha t  happened   :  Human-machine   teaming   is  always  an  interesting  domain   to  surface   unexpect ed  results.   In  a  play  staged  in  the  YoungVic   in  London,   playwrigh ts  have  joined   forces   with   GPT-3  to  gener ate  a  scrip t  on  the  spot   which   is  enact ed  by  a  troupe   of  actors  without   any   rehear sals  giving   unique   plays  every  nigh t  that  they  are  on  stage.  Taking   on  an  uncensor ed  and   unfiltered  appr oach,   the  harsh  stage  lighting  of  the  YoungVic   will  also  lay  bare  the  biases   that   pervade  the  outputs   of  GPT-3  mirr oring   the  realities   of  the  world  outside   the  stage.  Jennif er   Tang,  the  director,  sees   this  as  an  exciting   foray  into  the  futur e  of  wha t  AI  can  do  for  the  creative   field.    Why  it  matters  :  While   we  have  seen   a  lot  of  deba tes  around   the  role  that  AI  will  play  in  the   creative  fields,   some thing   that  we’ve  covered  in  Volume   5  of  the  State  of  AI  Ethics   Report   as   well,  using   a  very  powerful   model   like  the  GPT-3  to  work  side-b y-side   with   humans   is  novel  in   gener ating  creative  outputs.   While   scholar s  interviewed  in  the  article   caution   against  attributing    creativity   to  the  outputs   from  the  system,  it  migh t  be  worth   considering   if  we  can  say  that  such    a  tool  help s  to  boos t  creativity   for  artis ts  by  expanding   the  solution   space   that  the  artis ts  can   then e xplor e.   Between  the  lines   :  Biases   in  the  outputs   from  GPT-3  are  very  problema tic  -  with   stereotypic al   dialogue   alloc ation  based   on  the  religion   of  the  actors  to  outrigh t  homophobia   and  racism,   the   issues   with   such   large-scale  models   are  numer ous.   How  such   problems   are  tackled   and  if  they   can  be  brough t  to  the  stage  wher e  they  become   trusted  tools  in  the  creative  process   remains   to   be  seen.   The  first  step  in  that  process   is  highligh ting  the  problems   and  beginning   to  build   tools   that can addr ess those issues be fore this bec omes a c ommon pr actice in the cr eative indus tries.    The St ealth y iPhone Hack s Tha t Apple Still Can't St op   [Original article b y  Wired  ]   Wha t  happened   :  In  a  not-so-surprising   revelation,   high-pr ofile   individuals   were  targeted  by  the   Bahr aini  governmen t  using   zero-click   attacks  that  targeted  vulner abilities   in  the  iMessag e  app   on  the  iPhone.   Dubbed   “Meg alodon”   and  “Forced  Entry”  by  Amnes ty  Interna tional   and  Citiz en   Lab  respectiv ely,  the  attacks  bypass   critic al  protections   created  by  Apple,   called   BlastDoor ,  to   guar d  against  these   kinds   of  attacks.  The  zero-click   attacks  don’t   requir e  any  interaction   from   the user and tha t’s wha t giv es them pot ency and e ffectiv eness.    T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 8 9   
 Why  it  matters  :  Though   these   attacks  cost  millions   of  dollar s  to  develop   and  they  often  have   short   shelf -lives  because   of  security   patches   and  upda tes  issued   by  manuf actur ers,  they  still   pose   an  immense   threat  to  the  targeted  individuals.   The  issue   is  exacerba ted  because   Apple    refuses   to  allow  user s  to  disable   apps  that  they  provide   natively  on  the  iPhone.   Past  releases    have  shown  that  the  attack  surface  for  such   kinds   of  apps  is  quite  large  and  protecting   against   such   threats  is  increasingly   difficult   and  requir es  signific ant  overhauls   of  the  core  architectur e,   which   will  requir e  tons  of  resour ces  from  Apple   to  mak e  the  necessar y  chang es,  some thing   that   they are unlik ely t o do in the short -run.    Between  the  lines   :  iMessag e  is  not  the  only   app  that  faces   such   zero-click   attacks;  there  are   other   apps  like  Wha tsapp   that  are  also  suscep tible   to  different  attacks  that  follow  similar    patterns.   With   the  utiliz ation  of  AI  to  discover  vulner abilities,   we  are  perhap s  entering   an  era   wher eby  the  detection   of  vulner abilities   is  greatly  acceler ated  allowing   malicious   actors  to  craft   even  more  sophis ticated  attacks  by  directing   their   ener gies  towards  developmen ts  of  those    exploits   more  so  than   having  to  discover  the  vulner abilities   first  before  crafting   the  attacks.  This   places   an  additional   burden  on  the  manuf actur ers  to  ensur e  that  they  have  more  robus t   developmen t practices guar anteeing security of the end user ’s devices and app s.   The limit ations of AI sa fety tools    [Original article b y  VentureBea t  ]   Wha t  happened   :  With   the  inclusion   of  the  word  “safety”  in  various   trustworth y  AI  proposals    from  the  EU  HLEG  and  NIST,  this  article   talks  about   the  role  some thing   like  the  Safety  Gym  from   OpenAI   can  play  in  achie ving  safety  in  AI  systems.   It  provides   an  environmen t  to  test   reinforcemen t  learning   systems  in  a  constrained   setting  to  evalua te  their   performance   and   assess   them   for  various   safety  concerns.   The  article   features  interviews  with   some   resear cher s   in  the  field   who   men tion  how  such   a  gym   migh t  be  inadequa te  since   it  still  depends   on   specif ying  rules   to  qualif y  wha t  constitut es  safe  beha vior.  And  such   rules   will  have  to  constantly   grow and adap t as the s ystems “ explor e” ne w ways of achie ving the specified t asks.   Why  it  matters  :  Environmen ts  like  the  Safety  Gym  help   to  provide   a  sandbo x  to  test  digit al  twins    of  systems  that  will  be  deplo yed  in  production,   especially   when   the  costs  of  such   testing  migh t   be  prohibitiv e  in  the  real-w orld  or  too  risky .  This  applies   to  cases   of  autonomous   driving ,   indus trial  robots   working   alongside   other   humans,   and  other   use-c ases   with   humans   and   machines oper ating in a shar ed en vironmen t.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 9 0   
 Between  the  lines   :  A  single   environmen t  with   a  specific   modality   of  oper ation  can  never   compr ehensiv ely  help   to  iden tify  all  the  places   wher e  alignmen t  problems   migh t  arise   for  an  AI   system,  but  they  do  provide   a  diagnos tic  test  to  at  least  iden tify  how  the  system  can  misbeha ve   or  deviate  from  expect ations.   Using   that  informa tion  to  iterate  is  a  useful  outcome   from  the  use   of  such   an  environmen t.  Mor e  so,  such   environmen ts  offer  a  much   more  practic al  way  to  go   about   safety  testing  rather   than   just  theor etical  formula tions   which   to  a  certain  extent  are   limit ed  by  the  ingenuity   of  the  testers  and  developer s’  to  imagine   how  an  AI  system  migh t   beha ve.   AI  fake-face  gener ators  can  be  rewound   to  reveal  the  real  faces   they   trained on    [Original article b y  MIT T echnology R eview  ]   Wha t  happened   :  The  article   covers  a  recen t  paper   that  utiliz ed  member ship  inference   attacks   to  determine   wha t  face  imag es  migh t  have  been   used   in  training   a  facial   recognition   technology    system.  Ther e  are  man y  websites  like  This  Person  Does   Not  Exist  that  offer  AI-gener ated  faces    by  utilizing   GANs,   but  some   of  them   resemble   real  people   too  closely .  The  paper   sough t  to   demons trate  that  by  gener ating  faces   from  the  GAN   and  then   using   a  separ ate  facial   recognition    system t o see if an y of them w ere a ma tch.   Why  it  matters  :  Such   a  technique   has  the  potential  to  allow  people   to  check   if  their   imag e  has   been   used   in  training   an  AI  system.  But,  it  also  showcases   latent  vulner abilities   in  such   systems   when   they  can  leak  wha t  kind   of  data  was  used   to  train  them.   Especially   when   you  have   pre-trained   models   that  are  re-used   downstream   by  other   developer s.  Other   techniques   like   model   inversion  and  model   stealing ,  falling   in  the  broad  category  of  machine   learning   security    demons trate  such   weaknesses   in  AI  systems  today  and  provide   us  with   a  pathway  towards   building mor e robus t AI s ystems.    Between  the  lines   :  The  area  of  machine   learning   security   today  is  highly   under -explor ed  with    mos t  of  the  focus   on  issues   like  fairness   and  privacy,  which   while   import ant  don’t   cover  the  full   gamut   of  ethical  issues   with   AI  systems.   We  need   to  ensur e  that  the  systems  are  robus t  as  well   and  today  we  are  in  the  early   stages  of  machine   learning   security   attacks  and  defenses   as  was   the c ase with cyber security f or mor e traditional softw are systems a f ew years ago.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 9 1   
 Wha t Apple’ s Ne w Repair Pr ogram Means f or You (And Y our iPhone)    [Original article b y  NYTimes   ]   Wha t  happened   :  Apple   has  announced   a  new  program  under   which   they  are  making    replacemen t  parts   available   to  a  wider   set  of  repair   services   provider s,  including   to  consumer s   so  that  they  can  mak e  minor   repair s  either   themselv es,  or  take  it  to  other   repair   shop s  to   extend  the  life  of  their   devices.   This  has  direct  implic ations   in  terms   of  increasing   accessibility   of   these   devices,   since   consumer s  who   were  char ged  a  lot  of  mone y  at  Apple   stores  or  authoriz ed   services   can  now  get  a  cheaper   pathway  to  continue   using   their   devices.   And  mos t  import antly,   extending   the  lifespan   of  the  device  means   that  we  will  reduce   the  impact   on  the  plane t,  given   that  the  embodied   carbon   emissions   constitut e  a  major   chunk   of  the  environmen tal  impacts   of   technology , this is a gr eat step forward.   Why  it  matters  :  This  is  a  huge  win  for  advocates  of  the  “Righ t  to  Repair ”  movemen t,  and  as   men tioned   in  the  article,   a  huge  compan y  like  Apple   making   such   a  move  can  act  as  a   trendse tter  for  other   companies   to  follow  suit  and  offer  similar   services.   Given  that  we  cycle   through   our  devices   fairly   quickly ,  extending   the  lifespan   of  these   devices   can  have  an  indir ect   impact   also  on  the  kind   of  softw are  that  is  developed   which   can  continue   to  leverage  older    hardware  rather   than   constantly  creating  backw ard-inc ompa tible   upda tes  that  necessit ate   moving t o ne wer de vices.    Between  the  lines   :  The  concerns   that  are  usually   flagged  for  not  offering   such   programs  has   traditionally   been   that  unauthoriz ed  repair   centers  migh t  pose   security   and  privacy  risks  to  the   data  of  the  consumer s  on  those   devices.   The  current  move  migh t  be  coming   on  the  heels   of   hints  from  the  FTC  that  they  migh t  mak e  more  string ent  regula tions   that  manda te  providing    options   to  consumer s  to  be  able   to  repair   their   devices   either   on  their   own  or  get  access   to   replacemen t  parts   so  that  they  can  pick  replacemen t  service  provider s  outside   of  those    authoriz ed b y the manuf actur er.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 9 2   
 Closing R emark s   Congr atulations   for  making   it  all  the  way  to  the  end!   This  was  our  long est  edition   of  the  State  of   AI Ethics R eport   thus f ar!   Every  few  mon ths  as  we  embark   on  capturing   the  latest  in  resear ch  and  reporting   in  the  domain    of  AI  ethics,   we  are  surprised   by  the  richness   of  the  domain,   the  inde fatigable   efforts  of   activis ts,  resear cher s,  and  practitioner s  around   the  world,   and  above  all  the  amazing   repert oire   of  ideas   permea ting  the  domain.   We  count  ourselves  lucky   to  have  all  of  you  as  our  support ers   who   encourage  us  to  continue   our  explor ation  and  help   us  realiz e  our  mission   to  Democr atize   AI Ethics Lit eracy  .   Man y  pieces   in  this  edition   of  the  report   resona ted  deeply   with   me  and  made   me  pause   and   wonder   wha t  it  is  that  we  can  be  doing   to  elevate  the  level  of  conversation  in  the  field   and   provide   more  meaningful   ways  for  the  community   to  engage  with   each   other   and  support   each    other   as  we  all  strive  to  mak e  socie ty  better,  using   technology   to  solve  problems   rather   than    create ne w ones.    The  report   is  one  amongs t  man y  different  ways  that  you  can  stay  connect ed  with   us.  We  also   publish   The  AI  Ethics   Brief  that  is  read  by  technic al  leader s  and  policymak ers  from  around   the   world.   We  invite  you  to  stay  in  touch   with   us  between  reports   through   that.  Above  all,  if  you’re   working on in teresting pr oblems and ar e looking f or sounding boar ds,  we’re around   !   Until  the  next  report,   hope   you  stay  safe  and  health y,  and  let’s  all  work  together   to  Mak e   Responsible AI the Norm r ather than the Ex ception  !   Abhishek Gupta (  @atg_abhishek  )   Founder, Director, & Principal Researcher,   Montreal AI Ethics Institute   Abhishek  Gupta  is  the  founder,  director,  and  principal  researcher  at  the   Montreal  AI  Ethics  Institute.  He  is  also  a  machine  learning  engineer  at   Microsoft,  where  he  serves  on  the  CSE  Responsible  AI  Board.  He  also  serves  as   the Chair of the Standards Working Group at the Green Software Foundation.   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 9 3   
 Support Our W ork   The  Mon treal  AI  Ethics   Institut e  is  commit ted  to  democr atizing   AI  Ethics   literacy.  But  we  can’t  do   it alone.    Every  dollar   you  dona te  help s  us  pay  for  our  staff  and  tech  stack,  which   mak e  everything   we  do   possible.    With y our support, w e’ll be able t o:   ●  Run mor e events and cr eate mor e content   ●  Use softw are tha t respects our r eader s’ da ta priv acy   ●  Build the mos t eng aged AI E thics c ommunity in the w orld   Please mak e a dona tion t oday at  mon treale thics.ai/dona te  .   We  also  encourage  you  to  sign  up  for  our  weekly   newsletter  The  AI  Ethics   Brief  at   brief.mon treale thics.ai   to  keep  up  with   our  latest  work,  including   summaries   of  the  latest   resear ch & r eporting , as w ell as our upc oming e vents.   If  you  want  to  revisit  previous   editions   of  the  report   to  catch  up,  head   over  to   mon treale thics.ai/ state  .   Please   also  reach   out  to  Masa   Sweidan   masa@mon treale thics.ai   for  providing   your   organizational support f or upc oming quart erly editions of the   State of AI E thics R eport.    Note:  All  dona tions   made   to  the  Mon treal  AI  Ethics   Institut e  (MAIEI)   are  subject   to  our   Contributions P olicy   .   T h e  S t a t e  o f  A I  E t h i c s  R e p o r t ,  V o l u m e  6  ( J a nu a r y  2 0 2 2 )   2 9 4   
