DATA & SOCIETY   1 GOVERNING ARTIFICIAL INTELLIGENCE Governing Artificial  Intelligence:  UPHOLDING  HUMAN RIGHTS  & DIGNITY Mark Latonero
Governing Artificial   Intelligence:  UPHOLDING   HUMAN RIGHTS   & DIGNITY Mark Latonero EXECUTIVE SUMMARY Can international human rights help guide and govern artificial intelligence (AI)?      Currently, much of society is uncertain about the real human impacts of AI systems.  Amid hopes that AI can bring forth “global good” there is evidence that some AI sys tems are already violating fundamental rights and freedoms. As stakeholders look for  a North Star to guide AI development, we can rely on human rights to help chart the  course ahead. International human rights are a powerful tool for identifying, prevent ing, and redressing an important class of risks and harms. A human rights-based  frame could provide those developing AI with the aspirational, normative, and legal  guidance to uphold human dignity and the inherent worth of every individual regardless of country or jurisdiction. Simply put: In order for AI to benefit the common good, at the very least its  design and deployment should avoid harms to fundamental human  values. International human rights provide a robust and global  formulation of those values. This report is intended as a resource for anyone working in the field of AI and gover nance. It is also intended for those in the human rights field, outlining why they should  be concerned about the present-day impacts of AI. What follows translates between  
DATA & SOCIETY   2 GOVERNING ARTIFICIAL INTELLIGENCEthese fields by reframing the societal impact of AI systems through the lens of human  rights. As a starting point, we focus on five initial examples of human rights areas –  nondiscrimination, equality, political participation, privacy, and freedom of expression  – and demonstrate how each one is implicated in a number of recent controversies  generated as a result of AI-related systems. Despite these well-publicized examples  of rights harms, some progress is already underway. Anticipating negative impacts  to persons with disabilities, for example, can lead designers to build AI systems that  protect and promote their rights. This primer provides a snapshot of stakeholder engagement at the intersection of  AI and human rights. While some companies in the private sector have scrambled  to react in the face of criticism, others are proactively assessing the human rights  impact of their AI products. In addition, the sectors of government, intergovernmental  organizations, civil society, and academia have had their own nascent developments.  There may be some momentum for adopting a human rights approach for AI among  large tech companies and civil society organizations. To date, there are only a few,  albeit significant, number of examples at the United Nations (UN), in government, and  academia that bring human rights to the center of AI governance debates. Human rights cannot address all the present and unforeseen concerns pertaining to  AI. Near-term work in this area should focus on how a human rights approach could  be practically implemented through policy, practice, and organizational change. Fur ther to this goal, this report offers some initial recommendations:  • Technology companies should find effective channels of communication with local  civil society groups and researchers, particularly in geographic areas where human  rights concerns are high, in order to identify and respond to risks related to AI deploy ments.  • Technology companies and researchers should conduct Human Rights Impact As sessments (HRIAs) through the life cycle of their AI systems. Researchers should  reevaluate HRIA methodology for AI, particularly in light of new developments in algorithmic impact assessments. Toolkits should be developed to assess specific industry  needs. • Governments should acknowledge their human rights obligations and incorporate a  duty to protect fundamental rights in national AI policies, guidelines, and possible regulations. Governments can play a more active role in multilateral institutions, like the  UN, to advocate for AI development that respects human rights. • Since human rights principles were not written as technical specifications, human  rights lawyers, policy makers, social scientists, computer scientists, and engineers  should work together to operationalize human rights into business models, workflows,  and product design.
DATA & SOCIETY   3 GOVERNING ARTIFICIAL INTELLIGENCE• Academics should further examine the value, limitations, and interactions between human rights law and human dignity approaches, humanitarian law, and ethics in relation  to emerging AI technologies. Human rights and legal scholars should work with other  stakeholders on the tradeoffs between rights when faced with specific AI risks and  harms. Social science researchers should empirically investigate the on-the-ground  impact of AI on human rights. • UN human rights investigators and special rapporteurs should continue researching  and publicizing the human rights impacts resulting from AI systems. UN officials and  participating governments should evaluate whether existing UN mechanisms for inter national rights monitoring, accountability, and redress are adequate to respond to AI  and other rapidly emerging technologies. UN leadership should also assume a central  role in international technology debates by promoting shared global values based on  fundamental rights and human dignity. 
DATA & SOCIETY   4 GOVERNING ARTIFICIAL INTELLIGENCETABLE OF CONTENTS EXECUTIVE SUMMARY .................................................................................................... 1 INTRODUCTION .............................................................................................................. 5 BRIDGING AI AND HUMAN RIGHTS ................................................................................ 7 A HUMAN RIGHTS FRAME FOR AI RISKS AND HARMS ................................................. 10 Nondiscrimination and Equality ........................................................................................ 10 Political Participation .......................................................................................................... 12 Privacy .................................................................................................................................. 13 Freedom of Expression ...................................................................................................... 14 The Disability Rights Approach and Accessible Design ................................................. 15 STAKEHOLDER OVERVIEW ............................................................................................ 17 Business ............................................................................................................................... 18 Civil Society ......................................................................................................................... 20 Governments ...................................................................................................................... 20 United Nations .................................................................................................................... 21 Intergovernmental Organizations ..................................................................................... 22 Academia ............................................................................................................................ 23 CONCLUSION ................................................................................................................ 24 ENDNOTES .................................................................................................................... 26 ACKNOWLEDGMENTS .................................................................................................. 36 Author: Mark Latonero; Research Lead, Data & Society; PhD Annenberg School for   Communication, University of Southern California.
DATA & SOCIETY   5 GOVERNING ARTIFICIAL INTELLIGENCE INTRODUCTION Can international human rights help guide and govern artificial intelligence (AI)?  According to the global ethics initiative of the Institute of Electrical and Electronics  Engineers (IEEE), the largest organization of technical professionals, the answer is  clear. The IEEE’s 2017 report on ethically aligned design for AI lists as its first principle  that AI design should not infringe upon international human rights.1 Yet some AI sys tems are already infringing on such rights. For instance, in March 2018, human rights  investigators from the United Nations (UN) found that Facebook – and its algorithmically driven news feed – exacerbated the circulation of hate speech and incitement to  violence in Myanmar.2 During a US Congressional hearing in April 2018, Senator Pat rick Leahy questioned CEO Mark Zuckerberg about the failure of Facebook’s AI for  content detection in the face of possible genocide against Myanmar’s Rohingya ethic  minority. While Zuckerberg initially told senators that more advanced AI tools would  help solve the problem, he later conceded to investors that Facebook’s AI systems will  be unable to detect “hate” in local contexts with reasonable accuracy anytime soon.3 Just a month after Zuckerberg’s hearing, the UN’s International Telecommunications  Union (ITU) hosted its second annual AI for Global Good summit in Geneva.4 For many  involved in the summit, AI is not just a source of potential risks, it can bring a better  future of worldwide benefits. Between these hopes and fears lies an increased sense  of uncertainty. As stakeholders look for a North Star to guide AI development, we can  rely on human rights to help chart the course ahead. Simply put: In order for AI to benefit the common good, at the very least its  design and deployment should avoid harms to fundamental human  values. International human rights provide a robust and global  formulation of those values. In bridging AI and human rights, what’s at stake is human dignity.* As an international  framework, human rights law is intended to establish global principles (“norms”) and  mechanisms of accountability for the treatment of individuals. As such, a rights-based  * The definition of human dignity is contested and its normative value is debated in an extensive literature that  is outside the scope of this report. For the present purposes, the term human dignity gestures towards its  usage in Western moral philosophy, such as Kant’s notions of dignity linked to human autonomy and agency,  while acknowledging that dignity has been linked to traditions such as Eastern philosophy as well. This report’s  usage of human dignity also evokes the United Nation’s charter, the Universal Declaration on Human Rights,  and the major rights treaties, which link fundamental human rights, the dignity and worth of the human person,  and the equal rights of men and women. The interactions between humans and AI may further challenge or  refine the concept of human dignity, which is an important topic for future work.
DATA & SOCIETY   6 GOVERNING ARTIFICIAL INTELLIGENCEapproach provides actors developing AI with the aspirational and normative guidance to uphold human dignity and the inherent worth of every individual, regardless  of country or jurisdiction. Implementing human rights can help identify and anticipate  some of AI’s worst social harms and guide those developing technical and policy safeguards to promote positive uses. Those working on AI accountability can activate the  international system of human rights practice – including binding treaties, UN investigations, and advocacy initiatives – to monitor social impacts and establish processes  of redress. Importantly, advocates can use human rights to focus attention on power  relationships and inequalities that impact vulnerable or marginalized groups around  the globe. IMPLEMENTING HUMAN RIGHTS CAN HELP  IDENTIFY AND ANTICIPATE SOME OF AI’S WORST  SOCIAL HARMS AND GUIDE THOSE DEVELOPING  TECHNICAL AND POLICY SAFEGUARDS TO  PROMOTE POSITIVE USES. Those working on AI commercially might wonder why they should care about human  rights. Increasingly, stakeholders are holding the private sector responsible for upholding rights.5 In 2011, the UN released a landmark document – The Guiding Princi ples on Business and Human Rights  – that calls on industry to respect, protect, and  provide remedies for human rights.6 These principles can provide AI executives and  developers alike with a template for conducting due diligence on human rights impacts. They provide guidelines for how businesses should assume a higher duty of  care when developing and deploying their products.7 Although a milestone in the field of business and human rights, the UN Guiding Princi ples  reflects but a starting point for the application of human rights in the tech sector.  Those working directly on AI need regulation, or “hard” laws, along with technical  standards, social norms, and market incentives, to effectively incorporate a respect  for human rights into their business models, policies, and practices. At the same time,  those working on human rights need to be actively engaged in AI governance and  monitoring. When necessary, they should be ready to invoke a human rights framework to challenge how AI is developed and deployed by business or government. Civil  society and AI developers should work together to help assess risk areas and anticipate the needs of vulnerable groups. Only when stakeholders are working across silos  to safeguard against harms can AI systems avoid human rights abuses and advance  the enjoyment of human rights. This report is intended as a resource for anyone working in the field of AI and gover nance. Anywhere that AI is being researched, developed, or deployed, a human rights  frame can identify, anticipate, and minimize an important class of risks and harms. This 
DATA & SOCIETY   7 GOVERNING ARTIFICIAL INTELLIGENCEwork is also intended for those in the human rights field, outlining why they should be  concerned about the present and near-term impacts of AI. What follows translates  between these fields by reframing the societal impact of AI systems through the lens  of human rights. For those seeking to govern AI – from governments looking to craft regulation to  companies looking to self-regulate – this document offers a perspective based on  established human rights accountability and norms. The field of human rights has limitations and will certainly not address all the ethical issues arising from AI. Yet it offers  a strong value proposition: an approach to AI governance that upholds human dignity  based on international human rights law. The first part of this report, “Bridging AI and Human Rights,” connects the entry points  between AI and human rights for governance discussions. Next, “A Human Rights  Frame for AI Risks and Harms” reviews a number of current AI risks and harms from  a human rights perspective, describing how such rights can be applied. Part three,  “Stakeholder Overview,” catalogues the current state of play among stakeholders  active in this space, with examples of progress and challenges. Finally, the conclusion  discusses the limitations and presents several recommendations for incorporating a  human rights approach for AI governance. BRIDGING AI AND HUMAN RIGHTS Human rights have only appeared at the periphery of our prominent AI debates.8 Both  AI and human rights are highly technical fields; to fully digest either would require far  more of an exegesis than can be attempted in this report. Instead, we shall draw on  basic entry points from both fields to inform AI governance discussions. Discussions about AI can be fragmented; some people speak of AI colloquially in  the popular press or in tech marketing materials, while others speak of concrete  methods in scientific proceedings.9 Moreover, the nuances of terminology and the  speed at which the field is moving can make cross-disciplinary discussions difficult to  
DATA & SOCIETY   8 GOVERNING ARTIFICIAL INTELLIGENCEhave. When considering social and policy implications, it is useful to think of “AI” as a  catchphrase for a cluster of technologies embedded in social systems. This includes  machine learning, natural language processing, computer vision, neural networks,  deep learning, big data analytics, predictive models, algorithms, and robotics—all of  which are intrinsically situated in the social contexts where they are developed and  deployed. While some areas of AI remain only theoretical, others, such as machine learning, are  already having an impact in real-world contexts.10 Machine learning systems process  large amounts of historical training data and “learn” from these examples to detect  patterns that can be useful in decision-making.11 All machine learning algorithms  contain some level of statistical bias, which produces incorrect decisions some of  the time.12 However, if the historical data are incomplete or are not representative of a  specified population, these biases can scale quickly and inexplicably across AI sys tems. Such systems can further entrench discriminatory outcomes in people’s lives.  How far should we as a society allow machine learning systems to influence human  decision-making or even make decisions on their own? These concerns are at the  heart of AI debates.13 While these questions have yet to be answered, the fact is that today, automated  systems are making predictions about human behavior and producing decisions and  recommendations that are impacting people in everyday life. These systems are  increasingly becoming embedded in a number of social contexts, from policing and  judicial sentencing to medicine and finance. We do not know the unintentional impacts  or unforeseen consequences of current or future AI systems. As this uncertainty has  brought urgent calls to govern AI, we can now turn to the value of human rights. The field of human rights can be complex for nonexperts. For the purposes of this  report, we anchor international human rights law in the drafting and implementation  of the Universal Declaration on Human Rights (UDHR) by the United Nations in 1948.  The UDHR’s aspirational language established that human rights were grounded in a  respect for all individuals that derived from our equal status as bearers of inherent human dignity. This was a response to the “disregard and contempt for human rights,”14  which precipitated two world wars and the Holocaust. Human dignity and fundamental rights are not tied to country citizenship, legal regime, or socioeconomic position.  These rights are universal in the sense that they apply to everyone, everywhere, which  provides a frame  for discussing global AI impact and governance. Over the last 70 years, human rights proponents have developed the principles of the  UDHR into a body of international human rights law that includes nine major human  rights treaties; regional rights instruments in the Americas, Africa, and Europe; incor poration in state constitutions and national laws; and customary and case law.15 Yet  because of a divergence in political ideologies and claims to sovereignty, governments  enforce international human rights law to wildly varying degrees.16 Thus, a human  rights framework  has emerged to monitor, promote, and protect human rights. This  involves the further development of international human rights law and the interaction 
DATA & SOCIETY   9 GOVERNING ARTIFICIAL INTELLIGENCEof a diverse network of actors in the UN system, nation-states, international organizations, NGOs, civil society, the private sector, academia, and advocates at the local or  individual level. Those looking for first principles to ground AI governance can use the language of  human rights. For example, one of the most hotly debated topics in AI is discriminatory algorithms and systems. This includes empirical research on facial recognition  systems that cannot “see” people, particularly women, with darker skin due to a lack  of adequate training data or to faulty models and therefore reproduce culturally engrained biases against people of color.17 Human rights principles of nondiscrimination  have been propagated through a multitude of UN treaties, national laws, UN commentary, academic interpretation, and other policies and guidelines. This body of work  offers not only a distinct value commitment but also a global perspective on how to  identify the impact of discrimination. Equality and nondiscrimination are foundational  to practically every major human rights instrument that has emerged from debate and  input from representatives from the world’s nations. The development of human rights  has its own controversies and politics, but over the last 70 years, international human  rights have come to represent shared global values.  Those working on technology policy are faced with the difficult task of deciding what  standards, values, or norms to apply in different social contexts. They need to balance  the tradeoffs of developing or deploying technologies. They need to understand the  potential misuses and abuses, unintended consequences, biases in sociotechnical  systems, and even the costs of not deploying a tool when it may help someone in  need. Human rights provide those working on AI with a basis for understanding why  governing systems – from technical standards to policy – should address values like  nondiscrimination in the first place. This is important for tech companies whose products will be used across national borders where laws and values vary. While it is outside the present scope of this report, an area that demands more foundational work concerns pathways for human rights accountability and remedy when  AI harms become manifest. A purely legal, regulatory, or compliance framework would  lag behind the velocity of change associated with emerging AI technologies. Thus,  other components of the human rights framework, such as UN special rapporteurs,  independent investigators, and monitors from civil society are crucial for calling  attention to AI risks and harms. As scholars Christiaan Van Veen of New York Univer sity (NYU) and Corinne Cath of Oxford Internet Institute state, “Human rights, as a  language and legal framework, is itself a source of power because human rights carry  significant moral legitimacy and the reputational cost of being perceived as a human  rights violator can be very high.”18 Thus, human rights can provide the link between an  AI system’s negative social impact on even one individual in places like Myanmar and  the most powerful companies in Silicon Valley.
DATA & SOCIETY   10 GOVERNING ARTIFICIAL INTELLIGENCEA HUMAN RIGHTS FRAME FOR AI RISKS AND HARMS This section reframes a number of well-publicized controversies generated by AI  systems. By taking up a human rights lens, we can see how these classes of risks  and harms fall within the purview of human rights. We will focus on rights found in the  UDHR and the most significant human rights treaties: The International Covenant on  Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESRC), which have been ratified by roughly 170 countries.  Together, these three documents make up the International Bill of Rights, which illus trates that human rights are “indivisible, interdependent, and interrelated.”19 This section provides the foundation for viewing these challenges (and future ones)  as not just local problems impacted by individual technologies but as concerns addressable through a framework of universal rights. Intended as a starting point rather  than an exhaustive analysis, this section will touch upon five areas of human rights:  nondiscrimination, equality, political participation, privacy, and freedom of expression.  In addition, we will examine how the rights of persons with disabilities can help us  anticipate harms to the human dignity of vulnerable groups and, in doing so, allow us  to develop AI technologies to advance human rights. NONDISCRIMINATION AND EQUALITY As mentioned above, bias and discrimination have become central topics for those  concerned with the governance and social impact of AI systems.20 A number of high  profile studies have demonstrated that, as in the case of detecting skin color, certain  AI systems are inherently discriminatory. Alarming reports have detailed how discriminatory algorithms are already deployed in the justice system, wherein judges use  these tools for sentencing that purport to predict the likelihood a criminal defendant  will reoffend.21 In Automating Inequality , Virginia Eubanks details how government actors implement  automated and surveillance technologies that harm marginalized groups.22 Eubanks  studied automated systems in the US that discriminated against the poor’s receipt of  
DATA & SOCIETY   11 GOVERNING ARTIFICIAL INTELLIGENCEgovernment assistance. Automating Inequality  includes a discussion of the Allegheny  Family Screening Tool (AFST), a predictive risk model deployed by the County Office  of Children, Youth, and Families to forecast child abuse and neglect. While the AFST  is only one step in a process that includes human decision-makers, Eubanks argues  that it makes workers in the agency question their own judgment and “is already  subtly changing how some intake screeners do their jobs.” Moreover, the system can  override its human co-workers to automatically trigger investigations into reports. The  model has inherent flaws: it only contains information about families who use public  services, making it more effective at targeting poor residents.23 Such discriminatory  effects can lead to harms in other human rights areas, such as education, housing,  family, and work. Some governments are already using algorithmic systems to classify people based on  problematic categories. For example, there are reports that the government of China is deploying systems to categorize people by social characteristics.24 This Social  Credit System is being developed to collect data on Chinese citizens and score them  according to their social trustworthiness, as defined by the government. The system  has punitive functions, such as shaming “debtors” by displaying their faces on large  screens in public spaces or blacklisting them from booking trains or flights.25 Historically, we have seen how governments’ use of national systems of social sorting  along predetermined physical categories can lead to discrimination against marginalized groups. In South Africa, a classification system built on databases that sorted  citizens by pseudoscientific racial taxonomies was deployed to implement the racist  and violent policies of the apartheid regime. This well-documented case serves as an  important cautionary tale for any widespread deployment of AI social scoring sys tems.26 Without safeguards, even AI systems built for mundane bureaucratic functions  can be repurposed to enact discriminatory policies of control. The importance of equality and nondiscrimination has filtered down through the ratification of treaties to provide the basis for post-war constitutions, state law, and judicial  interpretation.27 For example, the South African constitution, adopted in 1996, directly  accounted for the discriminatory policies of the past. The constitution establishes  equality, human dignity, and human rights as its legal foundations and core values. There have been attempts to frame discrimination in machine learning algorithms as  a human rights issue, as in a recent World Economic Forum (WEF) report that raised  both concerns and possible solutions for biased decision-making.28 The report calls  for human rights to move to the center of AI discussions: even when there is no intention for discrimination, ML [machine  learning] systems for which success is strictly measured in terms of  efficiency and profit may end up achieving these at the expense of a  company’s responsibility to respect human rights.29
DATA & SOCIETY   12 GOVERNING ARTIFICIAL INTELLIGENCEThe report challenges companies to prioritize compliance with human rights standards and to perform rights-based due diligence. Among the recommendations is a  call for companies to actively include a diversity of input and norms in systems design.  Companies are also encouraged to provide mechanisms of access and redress that  make developers responsible for discriminatory outputs. In May 2018, Amnesty International and Access Now led the drafting of The Toron to Declaration: Protecting the Rights to Equality and Non-Discrimination in Machine  Learning Systems .30 The document grounds the current attention on AI bias in binding  international legal principles. The Toronto Declaration outlines the responsibilities  of both states and private sector actors in respect to the use of machine learning  systems, including mitigating discriminatory effects, transparency, and provision of  effective remedies to those harmed. It remains to be seen how influential the declaration will become, as organizers are currently in the process of seeking endorsements,  particularly from AI companies. Even so, it represents a significant effort to translate  fundamental human rights for the AI space. POLITICAL PARTICIPATION A report from the Brookings Institution states that “advancements in artificial intelligence and cyber capabilities will open opportunities for malicious actors to undermine  democracies more covertly and effectively than what we have seen so far.”31 Russian  disinformation campaigns through automated bots on social media have been highlighted by researchers as attempts to interfere with the 2016 American presidential  election.32 Because AI is being designed to mimic human behavior in online conver sations, detecting those online bots that are weaponized to spread disinformation in  political discourse could become more difficult. Bots have many useful purposes, including helping search engines find content. Yet those designed for malicious purpos es, such as spreading disinformation, have been identified on platforms like Twitter,33  which undercuts the possibility of an informed citizenry as needed for meaningful  democratic elections. VIEWED THROUGH THIS HUMAN RIGHTS LENS, THE  CO-OPTED USE OF AN AUTOMATED SYSTEM BY  A BAD FAITH ACTOR CREATES A HUMAN RIGHTS  LIABILITY THAT DEMANDS REDRESS. Mark Zuckerberg’s written submission to the US Congress in 2018 stated how badfaith actors, in this instance operatives of the Russian government, have been able  to manipulate the political process through Facebook.34 Further studies have demonstrated that bots have and continue to be used to manipulate the media in countries  across the world to interfere with the outcomes of democratic elections.35
DATA & SOCIETY   13 GOVERNING ARTIFICIAL INTELLIGENCEThe rights around political participation are referenced, for example, in the right to  self-determination and the right to equal participation in political and public affairs in  the ICCPR. Viewed through this human rights lens, the co-opted use of an automated  system by a bad-faith actor creates a human rights liability that demands redress. Yet  finding the right remedy is one of the most contentious areas in platform technology  today. Platforms are more likely to remove bots because they violate their terms of  service rather than to protect users’ right to political participation. Further exploration  would be needed to see how human rights principles could inform contractual dis putes or litigation in this context. PRIVACY Privacy has long been a major concern for a broad field that includes government,  business, academia, and civil society organizations. For example, there has been a  surge in interest from developers and engineers to follow privacy-by-design36 principles, which demonstrate how norms can be incorporated at the systems-design level.  Conducting a privacy impact assessment for technological deployments is an established tool for privacy compliance. Yet we already see tensions around the human  right to privacy and AI development. For instance, Stanford University researchers trained a deep neural network to “predict” the sexual orientation of their subjects, without obtaining consent, using a set of  images collected from online dating websites.37 Beyond various methodological short comings, the research demonstrated how a disregard for privacy rights increases the  risks of algorithmic surveillance, where data that is collected and analyzed threatens  to reveal personal information about users. This can put individuals and groups at risk,  particularly those living under regimes that would use such information to repress and  discriminate.38 IF AI DEVELOPERS TREAT PRIVACY NOT AS AN  ETHICAL PREFERENCE BUT AS A FUNDAMENTAL  HUMAN RIGHT, IT WOULD STRENGTHEN THE  PRIVACY CONSIDERATIONS THAT ALREADY EXIST IN  INDUSTRY NORMS AND TECHNICAL STANDARDS. Another example is Amazon’s AI-powered facial recognition software, which was made  widely available in 2016. In July 2018, American Civil Liberties Union (ACLU) researchers  ran an experiment matching pictures of all 535 members of Congress to a database of  25,000 public images of arrested individuals. Researchers found that the software not  only produced 28 false matches but was also racially biased. Since Amazon has sold this  software to police departments, the ACLU expressed concern about further use of facial  recognition for government surveillance, which is pervasive, opaque, and unregulated.39
DATA & SOCIETY   14 GOVERNING ARTIFICIAL INTELLIGENCEIf AI developers treat privacy as a fundamental human right rather than an ethical  preference, the privacy considerations that already exist in industry norms and technical standards would be stronger.40 The right to privacy is found in Article 12 of the  Universal Declaration, Article 17 of the ICCPR , and in a number of other human rights  documents, national constitutions, and national laws.41 International human rights law and principles around privacy can help AI developers  analyze, identify, and respond to emerging risks. The AI capabilities demonstrated  by the Stanford study give a glimpse into how AI can threaten privacy: both through  the rampant collection of data and the capacity for de-anonymizing subjects. These  concerns have recently been documented in a report by human rights organizations  Article 19 and Privacy International, which notes that “AI-driven consumer products . . .  are frequently equipped with sensors that generate and collect vast amounts of data  without the knowledge or consent of those in its proximity.”42 The report states that  AI can be used to infer sensitive facts from relatively mundane data, learning about  people’s emotional states, health, politics, and others from data like location histories  and social media interactions.43 Protecting the right to privacy is key to the enjoyment  of a number of related rights, such as freedoms of expression, association, political  participation, and information. FREEDOM OF EXPRESSION The right to freedom of expression is particularly important in an environment wherein social media platforms use algorithms that decide whose voices we hear. In 2014,  researchers from Cornell collaborated with Facebook to undertake an emotional-contagion study, examining how emotions spread through the social network. The  researchers manipulated the experience of almost 700,000 Facebook users by using  a sentiment analysis tool to identify if friends posted negative comments or posts.  Those negative posts were then removed from users’ newsfeeds to test whether  algorithmically skewing the feed to display positive posts would keep users on the site  longer.44 This study demonstrates how platforms can make decisions based on users’  expressions that cause the world to appear in certain ways, strengthening one reality  while weakening another.45 The right to freedom of expression is a cornerstone of fundamental human rights  found in Article 19 of both the Universal Declaration and the ICCPR.46  As social media platforms become the central place where public discussion happens,  there is a strong debate about the role of platforms in content moderation.47 With hate  speech, false news, and media manipulation circulating on platforms like Facebook and  Twitter, legislators and the public are calling for companies to address the problem. These calls to action are met with concerns about how private companies can meaningfully determine the boundaries of speech. For example, David Kaye, the UN’s  special rapporteur on the right to freedom of opinion and expression, has expressed 
DATA & SOCIETY   15 GOVERNING ARTIFICIAL INTELLIGENCEconcern that content moderation systems could, even unintentionally, censor minority  opinions and other unpopular yet critical forms of free expression.48 The NYU’s Center  for Business and Human Rights argues for more governance of technology platforms  but states that government intervention can also do harm.49 The resource-intensive and relentless task of content moderation requires making  difficult decisions on both standards and their subsequent application. Kaye goes on  to indicate that in this murky environment, putting human rights at the center of this  debate offers states, companies, and key stakeholders practical standards to guide  content regulation, which would also apply when deploying automated systems.50  While this report cannot delve far into this contentious issue,51 it is important to note  that a human rights perspective informs us that few human rights are absolute. The  decisions about tradeoffs involve questions around proportionality, that is, balancing  the legal and social impact relative to multiple rights. A rights-based frame offers  language to analyze the balance between the right to the freedom of expression with  rights and freedoms such as political participation, information, assembly, association,  privacy, and security. THE DISABILITY RIGHTS APPROACH AND ACCESSIBLE DESIGN The Convention on the Rights of Persons with Disabilities, adopted at the UN in 2006,  reaffirms that anyone with a disability should be treated with human dignity and be  included in the enjoyment of fundamental human rights. Disability rights have become emblematic of how technological development increases the risk to vulnerable  groups. They also present a clear opportunity to enhance human rights. The convention has been signed by 161 countries and ratified by 177 countries; it reaffirms rights  such as nondiscrimination and establishes principles like universal design and accessibility of communications.52 With the backing of international human rights, country-specific regulation, industry standards, and guidance (or pressure) from disability  rights activists, AI developers could mitigate the risk of AI’s disparate impacts on  people with disabilities at the design stage rather than after deployment. In a 2018 report on new technologies and AI, the Australian Human Rights Commis sion identified lack of accessibility as a fundamental concern.53 The report observes  that almost one in five Australians live with a disability and argues that these citizens’  rights are protected under international human rights law and the Convention on the  Rights of Persons with Disabilities, which the government of Australia ratified, along  with related national laws. The commission lays out a plan that leverages international  human rights law, Australian law, nonbinding guidelines, compliance frameworks, accessibility models, and necessary stakeholder consultations to address the issue.54 The Convention on Disabilities was partly inspired by the American Disabilities Act  (ADA).55 A law backed by US enforcement mechanisms, the ADA has become influential for tech industries. Apple, for example, has become a market leader in accessibly  features such as automated programs that verbalize on-screen content for the blind.56 
DATA & SOCIETY   16 GOVERNING ARTIFICIAL INTELLIGENCEYet other companies, such as Netflix, did not automatically comply with the ADA simply because it became law. It took years of advocacy and public pressure from disability rights groups and a lawsuit by the National Association for the Deaf to force Netflix  to make its platform accessible by adding closed captioning to its online videos. The  industry has also been motivated to change its policies on accessible design because  of structured negotiations, a method of dispute resolution between advocates and  companies, rather than lawsuits.57 This underscores a broader point: Applying human  rights to AI governance on an international level entails more than law and regulation.  Change needs to come from additional forces, including market incentives, public  awareness, local activism, and the technological innovation that would make compliance easier. THOSE WORKING ON DISABILITY RIGHTS HAVE  MADE GREAT PROGRESS – AT LEAST WITH SOME  OF THE BIG TECH COMPANIES – AND THIS CAN BE  A MODEL FOR HOW PRODUCT DESIGN COULD BE  INTEGRATED WITH OTHER KINDS OF RIGHTS. This precedent illustrates how AI companies and developers can use an inherent  respect of human dignity and human rights to act on anticipated harms. By proactively  addressing negative impacts, as in the case of accessibility for people with disabilities,  developers can take steps to advance human rights. In a similar way, those working on  AI can look at any number of human rights issues to anticipate both social risks and  benefits—rights like health and education or issues like migration or the protection of  marginalized groups in conflict zones like Myanmar. This only scratches the surface of  how human rights can be applied to AI systems. Yet fully integrating a human rights approach to building and maintaining AI systems  would require change in tech industry culture and organizations. For example, we  would need to see human rights integrated into product and design teams, not just in  statements of corporate social responsibility. Human rights values would need to be  infused into the workflow of the organization as part of the jobs of employees working  on quality assurance, test suites, and product design documentation. Those working  on disability rights have made great progress – at least at some of the big tech companies – and this can be a model for how to work with other kinds of rights.58
DATA & SOCIETY   17 GOVERNING ARTIFICIAL INTELLIGENCESTAKEHOLDER OVERVIEW Recently, stakeholders have initiated a number  of activities at the intersection of AI and human  rights. This section provides a snapshot of the  current landscape for business, government,  intergovernmental organizations, civil society,  and academia. The discussion will focus on  AI and human rights activity in business, with  short examples of activities by civil society,  governments, the UN, intergovernmental  organizations, and academia. 
DATA & SOCIETY   18 GOVERNING ARTIFICIAL INTELLIGENCEBUSINESS WHILE AI “ETHICS” IS THE TOPIC DU JOUR FOR THE TECH  INDUSTRY, HUMAN RIGHTS IS BEGINNING TO EMERGE AS AN  ADDITIONAL PERSPECTIVE.  Microsoft completed the first Human Rights Impact Assessment (HRIA) on  AI for a major tech company, to be released in late 2018.59 HRIAs are an es tablished methodology for the business sector that are used to examine the  impact of a product or action from the viewpoint of the rights holders—whether  they be direct consumers or external stakeholders.60 Depending on how trans parent Microsoft’s HRIA report will be, others in the industry may learn how  to operationalize human rights due diligence for AI products. In addition, new  AI-specific approaches, such as algorithmic impact assessments, may inform  traditional HRIA practices.61 Increasingly, advocates and concerned employees have been able to exert  meaningful pressure on big tech companies. After political firm Cambridge  Analytica surreptitiously gained access to the private data of tens of millions  of Facebook users to influence their voting behavior, a number of nonprofits  and investment groups sent an open letter to Facebook’s largest institutional  shareholders. The letter claimed that the company “is failing to both assess  and address longstanding — yet urgent — human rights problems, including  critical concerns regarding civil, political and privacy rights.”62 Furthermore, tech workers have organized public campaigns to pressure their  employers to stop building technologies and AI that may be used by governments for social harm.63 In April 2018, around 4,000 Google employees sent a  letter to their CEO demanding the company cease its contract to participate in  an AI development project called Maven with the US Department of Defense.64  The letter cited “biased and weaponized AI.” Google later said it planned to  not renew its contract with project Maven. In June 2018, Google released a  statement of its AI principles, which said it would still work with the defense  industry but will not develop AI weapons.65 Importantly, Google states that it  will not design or deploy AI technologies “whose purpose contravenes widely  accepted principles of international law and human rights.”66 In August 2018,  1,400 Google employees signed another letter expressing concern about of  the company’s work on a search engine in China that would censor content.67  While Google’s AI principles indicate some engagement with human rights,  both of the Google employee letters appealed to moral and ethical concerns,  in vague terms, rather than human rights. 
DATA & SOCIETY   19 GOVERNING ARTIFICIAL INTELLIGENCEThese letters follow a trend of tech employees pressuring their companies to  live up to their aspirational missions on hot button issues such as migration  and refugees.68 In June 2018, Microsoft received a letter from 300 of its employees demanding that it cancel its contract with US Immigration and Cus toms Enforcement (ICE), citing human rights concerns related to ICE’s forced  separation of migrant and refugee parents from their children at the border.69  In July 2018, the president of Microsoft released a statement calling for gov ernment regulation of facial recognition technology with an explicit appeal to  protect fundamental human rights such as freedom of expression and privacy.  The statement reveals that customer requests for facial recognition have been  turned down by the company when the deployments were determined to pres ent a greater human rights risk.70 While it is unclear if these determinations  were made with a formal HRIA, they underscore the importance of developing  a litmus test for whether a given AI-driven technology should be deployed in a  specific context based on human rights risks. ONE OF THE TRAGEDIES OF THE FACEBOOK  MYANMAR ISSUE IS THAT THE COMPANY  APPARENTLY DID NOT RESPOND ADEQUATELY TO  REPEATED ATTEMPTS BY CIVIL SOCIETY, HUMAN  RIGHTS GROUPS, AND ACADEMIC RESEARCHERS TO  ALERT THE COMPANY THAT HATE GROUPS WERE  USING THE PLATFORM TO HARM OTHER USERS.   Some tech companies are already teaming up with civil society organizations  to address social harms. One of the tragedies of the Facebook Myanmar  issue is that the company apparently did not respond adequately to repeated  attempts by civil society, human rights groups, and academic researchers to  alert the company that hate groups were using the platform to harm other us ers.71 What would it take for Facebook or a similar technology organization to  meaningfully “hear” human rights activists and researchers? Far more work is  needed to develop a functioning model where tech industry leadership listens  and acts on these warning signs. Since the Senate hearings, Facebook stated  that it would implement positive changes, such as hiring more staff to monitor  the problem, including some with Burmese language skills, and working with  local groups to identify hate speech. Nonetheless, a report by Reuters and the  Human Rights Center at the University of California–Berkeley Law School has  subsequently found that the platform’s efforts have not effectively curtailed  hate speech in Myanmar.72
DATA & SOCIETY   20 GOVERNING ARTIFICIAL INTELLIGENCECIVIL SOCIETY WHILE SOME MAJOR INTERNATIONAL HUMAN RIGHTS  ORGANIZATIONS ARE STARTING TO FOCUS ON AI,  ADDITIONAL ATTENTION IS NEEDED FROM CIVIL SOCIETY ON  POTENTIAL RISKS AND HARMS.  It can be difficult for civil society organizations, especially smaller ones in the  Global South, to find ways to engage with AI. Organizations in developing  countries may therefore see the AI field dominated by powerful countries. • In 2017, Amnesty International launched an initiative on AI and human rights,  noting that “AI is built by humans and it will be shaped by human values. If we  build AI systems that are a mirror to our current societies, they will be riddled  with the historical biases and inequalities of our societies.”73 Human Rights  Watch is also building a program investigating AI impacts. • The 2017 Global Symposium on Artificial Intelligence and Inclusion in Brazil  highlighted the need to foster diverse voices in AI research, development,  policy-making, and advocacy.74 • The Digital Asia Hub in Hong Kong has been leading AI discussions in the  Asia-Pacific and provides a model of research and engagement on topics  important for the region. • The WEF is continuing its council to address the future of human rights and  developing a project on preparing civil society to respond to the challenges of  digital and emerging technologies such as AI.75 GOVERNMENTS DOZENS OF COUNTRIES HAVE INITIATED NATIONAL  STRATEGIES ON AI, YET HUMAN RIGHTS ARE NOT CENTRAL TO  MANY OF THESE EFFORTS.  Some governments are seeking more of a regulatory stance, while others are  starting to focus on the human rights impact of AI. • Few states have enacted legislation that would direct the human rights impact  of AI, yet the European Union has demonstrated an interest in regulating technology companies with an appeal to rights-based principles. The EU’s General  Data Protection Regulation (GDPR) establishes new protections for European  citizens’ rights around data protection and privacy, which impacts any organization collecting European residents’ data.76 
DATA & SOCIETY   21 GOVERNING ARTIFICIAL INTELLIGENCE• A Council of Europe study notes that “there is growing concern at the political  and public level globally regarding the increased use of algorithms and automated processing techniques and their considerable impact on the exercise of  human rights.”77 Moreover, in 2018, the Council of Europe’s Commissioner for  Human Rights argued for safeguarding human rights in the era of AI, particular ly the rights of privacy and equality and freedoms of expression and assembly. • In June 2018, Canada and France called “for the creation of an international  study group that can become a global point of reference for understanding  and sharing research results on artificial intelligence issues and best practices.”78 Global Affairs Canada’s Digital Inclusion Lab has been leading discus sions on AI and human rights, and Canada’s Treasury Board is exploring an  Algorithmic Impact Assessment for its procurement practices, which includes  how these systems impact individual liberty rights.79 • In 2018, the Australia Human Rights Commission launched a project to directly  address the human rights impact of AI and emerging technologies, which includes a robust engagement of international human rights law and may serve  as a guide for other countries.80 • In 2017, New York City passed a law that aims to help ensure that algorithms  used by city agencies are transparent, fair, and valid by setting up a task force  to make recommendations on algorithmic regulation, transparency, and bias.  While these rules apply only to New York and do not appeal to human rights  directly, this move to regulate AI may become a model for other cities.81 UNITED NATIONS THE UN HAS YET TO SUSTAIN A FOCUS ON AI FROM A  RIGHTS PERSPECTIVE82 WITH SOME NOTABLE EXCEPTIONS,  PARTICULARLY FROM UN INDEPENDENT INVESTIGATORS,  SPECIAL RAPPORTEURS, AND THE SECRETARY GENERAL’S  STRATEGY ON NEW TECHNOLOGY. • As noted, UN investigators found evidence that Facebook was used to exacerbate hate and violence in Myanmar, despite the platform’s use of algorithms  and AI to identify such hate speech.83 • The UN special rapporteur on the right to freedom of opinion and expression  David Kaye is expected to present a report in late 2018, which investigates  the impact of AI and the responsibilities of tech companies to protect human  rights. After his official 2017 visit to the US, UN special rapporteur on extreme  poverty and human rights Philip Alston urged actors to give more attention to  
DATA & SOCIETY   22 GOVERNING ARTIFICIAL INTELLIGENCEthe role of automation and robotization on job insecurity, as well as to examine  possible solutions like universal basic income.84 • The UN has addressed the pressing issue of autonomous weapons. In addition  to international humanitarian law of conduct in war, there is a strong human  rights dimension, such as the right to security of person for civilians caught in  conflict, which needs to be further examined.85 • In September 2018, The UN Secretary General released a strategy on new  technologies that seeks to align the use of technologies like AI with global  values found in the UN Charter, the UDHR, and international law.86 INTERGOVERNMENTAL ORGANIZATIONS INTERGOVERNMENTAL ORGANIZATIONS MAY PLAY AN  INFLUENTIAL ROLE, INCLUDING THE ORGANISATION FOR  ECONOMIC COOPERATION AND DEVELOPMENT (OECD),  WHICH IS PREPARING GUIDANCE RELATED TO AI FOR ITS 36  MEMBER COUNTRIES.87  The OECD’s 2011 Guidelines for Multinational Enterprises aligns with the  UN Guiding Principles for Human Rights by calling for companies to protect  human rights in the countries they operate in, conduct human rights due  diligence, and provide a mechanism for accountability. Notably, the OECD  guidance requires a system of National Contact Points, for which governments  in member countries appoint representatives that will hear grievances about  company conduct.88 The National Contact Point system is a potential example  of a mechanism for redress. Although the OECD produces “soft law” that is  non-judicial and nonbinding, it could provide a forum to address human rights  impacts that arise from AI deployed by companies.89 
DATA & SOCIETY   23 GOVERNING ARTIFICIAL INTELLIGENCEACADEMIA MORE WORK CAN BE DONE TO BRIDGE ACADEMICS IN  HUMAN RIGHTS LAW, SOCIAL SCIENCE, COMPUTER SCIENCE,  PHILOSOPHY, AND OTHER DISCIPLINES IN ORDER TO  CONNECT RESEARCH ON THE SOCIAL IMPACT OF AI, NORMS  AND ETHICS, TECHNICAL DEVELOPMENT, AND POLICY. • Scholars at Harvard University have published a report on AI and human rights  for the Canadian government; have called for an urgent human rights agenda  for AI; and have argued that any emerging model for AI governance “must be  situated in and interact with existing institutional frameworks of applicable  laws and policies, particularly human rights.” 90 • The University of Essex has urged the House of Lords of the United Kingdom  Select Committee on AI that “a human rights-based approach should sit at the  centre of the development and use of artificial intelligence, enabling a more  holistic, consistent, universal, and enforceable approach.”  91 • In June 2018, Stanford University’s Global Digital Policy Incubator held a  cross-disciplinary conference on AI, design, and human rights.92 The confer ence included a conversation between the UN High Commissioner for Human  Rights and a founder of the Silicon Valley organization OpenAI. The discussion  highlighted the need for more translation work between human rights proponents and technologists. 
DATA & SOCIETY   24 GOVERNING ARTIFICIAL INTELLIGENCECONCLUSION Today’s AI debates are searching for principles to govern emerging and future technological systems for the common good. If the “good” involves upholding human  dignity, then the international human rights system is fit for purpose. If AI researchers,  developers, and designers work to protect and respect fundamental human rights,  they could open the path for broad social benefit. To disregard human rights would be  to close off that path. IF AI RESEARCHERS, DEVELOPERS, AND  DESIGNERS WORKED TO PROTECT AND RESPECT  FUNDAMENTAL HUMAN RIGHTS, IT COULD OPEN  THE PATH FOR SOCIAL BENEFIT. TO DISREGARD  HUMAN RIGHTS WOULD BE TO CLOSE OFF THAT  PATH. Of course, human rights have limitations. While international human rights law and  principles cover a broad class of risks and harms, they are not equipped to address  all of the known and unknown concerns pertaining to AI. There will be instances  where AI systems have negative social impacts that are not identifiable or anticipat ed in terms of human rights. While the human rights systems are supported globally  by deliberative bodies such as the UN and have gained legitimacy through the treaty  system and national laws, there are still many critics. The very terms “human rights”  and “human dignity” have long histories replete with intense controversies about their  intrinsic philosophical and political value.93 And reflecting on our current geopolitical  moment, the outgoing UN High Commissioner for Human Rights has warned that the  universal rights system is under attack from chauvinistic nationalism that promotes  self-interest over the common good.94 
DATA & SOCIETY   25 GOVERNING ARTIFICIAL INTELLIGENCEEven if we bring human rights to the center of AI governance discussions, gaps would  remain between rights, principles, design and development, deployment, and usage.  Integrating any desirable value into a sociotechnical system is a perennial challenge.95  In essence, a human rights approach to AI would need to be fully integrated into the  organizational contexts of technologists, academics, and researchers, as well as in  the social contexts of users. In doing so, we may also find that AI might influence the  evolution of human rights and dignity in the future.  Near-term work in this area should focus on how a human rights approach could be  practically implemented through policy, practice, and organizational change. Further  to this goal, this report offers some initial recommendations: • Technology companies should find effective channels of communication with local civil  society groups and researchers – particularly in geographic areas where human rights  concerns are high – in order to identify and respond to risks related to AI deployments.  • Technology companies and researchers should conduct HRIAs throughout the life  cycle of their AI systems. Researchers should reevaluate HRIA methodology for AI,  particularly in light of new developments in algorithmic impact assessments. Toolkits  should be developed to assess specific industry needs. • Governments should acknowledge their human rights obligations and incorporate a  duty to protect fundamental rights in national AI policies, guidelines, and possible regulations. Governments can play a more active role in multilateral institutions, like the UN,  to advocate for AI development that respects human rights. • Since human rights principles were not written as technical specifications, human  rights lawyers, policy makers, social scientists, computer scientists, and engineers  should work together to operationalize human rights into business models, workflows,  and product design. • Academics should further examine the value, limitations, and interactions between human rights law and human dignity approaches; humanitarian law; and ethics in relation  to emerging AI technologies. Human rights and legal scholars should work with other  stakeholders on the tradeoffs between rights when faced with specific AI risks and  harms. Social science researchers should empirically investigate the on-the-ground  impact of AI on human rights. • UN human rights investigators and special rapporteurs should continue researching  and publicizing the human rights impacts resulting from AI systems. UN officials and  participating governments should evaluate whether existing UN mechanisms for inter national rights monitoring, accountability, and redress are adequate to respond to AI  and other rapidly emerging technologies. UN leadership should also assume a central  role in international technology debates by promoting shared global values based on  fundamental rights and human dignity. 
DATA & SOCIETY   26 GOVERNING ARTIFICIAL INTELLIGENCENOTES 1  “Ethically Aligned Design: A Vision for Prioritizing Human Well-Being with Autonomous and Intelligent Systems (version 2),” IEEE, December 2017, http:/ /standards.ieee.org/develop/indconn/ec/ ead_brochure_v2.pdf . The IEEE’s “Ethically Aligned Design” provides the organizing ideas for a series  of proposed technical standards governing ethical AI. While this document is still in development, it  represents over two years of stakeholder consultations and a milestone in bridging technologists  to human rights principles. It should be noted, however, that this document is not an official code of  conduct or formal code of ethics for IEEE members. 2  “Statement by Mr. Marzuki Darusman, Chairperson of the Independent International Fact-Finding  Mission on Myanmar, at the 37th session of the Human Rights Council,” United Nations Office of the  High Commissioner for Human Rights, March 12, 2018, https:/ /www.ohchr.org/EN/HRBodies/HRC/ Pages/NewsDetail.aspx?NewsID=22798&LangID=E . 3  “Facebook, Social Media Privacy, and the Use and Abuse of Data,” US Senate Committee on the  Judiciary, April 10, 2018, https:/ /www.judiciary.senate.gov/meetings/facebook-social-media-privacy-and-the-use-and-abuse-of-data ; “Transcript of Mark Zuckerberg’s Senate Hearing,” The Wash ington Post , April 10, 2018, https:/ /www.washingtonpost.com/news/the-switch/wp/2018/04/10/ transcript-of-mark-zuckerbergs-senate-hearing/?utm_term=.b0a2f734d2dc; “Facebook, Inc.  (FB) First Quarter 2018 Results Conference Call,” Facebook, April 25, 2018, https:/ /s21.q4cdn. com/399680738/files/doc_financials/2018/Q1/Q1- 18-Earnings-call-transcript.pdf . 4  “AI for Good Global Summit 2018,” ITU, last accessed August 26, 2018, https:/ /www.itu.int/en/ ITU-T /AI/2018/Pages/default.aspx. 5  While the reputational risks are clear, there is a debate on whether or not corporations can be  held legally accountable under international human rights law. For an overview that suggests that  corporations are accountable, see International Human Rights Law and Corporations and Sufyan  Droubi, “Transnational Corporations and International Human Rights Law,” Notre Dame Journal of  International and Comparative Law vol. 6, issue 1 (September 2016), https:/ /scholarship.law.nd.edu/ cgi/viewcontent.cgi?article=1048&context=ndjicl. 6  The Guiding Principles on Business and Human Rights , United Nations, June 16, 2011, https:/ / www.ohchr.org/Documents/Publications/GuidingPrinciplesBusinessHR_EN.pdf . 7  Other industry guidelines, also known as “soft” law, include the OECD Guidelines for Multinational  Enterprises (2011), http:/ /mneguidelines.oecd.org/guidelines and “ISO 26000:2010,” International  Organization for Standardization, last reviewed and confirmed in 2014, https:/ /www.iso.org/standard/42546.html. 8  Christiaan van Veen and Corinne Cath, “Artificial Intelligence: What’s Human Rights Got T o Do  With It?,” Data & Society Points, May 14, 2018, https:/ /points.datasociety.net/artificial-intelligencewhats-human-rights-got-to-do-with-it-4622ec1566d5.
DATA & SOCIETY   27 GOVERNING ARTIFICIAL INTELLIGENCE9  See Madeleine Elish and danah boyd, Situating Methods in the Magic of Big Data and Artificial  Intelligence. Communication Monographs , September 19, 2017, https:/ /www.tandfonline.com/doi/abs /10. 1080/03637751.2017. 1375130 . 10  See, e.g., Kristian Lum and William Isaac. T o Predict and Serve? The Royal Statistical Society ,  October 7, 2016, https:/ /rss.onlinelibrary.wiley.com/doi/full/10. 1111/j. 1740-9713.2016.00960.x; Solon Barocas and Andrew D. Selbst, “Big Data’s Disparate Impact,” California Law Review  104, 671  (September 2016), https:/ /papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899##; Arvind  Narayanan, “Translation Tutorial: 21 Definitions of Fairness and Their Politics,” presented at the  Conference on Fairness, Accountability, and Transparency, New Y ork, April 18, 2018, https:/ /www. youtube.com/watch?v=wqamrPkF5kk . 11  According to the Royal Society definition: “Machine learning systems are set a task, and given  a large amount of data to use as examples of how this task can be achieved or from which to detect  patterns. The system then learns how best to achieve the desired output. It can be thought of as  narrow AI: machine learning supports intelligent systems, which are able to learn a particular function,  given a specific set of data to learn from.” Machine Learning: The Power and Promise of Computers  That Learn by Example , The Royal Society (April 2017): 19, https:/ /royalsociety.org/~/media/policy/ projects/machine-learning/publications/machine-learning-report.pdf . 12  It is important to note that bias in a technical sense is not necessarily negative or wrong. For  example, we may want to introduce bias in a model to correct for inequitable outcomes for an under represented group. Then again, it is difficult to separate statistical bias from social bias in our current  debates. For a brief discussion of the nuances of bias in AI discussions see AI Now 2017 Report ,  https:/ /ainowinstitute.org/AI_Now_2017_Report.pdf . 13  A vibrant community of academic researchers and practitioners are focused on fairness, accountability, and transparency. See, e.g., Proceedings of Machine Learning Research , vol. 81 (February 2018), http:/ /proceedings.mlr.press/v81 . 14  Universal Declaration of Human Rights, United Nations, adopted December 10, 1948, http:/ / www.un.org/en/universal-declaration-human-rights/. 15  For an overview of regional human rights implementation in the Americas, Europe, and Africa,  see David C. Baluarte and Christian De Vos, From Judgment to Justice: Implementing International  and Regional Human Rights Decisions , Open Society Justice Initiative (November 2010), https:/ / www.opensocietyfoundations.org/sites/default/files/from-judgment-to-justice-20101122.pdf . 16  For an overview of the challenges of implementation, see International Institutions and Global  Governance Program, “The Global Human Rights Regime,” Council on Foreign Relations, May 11,  2012, Web, August 31, 2018. 17  See, e.g., Joy Buolamwini and Timnit Gebru “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” Proceedings of Machine Learning Research 81:1– 15, 2018  Conference on Fairness, Accountability, and Transparency; by developing a new face dataset that is  “more phenotypically balanced on the basis of skin type than existing benchmarks,” the researchers 
DATA & SOCIETY   28 GOVERNING ARTIFICIAL INTELLIGENCEwere able to demonstrate that the commercially available gender classifying algorithms they tested  have the lowest accuracy on darker skinned females. See also T om Simonite, “Machines T aught by  Photos Learn a Sexist View of Women,” Wired, August 21, 2017, https:/ /www.wired.com/story/machines-taught-by-photos-learn-a-sexist-view-of- women/. 18  van Veen and Cath, “Artificial Intelligence: What’s Human Rights Got T o Do With It?,” supra note  8; also see Jason Pielemeier, “The Advantages and Limitations of Applying the International Human Rights Framework to Artificial Intelligence.” Data & Society Points, June 6, 2018, https:/ /points. datasociety.net/the-advantages-and-limitations-of-applying-the-international-human-rights-framework-to-artificial-291a2dfe1d8a .  19  “World Conference on Human Rights, 14-25 June 1993, Vienna, Austria,” Office of the United  Nations High Commissioner for Human Rights, last accessed August 26, 2018, https:/ /www.ohchr. org/en/aboutus/pages/viennawc.aspx. 20  E.g., Joshua A. Kroll, et al., “Accountable Algorithms,” University of Pennsylvania Law Review  vol.  165, issue 3 (2017), https:/ /scholarship.law.upenn.edu/penn_law_review/vol165/iss3/3/; Borocas and  Selbst, “Big Data’s Disparate Impact,” supra  note 10; Frank Pasquale, Black Box Society (Cambridge,  Harvard University Press, 2015); “Workshop Primer: Ethics and AI,” AI Now, July 7, 2016, https:/ /ainow institute.org/AI_Now_2016_Primers.pdf ; Alex Campolo, et al., “AI Now 2017 Report,” AI Now , https:/ / ainowinstitute.org/AI_Now_2017_Report.pdf . 21  Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin, “How We Analyzed the COMPAS  Recidivism Algorithm,” ProPublica , May 23, 2016, https:/ /www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm. 22  Virginia Eubanks, Automating Inequality  (St. Martin’s Press: 2018); see also Safiya Umoja Noble,  Algorithms of Oppression: How Search Engines Reinforce Racism  (New Y ork University Press: 2018)  and Cathy O’Neil, Weapons of Math Destruction (Crown Random House: 2016). 23  Eubanks, Automating Inequality , chapter 4. 24  “Big Brother Is Watching: How China Is Compiling Computer Ratings on All Its Citizens,” South  China Morning Post International Edition , November 24, 2015, https:/ /www.scmp.com/news/china/ policies-politics/article/1882533/big-brother-watching-how-china-compiling-computer and “Notice  of the State Council on Printing and Distributing the Outline of the Construction of the Social Credit  System (2014-2020),” The State Council, the People’s Republic of China, June 27, 2014, http:/ /www. gov.cn/zhengce/content/2014-06/27/content_8913.htm. 25  Meg Jing Zeng, “China’s Social Credit System Puts Its People Under Pressure to Be Model  Citizens,” The Conversation, January 23, 2018, https:/ /theconversation.com/chinas-social-creditsystem-puts-its-people-under-pressure-to-be-model-citizens-89963. 26  Geoffrey C. Bowker and Susan Leigh Star, Sorting Things Out — Classification and Its Conse quences (MIT Press: 1999). 27  The Human Rights Council defines discrimination as any distinction, exclusion, restriction, or 
DATA & SOCIETY   29 GOVERNING ARTIFICIAL INTELLIGENCEpreference that is based on any ground, such as race, color, sex, language, religion, political or other  opinion, national or social origin, property, birth, or other status and which has the purpose or effect  of nullifying or impairing the recognition, enjoyment, or exercise by all persons, on an equal footing,  of all rights and freedoms. “CCPR General Comment No. 18: Non-Discrimination,” UN Human Rights  Committee (HRC), November 10, 1989, http:/ /www.refworld.org/docid/453883fa8.html. 28  How to Prevent Discriminatory Outcomes in Machine Learning , World Economic Forum, March  12, 2018, http:/ /www3.weforum.org/docs/WEF_40065_White_Paper_How_to_Prevent_Discriminatory_Outcomes_in_Machine_Learning.pdf . 29  Ibid. 30  “The T oronto Declaration: Protecting the Rights to Equality and Non-Discrimination in Machine  Learning systems,” May 2018, https:/ /www.accessnow.org/cms/assets/uploads/2018/05/T oronto-Declaration-D0V2.pdf . 31  Alina Polyakova and Spencer P . Boyer, “The Future of Political Warefare: Russia, the West, and  the Coming Age of Global Digital Competition,” 5, Brookings, March 2018, https:/ /www.brookings. edu/wp-content/uploads/2018/03/the-future-of-political-warfare.pdf . 32  Amelia Acker, “Tracking Disinformation by Reading Metadata,” D&S Media Manipulation: Dis patches from the Field , July 17, 2018, https:/ /medium.com/@MediaManipulation/tracking-disinformation-by-reading-metadata-320ece1ae79b . 33  Jon Swaine, “Twitter Admits Far More Russian Bots Posted on Election Than It Had Disclosed,”  The Guardian , January 19, 2018, https:/ /www.theguardian.com/technology/2018/jan/19/twitter-admits-far-more-russian-bots-posted-on-election-than-it-had-disclosed. 34  “Transcript of Mark Zuckerberg’s Senate Hearing,” The Washington Post,  supra note 3. 35  Alice Marwick and Rebecca Lewis, “Media Manipulation and Disinformation Online,”  Data &  Society, May 15, 2017, https:/ /datasociety.net/output/media-manipulation-and-disinfo-online/; see  also Jonathan Ong, “Architects of Networked Disinformation,” Newton T ech4Dev, February 5, 2018,  http:/ /newtontechfordev.com/newton-tech4dev-research-identifies-ad-pr-executives-chief-architects-fake-news-production-social-media-trolling. 36  See, e.g., Ann Cavoukian, Privacy by Design: The 7 Foundational Principles, Information and  Privacy Commissioner of Ontario, 2011, https:/ /www.ipc.on.ca/wp-content/uploads/Resources/ 7foundationalprinciples.pdf ; “Privacy and Data Protection by Design – From Policy to Engineering,”  European Union Agency for Network and Information Security, December 2014, https:/ /www.enisa. europa.eu/publications/privacy-and-data-protection-by-design. 37  Yilun Wang and Michal Kosinski, “Deep Neural Networks Are More Accurate Than Humans  at Detecting Sexual Orientation from Facial Images,” Journal of Personality and Social Psychology  (preprint), https:/ /osf.io/zn79k/.
DATA & SOCIETY   30 GOVERNING ARTIFICIAL INTELLIGENCE38  For an analysis and critique, see Jake Metcalf, “‘The Study Has Been Approved by the IRB’:  Gayface AI, Research Hype and the Pervasive Data Ethics Gap,” Pervade T eam , November 30, 2017,  https:/ /medium.com/pervade-team/the-study-has-been-approved-by-the-irb-gayface-ai-researchhype-and-the-pervasive-data-ethics-ed76171b882c; see also Melanie Penagos, “AI Systems and  Research Revealing Sexual Orientation Case Study,” AI and Human Rights Workshop, Data & Society  Research Institute, April 26-27, 2018, https:/ /datasociety.net/wp-content/uploads/2018/05/AI-Sys tems-and-Research-Revealing-Sexual-Orientation_Case-Study_Final_CC.pdf . 39  Jacob Snow, “Amazon’s Face Recognition Falsely Matched 28 Members of Congress with  Mugshots,” ACLU, July 26, 2018, https:/ /www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28. 40  “7002 – Data Privacy Process,” IEEE Standards Association, last accessed August 26, 2018,  https:/ /standards.ieee.org/develop/project/7002.html. 41  Article 17 of the ICCPR states: “No one shall be subjected to arbitrary or unlawful interference  with his privacy, family, home or correspondence, nor to unlawful attacks on his honour and reputation. Everyone has the right to the protection of the law against such interference or attacks.” “Inter national Covenant on Civil and Political Rights,” Office of the United Nations High Commissioner for  Human Rights, adopted December 16, 1966, https:/ /www.ohchr.org/en/professionalinterest/pages/ ccpr.aspx. 42  “Privacy and Freedom of Expression in the Age of Artificial Intelligence,” 17, Privacy International,  and Article 19, April 2018, https:/ /privacyinternational.org/sites/default/files/2018-04/Privacy%20 and%20Freedom%20of%20Expression%20%20In%20the%20Age%20of%20Artificial%20Intelligence.pdf . 43  Ibid. 44  Adam D. I. Kramer, et al., “Experimental Evidence of Massive-Scale Emotional Contagion Through Social Networks,” PNAS vol. 111 no. 24 (2014), http:/ /www.pnas.org/content/ pnas/111/24/8788.full.pdf . 45  T aina Bucher, If . . .  Then  (Oxford University Press: 2018). 46  Article 19 states: “Everyone shall have the right to hold opinions without interference . . . Every one shall have the right to freedom of expression; this right shall include freedom to seek, receive and  impart information and ideas of all kinds, regardless of frontiers, either orally, in writing or in print, in  the form of art, or through any other media of his choice.” “International Covenant on Civil and Political  Rights,” Office of the United Nations High Commissioner for Human Rights, adopted December 16,  1966, https:/ /www.ohchr.org/en/professionalinterest/pages/ccpr.aspx. 47  Philip M. Napoli and Robyn Caplan, “Why Media Companies Insist They’re Not Media Companies, Why They’re Wrong, and Why It Matters,” First Monday vol. 22, no. 5, May 1, 2017, http:/ /firstmonday.org/ojs/index.php/fm/article/view/7051/6124.
DATA & SOCIETY   31 GOVERNING ARTIFICIAL INTELLIGENCE48  Report of the Special Rapporteur on the Promotion and Protection of the Right to Freedom of  Opinion and Expression , United Nations General Assembly, April 6, 2018, https:/ /www.ohchr.org/EN/ Issues/FreedomOpinion/Pages/ContentRegulation.aspx. 49  NYU Stern Center for Business and Human Rights, Harmful Content: The Role of Internet Plat form Companies in Fighting T errorist Incitement and Politically Motivated Disinformation, November  3, 2017. 50  Report of the Special Rapporteur on the Promotion and Protection of the Right to Freedom of  Opinion and Expression ,  United Nations General Assembly, April 6, 2018, https:/ /www.ohchr.org/EN/ Issues/FreedomOpinion/Pages/ContentRegulation.aspx. 51  See, e.g. , Robyn Caplan, Lauren Hanson, and Joan Donovan, Dead Reckoning Navigating Con tent Moderation After Fake News  (Data & Society, 2018), https:/ /datasociety.net/output/dead-reck oning/. 52  Convention on the Rights of Persons with Disabilities and Optional Protocol, United Nations,  http:/ /www.un.org/disabilities/documents/convention/convoptprot-e.pdf . 53  Human Rights and T echnology Issues Paper, Australian Human Rights Commission , July 2018,  https:/ /www.humanrights.gov.au/sites/default/files/document/publication/AHRC-Human-RightsT ech-IP .pdf . 54  Ibid. 55  While the ADA may have influenced the Convention on the Rights of Persons with Disabilities,  President Barack Obama signed the Treaty in 2009, but the U.S. Senate has yet to ratify it. “Convention on the Rights of Persons with Disabilities,” United Nations Treaty Collection, status as of August  26, 2018, https:/ /treaties.un.org/Pages/ViewDetails.aspx?src=TREA TY&mtdsg_no=IV- 15&chapter=4&clang=_en. 56  “Accessibility,” Apple, last accessed August 26, 2018, https:/ /www.apple.com/accessibility ; Microsoft is also a market leader, e.g.,  see “AI for Accessibility,” Microsoft, last accessed August 26, 2018,  https:/ /www.microsoft.com/en-us/ai-for-accessibility . 57  Lainey Feingold, Structured Negotiation: A Winning Alternative to Lawsuits  (American Bar  Association: 2016); see also Lainey Feingold, “Shifting from Fear to Motivation when T alking about  Digital Accessibility Law,” 24 Accessibility , December 11, 2017, https:/ /www.24a11y.com/2017/shift ing-fear-motivation-talking-digital-accessibility-law/. 58  Keith Hiatt, personal interview, August 25, 2018. 59  “Microsoft Global Human Rights Statement,” Microsoft, last accessed Aug. 12, 2018, https:/ / www.microsoft.com/en-us/about/corporate-responsibility/human-rights-statement . 60  See “Conducting an Effective Human Rights Impact Assessment Guidelines, Steps, and 
DATA & SOCIETY   32 GOVERNING ARTIFICIAL INTELLIGENCEExamples,” Business for Social Responsibility , March 2013, https:/ /www.bsr.org/reports/BSR_Human_Rights_Impact_Assessments.pdf ; Human Rights Impact Assessment: Guidance and T oolbox,  The Danish Institute for Human Rights , 2016, https:/ /www.humanrights.dk/sites/humanrights.dk/ files/media/dokumenter/business/hria_toolbox/hria_guidance_and_toolbox_final_may22016. pdf_223795_1_1.pdf. 61  See Andrew D. Selbst, “Disparate Impact in Big Data Policing,” 52 Georgia L. Rev. 109 (2017),  https:/ /ssrn.com/ abstract=2819182; Dillon Reisman, Jason Schultz, Kate Crawford, and Meredith  Whittaker, Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability,  AI Now Institute (April 2018), https:/ /ainowinstitute.org/aiareport2018.pdf . 62  An Open Letter to the CEOs of Facebook’s Largest Institutional Shareholders” May 2,  2018, https:/ /static1.squarespace.com/static/57693891579fb3ab7149f04b/t/5ae9e204aa4a99634fa0a213/1525277190443/Open+Letter+to+Facebook+Investors_FinalFormat.pdf . 63  Ron Amadeo, “Google Employees Revolt, Say Company Should Shut Down Military Drone  Project,” Ars T echnica , April 4, 2018, https:/ /arstechnica.com/gadgets/2018/04/google-should-notbe-in-the-business-of-war-googlers-decry-pentagon-project and “Letter of Microsoft Workers to  Satya Nadella,” June 19, 2018, https:/ /int.nyt.com/data/documenthelper/46-microsoft-employee-let ter-ice/323507fcbddb9d0c59ff/optimized/full.pdf#page=1. 64  Daisuke Wakabayashi and Scott Shane, “Google Will Not Renew Pentagon Contract That Upset  Employees,” The New Y ork Times , June 1, 2018, https:/ /www.nytimes.com/2018/06/01/technology/ google-pentagon-project-maven.html. 65  Sundar Pichai, “AI at Google: Our Principles,” The Keyword , Google, June 7, 2018, https:/ /blog. google/technology/ai/ai-principles/. 66  For a further critique of Google’s principles from a human rights perspective, see Lorna McGregor and Vivian Ng, “Google’s New Principles on AI Need to Be Better at Protecting Human Rights,”  The Conversation , June 15, 2018, https:/ /theconversation.com/googles-new-principles-on-ai-needto-be-better-at-protecting-human-rights-98035. 67  Kate Conger and Daisuke Wakabayashi, “Google Employees Protest Secret Work on Censored Search Engine for China,” The New Y ork Times , August 16, 2018, https:/ /www.nytimes. com/2018/08/16/technology/google-employees-protest-search-censored-china.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT .nav=top-news. 68  Mark Latonero, “T ech Companies Should Speak Up for Refugees, Not Only High-Skilled Immigrants,” Harvard Business Review , May 16, 2017, https:/ /hbr.org/2017/05/tech-companies-shouldspeak-up-for-refugees-not-only-high-skilled-immigrants. 69  Letter of Microsoft Workers to Satya Nadella, June 19, 2018, https:/ /int.nyt.com/data/documenthelper/46-microsoft-employee-letter-ice/323507fcbddb9d0c59ff/optimized/full.pdf#page=1.
DATA & SOCIETY   33 GOVERNING ARTIFICIAL INTELLIGENCE70  Brad Smith, “Facial Recognition T echnology: The Need for Public Regulation and Corporate  Responsibility,” Microsoft, July 13, 2018, https:/ /blogs.microsoft.com/on-the-issues/2018/07/13/facial-recognition-technology-the-need-for-public-regulation-and-corporate-responsibility/. 71  Libby Hogan, “Myanmar Groups Criticise Zuckerberg’s Response to Hate Speech on Facebook,” The Guardian , April 5, 2018, https:/ /www.theguardian.com/technology/2018/apr/06/myanmar-facebook-criticise-mark-zuckerberg-response-hate-speech-spread. 72  Steve Stecklow, “Hatebook: Why Facebook Is Losing the War on Hate Speech in Myanmar,”  Reuters , August 15, 2018, https:/ /www.reuters.com/investigates/special-report/myanmar-facebook-hate/. 73  Salil Shetty, “Artificial Intelligence for Good,” Amnesty International, June 9, 2017, https:/ /www. amnesty.org/en/latest/news/2017/06/artificial-intelligence-for-good/. 74  David T albot, Levin Kim, Elena Goldstein, and Jenna Sherman, “Charting a Roadmap to Ensure Artificial Intelligence (AI) Benefits All,” Berkman Klein Center, November 30, 2017, https:/ / medium.com/berkman-klein-center/charting-a-roadmap-to-ensure-artificial-intelligence-ai-benefits-all-e322f23f8b59 . 75  “Preparing Civil Society for the Fourth Industrial Revolution,” World Economic Forum, https:/ / www.weforum.org/projects/preparing-civil-society-for-the-fourth-industrial-revolution. 76  “Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016  on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the  Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation),” EUR-Lex, last accessed August 26, 2018, https:/ /eur-lex.europa.eu/legal-content/EN/TX T /?qid=1532348683434&uri=CELEX:02016R0679-20160504. 77  Council of Europe, Algorithms and Human Rights: Study on the Human Rights Dimensions of  Automated Data Processing T echniques and Possible Regulatory Implications , March 2018, https:/ / rm.coe.int/algorithms-and-human-rights-en-rev/16807956b5. 78  “Canada-France Statement on Artificial Intelligence,” Government of Canada, last modified July  6, 2018, http:/ /international.gc.ca/world-monde/international_relations-relations_internationales/ europe/2018-06-07-france_ai-ia_france.aspx?lang=eng. 79  Digital Inclusion Lab, Global Affairs Canada, Artificial Intelligence and Human Rights: T owards  a Canadian Foreign Policy (unpublished draft), 2018; Michael Karlin, “A Canadian Algorithmic Impact  Assessment,” March 18, 2018, https:/ /medium.com/@supergovernance/a-canadian-algorithmic-impact-assessment- 128a2b2e7f85 (note this is a personal blog post rather than a government statement). 80  “Human Rights and T echnology,” Australian Human Rights Commission, May 22, 2018, https:/ / www.humanrights.gov.au/our-work/rights-and-freedoms/projects/human-rights-and-technology .
DATA & SOCIETY   34 GOVERNING ARTIFICIAL INTELLIGENCE81  Julia Powles, “New Y ork City’s Bold, Flawed Attempt to Make Algorithms Accountable,” The  New Y orker , December 20, 2017, https:/ /www.newyorker.com/tech/elements/new-york-citysbold-flawed-attempt-to-make-algorithms-accountable and “A Local Law in Relation to Automated  Decision Systems Used by Agencies,” The New Y ork City Council, http:/ /legistar.council.nyc.gov/ LegislationDetail.aspx?ID=3137815&GUID=437A6A6D-62E1-47E2-9C42-461253F9C6D0 . 82  van Veen and Cath, “Artificial Intelligence: What’s Human Rights Got T o Do With It?,”   supra note 8. 83  Statement by Mr. Marzuki Darusman, Chairperson of the Independent International Fact-Finding Mission on Myanmar, at the 37th session of the Human Rights Council, United Nations Human  Rights Council, March 12, 2018, https:/ /www.ohchr.org/EN/HRBodies/HRC/Pages/NewsDetail. aspx?NewsID=22798&LangID=E . 84  Report of the Special Rapporteur on Extreme Poverty and Human Rights, United Nations  Human Rights Council , March 22, 2017, http:/ /ap.ohchr.org/documents/dpage_e.aspx?si=A/ HRC/35/26 . 85  “Pathways to Banning Fully Autonomous Weapons,” United Nations Office for Disarmament  Affairs, October 23, 2017, https:/ /www.un.org/disarmament/update/pathways-to-banning-fully-autonomous-weapons/ and T ed Piccone, “How Can International Law Regulate Autonomous  Weapons?,” Brookings Institution, April 10, 2018, https:/ /www.brookings.edu/blog/order-from-chaos/2018/04/10/how-can-international-law-regulate-autonomous-weapons/. 86  “UN Secretary-General’s Strategy on New T echnologies,” United Nations, http:/ /www.un.org/ en/newtechnologies/images/pdf/SGs-Strategy-on-New-T echnologies.pdf . 87  The OECD has previously released guidance on big data. See “OECD Guidelines for Multinational Enterprises,” OECD, http:/ /mneguidelines.oecd.org/guidelines and “Big Data: Bringing Competition Policy to the Digital Era,” OECD, November 2016, http:/ /www.oecd.org/competition/big-data-bringing-competition-policy-to-the-digital-era.htm. 88  “Implementing the OECD Guidelines for Multinational Enterprises: The National Contact Points  from 2000 to 2015,” OECD, 2016, http:/ /mneguidelines.oecd.org/OECD-report- 15-years-NationalContact-Points.pdf . 89  OECD Guidelines for Multinational Enterprises: IV . Human Rights , OECD, 2011, http:/ /mneguidelines.oecd.org/2011humanrights.pdf . 90  See F. Raso, V . Krishnamurthy, et al., “Artificial Intelligence and Human Rights: Opportunities &  Risks,” Berkman Klein Center for Internet & Society Research Publication (forthcoming ); Urs Gasser  and Virgilio A.F. Almeida, “A Layered Model for AI Governance,” Harvard University, November 2017,  https:/ /dash.harvard.edu/bitstream/handle/1/34390353/w6gov- 18-LA TEX.pdf?sequence=1; see  separately Matthias Risse, Human Rights and Artificial Intelligence: An Urgently Needed Agenda ,  Harvard Kennedy School, May 2018, https:/ /research.hks.harvard.edu/publications/getFile.aspx ?Id=1664.
DATA & SOCIETY   35 GOVERNING ARTIFICIAL INTELLIGENCE91  Submission to the House of Lords Select Committee on Artificial Intelligence by the Human  Rights, Big Data and T echnology Project – Written Evidence (AIC0196), The Human Rights, Big Data  and T echnology Project, September 6, 2017, http:/ /data.parliament.uk/writtenevidence/commit teeevidence.svc/evidencedocument/artificial-intelligence-committee/artificial-intelligence/writ ten/69717.html. 92  “Human-Centered AI: Building Trust, Democracy, and Human Rights by Design,” Stanford Center on Democracy, Development, and the Rule of Law, https:/ /cddrl.fsi.stanford.edu/global-digital-policy-incubator/content/human-centered-ai-program-and-schedule . 93  Christopher McCrudden, Understanding Human Dignity  (Oxford University Press: 2014). 94  Opening Statement and Global Update of Human Rights Concerns by UN High Commissioner  for Human Rights Zeid Ra’ad Al Hussein at 38th Session of the Human Rights Council, United Nations  Human Rights Council, June 18, 2018, https:/ /www.ohchr.org/EN/HRBodies/HRC/Pages/NewsDetail.aspx?NewsID=23206&LangID=E . 95  For more on the socio-technical gap, see Mark Ackerman, The Intellectual Challenge of CSCW:  The Gap Between Social Requirements and T echnical Feasibility, Human Computer Interaction ,  2000, http:/ /web.eecs.umich.edu/~ackerm/pub/00a10/hci.final.pdf .
DATA & SOCIETY   36 GOVERNING ARTIFICIAL INTELLIGENCEACKNOWLEDGMENTS The author would like to express his appreciation to the individuals who generously  reviewed this report: Aaina Agarwal, Miranda Bogen, Corinne Cath, Tim Engelhardt,  Madeleline Elish, Eimear Farrell, Iason Gabriel, Janet Haven, Michael Karimian, Hibah Kamal-Grayson, Maroussia Levesque, Jake Metcalf, Carly Nyst, Maria Sapignoli,  Brittany Smith, and Cristiaan van Veen. Keith Hiatt lent his keen insight on technology  and disability rights. Zachary Gold made invaluable contributions and Melanie Penagos provided key research support. Patrick Davison provided astute editorial guidance  and critical feedback, and the entire team at Data & Society supported this project in  innumerable ways. Sue Glueck sparked key ideas that continue to inspire this work. This report was animated by the ideas generated at the workshop, Artificial Intelligence & Human Rights, held at Data & Society in April 2018. Workshop attendees  came from across sectors including, Global Affairs Canada, USAID, New York City  government, The Lisbon Council, OECD, United Nations Office of the High Commis sioner for Human Rights, Accenture, Microsoft, DeepMind, Google, Facebook, Gensler  Research Institute, Carnegie Mellon University, Cornell University, Oxford Internet  Institute, Princeton University, New York University, University of California Berkeley,  Max Planck Institute, Data & Society, Digital Asia Hub, Global Network Initiative, Business for Social Responsibility, World Economic Forum, Human Rights Watch, Privacy  International, Article 19, AccessNow, Amnesty International, IEEE, Open Society Foundations, Ford Foundation, and The Rockefeller Foundation.  Following the workshop, a body of published work emerged from Aubra Anthony,  Corinne Cath & Christiaan van Veen, Elizabeth Eagan, Sherif Elsayed-Ali, Jason  Pielemeier, Enrique Piracés, and Ben Zeverbergen which was highly instructive. Also  foundational were conversations with Dan Bross, Steve Crown, Eileen Donahoe, Hannah Hilligoss, Mark Hodge, Dunston Allison Hope, Alexa Koening, Vivek Krishnamurthy,  Dinah PoKempner, and Filippo Raso. The inclusion of the organizations and individuals above does not convey an endorsement of this report and any errors remain the  author’s alone.  For further correspondence about this report please contact Mark Latonero at mark@ datasociety.net.
DATA & SOCIETY   37 GOVERNING ARTIFICIAL INTELLIGENCEDATA & SOCIETY Data & Society is an independent nonprofit research institute that advances new  frames for understanding the implications of data-centric and automated technology.  We conduct research and build the field of actors to ensure that knowledge guides  debate, decision-making, and technical choices.  www.datasociety.net                                                                                                                 @datasociety
