Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 1     Artificial Intelligence    Australia’s Ethics  Framework   A Discussion Paper      
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 2  Citation   Dawson D and Schleiger E*, Horton J, McLaughlin J, Robinson C∞, Quezada G , Scowcroft J,  and Hajkowicz S†  (2019) Artificial In telligence: Australia’s Ethics Framework . Data61 CSIRO, Australia.   *Joint first authors   ∞CSIRO Land and Water   †Corresponding author   Copyright    © Commonwealth Scientific and Industrial Research Organisation 2019. To the ex tent permitted by law, all  rights are reserved and no part of this publication covered by copyright may be reproduced or copied in any  form or by any means except with the written permission of CSIRO.   Important disclaimer   CSIRO advises that the information  contained in this publication comprises general statements based on  scientific research. The reader is advised and needs to be aware that such information may be incomplete  or unable to be used in any specific situation. No reliance or actions must theref ore be made on that  information without seeking prior expert professional, scientific and technical advice. To the extent  permitted by law, CSIRO (including its employees and consultants) excludes all liability to any person for  any consequences, including  but not limited to all losses, damages, costs, expenses and any other  compensation, arising directly or indirectly from using this publication (in part or in whole) and any  information or material contained in it.   Acknowledgements   This study was funded by the Australian Government Department of Industry Innovation and Science. The  authors would like to express gratitude to the experts from industry, government and community  organisations who formed the steering committee that guided  the research.   The authors are grateful to the many researchers from CSIRO, Data61 and u niversities who shared their  expertise . In particular, t he authors would like to thank the reviewers, including Professor Toby Walsh  (Data61 and University of New Sout h Wales), Professor Lyria Bennett Moses (University of New South  Wales), Professor Julian Savulescu (University of Oxford and Monash University) and Dr Cecilia Etulain  (Alfred Hospital).   The authors would also like to thank the individuals who attended the  consultative workshops in Sydney,  Brisbane, Melbourne and Perth and those people who attended the Data61 Live Conference in Brisbane in  September 2018. By sharing your knowledge and perspective on artificial intelligence you have he lped  make this report p ossible.   Accessibility   CSIRO is committed to providing web accessible content wherever possible. If you are having difficulties  with accessing this document please contact csiroenquiries@csiro.au .    
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 3  Artificial Intelligence: Australia’s Ethics Framework   Public Consultation   Artificial Intelligence (AI) has the potential to increase our well -being;  lift our economy; improve society by,  for instance, making  it more inclusive; and help the environment by using the planet's  resources more  sustainably.  For Aus tralia to realise these benefits however, it will be important for citizens to have trust in  the AI applications developed by businesses, governments and academia. One way to achieve this is to  align the design and application of AI with ethical and inclus ive values .  Consultation Approach   The purpose of this public consultation is to seek your views on the discussion paper developed by Data61:  Artificial Intelligence: Australia’s Ethics Framework. Your feedback will inform the Government’s approach  to AI ethics in Australia.   As part of this consultation , the Department of Industry, Innovation and Science welcome s written  submissions , which will close on Friday,  31 May 2019 .   Please note that comments and submissions will be published on the department’s website unless, on  submission, you clearly indicate that you would like your comments to be treated as confidential .  Questions for consideration:   1. Are the principles put forward in the discussio n paper the right ones? Is anything missing?   2. Do the principles put forward in the discussion paper sufficiently reflect the values of the Australian  public?   3. As an organisation, if you designed or implemented an AI system based on these principles, would this  meet the needs of your customers and/or suppliers? What other principles might be required to meet  the needs of your customers and/or suppliers?   4. Would the proposed tools enable you or your organisation to implement the core principles for ethical  AI?   5. What other tools or support mechanisms would you need to be able to implement principles for ethical  AI?   6. Are there already best -practice models that you know of in related fields that can serve as a template  to follow in the practical application of ethi cal AI?   7. Are there additional ethical issues related to AI that have not been raised in the discussion paper? What  are they and why are they important?   Closing date for written submissions : Friday 31 May 2019   Email: artificial.intelligence@industry.gov.au    Website:  https://consult.industry.gov.au/    Mail : Artificial Intelligence    Strategic Policy Division    Department of Industry, Innovation and Science    GPO Box 2013, Canberra, ACT, 2601  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 4  Executive summary   The ethics of artificial intelligence are of growing importance . Artificial intelligence (AI) is changing  societies and economies around the world . Data61 analysis reveals that o ver the past few years, 14  countries and international organisations have announced AU$ 86 billion for AI programs . Some of these  technologies are powerful, which means they have considerable potential for both improved ethical  outco mes as well as ethical risks.  This report identifies key principles and measures that can be used to  achieve the best possible results from AI, while keeping the well -being of Australians as the top priority.   Countries worldwide are developing solutions.  Recent advances in  AI-enabled technologies ha ve  prompted a wave of responses across the globe, as nations attempt to tackle emerging  ethical issues   (Figure 1). Germany has delved into the ethics of automated vehicles, rolling out the most comprehensive  government -led ethical guidance on their development available  [1]. New York has put in place an  automated decisions task force, to review key systems used by government agencies for accountability and  fairness  [2]. The UK has a number of government advisory bodies, notably the Centre for Data Ethics and  Innovation  [3]. The European Union has explicitly highlighted ethical AI development as a source of  competitive advantage  [4].    Figure 1. Map of r ecent developments in artificial intelligence ethics worldwide   Data sources: Pan -Canadian AI Strategy  [5], Australian Federal Budget 2018 -2019  [6] German Ministry of Transport  and Digital Infr astructure  [1], National Institute for Transformation of India  [7], The Villani Report  [8], Reuters  [9],  Japanese Society for Artificial Intelligence  [10], European Commission  [11] UK Parliament  [12], Singapore Government   [13] China’s State Council  [14] New York City Hall  [2]    Canada, November 2017, AI & Society  program announced by Canadian  Government to support research into  social, economic and philosophical issues  of AI  United Kingdom , November 2018,  The Centre for Data Ethics and  Innovation is announced to advise  government on governance,  standards and reg ulation to guide  ethical AI   India, June 2017, National  Institute for Transformation of  India publish their National  Strategy for AI  with a focus on  ethical AI for all   European Union, October 2018,  European Commission appoints an  expert group to develop ethical, legal  and social policy recommendations for AI   Germany, June 2017, Federal  Ministry of Transport release  guidelines for t he use of  autonomous vehicles including 20  ethical rules   New York, May 2018, Mayor  De Blasio announces  Automated Decisions Task  Force to develop transparency  and equity in the use of A I    Japan, February 2017, The Ethics  Committee of Japanese Society  release Ethical Guidelines with an  emphasis on public engagement   Australia,  2018, Federal Government  announces funding for the  development of a national AI ethics  framework   France, March  2018, President  Macron announces AI strategy to  fund research int o AI ethics and  open data based on the Villani  report recommendations    Singapore, August 2018, Singapore Advisory  Council on the Ethical Use of AI and Data  appointed by the Minister for Communications  and Information   China, April 2018, Ministry of  Tran sport releases standards for the  testing of automated vehicle s.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 5  An approach based on case studies. This report examines key issues through exploring a series of case  studies and trends that have prompted ethical debate in Australia and worldwide (see Figure 2).        Examples of case studies  Most relevant principles   Data  governance  and AI  Identifying de -identified data   In 2016, a dataset that included de -identified health information was uploaded to  data.gov.au. It was expected that the data would be a useful tool for medical  research and policy development. Unfortunately, it was discovered that in  combination with other  public ly available information, researchers were able to  personally identify individuals from the data source. Quick action was taken to  remove the dataset from data.gov.au.    Privacy protection   Fairness   Automated  decisions  Houston teachers fired by automated system   An AI was used by the Houston school district to assess teacher performance  and  in some cases fire them. There was little transparency regarding the way that the  AI was operating. The use of this AI  was challenged in court by the teacher’s  union, as the system was proprietary software and its inner workings were  hidden. The case was settled and the district stopped using it  [15]. Fairness    Transparency  and  explainability   Contestability   Accountability   Predicting  human  behaviour  The COMPAS sentencing tool   COMPAS is a tool used in the US to give recommendations to judges about  whether prospective parolee will re -offend. There is extensive debate over the  accuracy of the system and whether it is fair to African Americans. Investigations  by a non -profit outlet have indicated that incorrect predictions unfairly categorise  black Americans as a higher risk. The system is proprietary  software  [16-19]. Do no harm   Regulatory and legal  compliance   Privacy protection    Fairness   Transparency  and  explainability   Figure 2. Table of k ey issues  examined in chapters , case studies and relevant principles   Data sources: Office of the Australian Information Commissioner  [20], US Senate Community Affairs Committee S ecretariat  [15], ProPublica   [16,18,19] , Northpointe  [17]  Artificial intelligence (AI) holds enormous potential to improve society. While a “general AI” that  replicates h uman intelligence is seen as an unlikely prospect in the coming few decades, there are  numerous “narrow AI” technologies which are already incredibly sophisticated at handling specific tasks  [3].  Medical AI technologies and autonomous vehicles are just a few high profile examples of AI that have  potential to save lives and transform society.   The benefits come with risks.  Automated decisions systems can limit issues associated with human bias,  but on ly if due care is focused on the data used by those systems and the ways they assess what is fair or  safe. Automated vehicles  could save thousands of lives by limiting accidents caused by human error, but as  Germany’s Transport Ministry has highlighted in its ethics framework for AVs, they require regulation to  ensure safety  [1].  Existing ethics in context, not reinvented. Philosophers, academics, political leaders and ethicists have  spent centuries developing ethical concepts, culminating in the human -rights based framework used in  international and Australian law. Australia is a party to seven core human rights agreements which have  shaped our law s [21]. An ethics framework for AI is not about rewriting these laws or ethical standards, it is 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 6  about updating them to ensure that existing laws and ethical principles can be applied in the context of  new AI technologies.     Core principles for AI   1. Generates net -benefits. The AI system must  generate benefits for people that are greater than  the costs.  2. Do  no harm . Civilian AI systems must not be  designed to harm or deceive people and should be  implemented in ways that minimise any negative  outcomes.   3. Regulatory and legal compliance.  The AI system   must comply with all relevant international,  Australian Local, State/Territory and Federal  government obligations, regulations and laws.  4. Privacy protection. Any system , including AI  systems,  must ensure people’s private data is  protected and kept confi dential plus prevent data  breaches which could cause reputational,  psychological , financial, professional or other types  of harm.    5. Fairness. The development or use of the AI  system must not result in unfair discrimination  against individuals, communiti es or groups. This  requires particular attention to ensure the  “training data” is free from bias or characteristics  which may cause the algorithm to behave unfairly.  6. Transparency  & Explainability . People must be  informed when an algorithm is being used  that  impacts them and they should be provided with  information about what information the algorithm  uses to make decisions .  7. Contestability.  When an algorithm impacts a  person there must be an efficient process to allow  that person to challenge the use  or output  of the  algorithm.  8. Accountability.  People and organisations  responsible for the creation and implementation of  AI algorithms should be identifiable and  accountable for the impacts of that algorithm , even  if the impacts are unintended.     Data i s at the core of AI . The recent advances in key AI capabilities such as deep learning have been made  possible by vast troves of data. This data has to be collected and used, which means issues related to AI are  closely intertwined with those that relate to  privacy and data. The nature of the data used also shapes the  results of any decision or prediction made by an AI, opening the door to discrimination when inappropriate  or inaccurate datasets are used. There are also key requirements of Australia’s Privac y Act which will be  difficult to navigate in the AI age  [22].  Predictions about people have added ethical layers.  Around the world, AI is making all kinds of predictions  about people, ranging from potential health issues through to the probability that they will end up re appearing in court  [16]. When it comes to medicine, this can provide enormous benefits for healthcare.  When it comes to human behaviour, however, it’s a challenging philosophical question with a wide range of  viewpoints  [23]. There ar e benefits, to be sure, but risks as well in creating self -fulfilling prophecies  [24].  The heart of big data is all about risk and probabilities, which humans struggle to accurately assess.   AI for a fairer go. Australia’s colloquial motto is a “fair go” for all. Ensuring fairness across the many  different groups in A ustralian society will be challenging, but this cuts right to the heart of ethical AI. There  are different ideas of what a “fair go” means. Algorithms can’t necessarily treat every person exactly the  same either; they should operate according to similar pr inciples in similar situations. But while like goes 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 7  with like, justice sometimes demands that different situations be treated differently. When developers  need to codify  fairness  into AI algorithms , the re are various  challenge s in managing  often inevitable  trade offs and s ometimes there’s no “right” choice  because what is considered optimal may be disputed . When  the stakes are high, it’s imperative to have a human decision -maker accountable for automated decisions — Australian laws already mandate it to a deg ree in some circumstances  [25].   Transparency is key, but not a panacea.  Transparency and AI is a complex issue. The ultimate goal of  transparency measures are to achieve accountability, but the inner workings of some AI technologies defy  easy explanation. Eve n in these cases, it is still possible to keep the developers and users of algorithms  accountable  [26]. An analogy can be drawn with people: an explanation of brain chemistry when making a  decision doesn’t necessarily help you understand how that decision was made —an explanation of that  person’s priorities is much more helpful. There are also complex issues relating to commercial secrecy as  well as the fact that making the inner workings of AI open to the public would leave them susceptible to  being gamed  [26].  Black boxes pose risks . On the other hand, AI “black boxes” in which the inner workings of an AI are  shrouded in secr ecy are not acceptable when public interest is at stake. Pathways forward involve a variety  of measures for different situations, ranging from explainable AI technologies  [27], testing, regulation that  requires transparency in the key priorities and fairness measures used in an AI system, through to  measures enabling external review and monitoring  [26]. People should always be aware when a decision  that affects them has been made by an AI, as difficulties with automated decisions by government  departments have already been before Austral ian courts  [28].  Justifying decisions.  The transparency debate is one component feeding into another debate: justifiability.  Can the designers of a machine justify what their AI is doing? How do we know what it is doing? An  independent, normative framework can serve to inform the development o f AI, as well as justify or revise  the decisions made by AI. This document is part of that conversation.   Privacy measures need to keep up with new AI capabilities. For decades, society has had rules about how  fingerprints are collected and used. With new A I-enabled facial recognition, gait and iris scanning  technologies, biometric information  goes well beyond  fingerprint s in many respects  [29]. Incidents like the  Cambridge Analytica scandal demonstrate how far -reaching privacy breaches can be in the modern  age,  and AI technologies have the potential to impact this in significant ways.  We may need to further explore  what privacy means in a digital world.   Keeping the bigger picture in focus.  Discussions on the ethics of autonomous vehicles tend to focus on  issues like the “trolley problem” where the vehicle is given a choice of who to save in a life -or-death  situation. Swerve to the right and hit an elderly person, stay straight and hit a child, or swerve to the left  and kill the passengers? These are importan t questions worth examining  [30], but if widespread adoption  of autonomous vehicles can improve safety and cut down on the hundreds of lives los t on Australian roads  every year, then there is a risk that lives could be lost if relatively far -fetched scenarios dominate the  discussion and delay testing and implementation.  The values programmed into autonomous vehicles are  important, though they need  to be considered alongside potential costs of inaction.   AI will reduce the need for some skills and increase the demand for others  Disruption in the job market is  a constant . However, AI may fuel the pace of change.  There will be challenges in ensuring equality of  opportunity  and inclusiveness  [31]. An ethical approach to AI development requires helping people  who  are negatively impacted by automation transition their careers. This could involve training, reskilling and  new career pathways. Improved information on risks and opportunities can help workers take proactive  action. Incentives can be used to encourag e the right type of training at the right times. Overall, acting early  improves the chances of avoiding job -loss or ongoing unemployment.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 8  AI can help with intractable problems. Long -standing health and environmental issues are in need of novel  solutions, and AI may be able to help. Australia’s vast natural environment is in need of new tools to aid in  its preservation, some of which are  already  being implemented  [32]. People with serious disabilities or  health problems are able to participate more in society thanks to AI -enabled technologies  [33].  International coordination is crucial.  Developing standards for electrical and industrial products required  international coordination to make devices safe and functional across borders  [34]. Many AI technologies  used in Australia won’t be made here. There are already plenty of off -the-shelf foreign AI products being  used  [35]. Regulations can induce foreign developers to work to Australian standards to a point, but there  are limits. Internatio nal coordination with partners overseas , including the International Standards  Organisation (ISO) , will be necessary to ensure AI products and software meet the required standards.   Implementing e thical AI . AI is a broad set of technologies with a range of legal and ethical implications.  There is no one -size-fits all solution to these emerging issues.  There are, however, tools which can be used  to assess risk and ensure compliance and oversight. The most appropriate tools can be selected f or each  individual circumstance.   A toolkit for ethical AI   1. Impact Assessments: Auditable  assessments of the potential  direct and indirect impacts of AI,  which address the potential  negative impacts on individuals,  communities and groups, along  with mitigation procedures.    2. Internal or external review:  The use of specialised  professionals or groups to review  the AI and/or use of AI systems to  ensure that they adhere to ethical  principles and Australian policies  and legislation.  3. Risk Assessments: The use of  risk assessm ents to classify the  level of risk associated with the  development and/or use of AI.   4. Best Practice Guidelines : The  development of accessible cross  industry best practice principles to  help guide developers and AI  users on gold standard practices.  5. In dustry standards: The  provision of educational guides,  training programs and potentially  certification to help implement  ethical standards in AI use and  development  6. Collaboration: Programs that  promote and incentivise  collaboration between industry  and academia in the development  of ‘ethical by design’ AI , along  with demographic diversity in AI  development .  7. Mechanisms for monitoring  and improvement: Regular  monitoring of AI for accuracy,  fairness and suitability for the task  at hand. This should also involve  consideration of whether the  original goals of the algorithm are  still relevant.  8. Recourse mechanisms:  Avenues for appeal when an  automated de cision or the use of  an algorithm negatively affects a  member of the public.  9. Consultation: The use of public  or specialist consultation to give  the opportunity for the ethical  issues of an AI to be discussed by  key stakeholders.     Best practice based on ethical principles. The development of best practice guidelines can help industry  and society achieve better outcomes. This requires the identification of values, ethical principles and  concepts that can serve as their basis.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 9  About this report. This report  covers civilian applications of AI . Military applications  are out of scope. This  report also acknowledges research into AI ethics occurring as part of a project by the Australian Human  Rights Commission  [36], as well as  work being undertaken  by the recently established Gradient Institute.  This work complements research being conducted by the  Australian Council of Learned Academies (ACOLA)   and builds upon the Robotics Roadmap for Australia by the Australian Centre for Robotic Vision.  From a  research perspective, this framework sits alongside existing standards, such as  the National Health and  Medical Research Council (NHMRC) Australian Code for the Responsible Conduct of Research  and the  NHMRC’s National Statement o n Ethical Conduct in Human Research .    
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 10  A guide to this framework   In this evolving domain, there may be no single ethical framework to guide all decision making and  implementation of Artificial Intelligence. The chapters of this ethics framework provide a s trong foundation  for both awareness and achievement of better ethical outcomes from AI. AI is a broad family of  technologies which requires careful, specialised approaches. These chapters provide a broad understanding  of AI and ethics, which can be used to  identify and begin crafting those specialised approaches. This ethical  framework should not be used in isolation from key business or policy decisions, and will supplement fit for-purpose applications.     Chapter 2: Existing frameworks, principles and guide lines on AI ethics   This chapter identifies and summarises some of the key approaches to issues related to AI and ethics  around the world. It helps provide broader context for the current state of AI ethics and highlights  strategies that can be observed for  lessons on implementation and effectiveness.     Chapter 3: Data governance   This section highlights the crucial role of data in most modern AI applications. It explores the ways in which  the input data can affect the output of the AI systems, as well as the ways in which data breaches, consent  issues and bias can affect the outcomes derived from AI technologies.   - Data governance is crucial to ethical AI; organisations developing AI technologies need to ensure they have  strong data governance foundations or the ir AI applications risk being fed with inappropriate data and  breaching privacy and/or discrimination laws.   - AI offers new capabilities, but these new capabilities also have the potential to breach privacy regulations in  new ways. If an AI can identify anon ymised data, for example, this has repercussions for what data  organisations can safely use.   - Organisations should constantly build on their existing data governance regimes by considering new AI enabled capabilities and ensuring their data governance syste m remains relevant.     Chapter 4: Automated decisions   This chapter highlights the ethical issues associated with delegating responsibility for decisions to  machines.   - Existing legislation suggests that for government departments, automated decisions are suita ble when there  is a large volume of decisions to be made, based on relatively uniform, uncontested criteria. When discretion  and exceptions are required, automated decision systems are best used only as a tool to assist human  decision makers —or not used at  all. These requirements are not mandated for other organisations, but are a  wise approach to consider.   - Consider human -in-the-loop (HITL) principles during the design phase of automated decisions systems, and  ensure sufficient human resources are available  to handle the likely amount of inquiries.   - There must be a clear chain of accountability for the decisions made by an automated system. Ask: Who is  responsible for the decisions made by the system?    
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 11  Chapter 5: Predicting human behaviour   This chapter examin es the ethical difficulties that emerge when creating systems that are designed to take  input data from humans and make judgements about those people.   - AI is not driven by human bias but it is programmed by humans. It can be susceptible to the biases of its   programmers, or can end up making flawed judgments based on flawed information. Even when the  information is not flawed, if the priorities of the system are not aligned with expectations of fairness, then  the system can deliver negative outcomes.   - Justice means that like situations should deliver like outcomes, but different situations can deliver different  outcomes. This means that developers need to pay special care to vulnerable, disadvantaged or protected  groups when programming AI.   - Full transparency is  sometimes impossible, or undesirable (consider privacy breaches). But there are always  ways to achieve a degree of transparency. Take neural nets, for example: they are too complex to explain,  and very few people would have the expertise to understand any way. However, the input data can be  explained, the outcomes from the system can be monitored, and the impacts of the system can be reviewed  internally or externally. Consider the system, and design a suitable framework for keeping it transparent and  accoun table. This is necessary for ensuring the system is operating fairly, in line with Australian norms and  values.     Chapter 6: Current examples of AI in practice   This chapter examines two areas where AI technologies are having a significant impact at this poi nt in  time —autonomous vehicles and surveillance technologies.   - Autonomous vehicles require hands -on safety governance and management from authorities, because there  are competing visions of how they should prioritise human life and a system without a cohesi ve set of rules is  likely to deliver worse outcomes that are not optimised for Australian road rules or conditions.   - AI-enabled surveillance technologies should consider “non -instrumentalism” as a key principle —does this  technology treat human beings as one  more cog in service of a goal, or is the goal to serve the best interests  of human beings?   - In many ways, biometric data is replacing fingerprints as a key tool for identification. The ease at which AI enabled voice, face and gait recognition systems can i dentify people poses an enormous risk to privacy.    
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 12  Contents   1 Introduction  ................................ ................................ ................................ ......................  14  2 Existing frameworks, principles and guidelines on AI Ethics  ................................ ............  17  2.1 Australian frameworks  ................................ ................................ ........................  17  2.2 International frameworks  ................................ ................................ ....................  20  2.3 Organisational and institutional frameworks  ................................ ......................  22  2.4 Key themes  ................................ ................................ ................................ ..........  26  3 Data governance  ................................ ................................ ................................ ...............  27  3.1 Consent and the Privacy Act  ................................ ................................ ................  28  3.2 Data breaches  ................................ ................................ ................................ ...... 29  3.3 Open data sources and re -identification  ................................ .............................  30  3.4 Bias in data  ................................ ................................ ................................ ..........  31  4 Automated decisions  ................................ ................................ ................................ ........  33  4.1 Humans in the loop (HITL)  ................................ ................................ ...................  33  4.2 Black box issues and transparency  ................................ ................................ ...... 34  4.3 Automation bias and the need for active human oversight  ...............................  35  4.4 Who is responsible for automated decisions?  ................................ ....................  36  5 Predicting human behaviour  ................................ ................................ ............................  38  5.2 Bias, predictions and discrimination  ................................ ................................ ... 39  5.3 Fairness and predictions  ................................ ................................ ......................  41  5.4 Transparency, policing and predictions ................................ ...............................  42  5.5 Medical predictions  ................................ ................................ .............................  44  5.6 Predictions and consumer behaviour ................................ ................................ .. 45  6 Current examples of AI in practice  ................................ ................................ ...................  48  6.1 Autonomous vehicles  ................................ ................................ ..........................  48  6.2 Personal identification and surveillance  ................................ .............................  52  6.3 Artificial Intelligence and Employment  ................................ ...............................  54  6.4 Gender Diversity in AI workforces  ................................ ................................ .......  55  6.5 Artificial Intelligence and Indigenous Communities  ................................ ............  55  7 A Proposed Ethics Framework  ................................ ................................ ..........................  57  7.1 Putting principles into practice  ................................ ................................ ...........  58 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 13  7.2 Example Risk Assessment Framework for AI Systems  ................................ ........  63  8 Conclusion  ................................ ................................ ................................ .........................  66  9 References  ................................ ................................ ................................ ........................  67  Appendix Stakeholder and E xpert Consultation  ................................ ................................ ...........  75    Figures     Figure 1. Map of recent developments in artificial intelligence ethics worldwide  ........................  4  Figure 2. Table of key issues examined in chapters, case studies and relevant principles  ............  5  Figure 3. Chart indicating Australian knowledge about consumer data collection and sharing  .. 27  Figure 4. Pie chart showing reasons for Australian data breaches, April -June 2018  ...................  29  Figure 5. Infographic showing three phases in developing automated decision systems  ...........  33  Figure 6. Infographic showing the five levels of vehicle autonomy  ................................ .............  48  Figure 7. Pie charts of consultation attendee demographics  ................................ .......................  75   
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 14  1 Introduction   “The machine is only a tool after all, which can help humanity progress faster by taking some of  the burdens of calculations and interpretations off its back. The task of the human brain  remains what it has always been; that of discovering new data to be a nalyzed, and of devising  new concepts to be tested.”   I, Robot, Isaac Asimov     Throughout the 1940s and 1950s, science fiction writer Isaac Asimov published fictional tales of intelligent  robots and envisioned three rules to govern them.  He would later add a  fourth law to protect humanity  more broadly.  Then and now, it was clear that four  rules would be insufficient to handle the philosophical  and technical complexity of the task. Asimov’s laws pre -date decades of studies into the ethics of artificial  intelli gence, which arguably began in 1955 when the term artificial intelligence (AI) was coined by  mathematician J ohn McCarthy and his colleagues  [37]. Today, AI ethics remains a rich and highly relevant  field of inqu iry.   In this report AI is defined as:  A collection of interrelated technologies used to solve problems  autonomousl y and perform tasks  to achieve defined objectives without explicit guidance from a human being.   Today’s AI has capabilities for unaided machine learning and complex problem solving delivered by virtual  (e.g. automated online search tools, computerised game simulators) and mechanical systems (e.g. robots,  autonomous vehicles). This definition of AI en compasses both recent, powerful advances in AI such as  neural nets and deep learning, as well as less sophisticated but still important applications with significant  impacts on people, such as automated decision systems.   This report deals exclusively with civilian applications of AI and does not delve into the ethics of AI in the  military. This document  focuses on “narrow AI” which performs a specific function, rather than “general AI”  which is comparable to human intelligence across a range of fields and i s not seen as a likely prospect by  2030.   Enormous benefits are already accompanying the age of AI. New AI -enabled medical technologies have the  potential to save lives. There are persuasive indications that autonomous vehicles may cut down on the  road toll . New jobs are being created, economies are being rejuvenated, and creative new forms of  entertainment are emerging.   But some of these tools are powerful and very complex. That means that their design and use are both  subject to significant ethical conside rations.  The report , ‘Ethical by design: principles for good technology ’,  by the Ethics Centre in Sydney, provides an overview of the philosophical basis of why an ethical approach  to technology matters  [38]. It highlights the importance of coming to an “ethical equilibrium” that satisfies  a broad range of attitudes toward what is ethical and what is not  [38]. Although this AI Ethics Discussion  Paper was developed in keeping with this concept, there are  a few foundational assumptions that lie at the  heart of the document —that we do have power to alter the outcomes we get from technology, and that  technology should serve the best interests of human beings  and be aligned with human values.   The notion that  technology is value -neutral while people make all the decisions is a flawed one.  As historian  Melvin Kranzberg once said, “technology is neither good nor bad, nor is it neutral  [39].” Technol ogy sh apes 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 15  people just as people shape technology. Today, cities have been transformed by road infrastructure to  serve cars. Smartphones change our attention spans  and have evolved our workforce . Medical technologies  such as IVF have even changed the ways children can be conceived.  People were born into this transformed  world,  and it affected the ways they lived their lives . Not everyone gets  access to the most advanced  technologies, and not every body gets a say in how they are  used once they are  released into the public  domain.  This makes it all the more important to track and consider the implications of new technologies at  the time they are emerging. If we accept that we have the ability to dete rmine the outcomes we get from  AI, then there is an ethical imperative to try to find the best possible outcomes and avoid the worst.   Around the world, people are being given prison sentences based on  assessments  from autonomous  systems. The world of trans portation faces a possible wave of disruption  as automated vehicles move on to  the roads, displacing jobs and creating new ones. AI is watching people through surveillance, sometimes  improving safety, sometimes encroaching on privacy. People are being asse ssed by AI for likely medical  problems, while others are being assessed to gauge their consumer preferences.   The effects of AI will be transformative for Australian society.  Countries  everywhere  are developing plans  for an AI -enabled era. In the past two y ears the United States, China, the United Kingdom, India, Finland,  Germany, the European Commission and other countries and organisations ha ve published AI strategies   [40]. An important component of these national strat egies is the ethical issues raised by the advancement  and adoption of AI technologies.   This ethics framework highlights the ethical issues that are emerging or likely to emerge in Australia from AI  technologies and outline s the initial steps toward mitigat ing them. It does not reinvent ethical concepts,  but contextualises existing ethical considerations developed over centuries of practice in order to keep  pace with the new capabilities that are emerging via AI.  It seeks pragmatic solutions and  future  pathw ays in  this rapidly evolving area by analysing case studies, while acknowledging the importance of ongoing  theoretical and philosophical discussions of the implications of AI technology.   The development and adoption of advanced forms of narrow AI will not wait for government or society to  catch up —these technologies are already here and developing quickly. Blocking all of these technologies is  not an option, any more than cutting off acces s to the internet would be , but there may be scope to ban  particularly harmful technologies if they emerge.  As with the internet, there are risks involved in the use of  AI, but they should not be seen as a reason to reject it  entirely . Many AI -driven technologies have been  proven to save lives and reduce human suffering, thus, an ethical approach to AI is not a restrictive one.  There have already been cases where the slow pace of regulatory adaptation has hindered the  development of potentia lly life -saving AI technologies  [41]. Numerous stakeholders consulted during the  formulation of this report expressed the concern that over -regulating this space could have negative  consequences and drive innovation offshore, to the detriment of smaller Australian companies and to the  advantage of established multinationals with more resources.   With that in mind, it is also important to consider the consequences of taking no action in steering the  ethical development and use of AI in Australia. As the case studies i n this document demonstrate, AI  technologies are already having a range of effects on people around the world. The developers of these  technologies are working in an area that is not yet well regulated, which means they are exposed to added  risk. If any ba cklash occurs, they run the risk of making mistakes or being scapegoated for problems which  could potentially be avoided if the area was well understood and proper rules, regulations or ethical  guidance were in place .  This report emphasises real world case  studies specifically related to AI and automated systems, rather  than a detailed exploration of the philosophical implications of AI, but those philosophical inquiries are also  important. The goal of this document is to provide a pragmatic assessment of k ey issues  to help foster  ethical AI development in Australia . It has been written with the goal of creating a  toolkit of practical and 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 16  implementable methods (such as developing best practice guidelines or providing education and training)  that can be used to support core ethical principles designed to assist both AI developers and Australia as a  whole. Further research and analysis by pro fessional ethicists will be necessary as AI technologies continue  to shape Australian society.   This Ethics Framework provides guidance on how to approach the ethical issues that emerge from the use  of AI. This report argues  that AI has the potential to pro vide many social, economic, and environmental  benefits, but there are also risks and ethical concerns regarding privacy, transparency, data security,  accountability, and equity.    An ethical framework such as this is one part of suite of governance mechanis ms and policy tools which can  include laws, regulations, standards and codes of conduct.  An ethical framework on its own will not ensure  the safe and ethical development and use of AI. Fit for purpose, flexible and nimble approaches are  appropriate for the  regulation and governance of new and emerging digital technologies. Ethics both  inform and are informed by laws and community values . These principles take laws into account and can  form the groundwork for the formulation of more specific codes, laws or r egulation, but are intended as a  guide only.   In developing and governing AI technologies, neither over -regulation nor a laissez -faire approach is  sufficient. There is a path forward which allows for flexible solutions, the fostering of innovation and a fir m  dedication to aligning the development of AI with human values.    This document does not aim to provide legal guidance. Regulations and possibly legal reform should be  formulated as needed by the appropriate legal and governing bodies, for each specific d omain or  application. The goal of this document is to help identify ethical principles and to elicit discussion and  reflection of how AI should be developed and used in Australia.   With a proactive approach to the ethical development of AI, Australia can do  more than just mitigate  against risks —if we can build AI for a fairer go, we can secure a competitive advantage as well as safeguard  the rights of Australians.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 17  2 Existing f rameworks, principles and guidelines  on AI Ethics   The following documents and publica tions provide an outline of relevant legislation and ethical principles  relating to the use and development of AI. The literature is sourced from governments and multilateral  organisations both within Australia and internationally. This summary is not a sy stematic review of all  available literature relating to the ethical use of AI, but a collection of key documents that give a high -level  overview of the current state of AI ethics . They have been selected on the basis of impact and visibility.    2.1 Australian f rameworks   Artificial intelligence is a broad set of technologies with applications across virtually all industries and  aspects of government and society. Government agencies are already using automated decisions systems  to streamline the provision of servi ces, and there is existing advice that provide s some insight on  governance and oversight  of AI.  2.1.1 Government and automated decisions   Some key documents authored by government bodies provide background on how agencies should use AI.  This includes section 6A o f the Social Security (Administration) Act 1999, which states:   1. The Secretary may arrange for the use, under the Secretary’s control, of computer programs for  any purposes for which the Secretary may make decisions under the social security law.   2. A decision made by the operation of a computer program under an arrangement made under  subsection (1) is taken to be a  decision made by the Secretary  [25].  This is just one of numerous legislative clauses allowing government agencies to use computers for  decision -making – since 2010, the departments of Social Services, Health, Education and Training,  Immigration and Border Protection, Agriculture and Water Resources and Veterans’ Affairs have all been  given some authority to let a utomated systems make decisions  [42]. This law clarifies an imp ortant aspect  of AI ethics as expressed in Australian legislation: when decisions are made by automated systems, a  human being with authority must be accountable for those decisions.   In 200 3, a Department of Finance working group for Automated Assistance in Administrative Decision  Making released a best practice guide for government agencies seek ing to use AI to make decisions  [43].  The guide , updated in 2007,  outlines 27 principles covering a range of  issues, from review mechanisms  through to the appropriate ways to override a decision made by an automated system. The guidelines  include flow charts of how automated decisions should be made, and checklists to help ensure that  automated decisions are bei ng made according to the values of administrative law. These checklists can  help serve as a valuable starting point for developing toolkits for AI use in administration.   The guide distinguishes between two key types of decisions: administrative decisions f or which the  decision -maker is required to exercise discretion; and those for which no discretion is exercisable once the  facts are established. Given the high volume of routine decisions that need to be made by some agencies,  the guide judged it suitable to use automated systems in making decisions where no discretion was  required. In other cases, automated decision -making systems were determined to be best used as  ‘decision -making tools’ for human supervisors. This distinction clarifies that while AI can be a valuable tool 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 18  in decision -making, there are some decisions requiring human judgment, particularly in the context of  public policy administration.   Federal government agencies are also developing AI -specific practices. An interdepartmental committee on  AI regularly convenes to discuss how government agencies can utilise AI. As automation becomes more  pervasive within government, industry, and broader society, frameworks such as the best practice guide on  automated decision -making can help to ensure that government bodies remain accountable to the public.   Guidance may also be sought from other examples of government action around automated decision making. For instance, the New York City government is the first American government body to set up a task  force specifically to examine accountability in automated decisions. The Automated Decisions Task Force  will examine automated systems through the lens of equity, fairness and accountability, and is set to  release a report in December 2019 that will recommen d procedures for reviewing and assessing  algorithmic tools used by the city  [2].  2.1.2 Australia’s international human rights obligations and anti -discrimination  legislation   Australia is a signatory to seven core intern ational human rights agreements  [21]   The International Covenant on Civil and Political Rights  (ICCPR)  [44]    The Interna tional Covenant on Economic, Social and Cultural Rights  (ICESCR)  [45]    The International Convention on the Elimination of All Forms of Racial Discrimination  (CERD)  [46]    The Convention on the Elimination of All Forms of Discrimination against Women  (CEDAW)  [47]    The Convention against Torture and Other Cruel, Inhuman or Degrading Treatme nt or Punishment   (CAT)  [48]    The Convention on the Rights of the Child  (CRC)  [49]    The Convention on the Rights of Persons with Disabilities  (CRPD)  [50]    These agreements are a ll derived from the Universal Declaration of Human Ri ghts which was  released in 1948  [36,51]   Australia is also a party to a number of related protocols.   Under Australia’s Human Rights (Parliamentary Scrutiny) Act 2011, new bills must be accompanied by a  statement of compatibility that demonstrates how they align with the seven aforeme ntioned hu man rights  agreements  [52,53] . The Parliamentary Joint Committee on Human Rights scrutinises laws to confirm they  are compatib le with Austr alia’s human rights obligations  [52]. Any future Australian legislation will need to  abide by these principles amid change occurring due to AI.    The Australian Human Rights Commission is currently in the process of developing a report examining  Australia’s human rights obligations in the cont ext of emerging technological issues. The report will be  released in 2020 after public consultation, but an issues paper has alrea dy been released for discussion   [36].  In addition, Australia has a number of anti -discrimination laws at both  state and federal levels. Federal laws  include the Age Discrimination Act 2004, the Disability Discrimination Act of 1992, the Racial Discrimination  Act of 1975 and the Sex D iscrimination Act of 1984  [54]. Measures to combat discrimination are highly  relevant to AI, as AI systems are vulnerable to discriminatory outcomes – for instance, there have been  cases where AI systems have used historical data, leading to results that replicated the biases or prejudices  of that original data, as well as any flaws  in the collection of that data  [55]. In ensuring that AI systems and 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 19  programs are created in accordance with existing anti -discrimination laws, designers will need to cons ider  the likely outcomes caused by their algorithms during the design phase.   2.1.3 Data -sharing legislation in Australia   Data is a key component of AI. It is necessary for both developing the skills needed to work on AI and the  technology itself, as large datas ets are often required to ‘teach’ machine learning technologies. Legislation  that guides data -sharing therefore affects the development of AI, but is also highly relevant to the privacy  of all Australians.   A key document on data -sharing in Australia is a 2 017 report from the Productivity Commission, Data  Availability and Use  [56]. The report focuses on ways to streamline access to data, as wel l as exploring the  economic benefits that could be gained through improved data access. The report covers several areas of  particular relevance to this ethics framework, including:    Assisting individuals to access their personal data being held by public ag encies    Identifying datasets with high value to the public    The role of third -party intermediaries in assisting consumers to make use of their data    The benefits and costs of data standardisation and public releases (which has relevance for the  broader develo pment of AI and how personal information may be handled by AI systems)   As a part of the Australian Government’s data reform efforts, a Data Sharing and Release bill is being  formulated. The Department of Prime Minister and Cabinet has released a discussion  paper outlining some  key principles of the bill , including the following goals  [57]:    To safeguard data sharing and release in a consistent an d appropriate way    To enhance the integrity of the data system    To build trust in use of public data    To establish institutional arrangements for data governance, via a National Data Commissioner and  its supporting office    To promote better sharing of public sector data   The Office of the Victorian Information Commissioner has also released an issues paper  outlining key  questions  relating to data used in AI  [58]. The report is pa rticularly concerned with exploring potential  privacy issues arising from the development and use of AI. It promotes the use of ‘ethical data  stewardship’, which requires a commitment to transparency and accountability in the way data is collected  and used . The report also proposes the need for independent governance and oversight of the AI industry,  to ensure that the principles of ethical data stewardship are adhered to.   Data -sharing practices are an integral aspect of AI ethics. AI systems require effect ive facilitation of data sharing and collection in order to function and develop – however, it is crucial that this process does not  compromise privacy. Comprehensively reviewing and reforming Australia’s data -sharing practices in order  to strike this bala nce would help resolve some key ethical issues associated with AI development, by  reducing the possibility that AI programs could access and misuse personal information.   2.1.4 Privacy Act   Privacy issues associated with the internet are not new but AI has the pot ential to amplify existing  challenges.  The Australian Privacy Act 1988  (Privacy Act) regulates how personal information is handled.  The Privacy Act defines personal information as  [59]: 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 20  …information or an opinion, whether true or not, and whether recorded in a material form or not, about an  identified individual, or an individual who is reasonably identifiable.   Common e xamples are an individua l’s name, signature, address, telephone number, date of birth, medical  records, bank account details and commentary or opinion about a person.   The Privacy Act includes thirteen Australian Privacy Principles  (APPs)  [60] , which apply to some private  sector organisations, as well as most Australian and Norfolk Island Government agencies. These are  collectively referred to as  ‘APP entities ’. The Privacy Act also regulates the privacy component of the  consumer credit reporting system  [61], tax file numbers  [62], and health and medical research  [63].  The Office of the Australian Information Commissioner is responsible for privacy functions that are  conferred by the Privacy Act.   2.2 International frameworks   Many of the AI strategies developed by governments aroun d the world include a discussion of ethics, and  this information is  important in framing the international context for Australia’s approach. In particular, key  ethical questions are explored in the national strategies of the United Kingdom, France and Germany, all of  which have been shaped by the European Union’s data pro tection laws.   In 2018, the EU began implementing its General Data Protection Regulation (GDPR) which is among the  largest, most far -reaching data -sharing laws in the world. It includes the ‘right to be forgotten’, which  requires organisations with data ope rations in the EU to have measures in place allowing members of the  public to request the removal of personal information held on them. Another element of the GDPR is  ‘privacy by design’, which clarifies statutory requirements for privacy at the system des ign phase, The GDPR  also encourages (but does not enforce) certification systems.  The GDPR also includes sections relevant to  automated decisions, indicating that automated decisions systems cannot be the sole decision -making  entity when the decision has l egal ramifications. Article 22 states: “The data subject shall have the right not  to be subject to a decision based solely on automated processing, including profiling, which produces legal  effects concerning him or her or similarly significantly affects h im or her  [64].”  Academics have pointed out that the language of this article is vague and that a right to explanations from  automated system may not actually exist under the GDPR  [65].  The European Union also has an official plan for AI development – Artificial Intelligence for Europe  – which  explicitly highlights the digital single market as a key driver of AI development, and emphasises the creation  of ethic al AI as a competitive advantage for Euro pean nations  [4]. In a 2018 statement, the European Group  on Ethics in Science and New Technologies suggested that a global standard of fundamental principles for  ethical AI, supported by legislative action, is required to ensur e the safe an d sustainable development of AI   [66].  The European Commission has also issued Draft Ethics Guidelines for Trustworthy AI , which emphasise that  AI should be “human centric” and “trustworthy  [67].” These two points emphasise not only the ethics of AI,  but also certain technical aspects of AI, because “a lack of technological mastery can cause unintentional  harm.” It outlines a frame work for trustworthy AI that begins with an “ethical purpose” for the AI, then  moves to the realisation of that AI, followed by requirements and finally technical and non -technical  methods of oversight  [67].  The United Kingdom’s national plan for AI ( AI in the UK, Ready, Willing and Able?)  explores AI ethics from  numerous angles, with sections on inequality, social cohesion, prejudice, data monopolies, criminal misuse  of data, and suggest ions for the development of an AI Code. The report points out that there are numerous  state and non -state actors developing ethical principles for the use of AI, but a coordinated approach is  lacking in many cases. According to the report, “mechanisms must  be found to ensure the current trend for 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 21  ethical principles does not simply translate into a meaningless box -ticking exercise.”  [3]  The report also  nominates the Alan Turing Institute as the national centre for AI research, with part of its mandate being  further exploration of the ethics of artificial intelligence. The document includes a code with five key  elements:   1. Artificial intelligence should be developed for the common good and benefit of humanity.   2. Artificial intelligence should operate on principles of intelligibility and fairness.   3. Artificial intelligence should not be used to diminish the data rights or priva cy of individuals, families  or communities.   4. All citizens have the right to be educated to enable them to flourish mentally, emotionally and  economically alongside artificial intelligence.   5. The autonomous power to hurt, destroy or deceive human beings shou ld never be ve sted in artificial  intelligence.  [3]   The French national report on AI examines a number of key ethical issues and pro poses measures to  address these  [68]. For instance, ‘dis crimination impact assessments’ are suggested as one possible  measure to address hidden bias and discrimination in AI, citing the existence of ‘privacy impact  assessments’ in European law. The report also explores the ‘black box problem’ —it is easy to expl ain the  data going in to the AI program and easy to explain the data that comes out, but what occurs within is  difficult for most people to understand. As such, technologies that ‘explain’ AI processes will be increasingly  important as AI becomes more comm only used. The report also extensively canvasses the issue of  automation, and the need for retraining measures to mitigate its impact on the workforce. At a regulatory  level, the report emphasises that designing procedures, tools, and methods that allow fo r the auditing of AI  systems will be key in ensuring that the systems conform to legal and ethical frameworks.  It also suggests  that it will be necessary to “instate a national advisory committee on ethics for digital technology and  artificial intelligence , within an institutional framework  [68].”  In Germany, the national report Automated and Connected Driving is the world’s most comprehensive  ethics report into autonomous vehicles (AVs) to date. The report lays out key principles for the  developme nt of AVs, explicitly stating that the public sector is responsible for safety and that licencing of  automated systems is a key requirement. The report emphasises that while the personal freedom of the  individual is a paramount concern of government, this must be pursued within the context of public safety.  The prioritisation of human life is a key element of this ethical framework – where damage is inevitable,  animals or property should never be placed above human life. When human life must be damaged, the   German ethics framework states that: “any distinction based on personal features (age, gender, physical or  mental constitution) is strictly prohibited. However, general programming to reduce the number of  perso nal injuries may be justifiable ” [1] . The report also notes that ethical ‘dilemma situations’ depend on  the actual specific situation and cannot be standardised or programmed – as such, it would be desirable for  an independent public sector agency to systematically process the lessons learned  from these situations.   However,  it may still prove necessary to program vehicles to deal with these ethical dilemma situations,  which would indicate some degree of standardisation. While humans are not expected to be able to make  well-reasoned decisions i n the brief moment before  an accident, this may not be the case for autonomous  vehicles  which can act rapidly but require programming beforehand . “The court understands that if you’ve  only been given one second to make a decision, you might make a decision  that another reasonable person  might not have made,” Dr Finkel told media   [69]. “Will we be as generous to a computerised algorithm  that can run at much faster speeds than we can? I don’t know.”  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 22  2.3 Organisationa l and institutional frameworks   2.3.1 Australian Council of Learned Academies   The Australian Council of Learned Academies (ACOLA) is compiling a comprehensive horizon scan of issues  affecting the development of AI in Australia. It identifies social impacts of AI that will affect Australia and  New Zealand, with input from key academics in the field of AI. The report covers the relevance of AI to key  industries like agriculture, fintech, and transport, as well as the ways in which AI affects government and  social po licy.  This report is being prepared concurrently with the ACOLA report . Of particular relevance to this ethical  framework are discussions of individual agency and autonomy, and of how AI can affect an individual’s  sense of self. Other elements of the report cover social licence, inclusion, privacy and data bias in AI, as well  as the differing concepts of fairness in algorithms. The ACOLA report should be considered complementary  to this framework, and when released will provide additional analysis that  can help policymakers  understand key issues relating to AI.   2.3.2 Nuffield Foundation’s roadmap for AI research   Ethical and societal implications of algorithms, data, and artificial intelligence: a roadmap for research  by  the Nuffield Foundation, examines the e thical implications of research into AI. It first examines the  ambiguity in many of the key concepts that are regularly brought up in discussions of AI ethics, such as  values and privacy, which can hold different meanings among different audiences. It aims  to ensure that  when discussing these issues, people do not “talk past one another”. It also makes the key point that a  number of values are often in conflict with each other and there will inevitably be tradeoffs —for example,  quality of services can often  be in conflict with privacy; convenience can be in conflict with dignity and  accuracy can b e in conflict with fairness  [70]. The inevitability of tradeoffs in AI algorithms is dis cussed  further in chapter 5.3 of this report.   2.3.3 Institute of Electrical and Electronics Engineers   Some of the most comprehensive documents regarding the ethical development of AI have been produced  by the Institute of Electrical and Electronics Engineers (IE EE) through their Global Initiative on Ethics of  Autonomous and Intelligent Systems, which is comprised of several hundred world leaders across ind ustry,  academia, and government  [71]. In 2016 the group released an initial report on ethical design,  [72]  and  based on public feedback released the se cond version for review in 2017  [73]. Their primary goal is to  produce an accessible and useful framework that can serve as a robust reference for the global  development of AI.   The IEEE outlines five core principles to consider in the design and implementation of AI:    Adherence to existing human rights frameworks    Improving human wellbeing    Ensuring accountable and responsible design    Transparent technology    Ability to track misuse   The comprehensive and collaborative approach to the development of the framework provides a well rounded frame of reference for company, governmental and academic ethical gui delines.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 23  2.3.4 AI Now   The US -based AI Now 2017 Report  [74] reviews current academic research around emerging and topical AI  issues. The report focuses on four key  issues: labour and automation; bias and inclusion; rights and  liberties; and ethics and governance. The discussion of bias and inclusion is the most comprehensive and  crucial section of the report, as this issue will impact AI design from the outset and w ill have long -running  negative consequences if not appropriately addressed. According to the report, the current known bias in  US-developed AI can be attributed to the lack of gender and ethnic diversity in the tech industry. However,  this issue may have g lobal reach, as many tech branches of international companies are based in the US  and are thus subject to the same problem. In December of 2018, AI Now issued its AI Now 2018 Report ,  which included a timeline of key ethical breaches involving AI technologies throughout the year  [75]. It also  highlighted key developments in ethical AI rese arch and emerging strategies to combat bias, such as  recognising allocative and representational harms, new observational fairness strategies, anti -classification  strategies (which focus on appropriate input data and measuring results), classification pari ty (equal  performance across groups, even at a cost to accuracy among certain groups in some cases) and calibration  strategies. The report included significant sections on hidden labour chains in the production of AI  technologies. It also highlighted the f act that ethics frameworks on their own are not enough, because  concrete actions need to be taken to ensure accountability and justice. AI Now has also produced a  template for Algorithmic Impact Assessments, which is discussed further in section 10 of this  report  [26].  2.3.5 The One Hundred Year Study on Artificial Intelligence   The One Hundred Year Study on Artificial Intelligence, based at Stanford University and launched in 2014, is  an effort to detail  the long -term influence of AI on society and individuals. A new report is scheduled for  release every five years, with the aim of creating a collection of reports that chronicle the development of  AI – and the issues raised by that development – over the course of one hundred years  [76]. Primarily  focussed on North American societies, the report ide ntifies eight areas that will likely undergo the biggest  transformation as a result of AI: transport; healthcare; education; low resource communities; public safety;  workplaces; homes; and entertainment. Ethical issues associated with each of these areas a re highlighted,  but the report focuses mainly on the current and future direction of AI in various domains. The authors  suggest that restrained government regulation and high levels of transparency around AI development will  provide the best climate for en couraging socially beneficial innovation.   2.3.6 The Asilomar AI Principles   In 2017, an AI conference hosted by US organisation the Future of Life Institute reviewed and discussed  some of the key literature on AI and developed a list of 23 key principles, known as the Asilomar AI  Principles  [77]. These have so far garnered 1,273 signatures of agreement from AI researchers and 2,541  signatures fr om other endorsers. There are 13 principles in the ‘ethics and values’ section of the report.  According to these, the onus is on the AI developer to adhere to responsible design, with the aim of  bettering humanity, and AI systems should be designed in line  with accepted values and cultural norms,  while protecting individual privacy and remaining transparent. Humans should also remain in control of  how and whether to delegate decisions to AI systems, with the goal of accomp lishing human -chosen  objectives  [77].   2.3.7 Universal Guidelines for Artificial Intelligence   The Public Voice coalition, a group of NGOs and representatives assembled by the Electronic Privacy  Information Center, in October 2018 issued 12 guidelines for the development of AI. These guidelines are 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 24  based on the premise that “primary responsibility f or AI systems must reside with those institutions that  fund, develop, and deploy these systems”  [78] . The 12 guidelines are:   1) Right to Transparency. All individuals have the right to know the basis of an AI decision that  concerns them. This includes access to the factors, the logic, and techniques that produced the  outcome.   2) Right to Human Determination. All individuals have the right to a final determination made by a  person.   3) Identification Obligation. The institution responsible for an AI system must be made known  to the  public.   4) Fairness Obligation. Institutions must ensure that AI systems do not reflect unfair bias or make  impermissible discriminatory decisions.   5) Assessment and Accountability Obligation. An AI system should be deployed only after an adequate  evalua tion of its purpose and objectives, its benefits, as well as its risks. Institutions must be  responsible for decisions made by an AI system.   6) Accuracy, Reliability, and Validity Obligations. Institutions must ensure the accuracy, reliability, and  validity o f decisions.   7) Data Quality Obligation. Institutions must establish data provenance, and assure quality and  relevance for the data input into algorithms.   8) Public Safety Obligation. Institutions must assess the public safety risks that arise from the  deploymen t of AI systems that direct or control physical devices, and implement safety controls.   9) Cybersecurity Obligation. Institutions must secure AI systems against cybersecurity threats.   10) Prohibition on Secret Profiling. No institution shall establish or maintain  a secret profiling system.   11) Prohibition on Unitary Scoring. No national government shall establish or maintain a general purpose score on its citizens or residents.   12) Termination Obligation. An institution that has established an AI system has an affirmative   obligation to terminate the system if human control of the system is no longer possible.   2.3.8 The Partnership on AI   Private companies are increasingly aware of the need for an ethical framework when using and developing  AI. The collegiate attitude adopted by t raditionally competitive tech companies is an indication of the  importance of openness and collaboration when developing said framework. For example, the Partnership  on AI, originally established by a handful of large tech companies, is now made up of a wi de variety of  industry and academic professionals working together to better understa nd the impacts of AI on society   [79]. Rather than a comprehe nsive ethics framework, the group has outlined eight tenets that their  members attempt to uphold. These tenets follow fairly standard topics on the ethical development and use  of AI, focusing in particular on technology that benefits as many people as poss ible; ensuring personal  privacies are protected; and encouraging transparency. At this point, the Partnership on AI has not  discussed the need to reduce bias and increase diversity in the tech industry.   2.3.9 Google   In June 2018, Google published its company pr inciples in r egards to the development of AI  [80], after staff  within the organisation protested . In addition to the f amiliar principles regarding safeguarding privacy, 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 25  developing AI that is beneficial for humanity, and addressing bias, Google has also released a list of AI  applications they have chosen not to pursue – including (but not limited to) weapons or other techn ologies  with the principal purpose of causing harm; technologies that gather surveillance in a way that violates  internationally accepted norms; and technologies whose purpose contravene principles of international law  and human rights.   The response to these principles has not been without scepticism, likely as a result of recent controversies  around Google’s contracts with the US military (which the company has  recently decided not to renew)  [81]  . Critics also noted that Google had the opportunity to be much more specific and actio n-oriented in their  principles and code, especially as they are touted as being concrete standards actively  governing Google’s  AI research  [82,83] . For instance, while the principles stated that Google will seek to avoid bias when  developing AI algorithms , a meaningful explanation of how this will be achieved was not addressed. In  addition, the proviso for independent review of Google’s AI technology development would likely be well  received.   Google’s subsidiary company Deepmind has also created an ethics board, however it has been criticised for  a lack of transparency in both membership and decision -making  [84].   2.3.10  Microsoft   Microsoft has also been a prominent voice in the AI ethics debate. In December 2018, Microsoft President  Brad Smith wrote on the company’s blog that Microsoft believed governments needed to regulate facial  recognition and that it was necessary to “ensure that this technology, and the organizations that develop  and use it, are governed by the rule of law  [85]”. The company has also put together a number of principles  and tools geared toward ethical AI. Its site includes six key prin ciples: Fairness, inclusiveness, reliability and  safety, transparency, privacy and security, and accountability. It has also issued guidelines for responsible  bots, which examine how they can earn trust  [86].  2.3.11  IBM   As a key player in the computing space and the developer of the question -and-answer AI Watson, IBM has  also released a set of materials on AI and ethics. In addition to guidance on ethical AI research and trust and  transp arency measure, IBM has also released an AI ethics guide for developers. The guide focuses on five  key areas for developers: Accountability, Value Alignment, Explainability, Fairness and User Data Rights   [87]. It stresses that the ethical development of AI cannot solely be viewed as a “technical” problem to be  resolved, and instead requires a strong focus on the communities it affects.   2.3.12  The Future of Humanity Research Institute   The University of Oxford’s Future of Humanity Institute calls for research on building frameworks that  ensure the socially beneficial development  of AI  [88]. Their report AI Go vernance: A Research Agenda   focuses on developing a global governance system to protect humanity from extreme risks posed by future  advanced AI. The report highlights the need for AI leaders to constitutionally commit to developing AI for  the common good. While the authors acknowledge that a solution that satisfies the interest of such a  diverse range of stakeholders will be exceedingly difficult and complicated, they argue that the potential  benefits to society make it a worthy endeavour.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 26  2.3.13  The Ethics of Art ificial Intelligence   In the first section of their 2014 publication Bostrom and Yudkowsky discuss the ethical issues associated  with machine learning AI developed in the near future  [89]. They make the observatio n that if a machine is  going to carry out tasks previously completed by humans then the machine is required to complete the  function to the same level that humans do, with “responsibility, transparency, auditability, incorruptibility,  predictability, and a  tendency to not make innocent victims sc ream with helpless frustration”  [89] . The  latter sections of their publication address potential ethical issues associated with super -intelligent  machines of the future, but this is out of scope for the current report.   2.3.14  The AI Initiative   Based at Harvard Kennedy School, the AI Initiative has developed a short series of recommendations to  help shape global  AI policy framework. These are  [90]:   Convene a yearly interdisciplinary meeting to discuss the pressing ethical issues in the development  of AI.     Create a global framework  that supports the ethical development of AI, including agreement on  beneficial safeguards, transparency standards, design guidelines, and confidence -building  measures.    Implement agreed -upon rules and regulations at local and international levels.   2.4 Key themes   While it is important to note that there exists other relevant work which cannot be reviewed here due to  length considerations, the publications discussed provide a snapshot of the current state of AI ethics  frameworks, and assist in framing the con text of Australia’s own uniquely tailored framework. Collectively,  the literature emphasise that the principles required for developing ethical AI centre on responsible design  that benefits humanity. This benefit is achieved through protecting privacy and human rights, addressing  bias, and providing transparency around the workings of machines. A number of tools have been suggested  to support the ethical development and use of AI, including impact assessments, audits, consumer data  rights, oversight mechani sms, and formal regulation.     
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 27  3 Data  governance   “But the plans were on display … on display in the bottom of a locked filing cabinet stuck in a  disused lavatory with a sign on the door saying ‘Beware of the Leopard”   Hitchhikers Guide to the Galaxy, Douglas Adams     Issues relating to AI ethics are intertwined with data sharing and use . The age of big data is here and  people’s opinions, interactions, behaviours and biological processes a re being tracked more than ever  [91].  However, Australians ar e largely unaware of the scale and degree to which their data is being collected,  sold, shared, collated and used. In one 2018 study, most people surveyed were aware that data generated  from their online activities could be tracked, collected and shared by  organisations (see Figure 3). However,  they frequently reported being unaware of the extent and purpose of for which their data  was collected,  used and shared  [91]. In addition, the study found that Austra lians were rarely able to grasp the full  implications of the terms of use applying to many services such as social medi a, or products like  smartphones  [91].    Figure 3. Chart indicating Australian knowledge about consumer data collection and sharing   Data source: Consumer data and the digital economy - Emerging issues in data collection, use and sharing  [91]  Despite low levels of public understanding, data governance issues are crucial and will on ly become more  important as AI development gains pace. Data has immense and growing value as the input for AI  technologies. As the value and the potential for exploitation of data increases, so does the need to protect  the data rights and privacy of Austra lians.  0% 20% 40% 60% 80% 100%Companies today have the ability to follow my activities across many sites on the webIn store shopping loyalty card providers like Flybuys and Everyday Rewards have the ability to collect and combine information about me from third partiesSome companies exchange information about their customers with third parties for purposes other than delivering the product or service the customer signed up forAll mobile/tablet apps only ask for permission to access things on my device that are required for the app to workWhen a company has a privacy policy, it means the site will not share my information with other websites or companies Correct Incorrect Don't Know
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 28  3.1 Consent and the Privacy Act   Personal data is regulated by the Australian Privacy Act, which classifies it as “information or an opinion,  whether true or not, and whether recorded in a material form or not, about an identified individual, or an  indiv idual who is reasonably identifiable”  [22] .  Privacy itself is a contested term, subject to many varying interpretations. It is far more than a right to a  degree of secrecy . Privacy is explicitly stated to be a human right under Article 12 of the Universal  Declaration of Human rights.   When working with personal data, protecting the consent process is fundamental to protecting privacy.  Due to the sensitive nature of personal data, consent should be adequately addressed at the point of data  collection. The Privacy Act stipulates that consent m ay be express or implied, and that it must abide by four  key terms:    The individual is adequately informed before giving consent    The individual gives consent voluntarily    The consent is current and specific    The individual has the capacity to understand and c ommunicate their consent  [22]  The third t erm of the Privacy Act states that consent must be current, however, at the time of writing there  are no specific provisions for the ‘right to be forgotten’, which features in the EU’s recently established  General Data Protection Regulation  [92] and the UK’s updated data protection laws  [93]. To align with this  international legislation, the ‘right to be forgotten’ could be considered for future incorporation into  Australia’s Privacy Act , but there may be other measures that are more  suitable in the Australian context .  Although this right affords individuals the greatest control over their data, it may be difficult to enforce and  adhere to, especially if the data has already been integrated into an AI system and a model has already  been trained.  It may be instructive to observe how the right to be forgotten is implemented and enforced  in the EU , as it is still in the early stages of implementation and review.   3.1.1 Case study: Cambr idge Analytica and public trust   The Cambridge Analytica scandal exemplifies the consequences of inadequate consent processes or privacy  protection. Through a Facebook app, a Cambridge University researcher was able to gain access to the  personal informatio n of not only users who agreed to take the survey, but also the people in those users’  Facebook social networks. In this way, the app harvested data from millions of Facebook users. Various  reports indicate that these data were then used to develop targete d advertising for various political  campaigns run by Cambridge Analytica.   When news broke of this alleged breach in privacy, many felt that Facebook had not provided a transparent  consent process. The ability for one user to effectively give consent for th e use of others’ data was  particularly concerning. The allegation that Cambridge Analytica used personal data to profile and target  political advertising to the users without appropriat e consent was widely criticised  [94] and both  Cambridge Analytica and Facebook were put under governmental and media scrutiny concerning their data  practices. Cambridge Analytica has now become ins olvent and Facebook stocks plummeted following the  publication of the story (although they recovered their full value eight weeks  later)  [95,96]  .   For industry, this incident serves as an example of the cost  of inadequate data protection policies  and also  demonstrates that it may not be sufficient to merely follow the letter of the law.  To avoid repeating these  mistakes, consent processes should ensure that consent is current, specific, and transparent. Regul ar  review of data collection and usage policies can help to safeguard against breaches. At a broader level, a 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 29  balance needs to be struck between protecting individual privacy and ensuring transparent consent  processes, while also encouraging investment and  innovation in new technologies that require rich  datasets.   3.2 Data breaches   With vast amounts of data being collected on individuals, the importance of protecting privacy – and of  knowing when privacy has been compromised – is crucial. A recent amendment to the Australian Privacy  Act addresses some of these concerns through the Notifiable Data Breaches (NDB) scheme, which  stipulates that if personal data is accessed or disclosed in any unauthorised way that may cause harm, all  affect ed individuals must be not ified  [97].  Between April and June 2018 there were 242 notification s of data breaches in Australia   [98]. The majority  of those breaches were a result of human error or malicious attacks (see Figure 4), suggesting that there  are security gaps in the storage and use of data. Data breaches are costly for organisations, with financial  and legal co nsequences as well as reputational damage. In 2017, the average cost of a data breach  in  Australia was $2.51 million  [99].    Figure 4. Pie chart showing reasons for  Australian data breaches, April -June 2018   Data source: Notifiable Data Breaches Quarterly Report, 1 April -30 June 2018   [98]  The mandatory reporting of breaches under the NDB scheme is a positive move towards ethical data  practices in Australia. However, these reforms should be supported with education and training on data  protection, as well as regular assessment of data practices to ensure that Australians can trust the security  of their private information.   3.2.1 Case study: Equifax data breach   In 2017 Equifax, a US -based credit reporting agency, experienced a data breach affecting at least 145.5  million individuals, with various degrees of sensitive p ersonal information compromised  [100] . This breach  was particularly concerning as Equifax had the opportunity to prevent the breach – via a patch that had  already been available for several months – but failed to identify vulnerabilities an d detect attacks to its  systems  [100] . In addition, due to the huge numbe r of people affected, it took several weeks to identify the  individuals and notify the public that the breach had occurred. The cost of the breach was estimated to be  in the realm of US$275 million.   It is widely speculated that Equifax did not have appropr iate measures or processes in place to adequately  protect the private data it held  [101,102] . This breach is an extreme example of the costs, consequences  and implications of inadequate data governance in a world increasingly reliant on the collection and use of  System fault , 5% Malicious or  criminal attack, 36% Human error , 59%
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 30  data to develop AI. Stronger data governance policies (including both technical fixes like segmenting  networks to isolate potential hackers and implementing robust data encryption, as well as external   legislation creating stronger repercussions for consumer data loss) can help to prevent th ese types of  breaches in future  [101] .  3.3 Open data sources and re -identification   The Australian Government has developed initiatives to better share and use reliable data sources. For  instance, in the 2015 Public Data Policy Statement, the Government committed to, “optimise the  use and  reuse of public data; to release non sensitive data as open by default; and to collaborate with the private  and research sectors to extend the value of public data for the benefit of the Australian public.”  [103]   This  announcement was backed up by several subsequent initiatives, culminating in the recent p ublication of  three key reforms  [104] :   A new Consumer Data Right, whereby consumers can safely share their data with trusted recipients  (e.g. co mparison websites) to compare prod ucts and negotiate better deals  [105] . The Consumer  Data Right is a right for consumers to consent to share their data with businesses – there is no  ‘implied consent’ for data transfers following the initial sharing. Consumers will also be ab le to keep  track of and revoke their consent  [105] .   A National Data Commissioner will implement and oversee a simpler, more efficient data sharing  and release framework. The National Data Commissioner will be the trusted overseer of the public  data system.    New legislative an d governance arrangements will enable better use of data across the economy  while ensuring appropriate safeguards are in place to protect sensitive information.   In addition to these reforms, tens of thousands of government datasets are available to the pub lic through  the data.gov.au website. This resource is one reason why Australia scores very highly on the international  Open Data Index (which measures government transparency online)  [106]   and may also be useful in  catalysing AI innovation and development using rich and diverse Australian datasets.   The publication of non -sensitive data is imperative to support research and innovation, but  there are  ethical issues to consider. Many of these forms of data could alone be considered de -identified or non personal, but the ability of AI to detect patterns and infer information could mean that individuals are  identified from non -personal data. Th is information can be exploited in unethical ways that infringe on the  right to privacy.   3.3.1 Case study: Ensuring privacy of de-identified data   In 2016, a dataset that included de -identified health information  was uploaded to data.gov.au. It was  expected that the data would be a useful tool for medical research and policy development . Unfortunately,  it was discovered that in combination with other public ly available information, researchers were able to  personally identify individuals from the data source. Quic k action was taken to remove  the dataset  from  data.gov.au .  The use of AI enabled devices and networks that can collate and predict data patterns has heightened the  risk of being able to identify individuals in what was considered a de -identified dataset.   A report from Australia’s Privacy Commissioner outline d the issues involved in the de -identification process  of the data release and propose d the use of rigorous risk management processes, with clear  documentation of the decision processes guiding the open  publication of de -identified data  [20]. 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 31  The Government’s Privacy Amendment (Re -identification Offence) Bill 2016 seeks to respond to a gap  identified in privacy legislation about the handling of de -identified personal information by making it an  offence to deliberately re -identify publicly released, de -identified government information.    The De -Identification De cision Making Framework by the O ffice of the Australian Information  Commissioner  and CSIRO can also assist in making decisions about these datasets.   Continued vigilance is required to ensure that de -identified datasets, that can be so useful to researchers ,  are adequately protected.     3.3.2 Case study: Locating people via geo -profiling   A recently published paper uses geo -profiles generated by public ly available information to (possibly)  identify the artist Banksy, who  has chosen to remain anonymous  [107] . The study was framed as an  investigation of the use of geo -profiling to solve a “mystery of modern art.” The authors suggest that these  methods could be used by law enforcement to locate terrorist bases based on terrorist graffiti. However,  the ability of  AI techniques to take public ly available data and make very personal inferences about  individuals poses a significant ethical issue about privacy and consent issues even when dealing with  public ly available, de -identified and non -personal data.   Any evalua tion of the identifiability of data should examine how non -personal data will be shared and with  whom. It should also consider how non -personal data could be used in conjunction with other data about  the same individual. The Office of the Australian Inform ation Commissioner has published a best practice  guide on the use of data analytics, which clearly outlines considerations and directives to ensure effective  data governance in line with the Australian Privacy Act  [108] . In particular, the guide promotes the use of  privacy -by-design to ensure that privacy is proactively managed and addressed through organisational  culture, practic es, processes and systems.   3.4 Bias in data   Machine learning and various other branches of AI are reliant on rich and diverse data sources to effectively  train algorithms to create an output. If the training data does not include a robust, inclusive sample, bi as  can creep in, resulting in AI outputs with implicit bias that can disadvantage or advantage certain groups.  Biased data inputs can lead to discrimination, most often against already vulnerable minority populations.  One of the most basic requirements for  preventing bias is controlling the data inputs to ensure they are  appropriate for the AI systems that they are used to train. Unbiased datasets, too, can yield unfair results.  This is explored more in chapter 5.3.   But simply using any and all input data i s not a solution either, as the case study below demonstrates.   3.4.1 Case study: The Microsoft chatbot   Tay the Twitter chatbot was developed by Microsoft as a way to better understand how AI interacts with  human users online. Tay  was programmed to learn to communicate through interactions with Twitter users  – in particular, its target audience was young American adults. However, the experiment only lasted 24  hours before Tay was taken offline for publishing extreme and offensive s exist and racist tweets.   The ability for Tay to learn from active real -time conversations on Twitter opened the chatbot up to misuse,  as its ability to filter out bigoted and offensive data was not adequately developed. As a result, Tay 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 32  processed, learned from and created responses reflective of the abusive content it encountered,  supporting the adage, ‘garbage in, garbage out’  [109] .  In addition to controlling the data inputs, consideration must be given to  the impact of indirect  discrimination. Indirect discrimination occurs as a result of the use of data variables highly correlated with  other variables that can lead to discrimi nation  [110] . For example, the neighbourhood that an individual  lives in is often highly correlated with their racial background, and the use of this data to make deci sions  can thereby lead to racial bias.   3.4.2 Case study: Amazon same -day delivery   Amazon recently rolled out same -day delivery across a select group of American cities. However, this  service was only extended to neighbourhoods with a high number of current Amazo n users. As a result,  predominantly non -white neighbourhoods were largely excluded from the service. The disadvantage to the  neighbourhoods excluded from same -day delivery further marginalised communities that are likely to  already be facing the impact of bias and discrimination.   Amazon could convincingly argue that it made its decision about where to roll out same -day delivery based  on logistic al and financial requirements. It did not intend to exclude non -white minorities, but because  racial demographics tend to correlate with location, the decision did result in indirect discrimination.   The above example demonstrates the need for critical assess ment of bias in data inputs used to make  decisions and create outputs (whether for AI or otherwise). In scientific research and AI programming,  strategies have been developed to optimise data inputs and sampling and reduce the impact of bias  [110] .  These issues need to be addressed at the development stage of AI, and as such, developers need  to be able  to appropriately assess their data inputs. Training and education systems that support the skills required to  address bias in sampling data would help to address this ethical issue.   3.5 Key points   - Data governance is crucial to ethical AI; organisat ions developing AI technologies need to ensure they have  strong data governance foundations or their AI applications risk being fed with inappropriate data and  breaching privacy and/or discrimination laws.   - Organisations need to carefully consider  meaningfu l consent when considering the input data that will feed  their AI systems.   - The nature of the input data affects the output. Indiscriminate input data can lead to negative outcomes, this  is just one reason why testing is important.   - AI offers new capabilitie s, but these new capabilities also have the potential to breach privacy regulations in  new ways. If an AI can identify anonymised data, for example, this has repercussions for what data  organisations can safely use.   - Organisations should constantly build on  their existing data governance regimes by considering new AI enabled capabilities and ensuring their data governance system remains relevant.     
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 33  4 Automated  decisions   “Big Data processes codify the past. They do not invent the future. Doing that requires moral  imagination, and that’s something only humans can provide. We have to explicitly embed  better values into our algorithms, creating Big Data models that follow our ethical lead.  Sometimes that will mean putting fairness ahead of profit.”   Weapons of Math Destruction, Cathy O’Neil     Humans are faced with tens of thousands of decisions each day. These decisions can be influenced by our  emotional state, fatigue, interest in the topic, internal biases and external influences. The decisions we  make in a pro fessional setting have the potential to significantly affect the greater community – for  example, an insurance adjustor’s decision about a claim, a judge’s decision about a legal case or a banker’s  decision about a loan application can have life -changing c onsequences for individuals.   Nationally and globally, AI is being used to guide decisions in government, banking and finance, insurance,  the legal system and mining sectors.   The number of decisions driven by AI will likely grow dramatically  with the develo pment and uptake of new  technology. When used appropriately, automated decisions can protect privacy, reduce bias, improve  replicability and expedite bureaucratic processes. Australia’s challenge lies in developing a framework and  accompanying resources to  aid responsible development and use of automated decision technologies.   4.1 Humans in the loop (HITL)   Automated decisions require data inputs which are analysed and assessed against criteria to create data  outputs and make decisions ( Figure 5). During the design process each of these steps requires evaluation  and assessment to ensure that t he system performs as intended.     Figure 5. Infographic showing three phases in developing a utomated decision systems     
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 34  The concept of ‘humans in the loop’ (HITL) was developed to ensure that humans maintain a supervisory  role over automated technologies  [111] . HITL aims to ensure human oversight of exception control,  optimisation and maintenance of automated decision systems to ensure that errors are addressed and  humans remain accountable.   Automated decisions that aff ect large and diverse groups of people require the design principle of ‘society  in the loop’ (SITL)  [111]  . For example, as discussed in Chapter 6.1, the automation of cars and the decisions  and predictions they make will have far reaching effects on society as a whole. Incorporating SITL when  designing automated vehicle s ystems involves considering the behaviour expected from the technology,  and how it aligns with the goals, norms and morals of various stakeholders.   Both HITL and SITL promote careful consideration of the programming used to generate automated  decisions to ensure that they reflect our laws, adhere to our human rights and social norms, as well as  protect our privacy. Problems can occur when AI is developed without critical assessment and monitoring  of the inputs, algorithms and outputs.  Article 22 of the EU’s  GDPR law states that human beings have the  right to not be subject solely to automated decisions when the decisions have legal ramifications  [64].  4.2 Black box issues and transparency   Various branches of artificial intelligence studies, such as “Explainable AI” and “Transparent AI”, seek to  apply new processes, technologies and even other layers of AI to existing AI programs in order to make  them understandable to users and other progra mmers  [112] .  This emerging area of research is likely to provide useful tools in understanding automated decision -making  mechanisms. However, it is important to consider that transparent systems can still operate with a high  error rate and significant bias. In addition, attempts to explain all of an algorithm’s processes can s low  down or hamper its effectiveness, and the explanations may still  be far too complex. As an analogy , if asked  why you chose a particular food, it wouldn’t be helpful to explain the chemical processes that occurred in  your brain while making that decisio n.   The question then becomes: what do we need to know about this algorithm to keep it accountable and  functioning according to laws, rights and social norms? Any effective approach to regulating AI will need to  be based on appropriate levels of transparen cy and accountability, which act in service of broad principles based objectives.   4.2.1 Case study: Houston teachers   A proprietary AI system was used by the Houston school district to assess the performance of their teaching  staff.  The system used student test scores over time to assess the teacher s’ impact.  The results were then  used to dismiss teachers deemed ineffective by the system  [113] .  The teacher’s union challenged the use of the AI system in court. As the algorithms used to assess the  teacher’s performance were considered proprietary information by the owners of the software, they could  not be scrutinised by humans. This inscrutability was deemed a potential violation of the teachers’ civil  rights, and the case was settled with the school distr ict withdrawing the  use of the system. Judge Stephen  Smith stated that the outputs of the AI systems could not be relied upon without further scrutiny, as they  may be “erroneously calculated for any number of reasons, ranging from data -entry mistakes to glitches in  the compu ter code itself. Algorithms are human creations, and subject to error like any other human  endeavour”  [113]  .  With the increasing development and uptake of decision support systems and automated decision systems,  similar cases will likely emerge in Australia in future – as such, it is important to understand the ethical 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 35  issues inherent in automated decision -making , and consider methods of addressing these. Resolving issues  of transparency associated with complex deep learning AI systems and proprietary systems will require  multi -disciplinary input.  Recourse mechanisms are one viable option to protect the interests of people  affected by the use of automated decision systems – for instance, some countries have implemented the  right to request human review of automated decisions. In the UK, when a decision is generated solely by  automated processing, the subject of the  decision must be made aware of this and has the ability (within a  month of being notified) to lodge a request to “( i) reconsider the decision, or (ii) take a new decision that is  not based solely on automated processing as a recourse when autonomous decis ions are used " [93].   4.3 Automation bias  and the need for active human oversight   Automation bias is “the tendency to disrega rd or not search for contradictory information in light of a  computer -generated solution that is accepted as correct”  [114]  . Relying on automated decisions in  situations where they cannot provide a consistently reliable outcome can result in increas ed errors by  commission or omission (following, or failing to, act on advice from an automated decision  system in error,  respectively)  [115] . These issues ar e particularly important when using automated decisions in situations  requiring discretion .  Good decision making based on advice from automated decision systems requires the humans involve d to  exercise active thinking and involvement, rather than passively allowing automated decision systems to  handle all of a task they are not suited for . When operators grow too reliant on automated systems and  cease to question the advice they are receivin g, problems can emerge, as demonstrated in the Enbridge  pipeline leak of 2010.   4.3.1 Case study: a utomation bias and the Enbridge pipeline leak   In July 2010, a  pipeline carrying crude oil ruptured near the Kalamazoo River in the US state of Michigan.  The resulti ng clean -up took over five years at  a cost of over US $737 million  [116] . The disaster prompted  numerous inquiries as well as academic papers. The large amount of environmental damage was caused by  the delayed reaction to the rupture, which allowed oil to pump into the surrounding area for over 17 hours.   During that time, there were two more “startup” moves to pump more oil, with the entire incident  releasing 843,444 gallons of oil into the area.   An automated system did provide warnings to control centre personnel. A review into the incident found  that op erators had heard the alarms from the system and seen the abnormally low amounts of oil reaching  the destination, but they had incorrectly attributed these warning signs to that planned shutdown. Reviews  of the incident found that it was not until an outsi de caller notified them of the leak that it was discovered  and action was taken  [116] .  Academics in 2017 analysed the disaster and suggested that regulators had overlooked complacency as a  key driving factor . They suggested that “ Industry, policy makers, and regulators need to  consider  automation bias when developing systems to reduce the likelihood of complacency errors  [117] .”  While the recommendations from review bodies highlighted the poor management of the incident, t he  academics pointed out that the people involved in the incident were all very experienced. They detailed  earlier incid ents in which more senior staff were more likely to overlook dangerous safety risks than junior  ones  [117] . They also pointed out that there had been frequent alarms in the past due to column  separation problems, which were resolved by pumping more oil down the line to clear the track.   Thus in th is case, experienced staff recommended a course of action that made the problem worse. This is  a difficult problem to resolve, because as the academics point out: “ According to some researchers, a 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 36  problem such as occurred in the Enbridge case cannot be add ressed by better training, given that it is  human nature to ignore frequent false alarms   [117] .”   Designers of automated systems need to carefully consider the way their systems will interact with human  operators, in order to counteract the damaging effects of overreliance and complacency, and ensure that  the recommendations provided by the system cannot easily be misinterpreted in harmful ways.  This will  mean careful consideration of ways to ensure HITL design principles are implemented.   4.4 Who  is responsible for automated decisions?   As AI systems  are further developed and more widely applied, it will be important to create policy outlining  where responsibility falls if things do go wrong. As an AI system has no moral authority, it cannot be held  accountable in a judicial sense for its decisions an d judgements. As such, a human must be accountable for  the consequences of decisions made by the AI.   The main question arising from liability decisions is: which entity behind the technology is ultimately  responsible, and at which point can a line be draw n between them? A recent Cambridge public law  conference highlighted the complexity of who is responsible when something goes wrong with auto mated  administrative decisions  [118] :   “To whom has authorit y been delegated, if that is indeed the correct analysis?    Is it the programmer, the policy maker, the authorised decision -maker, or the computer itself?    Is the concept of delegation appropriately used in this context at all? After all, unlike human  delegat es, a computer programme can never truly be said to act independently of its programmer  or the relevant government agency?    What if a computer process determines some, but not all, of the elements of the administrative  decision? Should the determination of those elements be treated as the subject of separate  decisions from those elements determined by the human decision -maker?”   4.4.1 Case study: Automated vehicles   In 2018, an Arizona pedestrian was killed by an automated vehicle owned by Uber.  A preliminary report   released by the National Transportation Safety Board  (NTSB)  in response to the incident states that there  was a human present in the automated vehicle, but the human was not in control of the vehicle when the  collision occurred  [119] . There are various reasons why the collision could have occurred, including poor  visibility of the pedestr ian, lack of oversight by the human driver, and inadequate safety systems of the  automated vehicle.   The complexities of attributing liability in instances of collisions involving automat ed vehicles are well  documented  [120] . In this case, although the legal matter was settled out of court and details have not  been released, the issue of liability is complex as the vehicle was operated by Uber, under the supervision  of a human driver and operated autonomously using components designed by various other tech  companies. Following their full investigative process, the NTSB will release a final report of the incident  identifying the factors that contributed to the collision.   The attribution of responsibility in regards to AI poses a  pressing dilemma.  There is a need for consistent  and universal guidelines, applicable across various industries utilising technology that is able to make  decisions significantly affecting human lives.  In addition, policy may provide a universal framework that aids  in defining appropriate situations where automated decisions and judgements may be used.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 37  4.5 Key points:   - Existing legislation suggests that for government departments, automated decisions are suitable when there  is a large volume of decisions to be m ade, based on relatively uniform, uncontested criteria. When discretion  and exceptions are required, automated decision systems are best used only as a tool to assist human  decision makers —or not used at all. These requirements are not mandated for other o rganisations, but are a  wise approach to consider.   - Consider human -in-the-loop (HITL) principles during the design phase of automated decisions systems, and  ensure sufficient human resources are available to handle the likely amount of inquiries.   - There must  be a clear chain of accountability for the decisions made by an automated system. Ask: Who is  responsible for the decisions made by the system?    
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 38  5 Predicting human behaviour   “Criminal sentences must be based on the facts, the law, the actual crimes committed, the  circumstances surrounding each individual case, and the defendant’s history of criminal  conduct. They should not be based on unchangeable factors that a per son cannot control, or on  the possibility of a future crime that has not taken place.”   Former US Attorney General, Eric Holder     In a similar manner to which AI systems are able to process information and use it to make decisions, they  are also able to extrapolate information and recognise patterns that can be used to make predictions about  future behaviours and events.  Although the  previously discussed concepts of HITL, transparency and black  box issues and accountability apply, the capability to predict future potential actions poses additional  specific ethical concerns related to bias and fairness that require consideration.   When used appropriately, AI -enabled predictions can be powerfully accurate, replicable and efficient. They  can be used to our advantage in place of human generated judgements and predictions, which can be  subject to various extraneous variables that can be thou ght of as noise, such as bias, fatigue and effort.  This  technology could be especially useful in industries that require decision makers to generate frequent,  accurate and replicable predictions and judgements such as the areas of justice, policing and med icine.   To appropriately assess the ethical issues associated with the use of AI enabled predictive and judgement  systems it is crucial to first acknowledge the inherent issues associated with human judgements and  predictions.   The Australian legal system uses precedent and sentencing guidelines to regulate decision making in an  effort to combine discretion and address the influence of bias. Although judges spend years training they  may still be impacted by cognitive biases, personal opinions, fluctuations in interest, fatigue and hunger   [121] .  Policies should promote Austr alia’s colloquial motto, everyone deserves a fair go. Ensuring that AI systems  are operating in a fair and balanced ways across the diverse Australian population is a cornerstone of  ethical AI. Establishing industry standards supported by up -to-date guidel ines could provide a baseline level  of assessment for all AI used in Australia to support the use of fair algorithms.    5.1.1 Case study: Israeli Judges and decision fatigue   In one high profile study from Israel, academics examined parole hearings in Israel to de termine what  factors were most likely t o result in a favourable ruling  [122,123] . After observing 1,112 rulings, the  researchers found that early in the day, and right after food breaks, the judges were far more l ikely to grant  parole. The difference was extreme, with over 65% of cases right after rest breaks receiving a favourable  ruling, compared to 0% just before a break.   The researchers suggested that the reason for this was that the judges simply became hungr y and tired,  resulting in harsher sentences. Other researchers have disputed the findings, indicating that the effect in  this particular study was more likely due to other factors such as prospective parolees only having legal  counsel at some times of day,  or cases being deferred  [124] .  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 39  The extent to which decision fatigue and general exhaustion affects judicial decisions is open to deba te, but  the problems associated with decision -fatigue are well supported in academic literature, as are the  difficulties in grappling with cognitive biases. Miscarriages of justice are frequently attributable to human  error  or misconduct.   Even the best dec ision -makers will sometimes resort to mental shortcuts, or “heuristics” to understand  a  situation and make decisions  [125] . Well -designed AI has the ability to perform in these situations with a  much higher degree of replicability and consistency  than humans.   5.2 Bias, predictions  and discrimination   Indirect discrimination occurs when data variables that are highly correlated with discriminatory var iables  are included in a model   [110]  – to give an example, an algorithm might not explicitly consider race as a  factor, but it might discriminate against a neighbourhood filled almost enti rely with people of one race ,  leading to a similar result . The superior ability of AI to recognise patterns creates serious potential ethical  issues when it is used to make predictions about human behaviour. To ensure that predictive systems are  not indire ctly biased, all variables used to develop and train the algorithms must be rigorously assessed and  tested. In cases with higher risk, it may be important to run smaller tests or simulations before using them  on the broader public.  In addition , the model i tself should be assessed and monitored to ensure that bias  does not creep in.   Australian legislation prohibits discrimination (unfair or unequal treatment of a group or individual) on the  basis of race, colour, sex, religion, political opinion, national ex traction, social origin, age, medical record,  criminal record, marital or relationship status, impairment, mental, intellectual or psychiatric disability,  physical disability, nationality, sexual orient ation, and trade union activity  [126] .   AI is set to use many of these indicators to make predictio ns about health or behaviour. Not all of these will  constitute discrimination. Race, for example, may prove to be a relevant indicator of a particular health  problem that could be avoided —consider fair skinned people, who are more at risk of skin cancer  [127] . An  AI that assesses skin cancer risk would need to take into account skin tone as a factor.   So when is the use of a particular input variable considered discrimination? Careful consideration will need  to be given as to what kind of outcomes constitute discrimination. This will be helpful in considering what  variables can be included, but even bey ond explicit variables there are also indirect ones.   Researchers have pointed out that there are many indicators, such as postcodes, education and family  history that can effectively indicate race without it explicitly needing to be included as an indicato r. In one  example from the US, geographic information was used to determine the cost of test preparation services.  It was revealed that this method unfairly discriminated against Asian American students who were charged  higher fees for academic thesis revi ew service s than other non -Asian students   [128] . Although ethnicity  was not specifically considered in the pricing structure of the service, the use of location based pricing  disproportionally impacted Asian -American s tudents with higher fees result ing in indirect discrimination.   This also prompts another ethical question for consideration: beyond racial discrimination, should location based discrimination be permissible or is this still discrimination?   The issue of rac ial bias has been exposed in AI used in the US, in courts to assess the likelihood that  someone will re -offend, and by police to focus on crime hotspots and identify potential suspects. Research  and debate is already occurring on the suitability of these t ools in the Australian context   [129,130] . 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 40  5.2.1 Case study: The COMPAS system and sentencing   The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) syste m is currently  used in many US courts to advise judges on sentencing and probation decisions. The system evaluates  individuals using 137 questions and assigns a risk score out of ten that indicates the probability of how  likely it is that a person will re -offend.   Although race is not directly assessed by the system,  zip code is.  Research by non -profit media outlet  ProPublica found that black people profiled by COMPAS were twice as likely to be incorrectly labelled as  being at high risk of committing violent  repeat offences than white people   [16].   The creators of the algorithm, Northpointe, responded to the report denying that their algorithm  is biased  against black people  [17]. Northpointe indicated that across their risk scores, ranging from one to ten, there  were equal levels of recidivism from black and white groups and that there was no racial bias in their  system  [23].  But the problem, according to ProP ublica, was not in the correct predictions, which they acknowledged  were reasonably fair across the two racial groups. The problem was in the incorrect predictions  (or false  positives ).  “When we analysed  the 40 percent of predictions that were incorrect, w e found a significant racial  disparity. Black defendants were twice as likely to be rated as higher risk but not re -offend. And white  defendants were twice as likely to be charged with new crimes aft er being classed as lower risk.”  [18]   Both ProPublica and  Northpointe were able to examine the same system and data and come up with  opposing findings on the racial bias of the system.  The discrepancy between the two analyses essentially  came down to the way each group assessed and measured fairness and balanced  it with accuracy of the  system.  It also suggests that internal reviews may miss key problems if they base their analysis on the same  assumptions of fairness as the original design.   The ability to assess bias in predictive systems is intrinsically linked w ith how fairness is measured along  with the level of transparency involved.   The way that the COMPAS system weighs and assess es the defendant’s input variables to calculate their  risk score is proprietary software, so assessment of the way that the system d eals with racial indicators  cannot be directly assessed. This lack of transparency presents significant ethical issues as the predictive  scores assigned to individuals can have significant effects on their lives. This lack of clarity on how the  system work s has already resulted in one challenge being filed with the US Supreme Court on the basis that  due to a lack of transparency, the accuracy and validity of COMPAS could not be d isputed. The case was not  heard  [131] .  The complex interactions between bias, fairness and transparency in AI enabled predictive systems are  exemplified in the COMPAS case study. Although the use of AI enabled predictive systems to help support  decision making processes poses great potential benefits in boosting replicability and reducing human error  and bias, there are inherent ethical issues that must be addressed , particular ly the effects of indirect  discrimination  and fairness . With appropriate transparency and accountability guidelines the use of these  systems could allow us to examine the way the predictions are made in turn giving us the ability to make  adjustments to address bias.   There may need to be ongoing conversations with the community about how their data is used. COMPAS,  for example, may not explicitly consider race, but it does explicitly consider the family background of the  offender and whether their parents are married or separated   [132] . Would this b e considered fair?   Australia must also c ontend with different sentencing outcomes for racial groups. In NSW a Suspect Target  Management P rogram  (STMP) was introduced in 2000 to identify and target repeat offenders to pro -
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 41  actively disrupt their criminal behaviour. This program has come under some  criticism for disproportionally  targeting Aboriginal people.    Finding the right balance between proactively preventing crime and inaccurate group profiling is an  ongoing ethical challenge across all jurisdictions.   The designers of algorithms need to pay  careful attention to how their systems come to a prediction and  there may be a role for government bodies in determining general boundaries and review and monitoring  processes —based on existing laws regarding discrimination —for how issues relating to  the separate but  related issues of  bias and fairness  can best be addressed and mitigated.    5.3 Fairness and predictions   The challenge of ensuring fairness in algorithms is not limited to biased datasets. The input data comes  from the world, and the data colle cted from the world is not necessarily fair.   The various population sub groups, such as men, women or people of a particular race or disability, are all  likely to be represented differently across datasets. When put into an AI or automated system, various subpopulations will rarely if ever show exactly the same input and  output results. This makes it likely that the  AI will be  inherently discriminatory in one way or another —and that is assuming that the input information  is reasonably accurate.   Accuracy in datasets is rarely perfect and the varying levels of accuracy can in and of themselves produce  unfair results —if an AI makes more mistake for one racial group, as has been observed in facial recognition  systems  [133] , that can constitute racial discrimination .   Then, there is the issue of relatively accurate data that represents and unfair situation in the real world.  Disadvantaged groups may be disadvantaged for historical or institutional reasons, but that information  isn’t necessarily underst ood or compensated for by an AI, which will assess the situation based on the data  alone.   So what is “fairness?” It really depends who you ask.   “Fairness” is a difficult concept to pin down and AI designers essentially have to reduce it to statistics.  Researchers have come up with many dozens  of mathematical definitions to define what fairness means in  an algorithm and many of them perform extremely well when measured from one angle, but from a  different angle can produce very different results. This concept of differing perspectives of fairness is  exemplified in the COMPAS case study. ProPublica and the Northpointe had diverging perspectives on how  to accurately judge parity between the assessment of white and black defendants resulting in completely  different answers as to whether the system was  biased.   Put simply: it will sometimes be mathematically impossible to meet every single fairness measure because  some of them contradict each other and multiple datasets will be used in systems, and these datasets will  almost never be exactly equal in acc uracy or representativeness. Tradeoffs will sometimes be necessary.   It is important for government and society to give consideration to the degree of flexibility that designers of  AI systems should have when it comes to making trade -offs between fairness m easures and other priorities  like profit . There needs to be serious consideration given to whether the net benefits of the algorithm  justify its existence, and whether it is justified in the ways it treats different groups .   Companies and consumers will be  faced with decisions to make about which algorithms best represent  their values. If a company prioritises profit ahead of various forms of fairness, can they justify it? A key  consideration will be how transparent they are in this decision so the broader public are able to make  informed choices, and companies are acting in accordance with public expectations.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 42  5.3.1 Case study: The Amazon hiring tool   In 2014, global e -commerce company Amazon began work on an automated resume selection tool. The  goal was to have a  system which could scan through large numbers of resumes, and determine the best  candidates for positions, expressed as a rat ing between one and five stars. The tool was given information  on job applicants over the preceding ten years.   In 2015, the resear chers realised that because of male dominance in the tech industry, the tool was now  assigning higher ratings to men than women. This was not just a problem of there being larger numbers of  qualified male applicants —the key word “women” appearing in resume s resulted in them being  downgraded.   The tool was designed to look beyond the common keywords, such as particular programming languages,  and focus on more subtle cues like the types of verbs used. Certain favoured verbs like “executed” were  more likely to appear on  the resumes of male applicants  [134] .   The technology  not only unfairly advantaged male applicants, it also put forward unqualified applicants .  The hiring tool was scrapped , likely due to these problems.   Talent websites such as LinkedIn use algorithms in their resume -matching systems, but key staff have said   that AI in its current state should not be making final hiring decisions on its own, and instead should be  used as a tool for human recruiters.   In Australia, ANZ  bank in 2018  indicated it is researching the use of AI in hiring practices  [135] . It states that  the goal is to find candidates in a “fair and unbiased” way. Algorithms used in such a manner can take  decisions out of  human hands, but as the Amazon case demonstrates, this does not necessarily mean they  are without human bias , especially in the training data . Hiring tools such as these will need to ensure that  the data inputs —which in this case , includes measures like t he time taken to answer certain questions —are  relevant and reflective of actual performance in the role. Testing this kind of outcome is difficult, as metrics  for employee performance are not always easily expressed in KPIs. There may indeed be potential f or  ethical outcomes from these tools,  but it should not be assumed that these tools will be more ethical or  less biased than human recruiters.   5.4 Transparency , policing  and predictions   Predictive analytics powered by big data can boost the accuracy and efficiency of policing in Australia, but  lessons learned overseas point to a need for strong transparency measures and for the public to be actively  involved in any program. Predictive pol icing programs are active in the US and in the UK, though there is  little peer -reviewed material on their effectiveness  [24].  Most predictive policing tools are not about predicting a specific crime —instead they can be used to either  profile people or places, and measure the effectiveness of particular policing initi atives. They aim to inform  police on crime trends and what is or isn’t working, and focus on long -term persistent problem s rather than  individual crimes  [24].   5.4.1 Case study: Predictive policing in the US   When data analysis company Palantir partnered up with the New Orleans police department, it began to  assemble a data base of local individuals to use as a basis for predictive policing. To do this it used  information gathered from social media profiles, known associates, licence plates, phone numbers,  nicknames, weapons and addresses  [136] . Medi a reports indicated that this database covered around 1%  of the population of New Orleans. After it had been in operation for six years there was a flurry of media 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 43  attention over the “secretive” program. The New Orleans Police Department (NOPD) clarified t hat the  program was not secret and had been discussed at technology conferences. But insufficient publicity meant  that even some city council members were unaware of the program.   A “gang member scorecard” became a focus of media coverage, and groups such a s the American Civil  Liberties Union (ACLU) pointed out that operations like this require more community approval and  transparency. The ACLU argued that the data used to fill out these databases could be based on biased  practices, such as stop -and-frisk po licies which disproportionately target African American males, and this  would feed into predictive policing databases.  [136]   This could mean more African Americans were  scanned by the system, creating a feedback loop in which the y were more likely to be targeted. This should  be a key consideration in any long -term use of an AI program —is the data being collected over time still  serving the intended outcome, and what measures ensure that this is being regularly assessed?   The NOPD t erminated its agreement with Palantir and some defendants that were identified through the  system have raised the use of this technology in their court defence and attempted to subpoena  documents relating to the Palantir algorithms from the authorities  [137] .  The Los Angeles Police Department (LAPD) still has an agreement with Palantir in relation to its LASER  predictive policing system. The LAPD also runs Predpol, a system which predicts crime by area, and suggests  locations for police to patrol based on previously re ported crimes, with the goal of creating a deterrent  effect. These programs have both prompted pushback from local civic organisations, who say that residents  are being unfairly spied upon by police because their neighbourhoods have been profiled  [138] , potentially  creating another feedback loop in which people are more likely to be stopped by police because they live in  a certain neighbourhood, and once they have been stopped by police they are more likely to be stopped  again.   In response, local police have invited reporters to see their predictive policing in action, arguing that it also  helps communities affected by crime and pointing out that early intervention can save lives and foster  positive links be twee n police and entire communities  [138] . Some police officers were also cited in media  pointing out that there needs to be sufficient public involvement and understanding and acceptance of  predictive poli cing programs in order for them  to effectively build those community links.   There are clear ethical issues that arise with the advent of predictive policing.  One of the key issues is the  need for transparency in how these systems work so that they can be adequately assessed and that they  remain accountable to the citizens affected by them.   In addition the exploitation of potentially personal data by these systems could infringe on privacy rights  accorded to Australians through the Privacy Act.  The trade -off between the increased ability of the police  to prevent and monitor crime and the protection of personal privacy is discussed in Chapter  6.2.  5.4.2 Case study: Predictive policing in Brisbane   One predictive policing tool has already been modelled to pre dict crime hotspots in Brisbane  [139] . Using  10 years of accumulated crime data, the system used 7 0% of the data to predict crime, with the researchers  seeing if its predictions correlated with the remaining 30%. The results proved more accurate than existing  models, with an improvement of 16% accuracy for assaults, 6% more accuracy for predicting unla wful  entry, 4% better accuracy for predicting drug offences and  theft, and 2% better for fraud  [139,140] . The  system can predict long term crime trends, bu t not short term ones  [140] . The Brisbane study used  information from location -based app foursquare, and incorporated information from both Brisbane and  New York.   Predictive policing tools typical ly use four broad types of information  [140] :    Historical data, such as the long term crime patterns recorded by p olice as crime hotspots.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 44   Geographical and demographic information, including distances from roads, median values of houses,  marriages, socioeconomic and racial makeup of an area.    Social media information, such as tweets about a particular location and keyw ords    Human mobility information, such as mobile phone usage and check ins and the associated distribution  in population   The Brisbane study primarily used human mobility information. Further research is needed, but this study  suggests that some types of inp ut information may be more effective at gauging accurate information than  others —so careful consideration may warrant emphasizing some types of input information over others,  given that they will have varying impacts on privacy and some less intrusive appr oaches may be less likely  to provoke public distrust. Public trust, as case studies from the US demonstrate, is a crucial element of the  effectivenes s of predictive policing tools.   The debate over the use of AI in policing is ongoing. One clear outcome has  been that if new technologies  are used in law enforcement without public endorsement, then those systems will not effectively serve the  police or general public. Transparency about how these systems operate and in what circumstances they  will be used are at the core of ensuring they remain ethical and accountable to the communities they  protect. It may be constructive for agencies to consider public engagement strategies and feedback  mechanisms before introducing new AI technologies that will significantly  affect the public. It may also be  prudent to consider risk analysis to provide objective information for the public on the beneficial outcomes  of using AI enabled systems versus traditional policing methods.   5.5 Medical predictions   AI predictions have the abi lity to add immense value to the Australian health care system in patient  management, diagnostics and care. However, like all new medical advances and methods, AI systems used  in health care require close management and gold standard research before implem entation.   5.5.1 Case study: Predicting coma outcomes   A program in China that analyses the brain activity of coma patients was able to successfully predict seven  cases in which the patients went on to recover, despite doctor assessments giving the m a much lower  chance of recover y [141] . The AI took examples of previous scans, and was able to detec t subtle brain  activity and patterns to determine which patients were likely to recover and which were not. One patient  was given a score of just seven out of 23 by human doctors, which indicated a low probability of recovery,  but the AI gave him over 20 p oints. He subsequently recovered. His life  may have been saved by the AI.   If this AI lives up to its potential, then this kind of tool would be of immense value in saving human lives by  spotting previously hidden potential for recovery in coma patients —those given high scores can be kept on  life support long enough to recover. But it prompts the question: what about people with low scores?   Rigorous peer -reviewed research should be conducted before such systems are relied upon to inform  clinical decisions.  Ongoing monitoring, auditing and research are also required.   Assuming the AI is accurate —and a number of patients with low scores would need to be kept on life  support to confirm the accuracy of the system —then the core questions will revolve around resour cing.  Families of patients may wish to try for recovery even if the odds of success are very small. It is crucial that  decisions in such cases are made among all stakeholders and don’t hinge solely on the results from a  machine. If resources permit, then f amilies should have that option.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 45  A sad reality of the hospital system is that every day, resources determine life and death decisions and  those hospital resources are not limitless. If an AI system has the potential to direct those resources into  situation s with the best chance of recovery, then that is a net gain in lives saved.   In many ways, an AI -enabled diagnostic tool is no different from other diagnostic tools —an abnormal  reading from an electrocardiography device has essentially made a similar predic tion with life or death  consequences , and assisted doctors in giving the best treatment they can t o the highest number of people.   5.5.2 AI and health insurance   If artificial intelligence can deliver more accurate predictions in areas like healthcare, then this has  ramifications for insurance. If an AI can assess someone’s health more accurately than a human physician,  then this is an excellent result for people i n good health —they can receive lower premiums, benefiting both  them and the insurance company due to the lower risk.   But what happens when the AI locates a hard -to-spot health problem and the insurer increases the  premium or denies that person coverage al together, in order to be able to deliver lower premiums to other  customers or increase profit? In Australia, there are prohibitions on discriminating against people with pre existing conditions, but it is permissible for insurers to impose a 12 -month waiti ng period for any payouts  for people with pre -existing conditions, to ensure they do not take out insurance just ahead of an expensive  procedure  [142] . Rules such as this one may become even more important moving forward.   Numerous medical technologies im prove the ability to diagnose health problems and thus improve the  ability to calculate risk, with genetic screening being just one recent example. AI technologies may boost  the accuracy of these risk assessments, but they do not necessarily change the nat ure of insurance beyond  this context.   In the event of a dramatic leap in the accuracy of health predictions, regulatory responses may be needed  to ensure people with health problems are not priced out of the insurance market. At the present stage,  absent a  far-reaching shift in the provision of healthcare, these responses are likely to fit comfortably  within existing legislation regarding the insurance industry.   5.6 Predictions and consumer behaviour   Whether they are aware of it or not, Australians who use social media or search engines are likely to have  received targeted advertisements. These include adverts from platforms giants such as Google or Facebook  that pitched a product based on their online activity. This can provoke mixed feelings among consumer s.  Writing in a journal on consumer preferences, several scholars recently examined the ability of AI -enabled  technologies to accurately predict consumer behaviour and provide targeted advertising. They state that  what may be a short -term boon to advertise rs can come at a cost beyond the immediate monetary input:   “We contend that some of the welfare -enhancing benefits of those technologies can backfire and generate  consumer reactance if they undermine the sense of autonomy that consumers seek in their deci sionmaking. That may occur when consumers feel deprived of their ability to control their own choices:  predictive algorithms are getting better and better at anticipating consumers’ preferences, and decision making aids are often too opa que for consumers to understand ” [143]  .  There is a lot of misunderstanding on the part of consumers regarding the techniques used to determine  their preferences for targeted advertising. When questioned by the US senate, Facebook CEO Mark  Zuckerberg had to repeatedly deny that Facebook messenger listens to audio messages between peo ple to  better sell them adverts  [144] . While Facebook does not appear to be  mining audio messages, the company  has utilised machine learning to predict when users might change brand as part of a “loyalty prediction” 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 46  program offered to advertisers, and these c laimed capabilities were only reported in  media through leaked  documents  [145] .  Advertising and data collection standards that address AI capabilities and ensure privacy is protected will be  crucial in building trust between consumers and comp anies, while ensuring a balance between intrusive  and beneficial targeted advertising.   5.6.1 Case study: Manipulating the mood states and perceptions of users   Controversial research , published in a peer -reviewed journal , used Facebook’s platform to demonstrate  that users’ moods can be manipulated by filtering their feeds (comments, videos, pictures and web links  posted by their Facebook friends).  [146]   Reduced exposure to feeds with positive content led to the user  posting fewer positive posts, and the same pattern occurred for negative content. This study was public ly  criticised for failing to gain informed consent from Facebook users. However, setting aside the issue of  informed consent, the study highlights the power of AI -driven ‘filtering practices’ to shape user mood,  which can be used to enhance the impact of ta rgeted advertising.   AI technology can go beyond filtering news feeds, to manipulating video content in real time. A  research  project called Deep Video Portraits was recently showcased at an international conference on innovations  in animation and computer  graphics, and showed videos of ‘talking heads’ being seamlessly altered  [147] .  The technology produced subtle changes in emotion and tone that are diff icult to distinguish from real  footage. While the researchers contend that the technology can be used by the film industry, the  technology also has implications for the fake news phenomenon. As one of the Deep Video Portraits  researchers Michael Zollhofer  [148]  stated in a press release, “With ever -improving video editing  technology, we must also start being more critical about the video content we consume every day,  especially if there is no proof of origin.”    These examples show that AI can be used to slant information without user knowledge, and for the  purpose of influencing how consumers of media feel and perceive reality. AI -based manipulations of this  calibre will require new controls to ensure users can trust the content they receive and are infor med of  advertising tactics.   One potential approach here could involve requirements for information to be p osted on websites like  Facebook revealing when  AI techniques have been used to enhance targeted advertisements. This measure  would be similar to the EU Cookie Law, which requires websites to explain to users what information is  captured  by the site and how it is used  [149] . More sophisticated techniques might be required for fake  videos. For example, the US Defence Advanced Research Projects Agency runs a program called Media  Forensics that is developing AI tool s to detect doctored video clips  [150] .  5.7 Key points   - AI is not driven by human bias but it is programmed by humans. It can be susceptible to the biases of its  programmers, or can end up making flawed judgments based on flawed information. E ven when the  information is not flawed, if the priorities of the system are not aligned with expectations of fairness, then  the system can deliver negative outcomes.   - Justice means that like situations should deliver like outcomes, but different situations can deliver different  outcomes. This means that developers need to pay special care to vulnerable, disadvantaged or protected  groups when programming AI.   - Full transparency is sometimes impossible, or undesirable (consider privacy breaches). But there are a lways  ways to achieve a certain degree of transparency. Take neural nets, for example: they are too complex to  open up and explain, and very few people would have the expertise to understand anyway. However, the  input data can be explained, the outcomes fr om the system can be monitored, and the impacts of the system 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 47  can be reviewed internally or externally. Consider the system, and design a suitable framework for keeping it  transparent and accountable. This is necessary for ensuring the system is operating fairly, in line with  Australian norms and values.   - Public trust is of key importance. Organisations would be well advised to go beyond the “letter of the law”  and instead follow best practice when designing AI.   - Not all input data is equally effective, but t here are also varying levels of invasiveness. Carefully consider  whether there are alternative forms of less invasive input data that could yield equal or better results. Ask:  “Is there less sensitive data that could deliver the necessary results?”   - Know th e trade -offs in the system you are using. Make active choices about them that could be justified in  the court of public opinion. If a poorly -designed AI system causes harm to the public, ignorance is unlikely to  be an acceptable defence.     
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 48  6 Current examples  of AI in practice    In addition to addressing the ethical issues that have arisen from data, automated decisions and predictive  technologies it is important to consider examples of how AI systems are integrated in ways that have  already or could potentially have an enormous impact on society. In this chapter we discuss the ethical  issues associated with AI enabled vehicles and surveillance.  These technologies have been a key area of  focus in ethical AI discussions and are often used as examples of  areas that need focussed attention from  governments to help regulate their use.   6.1 Autonomous vehicles   “There is a naïve view that AVs are in themselves beneficial. They can be beneficial only if we  deliberately make them so”   Fighting Traffic, Peter D. Norto n    Autonomous vehicles (AVs) represent a major possibility for artificial intelligence applications in transport.  However, the definition of ‘autonomy’ exists on a spectrum, rather than as a binary. There are five levels of  vehicle autonomy, defined by the  Society of Automotive Engineers  [151]  (Figure 6).     Figure 6. Infographic showing t he five levels of vehicle autonomy   Data source: Adapted from the Society of Automated Engineers  [151]   
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 49  6.1.1 Autonomous vehicles in Australia   In Australia, transport ministers have agreed to a phased reform program delivered by the National  Transport Commission (NTC) to support the safe and legal operation of automated vehicles from 2020   [152] .   The NTC recently published guidelines for trialling automated vehicles in Australia  [153] . The guidelines  describe a n application process to request the trial of automated vehicles on Australian roads. The criteria  that must be addressed in the applications include details of, the trial location, the technology being  trialled, the traffic management plan, the infrastruc ture requirements of the trial, how the organisations  will with engage with the public and how they will manage changes over the course of the trial. The  guideline intends to support innovation, create a national set of guidelines and encourage investment in  Australia as a ‘global testbed for automated vehicles’  [153] . Level 4 automated vehicles trials are currently  being run in Melbourne, Perth and Sydney.   The NTC also released a policy paper that helps clarify how traffic laws are applied to vehicles with  automated functions at this point in time . In particular the paper clarifies the responsibility of the human  driver “for compliance with road traffic laws when a vehicle has conditional automat ion engaged at a point  in time”  [154]  . The most recent release by the NTC addresses the laws that need to be changed to suppor t  the use of automated vehicles  [155] .  The NTC is currently working on several more reports including the need to address insurance issues and  safety and the regulation of vehicle data. In preparation for the changes automated cars will bring, the  Minister for Infrastructure, Transport and Regional Development announced the upcoming opening of a  new Office of Future Transpo rt Technologies in October 2018  [156] . These upcoming initiatives will help  provide the required foresight and support that will enable Australia to keep pace with the rapidly changing  capabilities of automated vehicles.   6.1.2 Costs and benefits of automated vehicles   There is growing commercial inte rest around AVs, with sales predicted to reach 1 million  by 2027 and 10  million by 2032  [157-161] . However, man y artificial intelligence experts caution that Level 5 autonomy is  still much further away than generally believed. As well as technological barriers, the affordability,  capability and accessibility of AVs, along with privacy concerns, coul d also impact fu ture uptake  [162] .  Howev er, AVs – even without reaching Level 5 autonomy – also have the potential to deliver numerous  social, environmental, and financial benefits. Research has found that AVs could ease congested traffic f low  and reduce fuel consumption [163]; reduce travel costs (through lowering the cost of crashes, travel time,  fuel, and parking)  [164]  ; and enable a smaller car fleet  [165] . (However, the additional convenience and  mobility afforded by AVs could also translate into greater demand for private vehicle travel over public   transport, walking, or cycling  [166-168] ).  Safety represents another major potential benefit of vehicle automation. US research has found that over  90% of car crashes result from human error [169] and 40% of fatal crashes are caused by distr action,  intoxication or fatigue  [164] . Removing the human driver from the equation will therefore eliminate these  incidents – some estimates suggest that full vehicle automation could reduce  traffic accidents by up to 90%   [170] . However, there are also safety concerns surrounding AVs, especially since the high -profile incident in  March 2018 when an  AV hit and killed a pedestrian  [171] . (The preliminary report on the accident does not  determine probable cause or assign culpability, but did note a number of design decisions that could be  characterised as questionable, such as the fact that the vehicle operator monitors the self-driving interface  via a screen in the car, but is also expected to apply emergency braking if necessary, and will not be alerted 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 50  by the system  if emergency braking is needed  [119] ). AVs also introduce the issue of cybersecurity, with the  2015 hacking of a Jeep Cherokee demonstrating the vulnerabil ity of digitally connected cars  [172] .  6.1.3 Ethical principles and a utomated vehicles   AVs, as machines which have to make decisions (in accordance with programming determined by humans)  are also subject to complex and difficult ethical considerations. Some key ethical questions surrounding AVs  include:    Should the car be pr ogrammed to take an egalitarian approach (maximising benefit for the highest  number of people) or a negative approach (maximising benefit for the occupant only, and increasing risk  for everyone else)?  [173]      Should car owners have a say in setting the car’s ‘moral code’?  [173]      In situations where harm to humans is unavoidable, would it be acceptable for AVs to perform some k ind  of prioritisation – e.g. based on age?    How should AVs distribute the risk of driving – for instance, would it be acceptable to program a car that  valued occupants over pedestrians, or vice versa?    In instances such as the fatal AV crash of March 2018,  who is responsible for the harm caused – the  operator or the car manufacturer?   It is relatively straightforward to program AVs in accordance with certain rules (e.g. do not cross a lane  boundary; do not collide with pedestrians, etc. – although, as the M arch 2018 crash shows, the technology  is still far from perfect in following these rules). ‘Dilemma situations’ represent cases where not all rules  can be followed, and some kind of decision has to be made in accordance with ethical principles. Usually, a  ‘hierarchy of constraints’ is needed to determine action. This has prompted debate over how an  autonomous vehicle should weigh outcomes that will cost human lives in various situations where an  accident is inevitable.   Utilitarianism – maximising benefits a nd reducing harm for the greatest number of people, without  distinction between them – is a strong principle underlying considerations of the ethics of AVs. Research by  MIT has found that most people favou r a utilitarian approach to AVs  [174] . However, while participants  approved of utilitarian AVs in theory and would like others to buy them, they  themselves would prefer to  ride in AVs that  protect occupants at all costs  [174] . Given that  car manufacturers will therefore be  incentivised to produce cars programmed to prioritise occupant safety, any realisation of utilitarian ethics  in AVs will likely be brough t about only through regulation  [175] .  Utilitarian principles are also complex to implement, and give rise to ethical conundrums of their own. For  instance, following the principle of harm reduction, should an AV be programmed to hit a motorcyclist with  a helmet instead of one without a helmet,  since the chance of survival is greater?  [176]   Alternatively, it  could be argued that AVs should, where possible, ‘choose’ to hit cars with greater crashworthiness – a  development which would disincenti vise the purchase of safer cars  [177] . A  ‘consequentialist approach’  that uses a single cost function (e.g. human harm) and encodes ethics purely  around the principle of  reducing that cost is  therefore not broadly feasible  [177].  Utilitarianism is not the only consideration in the ethics of AVs. A recent study surveyed millions of people  across hundreds of countries to gauge moral preferences in AVs and what priorities they should have in the  event of an unavoidable acciden t [30]. The researchers used an online survey to get over 39 million  responses to hypothetical ethical dilemmas for AVs. The strongest preferenc es were for sparing human  lives over animal lives, sparing more lives, and sparing young lives. The results indicated a popular  preference for sparing the lives of children over adults. Notably, not all parts of the world saw eye -to-eye  on how AVs should m ake such life -and-death decisions. In Eastern cultures, young lives and fit people were 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 51  not given the same preference for protection as in Western cultures, while pede strians were given extra  weight  [30]. Southern cultures expressed a stronger preference for protecting women  [30].  6.1.4 German y’s Ethics Commission report   In 2017, Germany became the first country in the world to attempt to answer and codify some of these  ethical questions in a set of formal ethical guidelines for AVs, drawn up by a government -appointed  committee comprised of leg al, technical, and ethical experts. The full report contains 20 propositions. Key  among these are  [1]:   Automated and connected driving is an ethical im perative if the systems cause fewer accidents than  human drivers (that is, a positive balance of risk).    In hazardous situations, the protection of human life must always have top priority. The system must be  programmed to accept damage to animals or proper ty in a conflict if this means that personal injury can  be prevented.    In the event of unavoidable accident situations, any distinction between individuals based on personal  features (age, gender, physical or mental constitution) is impermissible.    In every  driving situation, it must be clearly regulated and apparent who is responsible for the driving  task: the human or the computer.    Drivers must always be able to decide themselves whether their vehicle data are to be forwarded and  used (data sovereignty).    Genuine dilemma situations (e.g. the decision between human lives) depend on the actual specific  situation and cannot be standardised or programmed. It would be desirable for an independent public  sector agency to systematically process the lessons learned  from these situations.    In the case of automated and connected driving systems, the accountability that was previously the sole  preserve of the individual shifts from the motorist to the manufacturers and operators of the  technological systems and to the b odies responsible for taking infrastructure, policy and legal decisions.   One priority to keep in mind is the need for a uniform set of regulations for autonomous vehicles operating  on Australian roads, which takes into account that Australian vehicles may be operating under different  road rules than in the location manufactured —this has ramifications for which side of the road the vehicle  drives on, or the presence of roundabouts. There is also the global context to consider —most road rules  have a global co ntext. This necessitates international collaboration.   The Australian Government has been an active participant in work in  the UN World Forum for the  Harmon ization of Vehicle Regulations. These safety regulations are then incorporated into national law  across many countries. In Australia they are known as Australian Design Rules under the Motor Vehicle  Standards Act 1989.     In addition, another relevant UN working group is the Global Forum for Ro ad Traffic Safety . The outcomes  from this working group affect the rules made and implemented by Australian state governments. The  Australian government is becoming more involved with this group as rules are being considered relating to  autonomous vehicles .  Australia’s vehicle safety regulations are already based on international standards, so in the longer term  context of autonomous vehicles it is likely Australia will take up the appropriate standardised international  safety frameworks. There would be som e localised exceptions, such as local legislation on child seatbelts or  rules relating to the supply of vehicles for the appropriate side of the road.   In the interim period, regulatory processes are being developed within Australia  [178] . 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 52    The NTC is currently in the process of considering automated vehicle liability issues   [179] . It is also  considering as regulati on around safety assurance systems for AVs, with four potential reform options  ranging from the baseline option of using existing regulation to manage safety, right through to the  introduction of a regulatory system that is nationally managed from point of  supply and while in service.  This would impose a primary safety duty on the manufacturer of the automated driving system and require  them to certify that their AVs adhere to safety criteria  [180] . Interestingly, the report does not address  ethical considerations in regards to AVs, stating that “safety dilemmas with ethical implications are already  largely captured by the safety criteria”.  The safety criteria state that AVs must be able to:    “detect and appropriately respond to a variety of foreseeable and unusual conditions affecting its safe  operation, and to interact in a predictable and safe way with other road users (road users include other  automated and non -automated vehicle s and vulnerable road users)    take steps towards achieving a minimal risk condition when it cannot operate safely    prioritise safety over strict compliance with road traffic laws where necessary”  [180]    Uniformity is also necessary because regardless of the specific priorities that are chosen in certain “life -ordeath” scenarios, vehicles that are all using similar operating principles can more easily determine the  safest way to re spond in a given scenario. If different manufacturers are all creating autonomous vehicles  that operate to different specifications, it is likely that safety would suffer.   6.2 Personal identification and surveillance   The ability of AI -enabled face, voice and e ven gait   [181]  recognition  systems provide immense potential to  track and identify individuals.   In some cases, these technologies are already operating in Australia without significant problems or  widespread public objection. Facial recognition technologies are used in some Austra lian airports to aid  check in, security and immigration processes to speed up processing and reduce costs while maintaining  security  [182,183] . However, there are significant privacy implications over the widespread use of facial  recognition technology. There are extensive rules regarding earlier technologies that identify individuals,  such as fingerprints, but in many respects the  law has not caught up to technological capabilities such as  facial recognition  and the additional biometric information that is being collected, above and beyond  fingerprints.   Microsoft in particular has been vocal in expressing concern over three key implications of the use of facial  recognition technology. Microsoft President Brad Smith has stated   [85]:  “First, especially in its current state of development, certain uses of facial recognition technology increase  the risk of decisions and, more generally, outcomes that are bi ased and, in some cases, in violation of laws  prohibiting discrimination.   Second, the widespread use of this technology can lead to new intrusions into people’s privacy.   And third, the use of facial recognition technology by a government for mass surveilla nce can encroach on  democratic freedoms.”   These three uses of facial recognition technologies broadly encapsulate the challenges in rolling out this  technology without adequate oversight and accountability mechanisms.   In response to the growing interest in  these technologies there is a significant debate over how they should  be used in Australia . 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 53  6.2.1 Case study: Surveillance technology in crisis situations   When used in service of humanitarian objectives, surveillance technolog ies such as facial and pattern  reco gnition, geo -tracking and mapping can be a life -saving tool . In the event of a crisis there is an  overwhelming amount of data that can be collected and analysed to aid in resolution of the situation, AI is  well placed to manage this process.   A trial operation by police in India to test the use of facial recognition systems was able to scan the faces of  45,000 children in various children’s homes and establish the identities of 2,930 children who  had been  registered as missing  [184] . After bureaucratic difficulties between different agencies and the courts, the  Delhi Police were able to utilise two datasets —60,000 children registe red as missing, and 45,000 children  residing in care institutions. From these two databases they were able to identify almost 3,000 matches   [185] .  Discussions are underway on how to use this system to identify missing children elsewhere in India. A key  ingredient in this outcome was the ability for law enfor cement to access these datasets  [184] .    While AI enabled surveillance may increase personal safety and reduce crime , the need to ensure that  privacy is protected and that such technologies are not used to persecute groups is critical. Author ities  need to give careful consideration to the use of AI in surveillance to ensure an appropriate balance is struck  between protecting the safety of citizens and adopting intrusive surveillance measures that unfairly harm  and persecute innocent people.   6.2.2 Monitoring employee behaviour with AI   Westpac Bank is among companies in Australia that are exploring the use of AI -enabled facial recognition  technologies to monitor the moods of employees. Representatives have indicated that the goal is to “take  the pulse”  of teams across the organisation  [186] .  The use of facial recognition and mood detection AI to monitor employees can be used in ways that are  ethical or unethical, and one way to assess this is to look at the goal of the exercise. Is it to benefit the  welfare of employees, or is it to maximise profit? The Ethics Centre highlights the fact that AI technologies  should keep the principle of “non -instrumentalism” in mind when designing technology  [38]. This  effectively means that humans should not merely become another part of the machine —the machine  should serve people, not the other way aroun d. In addition, the  NHMRC  National Statement on Ethical  Conduct in Human Research states that “Respect for human beings involves giving due scope, throughout  the research process, to the capacity of human beings to make their own decisions”  [187]  . When  researching or utilising technologies that monitor people’s emotions, it is important to ensure that their  autonomy and right to make their own decisions are respected.   If, say, people’s smiles were being logged by a machine and emplo yees were disciplined for not being  happy enough, and the goal was to put on a masquerade of happiness to please customers for profit  reasons, then the machine is treating humans as another component of a profit -generating outcome. On  the other hand, if th e people’s emotional state was assessed in order to deliver timely psychological  assistance at the right time to people facing stress or an emotional breakdown, then the technology is  serving people instead and could be defended on ethical grounds  as long as it respected the autonomy of  individuals and their right to choose not to participate.   6.2.3 Police and AI -enabled surveillance   The ability of facial recognition technologies to identify and track suspects means that increased police  capabilities also need to  come with commensurate oversight mechanisms. This is particularly important 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 54  given that inaccuracies and inequities have been observed in the application of facial recognition  technologies overseas.   In the United Kingdom, privacy advocates used Freedom of Information requests to gain access to the  results of facial recognition programs by police. A report by Big Brother Watch indicated that in the London  Metro area, police use of facial recognition proved 98% inaccurate, and no arrests were made. In South  Wales, the technology was 91% inaccurate, and 15 arrests were made, roughly 0.005% of the number of  people who were scanned with facial recognition technology. 31 innocent members of the public were  asked to prove their identity  [188] . The report pointed out that there is a lack of any real statutor y  guidance for the use of facial recognition technologies and warned of a potential “chilling effect” on  people’s attendance in public spaces if they know they are being observed through surveillance.   Academics commissioned by police to assess the use of f acial recognition systems in South Wales found  that it had helped in the arrests of around 100 suspects, but they stressed that it required police to adapt  their operating methods to achieve results from the technology and this took time. They stated that at first,  only 3% of matches proved accurate, but this improved to 46% over the course of the project  [189] . They  suggest that these technologies are best thought of as “assisted” facial recognition tech nologies and that a  human is still required to confirm matches.   In Australia, government is considering the implications of facial recognition via the Identity Matching  Services Bill 2018, which is still under discussion.   6.2.4 Balancing privacy with security   Groups such as the Human Rights Law Council have raised concerns over the ways in which personal  biometric information may be shared between agencies  [29]. They suggest that agencies should consider a  framework put forward by the US -based Georgetown Law Cen ter on Privacy and Technology which  assesses the risks involved in police use o f facial recognition technology  [29,190] . This framework highlights  five key risk factors to consider:  [190]    1. Targeted versus dragnet searches (is the search just from convicted criminals and suspects, or innocent  people too?)   2. Targeted versus dragnet databases (does the database include as many people as possible, including  innocent people?)   3. Transparent versus invisible searches (do people know tha t their picture has been used in a search?)   6. Real time versus after the fact searches (is this search of past information, or tracking in real time?)   7. Established use versus novel use (how different is the application of this facial recognition compared to  previous applications like fingerprinting?)   Australian authorities need to give careful consideration to the use of AI in surveillance and security, to  ensure an appropriate balance is struck between protecting the safety of citizens and adopting intrusive   surveillance measures. A privacy framework  for law enforcement that incorporates the new capabilities  delivered by facial recognition technologies could incorporate approaches like the Georgetown Framework,  thus  help ing agencies ensure that facial recogni tion technologies are used in an appropriate manner.   6.3 Artificial Intelligence and Employment   In 2013 two University of Oxford academics, Carl Benedikt Frey and Michael Osborne, published a study   [191]  examining the impacts of automation on 702 unique occupation t ypes in the US economy. They found  47% were at risk of being replaced. They also found a strong negative relationship between automation  risks and wages (i.e. lower pay for jobs with a higher chance of being automated). This led to concern 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 55  around the world  about the possibilities of higher rates of unemployment and under -employment. The  University of Oxford study was replicated in multiple jurisdictions. The Committee for the Economic  Development of Australia (CEDA) commissioned a study  [192]  of the Australian economy and found a  similar result with 44% of the workforce at risk to automation  [193] .   Recent research published by the United Kingdom Global Innovation Foundation Nesta  [194]  suggest s that  the original University of Oxford paper  [191] , and many others that have used a similar methodology, have  overstated the job losses from automation. The Nesta report points out the AI will create many new jobs. It  also notes that jobs impacted by AI don’t  just disappear; automation often just requires new skills and ne w  tasks but the job stays in tact. Accountants did not lose their jobs to spreadsheets; rather they learned how  to use them and got better jobs. The coming decade of AI enablement will have a similar impact. A more  recent meta -level study by the OECD  [195]  published in 2018 found that 14% of jobs have a “high risk” of  automation and another 32% will be substantially changed .  Whilst there is  much debate, and many other estimates (higher and lower), the weight of evidence suggests  around half of all jobs will be significantly impacted  (positively or negatively)  by automation and digital  technologies.  A smaller, but still significant, number of  jobs are likely to be fully automated requiring  workers to transition into new jobs and new careers. Retraining, reskilling and strategic career moves can  help people achieve better employment outcomes. A recent study by Google and consulting firm Alpha  Beta  [196]  finds Australian workers will, on average, need to increase time spent learning new skills by 33%  over their lifetime and that job tasks will change 18% per decade.   There is much that can be done by governments, companies and individuals to improve the chances of job  retention and successful career transition in light of automation. One of the main issues is the importance  of acting early; well before job loss occurs.  An ethical approach to widespread AI -based automation of tasks  performed by human workers requires helping the workers transition smoothly and proactively into new  jobs and new careers.   6.4 Gender  Diversity  in AI workforces   Another aspect relating to employm ent ethics is associated with the gender balance within AI -technical  workforces. Australia’s Workplace Gender Equality Agency has indicated that the Professional, Scientific  and Technical Services sector is only 40.9% female, and that full time female work ers receive 23.7% less pay  on average than their male counterparts.  [197]   When this is broken down into Computer System Design  and related services, the proportion of women in the sector falls to 27% of employees in 2018  [197] . There  is a risk that a lack of diversity in AI designers and developers results in a lack of diversity in the AI products  they make. Many companies and research organisations in the technology sector ar e committed to  addressing the gender imbalance.   The Government has recognised that Australia must have a deeper STEM talent pool and this is why it has  supported the development of a Decadal Plan for Women in STEM to provide a roadmap for sustained  increa ses in women’s participation in STEM over the next decade.   The benefits of greater diversity in the ICT workforce will be felt across many dimensions of the Australian  economy, including AI.   6.5 Artificial Intelligence and Indigenous Communities   Discussions a nd protocols that have focused on knowledge sharing and management between Indigenous  people, science and decision -makers provide some valuable insights for AI frameworks and applications   [198]  in this context. This highlights three interrelated issues to consider:  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 56  1. AI based on data collected on and with Indigenous people needs to consider how data is collected and  used so that it complies with Indigenous cultural protocols and human ethics and appropriately  protects Indigenous intellectual property, knowledge and its use. As highlighted in a discussion paper  commissioned by IP Australia and the Department of Industry, Innovation and Science, “Indigenous  Knowledge is held for the benefit of a  community or group as a whole and there can be strict protocols  governing the use of Indigenous Knowledge, directed at gaining community approval”  [199]  .  Guidelines for ethical research in Australian Indigenous studies offer a useful starting point to guide this  effort  [200] .  2. Information is not intelligence and the analytical proces s by which Indigenous knowledge (is  categorised, labelled, shared and incorporated into AI learning and feedbacks should be guided by  cross -cultural collaborative approaches. The way in which indigenous knowledge  is used has a direct  bearing on the way it is collected —some uses of indigenous knowledge  would not be considered  acceptable to the communities they are drawn from, meaning that the uses would need to be clarified  upfront.   3. AI needs to be open and accountable so that Indigenous people and organisati ons are clear about how  AI learning is generated and why this information is used to inform decisions that affect Indigenous  estates and lives.   The principles outlined in this document can provide some guidance on how to properly collect and handle  indige nous knowledge , but are by no means the end point. Consideration of Net Benefits will need to place  a strong emphasis not only on the application of the information, but the impacts on the communities that  provide it. Further research into the relationship  between AI and indigenous knowledge will be crucial in  establishing proper standards and codes of conduct.   6.6 Key points   - Autonomous vehicles require hands -on safety governance and management from authorities, because  systems will need to make choices on how to react under different circumstances and a system without a  cohesive set of rules is likely to deliver worse outcomes that are not optimised for Australian road rules or  conditions.   - AI-enabled surveillance technologies should consider “non -instrumentalis m” as a key principle —does this  technology treat human beings as one more cog in service of a goal, or is the goal to serve the best interests  of human beings?   - In many ways, biometric data is replacing fingerprints as a key tool for identification as biome tric data (which  includes fingerprints) can now use other elements like facial recognition. The ease at which AI -enabled voice,  face and gait recognition systems can identify people poses an enormous risk to privacy.   - AI technologies cannot be considered in isolation, they also need to take into account the context in which  they will be used and the other technologies which will complement them.   - There are various factors that can be used to assess the risks of a facial recognition system. Designers of  thes e systems should consider these factors.   - Workers and society will get better outcomes if we take proactive measures to assist smooth career  transitions.   - There is a gender imbalance in terms of numbers and salaries in AI technical workforces which may need to  be addressed.   - AI applied within indigenous communities needs to take into account cultural issues of importance.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 57  7 A Proposed Ethics Framework   As always in life people want a simple answer. It's like that lovely quote, for every complex  problem in life there's always a simple answer and it's always wrong.   Susan Greenfield     AI is a valuable tool to be harnessed, and one that can be used for many different goals. Already,  companies and government agencies are finding that their increasing reliance on AI sy stems and  automated decisions is creating ethical issues requiring resolution. With significant ramifications for the  daily lives, fundamental human rights and economic prosperity of all Australians, a considered and timely  response is required.   The eight core principles referred to throughout this report are used as ethical framework to guide  organisations in the use or development of AI systems. These principles should be seen as goals that define  whether an AI system is operating ethically. In each chapt er of this report we highlighted specific principles  that are associated with the case studies and dis cussions contained within them.  It is important to note  that all the principles should be considered throughout the design and use of an AI system not jus t those  discussed in detail in each chapter.   1. Generates net -benefits. The AI system must generate benefits for people that are greater than the  costs.   2. Do no harm.  Civilian AI systems must not be designed to harm or deceive people and should be  implemented in ways that minimise any negative outcomes.   3. Regulatory and legal compliance.  The AI system must comply with all relevant international, Australian  Local, State/Territory and Federal government obligations, regulations and laws.   4. Privacy protection. Any system, including AI systems, must ensure people’s private data is protected  and kept confidential plus prevent data breaches which could cause reputational, psychological ,  financial, professional or o ther types of harm to a person.    5. Fairness. The deve lopment or use of the AI system must not result in unfair discrimination against  individuals, communities or groups. This requires particular attention to ensure the “training data” is  free from bias or characteristics which may cause the algorithm to beha ve unfairly.   6. Transparency  and explainability . People must be informed when an algorithm is being used that  impacts them and they should be provided with information about what information the algorithm  uses to make decisions .   7. Contestability.  When an algo rithm significantly impacts a person there must be an efficient process to  allow that person to challenge the use or output of the algorithm.   8. Accountability.  People and organisations responsible for the creation and implementation of AI  algorithms should be identifiable and accountable for the impacts of that algorithm.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 58  7.1 Putting principles into practice   The principles provide goals to work towards, but goals alone are not enough. The remainder of this section  will explore the ways in which individuals, team s and organisations can reach these goals. To support the  practical application of the core ethical principles, an AI toolkit has been referenced throughout the report  as potential instruments  of action.   The toolkit does not address all potential solution s regarding the governance of AI in Australia and is  intended to provide a platform upon which to build knowledge and expertise around the ethical use and  development of AI in Australia.  It is unlikely that there will be a one size fits all approach to add ress the  ethical issues  associat ed with AI  [201] . In addition the approaches taken to address these issues are unlikely  to remain static over time.   This chapter provides g uidance for individuals or teams responsible for any aspect of the design,  development and deployment of any AI -based system that interfaces with humans.  It can help AI  practitioners address three important questions:   - What is the purpose of the AI system?   - What are the  relevant principles to guide the ethical use and application of the AI system?   - How do you assess the requirements of meeting these ethical principles?   What are the tools and processes that can be employed to ensure the AI system is designed, implemented  and deployed in an ethical manner?  Additionally, there is a sample risk framework which can guide AI  governance teams in assessing the levels of risk in an AI system.   We would invite stakeholders as part of the public consultation to share their thoughts and expertise on  how ethical AI can be practically implemented.    7.1.1 Impact assessments   These are auditable assessments of the potential direct and indirect impacts of AI, which address the  potential negative impacts on individuals, communities and groups, alo ng with mitigation procedures.   Algorithmic impact assessments (AIA) are designed to assess the potential impact that an AI system will  have on the public. They are often used to assess automated decis ion systems used by governments   [26,202] . The AI Now Institute have developed an AIA and are urging the recently appointed New York City  (NYC) task -force to consider using their framework to ensure that all automated decision systems used by  the NYC government are made according to principles of equi ty, fair ness and accountability  [2,26] .  The four key goals of the AI Now Institute’s AIA are:    “Respect the public’s right to know which systems impact their lives by publicly listing and describing  automated decision systems that significantly affect individuals and communities    Increase public agencies’ internal expertise and capacity to evaluate the systems they build or procure, so  they can anticipate issues that might raise concerns, such as disparate impacts or due process violations    Ensure greater accountability of automated decision systems by providing a meaningful and ongoing  opportunity for external researchers to review, audit, an d assess these systems using methods that allow  them to identify and detect problems    Ensure that the public has a meaningful opportunity to respond to and, if necessary, dispute the use of a  given system or an agency’s approach to algorithmic accountabilit y.” 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 59  In addition to assessing algorithms , impact assessments can be designed to address other important ethical  issues associated with AI. The Office of the Australian Information Commissioner (OAIC) has developed a  privacy impact assessment to identify the  impact that a project could have on individual privacy  [203] .  There is also an affiliated eLearning guide, to help provide guidance to organisations about ‘privacy by   design’ approaches to data use  [204] .  The adoption and use of standard , auditable, impact assessments by organisations developing or using AI in  Australia would help encourage accountability and ensure that ethical principles are considered and  addressed before AI was implemented.   7.1.2 Review processes   Specialised professionals o r groups can review AI and/or use of AI systems to ensure that they adhere to  ethical principles and Australian policies and legislation.   In many cases, Australia is likely to be importing “off the shelf” AI developed internationally under different  regula tory frameworks.  In these cases adequate review process will be key to ensuring that the technology  meets Australian standards and adheres to ethical principles.   Alternatively, in some cases it may be permissible to use AI programs to review other AI syst ems. Several  companies have developed tools to that are able to effectively assess algorithms used by AIs and report on  how the system is operating and whether i t is acting fairly or with bias  [27]. IBM has released an open  source, cloud based software that creates an easy to use visual representation that shows how the  algo rithms are generating decisi ons [205] . In addition, it can assess the algorithm’s accuracy, fairness and  performance.  Microsoft and Google a re working on similar tool s to assess algorithms for bias  [27]. The use  of such technologies could  improve our ability to efficiently, effectively and objectively review the  components of AI to ensure that they ad here to key ethical principles.  However, if utilised, these are AI  enabled technologies would require a significant degree of scrutiny to ens ure that they did not have the  same flaws that they were purporting to assess.   7.1.3 Risk assessments   The assessment of AI is largely an exercise in accounting for and addressing risks pos ed by the use of the  technology  [201] . As such, consideration should be given to whether certain uses of AI require additional  assessment, these may be considere d to be threshold assessments. FATML have developed a Social Impact  Statement that details requirements of developers of AI to consider who will be impacted by the algorithm  and who  is responsible for that impact  [206] . Similar assessments may be well placed to identify high risk  applications and uses of AI th at require additional monitoring or review.   There are additional potential risks when AI is used in vulnerab le populations and minorities. In these cases  we should consider whether additional scrutiny is required to ensure it is fair. For example, when  conducting research involving human participants additional considerations must be made when dealing  with v ulnerable groups and minorities  [207] .  One argument against this concept of risk based levels of assessment is that the standard level of  assessment should ensure that AI across all spectrums is acting and used according to the key ethical  principles. Perhaps we should expect that a  standard prescribed course of action should be rigorous  enough to ensure that low to high risk AI adheres to core ethical principles.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 60  7.1.4 Best practice guidelines   This involves the development of accessible cross industry best practice principles to help guid e developers  and AI users on gold standard practices.   Best practice guidelines encompass the best available evidence and information to inform practice. For  example, The Office of Australia’s Fair Work Ombudsman has published various best practice guides f or  employers and employees to help identify and implement best practice in itiatives into their workplaces   [208] . Similar  guidelines could be developed to provide best practice initiatives that support the ethical use  and development of AI. The use of these adaptable and flexible best practice guides rather than rigid  policies fit well with the dynamic nature of AI and the d ifficulty in predic ting what is coming next  [209] . It  would be straightforw ard to adjust best practice guidelines as situations and scenarios change over time.   The Australian government has already developed a best practice guide to provide strategies and  information about the best practice use of technology to make automa ted de cisions by their agencies   [210] . Similar guidelines could be developed and promoted to support consideration of the core ethical  issues associated with AI use and development.   7.1.5 Education, training and standards   Standards and certification of AI systems is being actively explored both nationally and internationally.    In Australia, t he provision and certification of standards are generally overseen by relevant industry bodies.  Doctors are accountable to medical bo dies and there are extensive regulations on their behaviour, and the  Australian Medical Association ha s a code of ethics for guidance  [211] . Electricians, plumbers and people  involved in air -conditioning repair all require certificati on to demonstrate their ski lls and guarantee public  safety  [212] . States have various requirements regarding certificati on for repairing motor vehicles  [213] .  Engineers Australia provides accreditatio n for programs that train engineers in coordinati on with  international standards  [214] . There are industry bodies such as Data Governance Australia which a re  examining data principles  [215] .  One area where the implementation of standards could have a large positive impact on ethical AI in  Austral ia relates to data scientists. Currently, there is no agreed upon accreditation or standards that  govern data science as a profession. Designers of algorithms which may have significant impacts on public  well-being are operating within a profession with re latively limited guidance or oversight.   Australia’s national standards body, Standards Australia is working with industry stakeholders in developing  an AI Standards roadmap to guide the development of an Australian position on AI standards.   Dr Alan Finkel  (Australia’s Chief Scientist), has also proposed a framework for voluntary certification of  ethical AI by qualified experts  which his office is currently exploring further.    Internationally the International Standards Organisation (ISO) has a technical co mmittee, which Australia is  an observer, developing standards on AI ((ISO) (ISO/IEC JTC 1/SC 42 – Artificial Intelligence). This includes  both technical and ethical standards.   There is significant scope within Australia to provide more guidance for the fo rmulation of standards to  govern designers of AI systems.   7.1.6 Business and academic collaboration in Australia   A key focus of Australia’s National Innovation and Science Agenda is the promotion of collaboration  through, “funding incentives so that more university funding is allocated to research that is done in  partnership with industry; and invest over the long te rm in critical, world -leading research infrastructure to 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 61  ensure our researchers have access to the infrastructure they need “ [216] . One such initiative is the  provision of an Intellectual Property (IP) toolkit to help resolve complex issues that can arise over IP when  industry and academics collaborate  [217] .   The Australian Technology Network is a partnership between several innovative universities committed to  developing collaboration with industry.  In a recent  report they put forward five recommendations to foster  these relationships  [218] :   “Expand in place supporting structures to deepen PhD and university collaboration with indust ry   Ensure initiatives targeting PhD employability have broad scale    Link a portion of PhD scholarships to industry collaboration    Implement a national communication strategy to improve awareness and develop a deeper  understanding in industry of the PhD”   Qual ity research into addressing ethical AI by design and implementation is key to ensuring that Austr alia  stays ahead of the curve. Without methods of accessible transfer of knowledge from theory to practice the  impact is lost. Collaboration is increasingly i mportant between researchers and the tech industry to ensure  that AI is developed and used ethically and should be prioritised.   7.1.7 Monitoring AI   This consists of regular monitoring of AI or automated decision systems for accuracy, fairness and suitability  for the task at hand. This should also involve consideration of whether the original goals of the algorithm  are still relevant.   Promotion of regular assessment of AI systems and how they are being used will be a key tool to ensure  that all of the core ethical  princ iples are being addressed. Although initial assessments before the  deployment of AI are critical they are unlikely to provide the scope needed to assess the ongoing impact of  the AI in the changing world.   Regular monitoring of AI to assess whether it  is still suitable for the task at hand and whether it still adheres  to the core ethical principles could be encouraged in best practice guidelines or as part of ongoing impact  assessments.   7.1.8 Recourse mechanisms   Are there avenues for appeal when an automated  decision or the use of AI negatively affects a member of  the public?   The GDPR and the UK’s Data Protection Act both include the requirement for individuals to be informed  about the use of automated decisions that affect them and provide the opportunity to  contest those  findings   [92]. Recourse mechanisms he lp promote transparency between organisations using automated  decisions and the  users affected by the systems.  They also engender trust between individuals and  organisations and could be used to improve public acceptance of the use of AI. The provision of recourse  mechanisms has become increasingly important in cases of black box algorithms where the process of how  the system came to a decision or judgement cannot be elucidated.   It may be important to consider that there may be additional complexities assoc iated with the provision of  recourse mechanisms. In situations where the AI systems were found to be faulty, demands could be made  for compensation if any damages were incurred as a result of the impact of the AI system on the individual.  This ties into th e principle of suitability of AI systems and the need to ensure that they are appropriate for 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 62  the task at hand and can perform in a manner that does not cause unacceptable levels of harm when  weighed against the benefits of their use.   7.1.9 Consultation   Without public support there is no destination for the momentum which AI is building. Investing in avenues  for public feedback and dialogue on AI will be key to ensuring that the development and use of AI is in line  with what Australians want. This tool is related  to the principle of net benefit and the need for AI systems  to generate benefits greater than the costs. As discussed at the 2018, Global Symposium for Regulators in  regards to public consultation, “Keep an open door and an open mind. When it comes to AI,  no one  understands all of the problems, let alone all of the solutions. Hearing from as many perspectives as  possible will expose policymakers and regulators to issues that may not have been on their radar and  creative solutions they may not have tried ot herwise. And some of these solutions may  not require law or  regulation”  [219]  .  Regular large scale cons ultation with various stakeholders including the general public, academics and  indust ry members is of critical importance whe n developing AI regulations  [209] . A diverse range of inputs  can only be collected from a diverse group of stakeholders. Various organisations developing materials that  provide information about ethics and AI have included a lengthy consultation process and courted input  from diverse and va ried sources   [36,72,73] . Cons ultation is a valuable tool that can help  to better  understand the spectrum of ideas, concerns and  solut ions regarding ethic al AI.
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 63  7.2 Example Risk Assessment Framework for AI Systems   Risk assessments are commonly used to assess risk factors that have the potential to cause harm. The assessment of AI is larg ely an exercise in accounting for and  addressing risks  posed by the use of the AI system.  They are also useful to provide a threshold and trigger s for  additional action and risk mitigation processes. This  preliminary guide is for individuals or teams responsible for any aspect of the design, development and d eployment of any AI -based system that interfaces with  humans.  Its purpose is to guide AI practitioners to address three questions: What is the purpose of the AI system? What are the relev ant principles to guide the  ethical use and application of the AI sys tem? What are the tools and processes that can be employed to ensure the AI system is designed, implemented and  deployed in an ethical manner?    This is just one example framework, which cannot stand in for frameworks tailored for each individual applicatio n of AI.   The first table examines the probability of risk, together with the  consequence . When a risk has both a high probability of occurring and more negative outcomes,  the consequences become more severe.   Likelihood of risk  Consequence   Insignificant risk Minor risk  Moderate risk  Major risk  Critical risk   Rare  Low Low Moderate  High  High   Unlikely  Low Moderate  Moderate  High  Extreme   Possible  Low Moderate  High  High  Extreme   Likely  Moderate  High  High  Extreme  Extreme   Almost certain  Moderate  High  High  Extreme  Extreme     The second table examines the factors that can cause an AI application to contain more risk. The rows near the top carry rela tively little risk, while the rows near  the bottom contain more risk. Different scenarios may contain more or les s risk depending on the individual circumstances, but this guide provides a general  overview of areas likely to contain risks which ought to be considered before implementation.  It is also worth noting that although there is a column for the  number of peop le affected, severe repercussions for just a single person would still be viewed as a major or critical consequence.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 64    Privacy Protection   /Consequence  Fairness   /Consequence  Physical harm   /Consequence  Contestability   /Consequence  Accountability   /Consequence  Regulatory and legal  Compliance   /Likelihood  Transparency and  explainability   /Likelihood  Number of  people affected   Insignificant  – no  private or sensitive  data is used by the AI  Insignificant  – has  no effect on a  person’s human  rights  Insignificant  –  application cannot  control or influence  other systems  Insignificant – the  application operates on  an opt -in basis and no  intervention is required  to reverse outcome in  the event someone  decides to opt out  Insignificant –  clear   accountability of  outcomes  Insignificant  – consent  gained for use of data.  Application operates in  familiar legal territory.  Insignificant  – inputs,  algorithm and output  are bounded and well  understood  Insignificant  –  application will not  affect individuals   Minor  – uses small  number of people’s  private data  Minor  – clear opt in and opt -out of  application  Minor  – application  controls equipment  incapable of causing  significant harm, public  nuisance or surveillance  Minor  – application  automatically opts  people in, bu t there is  clear notification and it  is easy to opt out. It is  easy to obtain human  assistance to do so.  Minor  – there is  legal precedent  for similar  outcomes and a  clear chain of  responsibility  Minor  – an identical or  similar application of AI  has legal p recedent  demonstrating  compliance. Consent  has clearly been gained.  Minor  – the application  uses difficult -to-explain  AI like neural nets, but  the inputs are clear and  there are no cases of  totally unexpected,  inexplicable outputs  Minor  –the  application wi ll run  only within an  organisation and  affect a small  number of people   Major  – uses a large  number of people’s  private data  Major  – opt-in  but opt -out of  application is  unclear  Minor  – application  may control heavy  equipment or  hazardous material but  there is very limited  potential to harm  people or cause public  nuisance  Major  – application will  be used widely among  the public but there are  little or no human  resources available for  assistance or appeal  Minor  – there is  reasonably clear  delineation of  accountability  between users  and developers  Minor  – there is little  legal precedent for the  application, but  extensive legal advice  has been sought and  the application has  been reviewed by third  parties  Minor:  the application  has unexpected outputs  but they are  periodically reviewed  until understood.  External review and  collaboration is  fostered  Minor – the  algorithm will affect  a small community  of people who are  well-informed  about its use   Major  – application is  designed in a way  that makes it likely to  gather information on  individuals without  their express consent  Major  – person  has no choice to  opt-out of  application and AI  has an effect on a  single person’s  human rights  Major – application  controls heavy  equipment / hazardous  material and is  expected to operate in  a public space  Major  – there is no  easy way to opt out  Major  – there is  little or no legal  precedent for this  application and  there is no  separation of  accountability  between users  and developers  Major  – there is no  legal precedent for the  application and no third  parties or legal experts  have been consulted.  Consent is unclear.  Major: the application’s  outputs are  inexplicable. Review is  of limited effectiveness  in understanding the m Major: the  application will  affect a large  number of people  around the country   Critical  – uses a major  database of private or  sensitive (e.g. health)  data  Critical  – has an  effect on a large  population’s  human rights  Critical  – application  can control e quipment  that could cause loss of  life, or equipment is  designed to secretly  gather personal  information  Critical – outcomes of  the application not opt out and person affected  has no recourse to  change the outcome  Critical – unclear  legal  accountability fo r  outcomes  Critical  – no consent  gained for use of large  quantities of private  data. There is no clear  legal precedent.  Critical  – inputs are  uncontrolled, the  algorithm is not well understood and the  outputs are not  understood  Critical  – the  application affects a  national or global  audience of people  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 65    There are a variety of actions that can be taken to mitigate risk. These are just some of many that have been explored in thi s report.  These measures are by no  means comprehensive —consultation with external  organisations will yield additional solutions in many cases . Additional measures may also emerge wit h new  research, technologies and practices.   Risk Actions   Low  Internal Monitoring    Testing    Review industry standards    Moderate   Internal Monitoring    Consider  how to lower risk    Risk mitigation plan    Internal review    Testing     Impact assessments    External review   High   Internal/External Monitoring    Consider how to lower risk    Risk mitigation plan    Impact assessments    Internal and external review    Testing    Consultation with  specialists    Detailed appeals/opt out plan    Additional human resources to handle inquiries/appeals    Legal advice sought    Liaise with industry partners, government bodies on best practice   Extreme   Unacceptable risk     We invite stakeholders as part of the public consultation to share their thoughts and expertise on how ethical AI can be practically implemented.
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 66  8 Conclusion   “Humans are allergic to change. They love to say, "We've always done it this way." I try to fight  that. That's why I have a clock on my wall that runs counter -clockwise."   Grace Hopper     This framework  discussion paper  is intended to guide Australia’s first steps in the journey towards  integrating policies and strategies to provide a landscape that suppo rts the positive development and use  of AI.    The principles and tool kit items provide practical, accessible  approaches to harness the best that AI can  offer Australia, while addressing the risks. AI is an opportunity; one that has the potential to provide a  better future with fairer processes and tools to address important environmental and social issues.   Howev er, reaching this future will require input from stakeholders across government, business, academia  and broader society. AI developers cannot be expected to bear the responsibility for achieving these  outcomes all on their own.  Further collaboration will b e of the utmost importance in reaching these goals.    
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 67  9 References   1 Ethics Commission Federal Ministry of Transport and Digital Infrastructure. 2017. Automated and  connected driving. German Government. Germany.   2 New York City Hall. 2018. Mayor de Blasio announces first -in-nation Task Force to examine  automated decision systems used by the city. Mayor's Office.   3 House of Lords Select Committee on Artificial Intelligence. 2018. AI in the UK: Ready, willing and  able? UK Parliament. United Kingdom.   4 European Commission. 2018. Artificial Intelligence for Europe.   5 Canadian Institute for Advanced Research. 2017. Pan -Canadian artificial intelligence strategy.   6 Australian Government. 2018. 2018 -2019 Budget overview. Canberra.   7 National Institute for Transforming India. 2018. National strategy for artificial intelligence.   8 Villani C. 2018. For a Meaningful Artificial Intelligence.   9 Rosemain M and Rose M. France to spend $1.8 billion on AI to compete with U.S., Chi na. Reuters. 29  March 2018.   10 The Japanese Society for Artificial Intelligence. 2017. The Japanese society for artificial intelligence  ethical guidelines. Japan.   11 European Commission. 2018. High -level expert group on artificial intelligence.   12 UK Parl iament. 2018. World first Centre for Data Ethics and Innovation: Government statement.   13 Infocomm Media Development Authority. 2018. Composition of the Advisory Council on the Ethical  Use of Artificial Intelligence and Data. Minister for Communications a nd Information. Singapore.   14 The State Council: The People's Republic of China. 2018. Guidelines to ensure safe self -driving  vehicle tests. The State Council: The People's Republic of China. The People's Republic of China.   15 Senate Community Affairs Comm ittee Secretariat. 2017. Design, scope, cost -benefit analysis,  contracts awarded and implementation associated with the Better Management of the Social  Welfare System initiative. Parliament of Australia.   16 Angwin J, Larson J, Mattu S et al. 2016. Machine  bias risk assessments in criminal sentencing.  ProPublica.   17 Dieterich W, Mendoza C, Brennan T. 2016. COMPAS risk scales: Demonstrating accuracy and  predictive parity. Northpointe:   18 Angwin J, Larson J. 2016. ProPublica responds to company’s critique o f Machine Bias story.  ProPublica.   19 Angwin J, Larson J. 2016. Bias in criminal risk scores is mathematically inevitable, researchers say.  ProPublica.   20 Office of the Australian Information Commissioner. 2018. Publication of Medicare Benefits Schedule  and Pharmaceutical Benefits Schedule data: Commissioner initiated investigation report. Australian  Government.   21 Australian Government. International human rights system. Attorney General's Department.   22 Australian Government. 1988. Privacy Act 1988, No. 119, 1988 as amended. Australian Government.  Canberra.   23 Corbett -Davies S, Pierson E, Feller A et al. 2016. A computer program used for bail and sentencing  decisions was labeled biased against blacks. It’s actually not that clear. The Washington Post.    24 Moses L B, Chan J. 2016. Algorithmic prediction in policing: assumptions, evaluation, and  accountability. Policing and Society, 28(7): 806 -822.   25 Australian Government. 1999. Social Security (Administration) Act 1999: No 191, 1999. Federal  Register o f Legislation.   26 Reisman D, Schultz J, Crawford K et al. 2018. Algorithmic Impact Assessments: A Practical Framework  for Public Agency Accountability.   27 Kleinman Z. IBM launches tool aimed at detecting AI bias.   28 Khadem N. Tax office computer says ye s, Federal Court says no. ABC. 8 October 2018.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 68  29 Human Rights Law Centre. 2018. The dangers of unregulated biometric use: Submission to the  Inquiry into the Identity -matching Services Bill 2018 and the Australian Passports Amendment  (Identity -matching Ser vices) Bill 2018. Human Rights Law Centre. Australia.   30 Awad E, Dsouza S, Kim R et al. 2018. The Moral Machine experiment. Nature, 563(7729): 59 -64.  31 Berg A, Buffie E, Zanna L -F. 2018. Should we fear the robot revolution? (The correct answer is yes).  International Monetary Fund.   32 CSIRO. 2015. Robots to ResQu our rainforests. CSIRO.   33 Ray S. 2018. Data guru living with ALS modernizes industries by typing with his eyes. Microsoft News.   34 International Electrotechnical Commission. Functional Safety a nd IEC 61508.   35 Redrup Y. 2018. Google to make AI accessible to all businesses with Cloud AutoML. Australian  Financial Review.   36 Australian Human Rights Commission. 2018. Human rights and technology issues paper. Sydney.   37 McCarthy J, Minsky M L, Roch ester N et al. 1955. A Proposal for the Dartmouth Summer Research  Project on artificial intelligence.   38 Beard M, Longstaff S. 2018. Ethical by design: principles for good technology. The Ethics Centre.  Sydney, Australia.   39 Kranzberg M. 1986. Technology and History: "Kranzberg's Laws". Technology and Culture, 27(3):  544-560.   40 Dutton T. An Overview of National AI Strategies. Medium  [Internet]. 2018 Available from:  https://medium.com/politics -ai/an -overview -of-national -ai-strategies -2a70ec6edfd    41 British Broadcasting Corporation. 2017. Google DeepMind NHS app test broke UK privacy law.  United Kingdom.   42 Elvery S. 2017. How algorithms make important governmen t decisions — and how that affects you.  ABC. Australia.   43 Australian Government. 2007. Automated Assistance in Administrative Decision -Making.   44 UN. 1966. International Covenant on Civil and Political Rights. United Nations.   45 UN. 1966. International Covenant on Economic, Social and Cultural Rights. United Nations.   46 UN. 1966. International Convention on the Elimination of all Forms of Racial Discrimination. United  Nations.   47 UN. 1979. Convention on the Elimination of All Forms of Discrimination ag ainst Women. United  Nations.   48 UN. 1984. Convention against Torture and other Cruel, Inhuman or Degrading Treatment or  Punishment. United Nations.   49 UN. 1989. Convention on the Rights of the Child. United Nations.   50 United Nations. 2007. Convention on the rights of persons with disabilities. United Nations.   51 United Nations. 1948. The universal declaration of human rights. United Nations.   52 Australian Government. 2011. Human rights (Parliamentary Scrutiny) act 201 1, No 186, 2011.  Federal Register of Legislation.   53 Australian Government. Statements of compatibility. Attorney General's Department. Australia.   54 Attorney -General's Department. Australia’s anti -discrimination law. Australian Government.  Australia.   55 Cossins D. 2018. Discriminating algorithms: 5 times AI showed prejudice. New Scientist.   56 Australian Government - Productivity Commission. 2017. Data availability and use, productivity  commission inquiry report. 82):   57 Department of the Prime Minister and Cabinet. 2018. New Australian Government data sharing and  release legislation: Issues paper for consultation. Australian Government. Canberra.   58 Office of the Victorian Information Commissioner. 2018. Artificial intelligence and privacy. Office of  the Victorian Information Commissioner. Victoria.   59 Australian Government. Privacy Act 1988. Federal Register of Legislation.   60 Australian Government. Australian Privacy Principles. Office of the Australian Information  Commissioner.   61 Australian Governme nt. Credit Reporting. Office of the Australian Information Commissioner.   62 Australian Government. Tax File Numbers. Office of the Australian Information Commissioner.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 69  63 Australian Government. Health Information and Medical Research. Office of the Austr alian  Information Commissioner.   64 European Union. Art. 22 GDPR Automated individual decision -making, including profiling.   65 Wachter S, Mittlestadt B, Floridi L. 2017. Why a Right to Explanation of Automated Decision -Making  Does Not Exist in the General  Data Protection Regulation. International Data Privacy Law, 7(2): 76 99.  66 European Group on Ethics in Science and New Technologies. 2018. Statement on artificial  intelligence, robotics and 'autonomous' systems. European Commission. Brussels.   67 European  Commission, High Level Expert Group on AI. 2018. Draft Ethics guidelines for trustworthy  AI.   68 Villani C. 2018. For a meaningful artificial intelligence: Towards a French and European strategy.  French Government.   69 Belot H, Piper G, Kesper A. 2018. Yo u decide: Would you let a car determine who dies?   70 Whittlestone J , Nyrup R , Alexandrova A  et al. 2019. Ethical and societal  implications of algorithms, data,  and artificial intelligence: a roadmap for  research.   71 IEEE. The IEEE global initiative on e thics of autonomous and intelligent systems. IEEE Standards  Association.   72 The IEEE global initiative on ethics of autonomous and intelligent systems. 2016. Ethically aligned  design: A vision for prioritizing human well -being with autonomous and intellig ent systems.   73 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. 2017. Ethically aligned  design: A vision for prioritizing human well -being with autonomous and intelligent systems.   74 Campolo A, Sanfilippo M, Whittaker M et al.  2017. AI  now  2017  report. AI Now Institute.   75 Whittaker M, Crawford K, Dobbe R et al. 2018. AI Now Report 2018.   76 Stone P, Brooks R, Brynjolfsson E et al. 2016. One hundred year study on artificial intelligence:  Report of the 2015 -2016 study pane l. Stanford University. Stanford, USA.   77 Future of Life Institute. 2017. Asilomar AI principles.   78 The Public Voice. 2018. Universal Guidelines for Artificial Intelligence. Electronic Privacy Information  Center. Brussels, Belgium.   79 Partnership on AI. Partnership on AI web page. Available from:  https://www.partnershiponai.org/partners/ .  80 Pichai S. 2018. AI at Google: Our principles. Google.   81 Specktor B. 2018. Google will e nd its 'evil' partnership with the US Military, but not until 2019. Live  Science.   82 Newcomer E. 2018. What Google's AI principles left out. Bloomberg. U.S.A.   83 Greene T. 2018. Google’s principles for developing AI aren’t good enough. The Next Web.   84 Hern A. 2017. Whatever happened to the DeepMind AI ethics board Google promised?   85 Smith B. 2018. Facial recognition: It’s time for action. Microsoft On the Issues - the Official Microsoft  Blog   86 Microsoft. Our Approach to AI (website).   87 IBM. 2018. Ev eryday Ethics for Artificial Intelligence.   88 Dafoe A. 2018. AI governance: A research agenda. Future of Humanity Institute. Oxford.   89 Bostrom N, Yudkowsky E. 'The ethics of artificial intelligence' In: The Cambridge Handbook of  Artificial Intelligence.  Cambridge University Press; 2014.   90 The AI Initiative. The AI initiative recommendations. The Future Society. Harvard Kennedy School.   91 Nguyen P, Solomon L. 2018. Emerging issues in data collection, use and sharing. Consumer Policy  Research Centre. Aust ralia.   92 European Commission. European Commission GDPR Home Page.  [9 August 2018]. Available from:  https://ec .europa.eu/commission/priorities/justice -and-fundamental -rights/data -protection/2018 reform -eu-data -protection -rules_en .  93 UK Government. 2018. Data Protection Act.   94 Rosenberg M, Confessore N, Cadwalladr C. 2018. How Trump Consultants Exploited the Fac ebook  Data of Millions. The New York Times   95 Vengattil M. 2018. Cambridge Analytica begins insolvency proceedings in the UK. Financial Review.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 70  96 Bhardwaj P. 2018. Eight weeks after the Cambridge Analytica scandal, Facebook's stock price  bounces back to where it was before the controversy. Business Insider Australia. Australia.   97 Government A. 2017. Privacy Amendment (Notifiable Data Breaches) Act 2017.   98 Office of the Australian Information Commissioner. 2018. Notifiable data breaches quarterly  statis tics report, 1 April -30 June 2018. Australian Government.   99 Institute P. 2017. Ponemon Institute’s 2017 Cost of Data Breach Study: Australia.   100 United States Government Accountability Office. 2018. Data protection - actions taken by Equifax  and federa l agencies in response to the 2017 breach. U.S. Government.   101 Newman L H. 2017. Equifax officially has no excuse. Wired.   102 Fleishman G. 2018. Equifax data breach, one year later: Obvious errors and no real changes, new  report says. Fortune.   103 Aust ralian Government. 2015. Australian Government public data policy statement.   104 Department of the Prime Minister and Cabinet. 2018. The Australian Government’s response to the  productivity commission data availability and use inquiry. Australian Governme nt. Canberra.   105 The Treasury. 2018. Consumer data right. Australia.   106 Open Knowledge Network. 2016. Global Open Data Index, Place Overview.   107 Hauge M V, Stevenson M D, Rossmo D K et al. 2016. Tagging Banksy: using geographic profiling to  investigate  a modern art mystery. Journal of Spatial Science, 61(1): 185 -190.   108 Balthazar P, Harri P, Prater A et al. 2018. Protecting your patients’ interests in the era of big data,  artificial intelligence, and predictive analytics. Journal of the American Colleg e of Radiology, 15(3,  Part B): 580 -586.   109 Harvard Business School Digital Initiative. 2018. Tay: Crowdsourcing a PR nightmare. Harvard  Business School. U.S.A.   110 Calmon F, Wei D, Vinzamuri B et al. 2017. Optimized pre -processing for discrimination preve ntion.  Neural Information Processing Systems Conference   111 Rahwan I. 2018. Society -in-the-loop: Programming the algorithmic social contract. Ethics and  Information Technology, 20(1): 5 -14.  112 Gunning D. 2018. Explainable artificial intelligence (XAI). De fense Advanced Research Projects  Agency.   113 Langford C. 2017. Houston schools must face teacher evaluation lawsuit. Courthouse News Service.   114 Parasuraman R, Riley V. 1997. Humans and automation: Use, misuse, disuse, abuse. Human Factors,  39(2): 230 -253.  115 Skitka L J, Mosier K L, Burdick M. 1999. Does automation bias decision -making? International Journal  of Human -Computer Studies, 51(5): 991 -1006.   116 National Transportation Safety Board. Enbridge Incorporated Hazardous Liquid Pipeline Rupture and  Release.   117 Wesley D, Dau L A. 2017. Complacency and Automation Bias in the Enbridge Pipeline Disaster.  Ergonomics in Design, 25(1): 17 -22.  118 Perry M. 2014. iDecide: The legal implications of automated decision -making. Cambridge Public Law  Conference   119 National Transportation Safety Board. 2018. Preliminary report highway 18mh010.   120 Smith B W. 2017. Automated Driving and Product Liability. Michigan State Law Review, 1(   121 Burns K. 2016. Judges, ‘common sense’ and judicial cognition. Griffith Law Re view, 25(3): 319 -351.   122 Danziger S, Levav J, Avnaim -Pesso L. 2011. Extraneous factors in judicial decisions. PNAS, 108(17):  6889 -6892.   123 Corbyn Z. Hungry judges dispense rough justice. Nature  [Internet]. 2011 Available from:  https://www.nature.com/news/2011/110411/full/news.2011.227.html .  124 Weinshall -Margel K, Shapard J. 2011. Overlooked factors in the analysis of parole decisions. PNAS,  108(42):   125 Greenwood A. 2018. Sp eaking remarks: The art of decision -making. Federal Court of Australia. Digital  Law Library.   126 Australian Human Rights Commission. 2014. A quick guide to Australian discrimination laws.   127 World Health Organisation. Skin cancers: Who is most at risk of  getting skin cancer?  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 71  128 Angwin J, Larson J. 2015. The Tiger Mom tax: Asians are nearly twice as likely to get a higher price  from Princeton Review. ProPublica.   129 Stobbs N, Hunter D, Bagaric M. 2017. Can sentencing be enhanced by the use of artificial   intelligence? Criminal Law Journal, 41(5): 261 -227.   130 Australian Federal Police. 2017. Policing for a safer Australia: Strategy for future capability.  Australian Federal Police.   131 Supreme Court of the United States Blog. 2017. Loomis v. Wisconsin: Pe tition for certiorari denied  on June 26, 2017.   132 Carlson A M. 2017. The Need for Transparency in the Age of Predictive Sentencing Algorithms. Iowa  Law Review, 103(1):   133 Lohr S. Facial Recognition Is Accurate, if You’re a White Guy. New York Times.   134 Dastin J. Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. 10  October 2018.   135 ANZ. 2018. Organisations are turning to Artificial Intelligence to improve their recruitment  processes. Here’s how it can benefit candidates . efinancialcareers.   136 Stanley J. 2018. New Orleans Program Offers Lessons In Pitfalls Of Predictive Policing. ACLU.   137 Delaney J. France, China, and the EU all have an AI strategy. Shouldn't the US? Wired.com.   138 Lapowsky I. How the LAPD uses data to predict crime. WIRED. 22 May 2018.   139 Crockford T. App data predicts when, where Brisbane criminals will strike next. Sydney Morning  Herald. 31 October 2018.   140 Shakila Khan Rumi K D, Flora Dilys Salim. 2018. Crime event prediction with dynamic featur es. EPJ  Data Science, 7(43):   141 Chen S. Doctors said the coma patients would never wake. AI said they would - and they did. South  China Morning Post. 8 September 2018.   142 Federal Register of Legislation. 2007. Private Health Insurance Act 2007: Compliat ion No. 29.   143 André Q, Carmon Z, Wertenbroch K et al. 2017. Consumer choice and autonomy in the age of  artificial Intelligence and big data. Customer Needs and Solutions, 5(1 -2): 28 -37.  144 Bloomberg Government. 2018. Transcript of Mark Zuckerberg’s sen ate hearing. The Washington  Post.   145 Hern A. 2018. Facebook ad feature claims to predict user's future behaviour. The Guardian.   146 Kramer A D I, Guillory J E, Hancock J T. 2014. Experimental evidence of massive -scale emotional  contagion through social networks. PNAS, 111(24): 8788.   147 Kim H, Garrido P, Tewari A et al. 2018. Deep video portraits. Siggraph 2018   148 Just V. 2018. AI could make dodgy lip sync dubbing a thing of the past. University of Bath. United  Kingdom.   149 European Commission. 2016. Th e EU internet handbook: Cookies. European Commission.   150 Knight W. 2018. The Defense Department has produced the first tools for catching deepfakes. MIT  Technology Review. USA.   151 Society of Automotive Engineers. 2018. Taxonomy and definitions for terms  related to on -road  motor vehicle automated driving systems: J3016_201806.   152 National Transport Commission. 2018. Automated vehicles in Australia.   153 National Transport Commission. 2017. Guidelines for Trials of Automated Vehicles in Australia.   154 National Transport Commission. 2017. Clarifying control of automated vehicles: Policy paper.   155 National Transport Commission. 2018. Changing driving laws to support automated vehicles: Policy  paper.   156 Johnston M. 2018. Federal govt unveils future tran sport tech office. ITnews.   157 Bloomberg Philanthropies. 2017. Taming the autonomous vehicle: A primer for cities. Aspen Institute  Center for Urban Innovation. USA.   158 Ackerman E. 2017. Toyota's Gill Pratt on self -driving cars and the reality of full aut onomy. IEEE  Spectrum.   159 Mervis J. 2017. Are we going too fast on driverless cars? Science.   160 Marowits R. 2017. Self -driving Ubers could still be many years away, says research head. CTV News.  Canada.   161 Truett R. 2016. Don't worry: Autonomous cars a ren't coming tomorrow (or next year). AutoWeek.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 72  162 Litman T. 2018. Autonomous vehicle implementation predictions implications for transport planning.  Victoria Transport Policy Intitute. Victoria.   163 Stern R E, Cui S, Delle Monache M L et al. 2018. Dissipation of stop -and-go waves via control of  autonomous vehicles: Field experiments. Transportation Research Part C: Emerging Technologies,  89(205 -221.   164 Fagnant D J, Kockelman K. 2015. Preparing a Nation for Autonomous Vehicles: Opportunities,  Barrie rs and Policy Recommendations for Capitalizing on Self -Driven Vehicles. Transportation  Research167 -181.   165 International Transport Forum. 2015. Big data and transport: Understanding and assessing options.  OECD Publishing. Paris, France.   166 Schoettle B, S ivak M. 2015. Potential impact of self -driving vehicles on household vehicle demand  and usage. The University of Michigan Transportation Research Institute. Michigan, USA.   167 Trommer S, Kolarova V, Fraedrich E et al. 2016. Autonomous driving: The impact o f vehicle  automation on mobility behaviour. Institute for Mobility Research. Berlin, Germany.   168 Truong L T, De Gruyter C, Currie G et al. 2017. Estimating the trip generation impacts of autonomous  vehicles on car travel in Victoria, Australia. Transporta tion, 44(6): 1279 -1292.   169 U.S. Department of Transportation. 2015. Critical reasons for crashes investigated in the National  Motor Vehicle Crash Causation Survey. National Highway Traffic Safety Adiministration Center for  Statistics and Analysis. Washing ton DC, U.S.A.   170 Bertoncello M, Wee D. 2015. Ten ways autonomous driving could redefine the automotive world.  McKinsey & Company.   171 Overly S. 2017. Uber suspends testing of self -driving cars after crash. The Sydney Morning Herald.  Australia.   172 Green berg A. 2015. After Jeep hack, Chrysler recalls 1.4M vehicles for bug fix. Wired.   173 Bogle A. 2018. Driverless cars and the 5 ethical questions on risk, safety and trust we still need to  answer. Australian Broadcasting Corporation. Australia.   174 Bonnefo n J-F, Shariff A, Rahwan I. 2016. The social dilemma of autonomous vehicles. Science,  352(6293): 1573.   175 Ackerman E. 2016. People want driverless cars with utilitarian ethics, unless they're a passenger.  IEEE Spectrum.   176 Goodall N J. 'Machine ethics a nd automated vehicles' In: Road Vehicle Automation.  Springer; 2014.   177 Gerdes J C, Thornton S M. 'Implementable ethics for autonomous vehicles' In: Maurer Markus  et al.   Autonomes Fahren: Technische, rechtliche und gesellschaftliche Aspekte.  Berlin, Heid elberg:  Springer Berlin Heidelberg; 2015.   178 Commission N T. Topics: Safety Assurance System for Automated Vehicles.   179 Commission N T. Current projects: Motor accident injury insurance and automated vehicles.   180 National Transport Commission. 2018. S afety assurance for automated driving systems: Decision  regulation impact statement. National Transport Commission. Melbourne, Australia.   181 Reyes O C, Vera -Rodriguez R, Scully P et al. 2018. Analysis of spatio -temporal representations for  robust footstep  recognition with deep residual neural networks. IEEE.   182 QANTAS. 2018. Facial recognition. QANTAS, Travel Advice.   183 Department of Home Affairs. Why are we using face recognition technology in arrivals SmartGate?  Australian Government.   184 Times of I ndia. 2018. Delhi: Facial recognition system helps trace 3,000 missing children. Times of  India. India, Dehli.   185 Dockrill P. Thousands of Vanished Children in India Have Been Identified by a New Technology.  Sciencealert. 1 May 2018.   186 Eyers J. 2017. We stpac Testing AI to monitor staff and customers. Australian Financial Review.   187 National Health and Medical Research Council. 2007 (updated 2018). National Statement on Ethical  Conduct in Human Research.   188 Big Brother Watch. 2018. Face Off: The Lawle ss Growth of Facial Recognition in UK Policing.   189 Davies B, Dawson A, Innes M. 2018. How facial recognition technology aids police.   190 Georgetown Law Center. 2016. The perpetual line -up: Unregulated police facial recognition in  America. Georgetown Law  Center on Privacy and Technology. U.S.A.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 73  191 Frey C B, Osborne M A. 2013. The future of employment: How susceptible are jobs to  computerisation. Technology Oxford Martin Programme on the Impacts of Future: Oxford.   192 Durrant -Whyte H, McCalman L, O'Callag han S et al. 2015. The Impact of Computerisation and  Automation on Future Employment. Committee for Economic Development (CEDA).   193 Nedelkoska L, Quintini G. 2018. Automation, skills use and training. Working Paper Number 202.  Organisation for Economic C ooperation and Development Paris.   194 Nesta. 2017. The Future of Skills: Employment in 2030. Nesta ( https://www.nesta.org.uk ). London.   195 OECD. 2018. Transformative technologies and jobs of the future. Canadian G7 Innovation Ministers  Meeting. The Organisation for Economic Cooperation and Development. Paris.   196 AlphaBeta. 2019. Future Skills - To adapt to the future of work, Australians will undertake a third  more education and training and change what, when and ho w we learn. Prepared by AlphaBeta for  Google Australia. Sydney.   197 Workplace Gender Equality Agency. 2018. Professional, Scientific and Technical Services summary.   198 Robinson C, McKaige B, Barber M et al. 2016. Report on the national Indigenous fire kn owledge and  fire management forum: Building protocols from practical experiences Darwin, Northern Territory  9th–10th February 2016 CSIRO and Northern Australia Environmental Resources Hub. Australia.   199 Janke T. 2018. Legal protection of Indigenous Knowle dge in Australia.   200 Australian Institute of Aboriginal and Torres Strait Islander Studies. 2012. Guidelines for ethical  research in Australian Indigenous studies. Australian Institute of Aboriginal and Torres Strait Islander  Studies. Canberra, Australia .  201 Reed C. 2018. How should we regulate artificial intelligence? Philosophical Transactions of the Royal  Society A: Mathematical, Physical and Engineering Sciences, 376(2128): 1 -12.  202 Government of Canada. Algorithmic impact assessment (v0.2). Governm ent of Canada. Canada.   203 Office of the Australian Information Commissioner. 2014. Guide to undertaking privacy impact  assessments. Australia.   204 Office of the Australian Information Commissioner. Privacy impact assessment eLearning.   205 Varshney K. 201 8. Introducing AI Fairness 360. IBM.   206 Fairness Accountability and Transparency in Machine Learning. Principles for accountable algorithms  and a social impact statement for algorithms. Fair and Transparent Machine Learning.   207 The National Health and Medical Research Council, The Australian Research Council and the  Australian, Vice -Chancellors’ Committee. 2007 (Updated May 2015). National statement on ethical  conduct in human research. National Health and Medical Research Council. Canberra.   208 Fair Wo rk Ombudsman. Best practice guides. Australian Government. Australia.   209 Erdelyi O, Goldsmith J. 2018. Regulating artificial intelligence: A proposal for a global solution.  Conference on Artificial Intelligence, Ethics and Society.   210 Administrative Rev iew Council. 2004. Automated assistance in administrative decision making:  Report no. 46. Administrative Review Council. Canberra.   211 Australian Medical Association. 2016. AMA code of ethics. Australian Medical Association. Australia.   212 Department of Ed ucation and Training. Licensing: Process for gaining a current, identified Australian  occupational licence. Australian Government. Australia.   213 Department of Mines Industry Regulation and Safety. Motor vehicle repairer's certificate.  Government of Wester n Australia. Australia.   214 Engineers Australia. Program accreditation overview. Available from:  https://www.engineersaustralia.org.au/About -Us/Accreditation .  215 Data Governance Australia. Leading Practice Data Principles. Available from:  http://datagovernanceaus.com.au/leading -practice -data -principles/ .  216 Department of Industry Innovation and Scie nce. 2015. National innovation and science agenda.  Australian Government. Australia.   217 IP Australia, Department of Industry Innovation and Science. 2018. The newly updated IP Toolkit is  now live. IP Australia. Australia.   218 Australian Technology Network  of Universities. 2017. Enhancing the value of PhDs to Australian  industry. Australian Technology Network of Universities. Australia.  
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 74  219 Gasser U, Budish R, Ashar A. 2018. Module on setting the stage for AI governance: Interfaces,  infrastructures, and ins titutions for policymakers and regulators. International Telecommunications  Union.       
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 75  Appendix Stakeholder and Expert Consultation   Targeted consultations with 91 invited representatives from universities and institutes, industry and  government were conducted across four Australian capital cities (Melbourne, Brisbane, Perth and Sydney).  The workshops were a key component to developing a cohesive and representative narrative that  accurately captured the perspectives and priorities of various Australian  stakeholders.  In addition to the  four consultation sessions, advisory and technical expert groups were engaged in the development of this  report  (Figure 7).    Figure 7. Pie charts of c onsultation attendee demographics   Note: A total of 91 persons were consulted at the workshops. Additional consultations were held with other industry, research  and government  experts.   Consultation approach   The consultations were run as informal workshops focused on a collaborative and generative approach.  Participants were encouraged to both share their ideas, and develop and collaborate with others.   The workshops began with a presentation introducing the topic of AI as well as the objectives and  structures of the AI Ethics Framework. Following the presentation participants were given opportunity to  question and interrogate the approach of both reports, and where appropriate, that feedback has been  integrated int o the reports.   Following the presentation, in the first discussion session, participants were given the opportunity to group  together and discuss their perspectives on the biggest opportunities for Australia in the use and adoption of  AI and the factors t hat could enable or inhibit that adoption. In the second discussion session participants  were asked to consider their perspectives on risk mitigation and measures needed to ensure wide -spread  societal benefits from the adoption of AI. Each of the workshops  provided robust dialogue and diverse  perspectives across both discussion sections that resulted in an informative snapshot of Australia’s unique  AI opportunities and challenges.   Key themes    Prioritization is key: Participants acknowledged the need to prior itise investment in AI on focussed,  strategic areas to take advantage of Australia’s unique opportunities and address its challenges.  Brisbane 24% Sydney 28%Perth 18%Melbourne 30% Industry 62% Government 16%Universities 22%
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 76  o Whilst the opinions on which domains should be of focus varied, it was reaffirmed that this  investment should be conducted  in conjunction with other major initiatives across the  Australian landscape.   o There were several discussions on the potential for Australia to be a leader in AI integration, in  addition to AI development, particularly across primary the industries.   o Suggestions were also made about Australia’s potential to play a world leading role in ethical  and responsible AI.    Need for skilled workers: All participants recognised a significant knowledge gap in the current  workforce as a whole, and that future -proofi ng skills and curriculum for the next generation of  knowledge workers needed to be addressed.   o The responsibility for ensuring Australia has a strong technically -enabled workforce was seen to  be a shared responsibility across all sectors.    Collaboration and multidisciplinary approaches are required: Across all sectors, in each city, the need  for Australia to develop much stronger collaboration within and between sectors was stated and  reiterated.   o There is strong appetite to connect between the industry and re searchers involved in the  workshops, but a lack of infrastructure to encourage this engagement and remove friction   o The need for initiatives that could help coordinate how sectors could work together on  projects, rather than compete for them.   o The multi -disciplinary nature of AI was discussed along with the need for collaborative  approaches to ensure that Australia can optimise their use and adoption of AI in the most  positive way.    Data governance: Discussion was focussed on the steps that need to be made to ensure that data  privacy regulations are adhered to, without limiting development or adoption of AI.   o In addition the need to address the lack of large datasets in Australia and how this will affect  our ability to compete in AI development on a global scale    Embracing AI: There was a healthy appreciation of strategies to mitigate risks, and demystify artificial  intelligence. These included ensuring that AI adhered to ethical principles such as fairness and  transparency.   o It was raised that over -regulation coul d limit the opportunity for Australia to play a leading role,  and having nimble and responsive frameworks would serve better.   o There was discussion around the need for a cohesive nationwide approach to addressing  ethical issues associated with AI use and ad option.   o In every session the need to use AI for good was suggested and discussed by participants. In  particular the need to use AI to address Australia’s sustainability and environmental issues was  discussed in each session.    

CONTACT US   t  1300 363 400    +61 3 9545 2176   e  csiroenquiries@csiro.au   w  www.data61.csiro.au   WE DO THE EXTRAORDIN ARY EVERY DAY   We innovate for tomorrow and help  improve today – for our customers,   all Australians and the world.   WE IMAGINE   WE COLLABORATE   WE INNOVATE     FOR FURTHER INFORMAT ION  Stefan Hajkowicz   t  +61 7 3833 5540   e  stefan.hajkowicz @data61.csiro .au  w  www.data61.csiro.au            
