Input on the European  Commission White Paper   “On Artificial Intelligence –   A European approach to  excellence and trust” JUNE 15, 2020BY: “WONKS AND TECHIES,”    a multi-disciplinary group at Stanford University, cooperating on international  technology and policy issues, led by Ms. Marietje SchaakePURPOSE:
21 All of the input, ideas, and recommendations included in this paper are the reflection of the contributing authors’ own research and analysis, and are not meant to reflect the views  of any institution or organization.Dear Commissioner Vestager, Margrethe,  I hope you are well despite the many challenges Covid-19 presents to Europeans and people worldwide. It is  my pleasure to shift focus to the EU AI Strategy, and to share with you input proposed by a multi-disciplinary  group of Stanford students and contributors interested in policy and technology.  We share these suggestions while building on a deep appreciation for the importance of European  leadership in implementing a values-based governance model for artificial intelligence. These suggestions  also support the ambition to strengthen the public interest and AI that contributes to people’s quality of  life. Before you is the result of months-long cooperation between students, from freshmen to PhD’s, from  computer scientists to law students, with expertise from Stanford faculty and staff. Although each of us  has our own emphases and priorities, we all believe in this collaborative effort and in the well-researched  proposals in the attached input.  We are available for follow up and happy to assist where suitable going forward.  Sincerely1,  Marietje Schaake, Policy Fellow, Stanford HAI, International Policy Director, Stanford Cyber Policy Center  Ruth Elisabeth Appel Dathan M. Duplichen Lisa Einstein Wren Elhai Muhammad Dhafer Muhammad Faishal Agata Foryciarz Sydney L. Frankenberg Toni Friedman Zoe Huczok Kyra Jasper Danielle Jablanski* *Editor, **Non-Stanford ContributorJennifer King Cindy Kuang Heajune Lee Shreya Mantha Vidyangi Patil** Gailyn Portelance Adriana Stephan Alex Tamkin Alessandro Vecchiato Eva Zhang Jason ZhaoPURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST” 
31. Executive Summary 4 2. On Trust, Transparency, and Accountability  9  2.1 Risk Management for Trust and Accountability  9  2.2 Regulation and Compliance Mechanisms  11  2.3 Considerations for Liability and Harm  12  2.4 New Governance Bodies and Responsibilities 13 3. AI Impacts for Production, Skills, and the Labor Markets 15  3.1 Digitization, Access, and Innovation  15  3.2 Jobs and Sectors Adopting Increased Automation  16  3.3 Skills Required for AI Expansion 18 4. Unknown Risks for Widespread AI Adoption in the EU 19  4.1 Unmarrying Military AI and the Broader Economic Market  19  4.2 Preparing Society for Pitfalls of Automation Bias  20  4.3 Testing and Operating in Multi-Systems Environments  21 5. Conclusion 23PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  Table of Contents
4PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  1. Executive Summary In our response to the White Paper “On Artificial Intelligence  – A European approach to excellence and trust,” we sought  to reflect on overarching themes and gaps illuminated by  the white paper, to bring attention to potential second and  third order effects and guide policymakers toward concrete  steps to take in the months and years ahead. As a multidisciplinary group of academics, military members, technical  and policy wonks, we had a diverse group of expertise  contributing to each of the sections below. We begin with a focus on risk and governance for AI in a broad sense, pivot  to the known unknowns related to jobs and the AI-enabled  economy, and end by outlining a few items categorized as  unknown unknowns for the future of AI in the EU. Below  is a table preceding our analysis, highlighting the specific  recommendations as they appear throughout each section.  We hope these reflections and recommendations help to  bolster the EU’s initiatives on artificial intelligence. High Level Recommendations: Page The determination to classify a system as “high risk” requires a greater distinction between its various  potential harms, along a spectrum ranging from high to low risk, and from material to immaterial harm.  Regardless of the standard for explainability, auditing should play a role in determining degrees of  transparency and ensuring the accountability of systems.9 As the EU continues developing regulations for automated decision-making systems and other applications  of machine learning, checkpoints for citizens’ data and privacy must be built into the process of research,  design, and production. These checkpoints could be part of an audit process, and need to be clearly  defined to ensure appropriate risk management mechanisms are institutionalized to mitigate harm and  enhance trust and accountability.10 Much like rules governing transparency, standards of “adequate and accessible redress” will be necessary  where applications of AI pose high risk to safety. “Ex ante” regulation of a high risk system could require a  system to meet certification requirements prior to its implementation in a given use case. 10 New technologies and the datasets that are used should be certified regularly before they can proceed in  development and deployment. 10ON TRUST , TRANSPARENCY , AND ACCOUNTABILITY
5PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  To assist regulators in determining appropriate accountability mechanisms, an “AI Accountability Act” can  set up compliance mechanisms with various laws that enable EU inspections or investigations of AI. Such a  law would serve to both cement risk management procedures and incentivize compliance with mandated  requirements for establishing safety, trust, transparency, and accountability in deployed AI systems. 11 Portions of these requirements can be addressed simultaneously through the widespread adoption of  detailed “Bias Impact Statements" for vendors and potentially future operators of AI systems.11 In order to establish liability for AI systems which may cause undue harm, the appropriate definition of  “damage” should consider inclusion of reference to dignitary rights, invoking Article 7 of the Charter of  Fundamental Rights of the European Union. The EU should further incorporate a statement on the need to  respect relevant human rights in the context of product liability. 13 The European Commission as well as governments of EU Member States should first conduct stakeholder  engagement exercises and additional public consultations.13 We recommend the creation of an inter-European Parliamentary Committee on AI. While the committee  would deal with implementation activities within and across member states, an AI Oversight Board could be  created to supervise implementation guidelines. 13 We propose the establishment of an Advisory Committee on Public Sector AI Use to guide actions to  responsibly adopt AI in the EU public sector. This advisory committee should also develop a route to advise  and share information with the aforementioned inter-European Parliamentary Committee on AI to aid  development and implementation principles for the public sector use of AI systems. 13 High Level Recommendations: Page One effort that the EU can establish is the development of a policy strategy to foster the digitization of  production more comprehensively, focusing on sectors that at this moment are lagging in data, computing  power, and digitization. 16AI IMPACTS FOR PRODUCTION, SKILLS, AND LABOR
6PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  It is crucial that the EU improves its investment efforts on AI through public and private initiatives. These  initiatives must be focused on inclusive innovation and fostering ideas that improve the lives of the many  and not the few. For this goal, the EU could institute specialized grant competitions that incentivize and  reward inclusive and equitable technological development.16 In order to protect the rights of workers on the job, it is necessary that the European Union develop a system  of regulation of the application of AI technologies for employment and management decisions.17 It is important to remember that there are many jobs that are impossible to automate and that will be in  higher demand in the future, such as jobs that require personal communication, empathy, and creativity.  Within the framework of the EU White Paper, the European Commission should include a specific goal to  bolster the development of such skills and expand education programs that focus on these skills.17 The European Union should also consider investment in AI projects aimed at extending the working life of  the older population and workers with disabilities. 18 The European Union should seek to develop and foster AI technologies that improve the production capacity  not just in terms of efficiency, but also in terms of the quality of the production process and end products,  incorporating new labor not typical for certain sectors once certain physical demands have been automated.18 The expansion of AI job opportunities will, therefore, require increased investment in educational programs  in universities and education centers, and online, especially for retaining midcareer workers. The training  will require newly adapted skills requisite to work in AI, both in the production and operation of AI systems.  This would imply a concerted investment from university consortiums in the European Union European  Union to expand graduate programs in fields related to AI, not only by replicating successful initiatives but  by also expanding current European research grants to attract students who might be otherwise enticed to  study outside the EU.18 To speed up the adoption of AI technologies by subject matter experts, the EU could develop a series of onthe-job training programs that facilitate the acquisition of AI skills for their workers.18
7PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  High Level Recommendations: Page It would benefit the EU to consider military AI applications in its broader plans, to prevent and avoid  fragmentation in the internal market. The EU now has a unique opportunity to steer the burgeoning field  of tech policy compliance, emerging as a parallel to international trade policy, by incorporating global  companies with global stakeholders to advance both market economies and regulation authorities on the  use of AI. 19 The Revised Coordinated Plan for AI should expand on requirement type C – ‘information provision’ – to  include training on automation bias, dual-use and malicious applications of AI.21 The Revised Coordinated Plan for AI should consider expanding on this leadership to develop widespread  and publicly available training on basic nomenclature, development plans, assumptions, and risks for a  general audience. The plan should consider dialogues which in turn bring technologists out of research labs  and into communities. 21 While systems on systems is clearly a technical and meta problem, the Revised Coordinated Plan for  AI should provide a step by step approach to testing systems which will inevitably operate in the same  environment. This area of study would be a boon for the member states’ Digital Innovation Hubs and could  appoint sector-specific research to each and entice talent to procure, train, and test contrasting models in a  sector-specific environment.22 There is a need for tiered testing phases to include testing systems on systems given that mistakes and  biases can result from AI learning in operation. AI systems may also learn from or contrast with other  AI systems. This requirement could be developed into its own conformity assessment cycle prior to  deployment, meeting requirements for addressees in a more complex testing environment and raising the  bar for both security by design and risk prevention.22 It is necessary to revalidate the European Union’s ability to interoperate digitally in order to execute  combined military tests and operations. Asynchronous capabilities currently exist among member states;  therefore, a standard for the use of AI in military operations must be established to ensure that new  developments do not inhibit interoperability.22UNKNOWN RISKS FOR WIDESPREAD AI ADOPTION IN THE EU
8PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  In the case of AI development for military weapons systems, a clear delineation should be drawn between  offensive and defensive systems. The EU should conduct a study to determine the areas where increased  automation would benefit or weaken infrastructure.22 Where offensive cyber-enabled capabilities can be enabled through AI, human in the loop decision  responsibility must be enshrined. In order to maintain control over military AI tools it is recommended that  the EU nest this capability and outline systems intent with an international body, such as the Tallinn Manual.  It is further recommended that AI-enabled capabilities remain egalitarian and distributed within the EU. 22 We want to see the EU promote an equitable distribution of AI research, development and deployment. We  encourage initiatives to increase public awareness, training, and literacy in response to advancements in  AI, and suggest the creation of new occupations in the data-driven future. These recommendations can be  coordinated and operationalized throughout the EU, made up of distinguished interdisciplinary experts, to  tackle the implementation of dynamic policies as they relate to the development and trade of AI hardware  and software, cooperation, and the capacity for change. We submit these recommendations for your  consideration, and look forward to the European Commission’s comments.23CONCLUSION
9PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  2. On Trust, Transparency, and  Accountability of AI Systems  The white paper’s human-rights based approach to  regulating Artificial Intelligence is pioneering. The ethical  guidelines mentioned, produced by the High-Level  Expert Group on AI, emphasize that transparency and  accountability of AI systems must adhere to the notion  of “trustworthy AI” . Ensuring trustworthiness for AI will  require decisions made by such systems be explainable  “in a manner adapted to the stakeholder concerned,” and  that mechanisms be put in place to ensure “adequate and  accessible redress.”2  2.1 RISK MANAGEMENT FOR  TRUST AND ACCOUNTABILITY  Before considering intended outcomes, there should be a  systematic assessment of each data set utilized in AI systems  to help mitigate against potential risks of AI developments.  Some data sets required for training or testing systems  could be considered too sensitive to deploy in comparison  to the benefits promised by systems. For example, data  used in an algorithm to determine individual and aggregate  shopping preferences might be determined to be “low risk”  because the potential negative impacts or harms from this  information can be mitigated – i.e. personal identifiable  information (PII) and privacy can be removed/anonymized.  Conversely, sensitive data related to weapons systems for  national defense, and the potential for manipulation or accidents in deployment must be assessed with greater  scrutiny. Thus, risk must be assessed in the contexts of each  use case, taking into account the consequences of new  datasets, other AI systems, and operational environments.   The determination to classify a system as “high risk” requires  a greater distinction between its various potential harms,  along a spectrum ranging from high to low risk, and from  material to immaterial harm.3 This tiered approach would  allow for classification of risks associated with violations  of, for example, the right to life, to be treated separately  from those associated with violations of the right to privacy,  violations to freedom of expression, human dignity and  nondiscrimination.  Moving beyond data assessments, transparency in AI  decision-making involves a full account of a system’s inputs,  outputs, and the factors that led to its decision(s).4 The EU’s  conceptualization of trustworthy AI emphasizes the notion  that systems be “explainable” to some degree. This may  involve the ability to understand which data inputs have the  most impact on an outcome, or whether a specific factor  had an outsized effect on an outcome.5 An interdisciplinary  team of scholars at Harvard University recommends that AI  systems be explainable a “proportion of the time.”6 In lieu  of  an explanation for a system’s specific decision or output,  a better understanding of the underlying technology itself  should be a bare minimum requirement.7  2 “Ethics Guidelines for Trustworthy AI (High-Level Expert Group on AI, April 2019), https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai. 3 “White Paper on Artificial Intelligence: A European approach to excellent and trust,” (European Commission, February 2020), 10,   https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf , 4 Doshi-Velez, Finale, and Mason Kortz, “Accountability of AI Under the Law: The Role of Explanation” (Berkman Klein Center Working Group on Explanation and the Law, 2017), 4. 5 Doshi-Velez and Kortz, “Accountability of AI Under the Law,” 7. 6 Doshi-Velez and Kortz, “Accountability of AI Under the Law.” 7 Engstrom, David, Ho, Daniel E., Sharkey, Catherine M., and Cuéllar, Mariano-Florentino, “Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies,” (Report  Submitted to the Administrative Conference of the United States, February 2020), 75.
10PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  While the EU’s General Data Protection Regulation (GDPR)  currently provides a framework for the “right to explanation,”  it is limited mainly to privacy concerns. The Article 29  Data Protection Working Party Guidelines on Automated  Individual-Decision-Making and Profiling, however, could  serve as a model for expanding the right to explanation more  broadly, and particularly when it comes to AI. Regardless  of the standard for explainability, auditing should play a  role in determining degrees of transparency and ensuring  the accountability of systems. Audits are currently the  most effective means of detecting discrimination or other  harms in AI systems.8 Audit trail requirements developed  in consultation with academia and industry are desirable,  particularly for safety-critical applications of AI.9  Under  this framework, national governments would take on the  task of coordinating the regular audit of applications of AI  themselves and would certify third-party auditors to carry  out assessments. Determined high-risk applications would  require government certification while low-risk applications  and technologies would simply be required to publish a  verified third-party audit.  In addition, the white paper aptly recognizes the potential  risks of AI as it pertains to fundamental freedoms as well  as personal and collective privacy. The EU should consider  adaptations to the way it categorizes citizen data, and invests  in data privacy regulation for AI systems. As the data produced  by citizens becomes increasingly valuable, companies hold  a monopoly on the value they can derive from it. States may  begin to classify data as a national resource, and perhaps  institute further mechanisms such as data trusts or a “data  tax” to be paid to the EU to enable equitable distribution  between states. The funds collected from this tax could  be used to ameliorate the harmful effects of an economic  transition spurred by AI, to conduct research, and to support  and retrain workforces. As the EU continues developing regulations for automated decision-making systems and  other applications of machine learning, checkpoints for  citizens’ data and privacy must be built into the process of  research, design, and production. These checkpoints could  be part of an audit process, and need to be clearly defined  to ensure appropriate risk management mechanisms are  institutionalized to mitigate harm and enhance trust and  accountability. Much like rules governing transparency, standards of  “adequate and accessible redress” will be necessary where  applications of AI pose high risk to safety. “Ex ante” regulation  of a high risk system could require a system to meet  certification requirements prior to its implementation in a  given use case. If the system is assessed as posing a high risk  for safety, regulators may establish requirements for increased  explanation of the system, testing phases, and/or education  for end users, to bolster explainability and reduce safety risks.  Alternatively, an assessment of high risk for discrimination  may require a comparison with the human equivalent to the  system, whereby AI outcomes are measured against human  decisions and results.10 There should be an independent third-party institution  responsible for making risk assessments for safety, trust,  transparency and accountability. New technologies and the  datasets that are used should be certified regularly before  they can proceed in development and deployment. This  risk classification system would not relieve companies of  prospective consequences related to potential harms, but can  be developed to mitigate the most extreme cases of harm. It  can also serve to promote and proliferate the idea of privacy  by design in engineering.  Ensuring comprehensive accountability for AI depends on  understanding which elements of a system can be clearly and  8 Casey, Bryan, Farhangi, Ashkan, and Vogl, Roland, Rethinking Explainable Machines: GDPR’s ‘Right to Explanation’ Debate and the Rise of Algorithmic Audits in Enterprise (Berkeley  Technology Law Journal 34, no. 145), 183. 9 Brundage, Miles, et al., “Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,” (Cornell University, April 2020), 3. 10 Engstrom, “Government by Algorithm,” 77.
11PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  credibly demonstrated, through what mechanisms, and with  what tradeoffs.11 A broad range of accountability tools are  available to regulators today, ranging from Algorithmic Impact  Assessments to “Bias and Safety Bounties” that incentivize  sharing of unexpected and/or unintended behavior by AI  systems.12 To assist regulators in determining appropriate  accountability mechanisms, an “AI Accountability Act” can set  up compliance mechanisms with various laws that enable EU  inspections or investigations of AI.13 Such a law would serve  to both cement risk management procedures and incentivize  compliance with mandated requirements for establishing  safety, trust, transparency, and accountability in deployed AI  systems.  2.2 REGULATION AND   COMPLIANCE MECHANISMS  Three levels of analysis requirements outlined in the white  paper provide a basis for addressing algorithmic bias and  discrimination: training data, keeping of records and data,  and information provision. Portions of these requirements  can be addressed simultaneously through the widespread  adoption of detailed “Bias Impact Statements” for vendors  and potentially future operators of AI systems. Features of Bias Impact Statements:  •   Detailed description of the AI application’s designed  use cases and use domain  •    Considerations of diversity and equity in the  dataset(s), especially for requiring mandatory  reporting on data distributions of protected  characteristics such as ethnicity, age, and gender    º   A standard list of protected characteristics for  specific types of use cases can be established for AI systems, based on GDPR and other  international human rights frameworks  •   Results of the AI model from mandated technical  metrics used to define and quantitatively measure  fairness    º   We recommend establishing a requirement to  include multiple metrics to represent as many  aspects of equity as possible for determining  a fairness measure. Potential metrics could  include anti-classification, classification  parity, calibration, equality of opportunity,  and disparate impact  •   Establishing pathways for users to flag and challenge  discrimination issues  •    Evidence of a notice and commitment to solicit public  third-party review    º   We recommend the formation of a group  of engineers across Member States that  facilitates expert review of such notice and  commitment releases Bias Impact Statements, pioneered by the Brookings  Institution and AI Now14, include detailed information  regarding the datasets used, the intended use case and  domains, the process of training, and sample results as  evaluated on specific fairness metrics for developing AI  systems. As the white paper notes, the High-Level Expert  Group has published a set of guidelines and a corresponding  “assessment list for practical use by companies.” However,  this assessment list is intended to prompt general reflection,  and compliance is non-binding. Mechanisms for compliance,  such as Bias Impact Statements, should be mandated,  especially for designers and vendors whose systems are  classified as high risk.  11 Brundage et al., “Towards Trustworthy AI Development,” 4.  12 “Brundage et al., “Towards Trustworthy AI Development,” 19. 13 European Parliament, “Tools for Ensuring Implementation and Application of EU Law and Evaluation of their Effectiveness,” (Directorate-General for Internal Policies, 2013). 14 Nicol Turner Lee, Paul Resnick, and Genie Barton, “Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms,” Brookings Institution, May 22,  2019, https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/.
12PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  autonomous cars or weapons,16 this lens is not intuitive or  suitable to most of the legal community. A more viable option  would consist of applying strict liability, imposed on the  producer even when no fault is discovered on their part before  the harm is caused. Both would require a reporting mechanism  that would, if nothing else, allow for valuable lessons to be  drawn from the processes leading to the harms.  The strict liability model is employed in the EU Product Liability  Directive with respect to products (defined as “movables” in  the text.)17 Extending it to AI-powered software and systems  would solve part of the liability conundrum. Because of the  “many hands” problem distinctive of AI – i.e., the multiplicity  of people and parts involved in creating each system, and  the interdependence of systems on other systems – ascribing  liability to a single producer will be difficult. As a result, the  EP’s Committee on Legal Affairs18 has proposed a collective  responsibility framework in which producers of AI systems  pool their resources to compensate plaintiffs. This would need  to include third party suppliers of subcomponents. In such  cases, producers bear collective liability, without being morally  responsible for harming consumers. However, collective liability  may be counterproductive for product security. Producers  could see the obligation to pay out liability claims as an  excuse to forego implementing features of security by design  in production. To mitigate this, producers who demonstrate  a “good faith effort” in respect to guidelines for a system’s risk  assessment could reduce their degree of liability based on a set  incentives structure. The EU Product Liability Directive streamlines liability for  the producers of defective products throughout the EU by  introducing a system of strict liability, in which the injured  party is entitled to compensation if he or she demonstrates  15 Figure 1 is the author’s representation of this causal chain for products and services liability. This figure was informed by Keating, Gregory. “Strict Liability Wrongs,” in Philosophical  Foundations of the Law of Torts, edited by John Oberdiek (Oxford University Press: May 2014), 292-310, and “Products Liability,” Cornell Legal Information Institute, accessed May 31,  2020, https://www.law.cornell.edu/wex/products_liability . 16 Dremliuga, Roman & Kuznetcov, Pavel & Mamychev, Alexey. “Criteria for Recognition of AI as a Legal Person,” Journal of Politics and Law, Vol 12, No. 3, (August 18, 2019):10.5539/jpl. v12n3p105. 17 European Union, “Council Directive 85/374/EEC of 25 July 1985 on the approximation of the laws, regulations and administrative provisions of the Member States concerning liability  for defective products” (July 1985).  18 Committee on Legal Affairs, “Report with Recommendations to the Commission on Civil Law Rules on Robotics” (Report submitted to the European Parliament, May 2016). 2.3 CONSIDERATIONS FOR   LIABILITY AND HARM  The white paper correctly identifies the specific challenges  that AI systems pose to the classic legal liability model. Prior to  artificial intelligence systems not working as they are expected,  or causing harm, it is essential to determine who is responsible,  who is liable, and who must be held accountable for their  unintended consequences.  In the realm of products and services liability, the following  causal chain is often charted: Figure 1: Products and services liability causal chain15 When arbitrating liability in a court of law, the plaintiff must  demonstrate that they suffered tangible harm, demonstrable  either in monetary value, or bodily injury (3). Secondly, they  must show that the harm was caused by a default in the  product (2). Finally, it must be proven that the given default in  the product was in fact caused by a fault of the producer, via  negligence or tortious intent.    Due to the “autonomous” dynamics of machine learning,  harm produced by the outputs of an AI system is not always  predictable or foreseeable. One approach to liability would  be to consider that AI machines have moral agency and can  be treated as “artificial persons” and potentially ‘punished’  accordingly. Mostly inspired by theoretical cases involving PRODUCER PRODUCT CONSUMER Negligence/ tortious intentDefault in product Demonstrable harm 1 2 3
13PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  damage, a production defect in the product, and the causal  link to the  resulting damage or harm. In order to establish  liability for AI systems which may cause undue harm,  the appropriate definition of “damage” should consider  inclusion of reference to dignitary rights, invoking Article  7 of the Charter of Fundamental Rights of the European  Union. The EU should further incorporate a statement on  the need to respect relevant human rights in the context of  product liability. Extension of liability for damage caused  by AI-powered software and systems must be coupled with  acknowledgement of the broader harms to individuals or  societies that can be caused by developing technologies. Since the harms caused by AI will not always be physical in  nature, their harms will not always be clearly demonstrable.  For example, being mistakenly identified as an offender by an  AI-powered facial recognition technology system may be a  costly experience, even if the error is eventually corrected. Law  enforcement, healthcare, and other applications of AI systems  may cause harm to a person’s reputation or honor, or subject  them to “indignities” . In 2014, Google Spain v. AEPD19 ruled that  dignitary rights could provide a basis for liability, by upholding  the “right to be forgotten” implicit in Article 7 of the Charter  of Fundamental Rights of the EU. A similar approach could  be included in the European Civil Code when AI harms affect  individual dignities or inflict emotional distress.  2.4  NEW GOVERNANCE  BODIES AND  RESPONSIBILITIES Considering the different levels of government within the  EU, it may be difficult to consolidate the engagement of  all relevant stakeholders. To overcome this challenge, the  European Commission as well as governments of EU Member  States should first conduct stakeholder engagement exercises and additional public consultations. It will be vital to include  the full range of relevant stakeholders, including civil society  organizations. While EU laws apply across the Union, localized  contexts and implementation patterns for new technologies  may slightly vary. In order to encourage streamlining and  cooperation, we recommend the creation of an interEuropean Parliamentary Committee on AI. While the committee would deal with implementation activities  within and across member states, an AI Oversight Board  could be created to supervise implementation guidelines.  An existing model for such a board could build upon the  European Data Protection Board (EDPB) which oversees  the implementation of GDPR, and has outlined guidelines  on data privacy, while providing member states flexibility  in implementation. The committee, like the EDPB, would  be composed of EU representatives and regulators. The  implementation mechanisms of GDPR could be built upon to  consider enforcement mechanisms for AI regulations. Outlining  specific responsibilities between the implementation decisions  and enforcement bodies is essential for avoiding duplication of  efforts regarding the regulation of AI.   The white paper outlines the Commission’s goal to initiate  an open and transparent dialogue to help facilitate  “deployment, experimentation, and adoption” of AI by the  public sector, as well as an “Adopt AI Program” that will  support public procurement of AI systems. Such a dialogue  should be a continuous and evidence-based effort that  leverages available field expertise. Therefore, we propose the  establishment of an Advisory Committee on Public Sector AI  Use to guide actions to responsibly adopt AI in the EU public  sector. This advisory committee should also develop a route  to advise and share information with the aforementioned  inter-European Parliamentary Committee on AI to aid  development and implementation principles for the public  sector use of AI systems.  19 Google Spain SL and Google Inc. v Agencia Española de Protección de Datos (AEPD) and Mario Costeja González., Judgment of the Court (Grand Chamber), Audencia National,  (May 2014).
14PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  The Advisory Committee on Public Sector AI use will serve as a  platform to leverage multidisciplinary expertise to coordinate  policy across EU member states. Members could be recruited  experts who are offered grant funding for research in exchange  for serving on the committee. Specifically, the committee  should have the following responsibilities:  •   Assess and issue guidance on which use case for  AI, especially in context, should be promoted or  regulated in line with the treaties   •   ssue coordinated guidance on the procurement of AI  technologies in the public sector   •   Assess ways the EU and member state structures  could be adapted to facilitate novel applications of  AI – e.g., reduction of bureaucracy, easier access to  data for research purposes, increased funding for AI  initiatives, novel crowdsourcing initiatives   •   Share best practices and lessons learned in the  public sector deployment of AI  •   Monitor ongoing use of AI in the public sector and  potential issues like use case creep or uses of data  that were not intended   •   Make recommendations to governments and various  agencies on the regulation of AI in their respective  fields
15PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  20 Roland Berger and France Digitale, “The road to AI: Investment dynamics in the European ecosystem,” (2019), https://www.rolandberger.com/it/Publications/The-road-to-AI.html.  21 Roland Berger, “The road to AI: Investment dynamics in the European ecosystem.” 22 Perrault, Raymond et al., “Artificial Intelligence Index Report 2019,” (Stanford HAI, 2019), https://hai.stanford.edu/sites/default/files/ai_index_2019_report.pdf.  23 “Sizing the prize: What’s the real value of AI for your business and how can you capitalize?” (PWC, September 2018), https://www.pwc.com/gx/en/issues/analytics/assets/pwc-aianalysis-sizing-the-prize-report.pdf. 3. AI Impacts for Production, Skills,   and Labor  The EU White Paper on AI highlights the importance of  developing an ecosystem of excellence and an ecosystem  of trust in the implementation of policies that address  the challenges posed by AI. In order to achieve these two  important goals, it is crucial for the EU to design policies  that limit the threats to job security and displacement, and  to enact policies that foster research and development  while also allowing a whole of society approach to  reaping the benefits of transitioning sectors to these new  technologies. The adoption of AI is still at its infancy for  many sectors, and while the pace of adoption is likely to  rapidly increase, the EU has the opportunity to influence the  widespread adoption of AI in the EU by optimizing policies  that will provide the groundwork for equitable and diffused  growth in the AI-enabled future economy.  3.1 DIGITIZATION, ACCESS,  AND INNOVATION  A 2018 study from a European consulting group portrayed  a strong European AI ecosystem, with over 2,000 startups,  hundreds of labs and 3,000+ AI communities in 44 European  countries. “Trends in investment flows demonstrate the extent  of interdependency within the European ecosystem, as well  as its interconnections with the global leaders in AI, namely  the United States (US) and China. A coordinated investment,  talent and regulatory strategy would strengthen the European  AI ecosystem and set Europe on a clear path towards  global leadership.”20 Increased investment, research and development, and new patents could all stem from increased  focus on and partnership with European AI startups. Venture  capital (VC) funding is widespread in only eight member  states, Denmark, Finland, France, Germany, the Netherlands,  Spain, Sweden and the UK, with the average European  venture capitalist fund only equaling half of the average  American VC fund.21 The Stanford HAI Index Report22 further  reveals that most EU member states have no reported patents  as of 2019, and Germany, considered a leader in AI, currently  has half the number of AI patents per capita compared to the  United States. Some estimates suggest that AI “could contribute up to  $15.7 trillion to the global economy in 2030, more than the  current output of China and India combined.”23 Increased  productivity and increased consumption will be brought on  by the automation of tasks currently performed by humans,  and increased productivity of manufacturing and services.  These benefits will not apply to all companies and all workers  equally, and, in fact, the introduction of AI has been flagged  by many as a significant threat to equality overall. Even for  businesses, the barrier to entry for adoption of AI is not equal  for companies from a resource outlook, with adoption of AI  ultimately feasible at different points of production.  A prerequisite for AI is access to computing power for  research, development, testing and implementation. The  white paper reports that the EU is in a strategic position to  develop critical improvements in computing power, but it 
16PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  24 McKinsey Global Institute, “Jobs Lost, Jobs Gained: Workforce Transitions in a Time of Automation,” (McKinsey & Company, December 2017), https://www.mckinsey.com/~/media/ mckinsey/featured%20insights/Future%20of%20Organizations/What%20the%20future%20of%20work%20will%20mean%20for%20jobs%20skills%20and%20wages/MGI-JobsLost-Jobs-Gained-Report-December-6-2017.ashx.  25 Ibid. is crucial that access to computing power – which can be  expensive and environmentally costly to acquire – is available  for a broad range of institutions and companies within the EU  to ensure competitiveness in both industry and expertise. A  second prerequisite for the adoption of artificial intelligence  is the full digitization of processes of production, with  some industries leading the way. One effort that the EU can  establish is the development of a policy strategy to foster the  digitization of production more comprehensively, focusing on  sectors that at this moment are lagging in data, computing  power, and digitization.  If the EU hopes to remain competitive in the AI space, it must  create policies enabling greater investment in computer  chip design and production for AI, as the US holds significant  leverage in the marketplace considering its dominance in  the global AI chip supply chain. To overcome this gap and  become an influential leader in AI technology, it is crucial that  the EU improves its investment efforts in AI through public  private partnerships. These initiatives must be focused on  inclusive innovation and fostering ideas that improve the  lives of the many and not the few. For this goal, the EU could  institute specialized grant competitions that incentivize and  reward inclusive and equitable technological development. As technical developments soar, algorithms have the  potential to replace a great deal of human labor. While these  changes suggest many benefits, they will also inevitably  condemn many Europeans to face job displacement and  unemployment. Not only will workers be displaced from tasks  they previously performed, but there will also be a decrease  in demand for lower-skilled labor as industries develop  due to the increases of automation and AI. “The extent to  which these technologies displace workers will depend on  the pace of their development and adoption, economic  growth, and growth in demand for work. Even as it causes declines in some occupations, automation will change many  more – 60 percent of occupations have at least 30 percent of  constituent work activities that could be automated.”24 Yet,  automation may also create millions of new jobs globally, and  the equilibrium between creation and displacement will be  critically affected by the labor policies the EU will implement.  3.2 JOBS AND SECTORS  ADOPTING INCREASED  AUTOMATION  Increased automation not only threatens Europeans’ access  to employment, but will also force European economic and  political institutions to come to terms with supporting a larger  number of citizens who lack the necessary training and skills  to take on new economic opportunities. While some sectors  may absorb this shock, one prediction suggests that “globally,  up to 375 million workers may need to switch occupational  categories.”25 This was estimated before the COVID-19 crisis,  which will place additional pressure on job markets and  government finances. These changes will impact different  countries asymmetrically and at different times, as some  are more reliant on industries soon to be impacted. The EU  can adopt policies that will accommodate new AI-based  occupations by committing to the development of education  and training programs that will best enable humans to work  in conjunction with machines, and increase skill sets for AIenabled jobs. It could also ensure transferable benefits and  commit to increasing support for those navigating the future  of the labor market such as those EU member states that  have already begun testing ideas like universal basic income  and adaptive social safety nets.  Employment decisions and workforce management, 
17PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  26 McKinsey Global Institute, “Jobs Lost, Jobs Gained.” 27 McKinsey Global Institute, “Jobs Lost, Jobs Gained.”particularly in the service sector, is an additional area  for potential non-physical harms as a result of AI  implementation. Some companies in the U.S. are already  using AI systems to pre-screen candidates’ online profiles  and extract personality information. Others may use AI to  discourage some candidates from applying by selectively  advertising their job postings on aggregate job sites. These  and other forms of online discrimination should be carefully  considered by EU regulators as a potential threat to the  EU Single Market foundations as well as to fundamental  rights. In the service sector, AI is being used for increased  monitoring, real-time data analytics, and automatic business  adjustments. Some will seek to automate decisions that  apply to workforce management, such as promotion and  termination. In order to protect the rights of workers on  the job, it is necessary that the European Union develop a  system of regulation of the application of AI technologies  for employment and management decisions. Monitoring of  workers should not impede their right to privacy. Regulation  may also include a requirement for a “human in the loop” for  AI employment and human resource decisions. The expansion of AI and the inclusion of automation in  many other fields is seen as potentially threatening to the  current labor market, and specifically, to jobs that are more  susceptible to being replaced by such technologies. This  concern is certainly valid, as in many contexts AI is developed  with the explicit goal to substitute machine work for laborious  human tasks. In some of these contexts, the AI applications  serve to reduce error and human risk (e.g., advances in health  testing or recycling), but, in others, robots may simply replace  bodies. “ Almost one-fifth of the time spent in US workplaces  involves predictable physical activity and is prevalent in  such sectors as manufacturing and retail. These sectors  have a relatively high potential for automation given the  capabilities of current AI technologies. Even within sectors,  there is considerable variation. In manufacturing, for example, occupations that have a large proportion of physical activities  in predictable environments such as factory welders have a  technical automation potential above 90 percent, whereas for  customer service representatives that potential is less than  30 percent.”26 The same is true for these sectors in Europe. It is  therefore important that AI technologies are developed with  the intent of innovating and enhancing the current working  environment, solving for the creation of new job positions  within the same industry in which the technology is adopted,  or ways to retrain any displaced workforce.  “The changes in net occupational growth or decline imply  that a very large number of people may need to shift  occupational categories and learn new skills in the years  ahead. The shift could be on a scale not seen since the  transition of the labor force out of agriculture in the early  1900s in the United States and Europe, and more recently  in China. But unlike those earlier transitions, in which young  people left farms and moved to cities for industrial jobs,  the challenge, especially in advanced economies, will be to  retrain midcareer workers. There are few precedents in which  societies have successfully retrained such large numbers of  people. Frictions in the labor markets—including cultural  norms regarding gender stereotypes in work and geographic  mismatches between workers and jobs—could also impede  the transition.”27 A major concern with new AI technologies is their potential to  exacerbate inequality and generate division within political  and social systems. It is important to remember that there  are many jobs that are impossible to automate and that will  be in higher demand in the future, such as jobs that require  personal communication, empathy, and creativity. Within the  framework of the EU White Paper, the European Commission  should include a specific goal to bolster the development  of such skills and expand education programs that focus on  these skills.
18PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  3.3 SKILLS REQUIRED FOR AI  EXPANSION As highlighted in the white paper, the adoption of AI  technologies requires the extensive use of a large amount  of data that requires sophisticated analysis in order to be  interpreted. The expansion of AI job opportunities will,  therefore, require increased investment in educational  programs in universities and education centers, and online,  especially for retaining midcareer workers. The training will  require newly adapted skills requisite to work in AI, both  in the production and operation of AI systems. This would  imply a concerted investment from university consortiums in  the European Union to expand graduate programs in fields  related to AI, not only by replicating successful initiatives but  by also expanding current European research grants to attract  students who might be otherwise enticed to study outside the  EU. While university initiatives focused on AI might provide a  next generation of experts, today’s workforce should also  have company-driven training for retooling and AI skills.  Care providers, educators, managers and executives,  information technology professionals, builders, and more will  be impacted. To speed up the adoption of AI technologies  by subject matter experts, the EU could develop a series of  on-the-job training programs that facilitate the acquisition  of AI skills for their workers. The EU faces steep competition  for talent from many countries with attractive research and  industry leaders, and companies are in competition with  larger, international tech giants to attract the most highly  skilled workers. This concern is exacerbated in the context of  AI, where the qualified workforce is limited even in contexts at  the front-end of its development. SMEs will require extensive  support from the EU to fully overcome this difficult challenge.  We implore the EU to quickly create plans for training for  applied AI expertise, interaction with stakeholders, and transferable management skills related to fields that are  rapidly adopting automation throughout the EU.  The European Union should also consider investment  in AI projects aimed at extending the working life of the  older population and workers with disabilities. In one  example from the field of manufacturing, companies such  as 99DegreesCustom in the U.S. use highly-developed AI  technologies to increase the level of customization available  for their products. The customization affords workers within  the manufacturing sector with new and more interesting  opportunities on the job, requiring creativity over physical  strength. AI systems can be a powerful ally to human  operators, making certain dangerous or difficult tasks simpler,  and ensuring safer working conditions for individuals.  Older workers face a higher risk of obsolescence due to the  rapid change in the working environment and limited onthe-job educational opportunities offered. The European  Union should seek to develop and foster AI technologies  that improve the production capacity not just in terms of  efficiency, but also in terms of the quality of the production  process and end products, incorporating new labor not  typical for certain sectors once certain physical demands have  been automated.
19PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  Non-transparent and unaccountable decision making, as well  as potentially biased training data, are outlined as known  risks in current systems that employ algorithms for decision  making outputs. Gaps in legislation authority and regulation  for liability are included in the original white paper as  known unknowns. While the paper generally paints artificial  intelligence in a positive light, with sweeping beneficial  changes for citizens, companies, and the entire European  Union, there are three main gaps related to unknown  unknowns which require attention for risk analysis and  mitigation at the technical level for EU AI policy planning:  4.1 UNMARRYING MILITARY  AI AND THE BROADER  ECONOMIC MARKET  The decision to exclude the development and use of AI for  military purposes in the EU AI White Paper limits recognition  of the truly dual-use nature of the technology itself and  its drivers. It also omits the geopolitical dimensions of the  competition for values, interests and standards that are at  play at the time of writing this. Defense spending in both  research and development for AI, as well as off the shelf  commercial products and rapid prototyping, accounts for  trillions of dollars spent globally. Many military AI systems  are not limited to weapons systems and can be built or  adapted for military and non-military use, and many startup  companies rely on defense customers to develop, test, and  deploy early versions of their products. Many useful tools  and techniques have been born from military research  and development, including the internet itself. Separating  defense and non-defense AI applications presents a missed 4. Unknown Risks for Widespread  AI Adoption in the EU opportunity for close cooperation in understanding and  regulating what is expected to be the most disruptive dualuse technology in modern history. It also makes it more  challenging to ensure the use of AI in military contexts is done  in line with democratic oversight and respect for fundamental  rights.  Other parts of the world that are advancing plans for the  fourth industrial revolution will utilize commercial and  military investment and testing interchangeably, buoying  strongholds on certain industries, and potentially gaining  leads toward faster market entry. Advances in AI are being  implemented in non-weapons systems for emergency  response, search and rescue, software as a service platform  and more. In many cases, diffuse military departments and  allies have access to more and better data sets, and/or also  shed light on just how much data is yet to be digitized. It  would benefit the EU to consider military AI applications in  its broader plans, to prevent and avoid fragmentation in the  internal market.  The EU has championed much of tech policy to protect  the fundamental rights of humans, their data and  privacy. However, a lack of prioritization of sector-specific  advancements for AI could create a ‘race to the bottom’  scenario in an effort to deploy AI quickly at cost and scale.  With $1.5 billion invested it will be difficult to lead in all  areas; “industry, health, transport, finance, agrifood value  chains, energy/environment, forestry, earth observation and  space.”28 There is a risk that improvements in explainability  and bias for AI systems will take a back seat to market drivers.  Decentralized development and testing across member states  can be an effective strategy, but it is essential to scope the  28 European Commission, “EU AI White Paper.” 
20PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  29 European Commission, “EU AI White Paper.”  30 Alen Wagner, Jason Borenstein, and Ayanna Howard, “Overtrust in the Robotic Age, “ Communications of the ACM, Vol. 61, No.9, (September 2018), 10.1145/3241365.  31 European Commission, “EU AI White Paper.” 32 Ibidmost important use cases to properly vet the interoperability  and regulatory challenges. The EU and its member states  cannot effectively prioritize and mitigate risk without  prioritizing sectors which are most important for AI over the  next 5-10 years, and pairing risk analysis and compliance  mechanisms to those priorities, including development of  the conformity assessments, providing baselines for future  analysis.   Although the white paper mentions the legislative scope  of the EU extending to “all relevant economic operators  providing AI-enabled products or services in the EU,”29 it  does not address regulating EU exported technology and the  possible unintended consequences of systems developed  in the EU and used in unforeseen ways outside the EU, both  military and non, (e.g. facial recognition purchased from a  democratic state deployed for surveillance in an autocratic  state). The European Union, home to esteemed international  dual-use and export control regimes, has championed  success in dynamic trade policy and regulation. The EU now  has a unique opportunity to steer the burgeoning field of tech  policy compliance, emerging as a parallel to international  trade policy, by incorporating global companies with global  stakeholders to advance both market economies and  regulation authorities on the use of AI.  4.2 PREPARING SOCIETY FOR  PITFALLS OF AUTOMATION  BIAS  In considering ways to improve trust and accountability of  artificially intelligent systems, the paper does not address  automation bias – the potential for overtrust in AI, where  human operators expect a certain outcome based on  assumptions, and over time, eliminate mechanisms for quality control. Ongoing research looks at the “tendency  of humans to defer to technology when presented with  conflicting information,” and the phenomenon’s potential  impact to physical security.30 Without proper training and  indoctrination, over reliance on machine outputs can lead  to mistakes, and introduces a blind spot in risk management  across whole enterprises.  Enterprise adoption of AI systems cannot be naïve to  automation bias. “Building an ecosystem of trust is a policy  objective in itself and should give citizens the confidence  to take up AI applications and gives companies and public  organizations the legal certainty to innovate using AI.”31 The  AI value chain must take into account new user interface  dynamics when considering the factors surrounding human  interaction with AI. It is not enough to test the technological  innovations without users and human patterns of behavior.  Page 12 of the white paper outlines examples where material  and immaterial harm can be exacerbated by automation  bias, leading to potentially worse biased outcomes and  discrimination in sectors such as law enforcement and  the judiciary. The automation bias problem goes beyond  the “black box problem” in engineering, and may affect  legal thresholds and liability, since current legislation  only addresses safety risks at the time a system enters  the market, and remains immature on “transparency,  traceability and human oversight.”32 It will be difficult to  establish accountability over the lifecycle of a system. A law or  regulation must be vague enough to cover different systems  deployed across sectors and industries, specific enough  to govern lifecycles of systems, updates, and adaptations,  despite any explainability gaps.  In promoting the uptake of AI across the private and public  sector, not enough emphasis has been placed on the cultural  transitions necessary to safely achieve its ambitious goals 
21PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  33 European Commission, “EU AI White Paper.” 34 Ashok Chilakapati, “Concept Drift and Model Decay in Machine Learning,” Towards Data Science, April 25, 2019, https://towardsdatascience.com/concept-drift-and-model-decay-inmachine-learning-a98a809ea8d4.to introduce AI into every facet of human life. The paper  mentions attempts to hack or manipulate data or algorithms  deployed for intended outcomes, but does not address  hacking and manipulation of human behavior and interaction  with deployed systems. The Revised Coordinated Plan for  AI should expand on requirement type C – ‘information  provision’ – to include training on automation bias, dualuse and malicious applications of AI. Many comparisons  can be drawn from the development and patterns of  behavior catalyzed by the proliferation of the internet, from  e-commerce’s explosion as the result of online Beanie  Baby sales, or the nefarious use cases of child exploitation  and pornography online. As stated, requirement C could  be implemented as a type of surgeon general warning,  but should be enhanced to a defined body establishing  frameworks and best practices to coordinate with the public  and end users of systems. EU governance bodies cannot  ensure that a deployed system won’t deduce a more efficient  way to achieve its goals and objectives in a way uncaptured in  its description or confidence intervals, therefore, a governing  body must be entrusted to keep pace with developments.  “The plan will also increase awareness of AI at all levels  of education in order to prepare citizens for informed  decisions that will be increasingly affected by AI.”33 The  Revised Coordinated Plan for AI should consider expanding  on this leadership to develop widespread and publicly  available training on basic nomenclature, development  plans, assumptions, and risks for a general audience. A  “curriculum” for developers of AI turned into training materials  is not the same product for promoting tech literacy for the  whole of Europe. The plan should consider dialogues which  in turn bring technologists out of research labs and into  communities. Consider for example an AI developer whose  technology for autonomous vehicles struggles to identify  small dark objects visiting a predominantly African-heritage  neighborhood, to grapple with the reality of that system deployed in that environment which might not be able to  identify children.  4.3 TESTING AND OPERATING  IN MULTI-SYSTEMS  ENVIRONMENTS  Machine learning systems which are programmed with  different rules based on different values, dependent on  different inputs, risk calculations and tradeoffs, have not  been deployed at scale in the same environment. Discussion  of testing for AI systems often cite two phases, testing in  environments where the system has been trained to operate  (control), and testing in environments where the system  has been trained to learn based on controls. What is often  overlooked is environments which deploy multiple different  artificial intelligent systems, for example, testing various  autonomous vehicle systems operating simultaneously on  the same roads. This “systems on systems” black hole extends  to military applications, health care, finance, language,  predictive analysis, etc. and highlights the unique “many  hands” interoperability problem. Systems with the same objectives could have different  outputs, different biases, or one or both could lack  nuanced data. What happens when a linear system has  to communicate with a vector-type system, or a vectortype system needs to be interoperable with a decision tree  system? Can one system account for concept drift, a statistical  scenario where “our interpretation of the data changes with  time even while the general distribution of the data does  not,”34 in another? Supervised systems interacting with  unsupervised systems? Integration of AI systems will not  occur in a vacuum. As one researcher illustrates a portion of  this problem, “rather like having the Lakers play the Patriots in  the World Series, when both the concept/game and the data/
22PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  35 Chilakapati, “Concept Drift and Model Decay in Machine Learning” . 36 European Commission, “EU AI White Paper.”players change there would be a lot of head scratching in  the stands.”35 While systems on systems is clearly a technical  and meta problem, the Revised Coordinated Plan for AI  should provide a step by step approach to testing systems  which will inevitably operate in the same environment.  This area of study would be a boon for the member states’  Digital Innovation Hubs and could appoint sector-specific  research to each and entice talent to procure, train, and test  contrasting models in a sector-specific environment.  There is a need for tiered testing phases to include testing  systems on systems given that mistakes and biases can result  from AI learning in operation. AI systems may also learn from  or contrast with other AI systems. This phase of testing is  essential, “where the outcome could not have been prevented  or anticipated at the design phase, the risks will not stem  from a flaw in the original design of the system but rather  from the practical impacts of the correlation or patterns that  the system identifies in a large data set.”36 This requirement  could be developed into its own conformity assessment cycle  prior to deployment, meeting requirements for addressees in  a more complex testing environment and raising the bar for  both security by design and risk prevention. It is necessary to revalidate the European Union’s ability to  interoperate digitally in order to execute combined military  tests and operations. Asynchronous capabilities currently  exist among member states; therefore, a standard for the use  of AI in military operations must be established to ensure  that new developments do not inhibit interoperability. In the  case of AI development for military weapons systems, a clear  delineation should be drawn between offensive and defensive  systems. The EU should conduct a study to determine  the areas where increased automation would benefit or  weaken infrastructure. The implementation of AI in defensive  capabilities – known as Intrusion Detection Monitoring (IDM)  systems, should be standardized. The implementation of this capability has the potential to increase defense posture, limit  miscalculation, and deter intrusions. Command, Control,  Communications, Computers, Intelligence, Surveillance, and  Reconnaissance (C4ISR) capabilities are susceptible to cyber  enabled effects. This should not deter from the use of AI in  C4ISR for offense, but rather highlights the need to validate  C4ISR targeting data prior to its use in operational decisions.  This same susceptibility implies that the additional risk  introduced by AI in offensive platforms’ where deployment  and execution may exceed moderate risk tolerance. Where offensive cyber-enabled capabilities can be enabled  through AI, human in the loop decision responsibility must be  enshrined. In order to maintain control over military AI tools it  is recommended that the EU nest this capability and outline  systems intent with an international framework, akin to the  Tallinn Manual. This will provide a clear declaration of what is  and is not acceptable for deployment in the military context,  enhancing authority over the development of AI defense  trajectories. AI-enabled offensive weapons, if developed,  should abide by existing international laws and norms. It is  further recommended that AI-enabled capabilities remain  egalitarian and distributed within the EU. This will ensure that  these capabilities do not hinder military operations in the  absence of an EU AI military implementation strategy. Early  risk assessment measures here are also a must.
23PURPOSE: INPUT ON THE EUROPEAN COMMISSION   WHITE PAPER “ON ARTIFICIAL INTELLIGENCE –   A EUROPEAN APPROACH TO EXCELLENCE AND TRUST”  5. Conclusion As research and development produce more capable do main-specific AI systems, it is likely that the resulting economic effects will have significant impact on populations, markets,  and politico-economic relations between states. Advances  in robotics coupled with an increasing consumer preference  for quick delivery of customized products might reduce labor  costs and shift manufacturing toward localized economies.  This may pose challenges for member states highly dependent on manufacturing exports, increasing states’ dependence on natural resources and deepening inequality within  and between states.  These concerns are not limited to physical goods. As machine  learning becomes more sophisticated, it is likely to bring  disproportionate financial gains to internet companies that  already possess large amounts of data. These firms will then  have the greatest capacity to hire, cultivate, and retain talent,  potentially creating a small group of firms which own a lion’s  share of gains from AI. As a result, these firms could become  increasingly important to their relative states, both because  of national competition for “algorithmic supremacy,” but also  for the tax revenue these companies generate. States without  powerful AI firms could find themselves increasingly dependent on other states, which may find themselves exceedingly  dependent on powerful companies.  Aside from concerns about power, matters of equity and  fairness have become abundantly clear when considering  futures where technological prowess is concentrated in just  a few centers. Given the diversity within and between EU  member states, where to distribute headquarters for AI de velopment and regulation remains an open question. Could  a small number of member states be relied upon to serve as  responsible stewards for the specific needs and values we  have identified? States and their constituencies may have  different preferences regarding tradeoffs between fairness and accuracy, or privacy and productivity, which may not be easily  addressed at the EU level.  By no small effort, we invite the European Commission  to consider the comments and recommendations made  throughout as those of conscious global citizens. We want to  see the EU promote the transparent and equitable distribution of AI research, development and deployment. We encour age initiatives to increase public awareness, training, and  literacy in response to advancements in AI, and suggest the  creation of new occupations in the data-driven future. We also  hope for increased understanding of and export control over  military machinery and proliferation pathways for dual-use  technology. These recommendations can be coordinated and  operationalized throughout the EU, made up of distinguished  interdisciplinary experts, to tackle the implementation of  dynamic policies as they relate to the development and trade  of AI hardware and software, cooperation, and the capacity  for change. We submit these recommendations for your consideration, and look forward to the European Commission’s  comments.  
