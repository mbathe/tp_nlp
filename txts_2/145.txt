                 
 
       DGI(2017)12     STUDY  ON THE HUMAN RIGHTS DIMENSIONS  OF AUTOMATED DATA PROCE SSING  TECHNIQUES (IN PARTI CULAR ALGORITHMS )  AND POSSIBLE REGULAT ORY IMPLICATIONS          PREPARED BY THE COMMITTEE OF EXPERTS  ON   INTERNET INTERMEDIAR IES (MSI-NET)   
                                     ALGORITHMS AND HUMAN RIGHTS   Study on the human rights dimensions   of automated data processing techniques  and possible regulatory implications     English edition     The opinions expressed in this work are the  responsibility of the authors and do not  necessarily reflect the official policy of the  Council of Europe.     All rights reserved.   No part of this publication may be  translated, reproduced or transmitted in  any form or by any means without the  prior permission in writing from the  Directorate of Communications (F -67075  Strasbourg Cedex or publi shing@coe.int ).  Cover page photo: S hutterstock   Published by the Council of Europe   F-67075 Strasbourg Cedex, www.coe.int      © Council of Europe, March 2018  
Council of Europe study   Algorithms and Human Rights   1   TABLE OF CONTENTS   COMPOSITION OF THE M SI-NET ................................ .............................  2  I. INTRODUCTION  ................................ ................................ ..............  3  II. THE SCOPE OF THE STUDY  ................................ ..............................  5  1. AUTOMATION  ................................ ................................ ................................  5  2. DATA ANALYSIS  ................................ ................................ ............................  6  3. ADAPTABILITY ................................ ................................ ...............................  6  4. SOCIAL CONSTRUCTS AROUND ALGORITHMS  ................................ ...................  7  III.  IMPACTS OF ALGORITHM S ON HUMAN RIGHTS  .............................  10  1. FAIR TRIAL AND DUE PROCESS  ................................ ................................ ..... 10  2. PRIVACY AND DATA PROTECTION  ................................ ................................ .. 12  3. FREEDOM OF EXPRESSION  ................................ ................................ ...........  16  4. FREEDOM OF ASSEMBLY AND ASSOCIATION  ................................ ..................  22  5. EFFECTIVE REMEDY  ................................ ................................ .....................  23  6. PROHIBITION OF DISCRIMINATION  ................................ ...............................  26  7. SOCIAL RIGHTS AND ACCESS TO PUBLIC SERVICES  ................................ ........  28  8. THE RIGHT TO FREE ELECTIONS  ................................ ................................ ... 30  9. OTHER POSSIBLE IMPACTS  ................................ ................................ ...........  32  IV. REGULATORY IMPLICATI ONS OF THE USE OF AU TOMATED DATA  PROCESSING TECHNIQUE S AND ALGORITHMS  ..............................  34  1. TRANSPARENCY  ................................ ................................ ..........................  37  2. ACCOUNTABILITY  ................................ ................................ ........................  39  3. ETHICAL FRAMEWORKS AND IMPROVED RISK ASSESSMENT  ............................  40  V. MAIN FINDINGS AND CO NCLUSIONS  ................................ ............  43  BIBLIOGRAPHY  ................................ ................................ ....................  47  REFERENCES ................................ ................................ .........................  52 
Council of Europe study   Algorithms and Human Rights   2   In the terms of reference for the Steering Committee on Media and Information Society  (CDMSI) for the biennium 2016 – 2017, the Committee of Ministers asked the CDMSI to  “undertake work to study the human rights dimensions of automated data processing  techniques (in particular algorithms) and their possible regulatory implications” and  approved the committee of experts on internet intermediaries (MSI -NET) as a  subordinate structure to facilitate the work of the CDMSI. In its first meeting  on 17 -18  March 2016, the expert committee decided to  appoint Benjamin Wagner as rapporteur  for the study, while other members of the MSI -NET expressed the wish to support the  rapporteur in a small working group.   COMPOSITION OF THE MSI -NET  Wolfgang SCHULZ – Professor, Faculty of Law, University of Hamburg / Hans -Bredow Institut (Chair)   Karmen TURK – Partner at Trinity Tallinn  – Estonia (Vice -Chair)   Bertrand de la CHAPELLE – Co-founder /Director of the Internet & Jurisdiction - France   Julia HÖRNLE – Professor of Internet Law, Queen Mary University of London   Tanja KERŠEVAN -SMOKVINA – Principal Advisor to Director General, Agency for  Comm unication Networks and Services – Slovenia  (Gender Equality Rapporteur)   Matthias KETTEMANN – Postdoc Fellow, Cluster of Excellence “Normative Orders”  Univer sity of Frankfurt/Main (Rapporteur Recommendation)   Dörte NIELANDT  - Division  VI A3 ( Legal framework for digital services, media industry ),  Federal Ministry for Economic Affairs and Energy – Germany   Arseny NEDYAK – Deputy Director, Department of   Media State Policy, Ministry of  Telecommunication  - Russian Federation   Pēteris PODVINSKIS – Ministry of Foreign Affairs, International Organisations  Directorate,  Department for Public Policy related to Internet – Latvia   Thomas SCHNEIDER – Deputy Director of International Affairs, International Information  Society Coordinator, Federal Department of the Environment, Transport, Energy and  Communication D ETEC, Federal Office of C ommunications (OFCOM) - Switzerland   Sophie STALLA -BOURDILLON – Associate Professor in Information Technology /  Intellectual Property Law, Director of ILAWS, Southampton Law School University of  Southampton   Dirk VOORHOOF – Lecturer European Media Law, UCPH (Copenhagen University) /  Professor at Ghent University / Member of the CMPF Scientific Committee Centre for  Media Pluralism and Press Freedom   Benjamin WAGNER – Assistant Professor, Institute for Management Information  Systems,  Vienna University of Economics and Busines s (Rapporteur Study)  
Council of Europe study   Algorithms and Human Rights   3   I. INTRODUCTION   What information is made available to users on their Facebook newsfeeds? On what basis  is a person’s risk profile  determined  and what profiles provide best chances for obtaining  health insurance, or employment, or for being regarded a potential criminal or terrorist?   Automated data processing techniques , such as algorithms , do not only enable internet  users to seek and access information, they are also increasingly used  in decision -making  processes, that were previously entirely in the remit of human beings. Algorithms may be  used to prepare human decisions or to take them immediately through automated  means. In fact, boundaries between human and automated decision -makin g are often  blurred, resulting in the notion of ‘ quasi - or semi -automated decision -making’.   The use of a lgorithms  raises considerable challenges not only for the specific  policy area  in which they are operated , but also for societ y as a whole . How to safeguard human   rights and human dignity in the face of rapidly changing technolog ies? The right to life ,  the right to fair trial and the presumption of innocence, the right to privacy and freedom  of expression, workers’ rights, t he right to free ele ctions, even the rule of law itself are  all impacted . Respond ing to challenges associated with ‘algorithms’ used by the public  and private sector, in particular by internet platforms  is currently one of the most hotly  debated questions .   There is an increa sing perception that “software is eating the world”  (Andreessen 2011) ,  as human beings feel that they have no control over and do not understand the technical  systems that surround them.  While disconcerting , it is not always  negative . It is a by product of this phase of modern  life in which globali sed economic and technological  development s produce large numbers of software -driven technical artefacts  and “coded  objects ” (Kitchin and Dodge 2011)  embed key human rights relev ant decision -making  capacities . Which split -second choices should a software -driven vehicle make if it knows  it is going to crash? Is racial, ethnic or gender bias more likely or less likely in an  automated system ? Are societal inequalities merely replicat ed or amplified through  automated data processing techniques?   Historically , private companies decided how to develop software in line with the  economic, legal and ethical frameworks they deemed appropriate.  While there are  emerging framework s for the development of systems and processes that lead to  algorithmic decision -making  or for the implementation thereof , they are still at an early  stage  and do usually not explicitly address human rights concerns. In fact,  it is uncertain  whether and to what exte nt existing legal concepts can adequately capture the ethical 
Council of Europe study   Algorithms and Human Rights   4   challenges posed by algorithms . Moreover, it is unclear  whether a normative framework  regarding the use of algorithms or an effective  regulation of automated data processing  techniques is even feasible as  many technologies based on algorithms are still in their  infancy  and a greater understanding of their societal implications is needed . Issues  arising from use of algorithms as part of the  decision -making  process  are manifold and  complex . At the  same time, the debate about algorithms and their possible consequences  for individuals, groups and societ ies is at an early stage . This should not , however ,  prevent efforts towards understanding what algorithms actually do , which consequences  for society flow from  them and how possible human rights c oncerns could be addressed .   This study  identifies a number of human rights concerns triggered by the increasing role  of algorithms  in decision -making . Depending on the types of functions performed by  algorithm s and the level of abstraction and complexity of the automated processing that  is used , their impact on the exerc ise of human rights will vary. Who is responsible w hen  human rights are infringed based on algorithmically -prepared decisions ? The person who  programmed the algorithm, the operator of the algorithm, or the human being who  implemented the decision? Is there a difference between such a decision and a human made decision? What effects does it have on the way in which human rights a re  exercised  and guaranteed in accordance with well -established human rights standards,  including rule of law principles and judiciary processes?    Challenges related to the human rights impact of algorithms and automated data  processing techniques are boun d to grow as related systems are becoming increasing ly  complex and interact with each other’s outputs in ways that become progressively  impenetrable to the human mind. This report does not intend to comprehensively  address all aspects related to the human rights i mpact s of algorithms  but rather seeks to  map out some of the main current concerns from the Council of Europe’s human rights  perspective, and to look at  possible regulatory options that member states may consider   to minimise adverse effects , or to promote good practices . A number of related themes  will require more detailed research to more systematically assess their challenges and  potential from a human rights point of view, including questions related to big data  processing, machine learning, art ificial intelligence and the Internet of things.        
Council of Europe study   Algorithms and Human Rights   5   II. THE SCOPE OF THE STUDY   When assessing  automated data processing techniques and the algorithms  they use, it is  important to be clear on what types of algorithms are being discussed. This study will  build on existing well -established definitions , in particular  the work of Tarleton Gillespie  (2014) , Nicholas Diakopoulos (2015)  and Frank Pasquale (2015) . It is further important  to keep in mind that the term ‘algorithm’ is applied  widely and has a varied set of  meanings, depending on whether it is used  in the computer science community, among  mathematicians and information technologists , in communication and cultural media  studies or in public, including political  and social , discourse. Mapping out the human  rights dimensions of algorithms must also con sider the divergence between formal  definitions of algorithms and the popular usage of the term. In fact, many of the debates  about algorithms focus less on algorithms themselves and more broadly on the role of  technology in society (Bucher 2016) .   This study’s  basic approach starts from Tarleton Gillespie ’s assumption that “algorithms  need not be software: in the broadest sense, they are encoded procedures for  transforming input data into a desired output, based on specified calculations. The  procedures name both a problem and the steps by which it should be solved .” (Gillespie  2014:167)  Algorithms are thus perceived as “a series of steps undertaken in order to  solve a particular problem or accomplish a def ined outcome” (Diakopoulos 2015:400) .  This report will not discuss algorithms that automate manufacturing processes or perform  other such routine tasks. Rather,  it seems reasonable to limit the  discussion to  algorithms that are digital  and affect the public at large , thus focussing mainly on  algorithmic decision -making  that has  implications for human rights . Without being  exhaustive or aiming to predict all potential properties of algor ithms and their decision making in the future , the following characteristics of algorithms that engage in  automated data  processing and (semi -)automated decision making  are considered key  issues from a human rights perspective  for this report : automation, data analysis, and  adaptability . In addition, algorithms and data processing techniques are produced by  human  being s and  operated by human  being s. Their implications can  therefore not be  understood without acknowledgement of the social constructs that exist around them.     1. AUTOMATION   Automation  is one of the core ch aracteristics associated with algorithmic decision making. The ability of automated computing systems to replace human beings in a 
Council of Europe study   Algorithms and Human Rights   6   growing number of situations is a key characteristic of the practical implementation of  algorithms. The reasons for replacing human  being s with automated computing systems  can be  usually traced back to issues of large -scale data processing, speed , volume and  scale of decision -making, and in many cases to expectations of lower error rates  compared  to human  beings . Automated decision -making alg orithms  are used across a  variety of domains, from simplistic models that help online service providers to carry out  operations on behalf of their users (Kim et al., 2014) to more complex profiling  algorithms (Hildebrandt, 2008) that filter systems for per sonalised content.  Automated ,  algorithmic decision -making is usually difficult to predict for a human being and its logic  will be difficult to explain after the fact.   2. DATA ANALYSIS   Data analysis algorithms  are applied to large amounts of data to find patterns of  correlation within dataset s without necessarily making a statement on causation  (Grindrod, 2014). Their use of  data mining and pattern recognition without  “unde rstanding” their correlation or causal  relationships may lead to errors and raise  concerns about data quality. These algorithms replicate the functions previously  performed by human beings but involve a quantitatively and qualitatively different  decision -making logic to much larger amounts of data input.   It is noteworthy that effects of automated decision -making can be framed as interplay of  the applied analytics (based on algorithms) and the data sets used. An assessment of  human rights impacts should take both elements into account since, to take an example,  bias may be hidden  in the data set  and thus not found by analysing the algorithm itself.  When assessing the human rights impacts of algorithms, i t further must be considered  that d esigners of algorithmic systems have varying levels of  discretion when deciding, for  instance, what training data to use or how to respond to false positives , and that the  power of the operator of the algorithm may lie in his or her knowledge of the structure of  the data set , rather than in insight into the exact workings of the algorithms.          3. ADAPTABILITY   Adaptability is demonstrated in self-learning algorithms  that use data to develop novel  patterns and knowledge, and to generate new decision -making rules through ma chine  learning techniques (Williamson 2016) . By adopting various learning styles, algorithms  model problems based on data sets and produce new solutions tha t may be impossible 
Council of Europe study   Algorithms and Human Rights   7   for a human being to grasp . Essentially through constant trial and error techniques ,  algorithms detect patt erns in existing data, identify  similar pa tterns in future data and  make  data driven predictions .   Machine learning techniques are used, among others, in search engines  that auto -correct  spelling mistakes, as well as in more complex fields , such as fraud prevention, risk  analysis, advancement of insight into customer behaviour and enhancement of medical  science .     The predictabil ity of an algorithm’s outcome by the operator is important when  considering its accountability and the design of  adequate governance structures . The  progress of “deep learning ” technologies may lead to more systems that cannot be  understood by using the me ntal model of mechanical machines. There is considerable  debate in the academic community about the degree to which such systems can be made  intelligible to human beings and what consequences such intelligibility could have.1  4. SOCIAL CONSTRUCTS AROUND ALGORITHMS   While algorithmic decision -making is increasingly adept at replacing human  decision making, important elements (such as discretion) of decision -making processes cannot be  automated and often become  lost when human decision -making processes are  automated  (Spiekermann 2015) . Without judging their respective “quality”, d ecision making processes by humans and by algorithms are fundamentally and categorically  different, make different mistakes, and might have different outcomes and therefore  consequences. While societ y and governments have considerable experience  understanding human decision -making and its failures, they are only  beginning to  understand the flaws , limitations and boundaries  of algorithmic decision -making . One key  challenge is the frequent perception that algorithms are able to create neutral , non discriminatory  and independent predictions about future events.  The frenzy surrounding  the operation of Google Flu trends in 2011 , which later turned out to be unjustified as its  prediction ability was far lower than had been claimed , is one example of the on -going  struggle with assertions regarding the accuracy of predict ive algorithms (Lazer et al.  2014; Lazer and Kennedy 2015).  This challenge , however , relates less to algorithm s as a                                              1 See, for example, Yuan Stevens, ‘ The Promises and Perils of Artificial Intelligence: Why Human Rights and the  Rule of Law  Matter’,  https://medium.com/@ystvns/the -promises -and-perils -of-artificial -intelligence -whyhuman -rights -norms -and-the-rule-of-law-40c57338e806 , September 5, 2017.  
Council of Europe study   Algorithms and Human Rights   8   tool and more to their design as well as  human perception and interpretation of their  implementation and results.  Thus, the key to promotin g human rights compliance in the  use of algorithms may be to understand what algorithms can and cannot achieve and not  to let their use be dictated merely by considerations of efficiency  or effectiveness  alone .  Traditionally, developers have programmed algorithms by hand “to process and  transform input data into a desired output, based on specified calculations ” (Gillespie   2014). With technological evolution, however, the socio-technical systems like algorithms  are becoming increasingly opaque . This is n ot technically necessary, but rather a  frequent design choice leading to algorithmic systems whose inner workings cannot be  made transparent or accountable to the outside world.  Even when a human being  formally takes a decision, for instance the decision t o remove certain content from a  social media platform (see below 3.), the human being  may  often be led to  ‘rubber  stamp’ an algorithmically prepared decision , not having the time, context or skills to  make an adequate decision  in the individual case . Thus , while it may seem logical to  draw a  distinction between fully automated decision -making and semi -automated  decision -making , in practice the boundaries between the two are blurred. In neither case  will a human being be able to provide a reasoned  argument why a certain decision  needed to be taken in the specific case. This has repercussions for the right of the  concerned individual to seek an effective remedy against a human rights violation (see  below 5.).  It should be noted that algorithms as discussed here do not exist meaningfully without  interaction with human beings.  Mathematic or computational construct s do not by  themselves have adverse human right s impacts but their implementation and application  to hu man interaction  does. Technologies – in their application to human interaction - are  deeply social constructs (Winner 1980, 1986)  with considerable political implications  (Denardis 2012) . While a decision -making software , for example,   may be  “biased  but  ambivalent ” (McCarthy 2011:90) , it has no meaning without a social system around it  which provides meaning  and impact .   It is thus to o simple to b lame the algorithm or to suggest to no longer resort to  computers or computing.  Rather , it is the social construct and the specific norms and  values embedded in algorithms that need to be questioned, critiqued and challenged.    Indeed, it is not the algorit hms themselves but the decision -making processes around  algorithms that must be scrutinised in terms of  how they affect human rights.  
Council of Europe study   Algorithms and Human Rights   9   The question whether the quality of decisions with respect to human rights differs  between those taken by human and those taken by or based on algorithmic calculation  can only be answered if we know how human decision -making function s. There is  evidence that it is special (Tversky and Kahneman 1974)  as regards the use of tacit  knowledge and tacit norms  (Schulz and Dankert 2016) . This, to take an example,  enables humans to notice exceptional cases where the ap plication of a rule is not  appropriate even though the case falls within its scope.  The increasing importance of  algorithm s in decision -making calls for a better understanding of the design and  characteristics of decision making procedures.      
Council of Europe study   Algorithms and Human Rights   10   III. IMPACT S OF ALGORITHMS ON HUMAN RIGHTS    Reservations against  algorithms and automated data processing techniques  usually point  to their opacity and unpredictability.2 Beyond the se general  concerns , however, there is  an increasing awareness that specific  human  rights are particular ly affected.  These are  referenced below with practical examples as to how and why the use of algorithms may  lead to rights violations  or may otherwise undermine the effective enjoyment of these  human  rights .  1. FAIR TRIAL AND DUE PROCESS   The trend towards using  automated processing techniques and algorithms  in crime  prevention  and the criminal justice system is growing . Indeed, there may be some  benefits in such use as massive data sets may be processed more speedily  or flight risk s  assessed more accurately. Moreover, t he use of automated processing techniques for the  determination of the length of a prison sentence may allow more even approaches to  comparable cases.  Yet, growing national security concerns have led to ever more  ambitious applications of new technologies.  Following a string of terrorist  attacks in the  US and Europe , politicians have call ed for online social media platforms to use their  algorithms to identify  potential  terrorists  and to take action accordingly  (Rifkind 2014;  Toor 2016) . Some  such platforms are already using algorithms to identify accounts that  generate extremist content . Apart from the significant impact such application of  algorithms has for the freedom of expression (see below 3.), it also raise s concern s for  fair trial standards contained in Article 6 of the ECHR, notably the presumption of  innocence , the right to be informed promptly of the cause and nature of an accusation ,  the right to a fair hearing and the right to defend oneself in person.  Concerns may also  arise with respect to Article 5 of the ECHR, which protects against arbitrary dep rivation of  liberty, and Article 7 (no punishment without law). In the field of crime prevention, the  main policy debates regarding  the use of algorithms relate to predictive policing. This  approach  goes beyond the ability of human beings to draw conclusions from past  offences to predict possible future patterns of crime . It include s developed automated                                              2 See Tim O’Reilly, “ The great question of the 21st century: Whose black box do you trust?”, 13 September  2016, available  at: https://www.linkedin.com/pulse/great -question -21st-century -whose -black-box-do-youtrust-tim-o-reilly?trk=eml -b2_content_ecosystem_digest -hero-22null&midToken=AQGexvwxq0Q3iQ&fromEmail=fromEmail&ut=2SrYDZ8lkCS7o1  (last visited on 25 September  2017) . 
Council of Europe study   Algorithms and Human Rights   11   systems that predict which individuals are likely to become involved in a crime  (Perry  2013) , or are likely to become repeat o ffenders  and therefore  requir e more severe  sentencing .3 It also includes systems meant to predict where crime is likely to take place  at a given time which are then used for prioritising police time for investigations and  arrests. Such approaches may be hi ghly prejudicial in terms of ethnic and racial  backgrounds  and therefore require scrupulous oversight and appropriate safeguards .  Often the systems are based on existing police databases that intentionally or  unintentionally  reflect systemic biases.4 Depen ding on how crimes are recorded , which  crimes are selected to be included within the analys is and which analytical tools are used ,  predictive algorithms may thus contribute to prejudicial decision -making  and  discriminatory outcomes .  In addition, considerable concern s exist that the  operation of such assessments  in the  context of crime prevention is  likely to create echo chambers within which pre-existing  prejudice  may be further  cemented . Bias or prejudice  related , for example,  to racial or  ethnic background , may not be recogni sed as such by the police  when integrated into an  automated computer program  that is deemed independent and neutral  (see also 6.). As a  result, bias may become  standardised and may then  be less likely to be identified and  questioned as such.  While it is unclear how prevalent such decisions created by  algorithms are in the criminal justice system generally, the mere potential of their use  raises serious concerns with regard to Article 6 of the ECHR and the principle of equal ity  of arms and adversarial proceedings as established by the European Court of Human  Rights.5   Furthermore, algorithms are increasingly used in the context of the civil and criminal  justice systems where artificial intelligence is being developed to event ually support or  replace decision -making by human judges. Such systems are currently being tested to  identify decision outcomes with a view to detect patters in complex judicial decision making. Thus far, the reliable prediction rate is relatively low at 7 9%. It is therefore                                              3 See also Article 19, Algorithms and Automated Decision -Making in the Content of Crime Prevention: A Briefing  paper, 2016.  4 See, for example, William Issac, Kristian Lum Kristian Lum and William Isaac (2016), To predict and serve?  Significance, October 10, 2016, The Royal Statistical Society, available at:  http://onlinelibrary.wiley.com/doi/10.1111/j.1740 -9713.2016.00960.x/epdf  (last visited on 25 September  2017) .  5 See, for instance, in Jespers v. Belgium , 15 October 1980 , no 8404/78, Salduz v. Turkey , 17 November 2008,  no 36391/02 and Blokhin v. Russia , 13 April 2016, no 47152/06.  
Council of Europe study   Algorithms and Human Rights   12   considered premature at the current time to imagine such systems replacing judges.6  Nevertheless, it is suggested that such systems can support or assist judges (and  lawyers).7 Given the pressure of high caseloads and insufficient resou rces from which  most judiciaries s uffer, there is a danger that support systems based on artificial  intelligence are inappropriately used by judges to “delegate” decisions to technological  systems that were not developed for that purpose  and are perceived as being more  ‘objective' even when this is not the case. Great care should therefore be taken to assess  what such systems can deliver and under what conditions that may b e used in order not  to jeopardise the right to a fair trial. This is particularly the  case when such systems are  introduced mandatorily, as is the case for parole decisions in the United States. Concerns  about judicial bias around parole decisions have led to the mandatory introduction of  software to predict the likelihood of offenders reo ffending in many U.S. states.8 However  independent investigation of this software suggests that the “software used […] to  predict future criminals […] is biased against blacks” (Angwin, Mattu, and Kirchner  2016) .   2. PRIVACY AND DATA PROTECTION   The longest and most sustained human rights debate on automated data processing  and  algorithms relates to the right to privacy .9 Algorithms facilitate the collection, processing  and repurposing of vast amounts of data and images . This may have seri ous  consequences  on the enjoyment of the right to private and family life , including the right  to data protection,  as guaranteed in Article 8 of the ECHR . Algorithms are us ed in online  tracking and profiling of individuals whose browsin g patterns are recorded by “cookies ”10                                              6 Nikolaos Altreas et al “Predicting judicial decisions of the European Court of Human Rights: a Natural  Language Processing perspective” PeerJ  Computer Science  Open Access (Published 24. October 2016),  available at https://peerj.com/articles/cs -93.pdf  at p.2; see also The Law society gazette, Monidipa Fouzder,  “Artificial Intelligence mimics j udicial reasoning”, 22 June 2016, available at:   https://www.lawgazette.co.uk/law/artificial -intelligence -mimics -judicial -reasoning/5056017.ar ticle   (last visited on 25 September 2017) .  7 Ibid.  8 See GCN, Kevin McCaney, “Prisons turn to analytics software for parole decisions”, 1 November 2013,  available at https://gcn.com/articles/2013/11/01/prison -analytics -software.aspx  (last visited on 25 September  2017) .  9 See Sills 1970 .  10 A cookie is a small amount of data generated by a website  and saved by the web browser  with the purpose to  remember information about the user, similar to a preference file created by a software application . While  cookies may serve many functions, their most common purpose is to store login information for a specific site.  Cookies are also used to store user preferences for a specific site. For  example, a search engine  may store  search settings in a cookie.  
Council of Europe study   Algorithms and Human Rights   13   and similar technologies such as digital fingerprinting , aggregated with search queries  (search engines /virtual assistants ). Moreover, behavioural data is processed from smart  devices, such as location and other sensor  data through apps on mobile devices  (Tene  and Polonetsky 2012) , raising increasing challenges for privacy and data protection .   Applications of online tracking and profiling are also used in targeted advertising based  on the profile of a person’s presumed interests.  Here, user consent is an important  regulatory concern. Research at Berkeley in 2012 established, for instance, that th e use  of privacy -invasive tracking technologies that cannot be observed by users (such as  digital fingerprinting  and behavioural data generated by sensors ) has increased following  the greater awareness of consumers and the ir growing practice of deleting or  disabling   cookies as part of the “do -not-track” choice settings in internet browsers.11 Moreover,  extensive data processing through the use of algorithms may aggravate infringements of  other rights, as personal data is used to target individuals, such as i n the context of  insurance or employment applications.   One particular challenge of algorithmic processing of personal data is the generation of  new data. When a data subject shares a few discrete pieces of data, it is often possible  for those data to be me rged, generating second and even third generations of data about  the individual. Two innocuous pieces of data, when assessed in comparison with a much  larger data set can "breed" and generate "baby data", the nature of which can be entirely  unpredictable f or the data subject. This raises major issues for the notions of consent,  transparency and personal autonomy. Research from Cambridge and Stanford  Universities illustrate the scale of the challenge .12  Efforts are ongoing to modernise the 1981 Council of Eur ope Convention for the  Protection of Individuals with regard to Automatic Processing of Personal Data  (Convention ETS 108) in line with the technological evolution , and to further define the  rights of the data subject  with respect to the implications for p rivacy of contemporary  tools for data collection, processing, repurposing and profiling . Article 8 of the draft  modernis ed Convention establishes the explicit right of every individual n ot to be                                              11 CJ Hoofnagle “Behavioural Advertising: The Offer You Cannot Refuse” (2012) 6 Harvard Policy & Law Review  273-296  12 See Stanford news, “New Stanford research finds computers are better judges of personality than friends and  family”, available at: http://news.stanford.edu/2015/01/12/personality -computer -knows -011215/  (last visited  on 25 September 2017) . 
Council of Europe study   Algorithms and Human Rights   14   subject ed to a decision significantly affecting him or h er based solely on an automated  processing of data without having his or her views taken into consideration;  the right  to  obtain knowledge of the reasoning underlying data processing where the results of  such  processing are applied to him or her;  and to ob ject at any time, on grounds relating to  his or her situation,  and to the processing of personal data  concerning him or her , unless  the controller demonstrates legitimate grounds for the processing which  override his or  her interests or rights and fundamen tal freedoms . The modernisation proposals further  aim to provide complementary safeguards as regards transparency (Article 7bis) and the  need for an examination of the likely impact of data processing on the rights and  fundamental freedoms of the person prior to commencing such processing (Article  8bis).13   The “Guidelines on the protection of individuals with regard to the processing of personal  data in a world of Big data”  14 recently adopted by the Committee of the Convention for  the Protection of Individu als with regard to Automatic Processing of Personal Data  provide a general framework to apply appropriate policies and measures to continue to  make effective the data protection principles in the context of Big Data.   Data protection regulat ory frameworks  at EU level, such as the  General  Data Protection  Regulation  of April 2016  (Regulation (EU) 2016/679  on the protection of natural persons  with regard to the processing of personal data and on the free movement of such data ),  which will apply as of May 2018 , also establish standards for the use of algorithms in  data collection, including possibly a limited right to information or even a “right to  explanation” (Goodman  and Flaxman 2016)  with respect to decision -making processes  –  although the exact scope of this right to explanation is heavily contested ,15 (Wachter,                                              13 See the Draft modernised Convention for the Protection of Individuals with Regard to the Processing of  Personal Data, September 2016, available at:  https://rm.coe.int/CoERMPublicCommonSearchServices/DisplayDCTMContent?documentId=09000016806a616c   (last visited on 25 September 2017).   14 Council of Europe,  Guidelines on the Protection of Individuals with regard to the Processing of Personal Data  in a World of Big Data, 17 January 2017, available at:  https://rm.coe.int/CoERMPublicCommonSearchServices/DisplayDCTMContent?documentId=09000016806f06d0   (last visited on 25 September 2017) .  15 See Wachter, Mittelstadt and Floridi, 2016. See also Lilian Edvards and Michael Veale, 2017, available at:  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2972855  (last visited on 06 October 2017).  
Council of Europe study   Algorithms and Human Rights   15   Mittelstadt, and Floridi 2016)  - as well as the rig ht to access to “ knowledge of the logic  involved in any automatic processing of data concerning him .”16   Particular concerns arise from the use of data brokers who aggregate the information  contained in personal profiles . Profiling, in itself, means extrapo lation of data available on  the internet through processes of automated information gathering and subsequent  construction and application of profiles.  Profiling techniques can benefit individuals and  society by, for instance, leading to better market segme ntation or permitting an analysis  of risks and fraud. Yet, there are also important concerns  about the usage of the  technique . The Council of Eur ope Recommendation on Profiling17 addresses the risk that  profiles attributed to a data subject make it possible to generate new data, including  through data aggregation.  This information may then be mined through the use of  algorithms, which creates a risk of large -scale surveillance ( “data-veillan ce”) by private  entities and governments alike  (Rubinstein, Lee, and Schwartz 2008) . This view is  echoed by the United Nations Huma n Rights Council, which on 22 March 2017 noted with  concern “that automatic processing of  personal data for individual profiling may lead to  discrimination or  decisions that otherwise have the potential to affect the enjoyment of   human rights, including economic, social and cultural rights .”18  The ma in concern of using data from profiles for different purposes through algorithms is  that the data lose s its original context . Repurposing of data is likely to affect a person’s  informational self -determination . Search engines may have a similar effect on the right  to privacy and data protection  as they also fa cilitat e the aggregation of data about a  specific individual .   The use of data from profiles , including those established based on data collected by  search algorithms and search engines,  directly affects the right to a person’s  informational self -determination . The data subject will usually not b e aware of the                                              16 See for further details European Data Protection Supervisor, “ethics”, webpage, available at:  https://secure.edps.europa.eu/EDPSWEB/edps/EDPS/Ethics  (last visited on 25 September 2017) . Directive  (EU) 2016/680 on the protection of natural persons with regard to the processing of personal data by  competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal  offences or the execution of criminal penalties, and on the  free movement of such data provides a framework  for the processing of data in the course of actions that do not fall under Community Law, such as judicial  cooperation in criminal matters and police cooperation.     17 Recommendation CM/Rec(2010)13 of the Com mittee of Ministers to member states on the protection of  individuals with regard to automatic processing of personal data in the context of profiling.   18 U.N. Human Rights Council Resolution on the Right to Privacy in the Digital Age, U.N. Doc.  A/HRC/34/7, 23  Mar. 2017, para.2  
Council of Europe study   Algorithms and Human Rights   16   profiling itself a nd of the subsequent repurposing of data beyond its original context ,  making it easier to find information by reducing the practical obscurity  of anonymous  data. In addition, the results obtained through search algorithms m ay be incomplete,  inaccurate or out -dated, thereby placing individuals in a distorted light, which may be  prejudicial.19 Such profiles may have particularly serious consequences for children and  their future. Finally, there is increasing evidence that data  is harvested in order to gain  behavioural insights that can be used to target voters and – ultimately – even  manipulate elections (see below 8.).20   Another key aspect related to the usage of algorithms for automated data processing  focuses  on ‘cloud’ data storage. This refers to solutions whereby files and other data are  no longer stored on local storage but are stored remotely on servers accessible via the  Internet. However , by virtue of engaging in non -local storage practices, the data of  users  may be processed by algorithms while stored remotely in intrusive ways that would not  usually be practiced. Such automated data processing can take place  in two places : (1)  in transit to the remote network storage location and (2) on the remote serv ers where  the data is stored. It may be i ncreasingly difficult for users to ascertain whether they are  using local or remote services , as modern operating systems are gradually becoming  more deeply enmeshe d with ‘cloud’ remote services. With regard to data  in transit, it  may therefore be difficult to determine whether it is sufficiently protected through   technologies such as  strong end -to-end encryption , and whether it is not manipulated in  some form.21   3. FREEDOM OF EXPRESSION   The operation of algorithms  and data processing techniques has tremendous effects  on  the right to f reedom of expression , which includes the right to receive and impart  information . While the positive effects  of search algorithms and search engines for the                                              19 See Solove (2006), p. 547. As regards data processing in the course of judicial cooperation in criminal  matters and police cooperation, which do not fall under Community Law, Directive (EU) 2016/680 on the  protection of natural persons with regard to the processing of personal data by competent authorities for the  purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of  criminal penalties, and on the free movement of suc h data establish data protection safeguards.   20 See also The Guardian, “The great British Brexit robberty: how our democracy was hijacked”, 7 May 2017,  available at: https://www.theguardian.com/technology/2017/may/07/the -great-british -brexit -robbery hijacked -democracy  (last visited on 25 September 2017) .   21 For example, Microsoft’s cloud service ‘SkyDrive’ operates an automated process designed to remove certain  content (such as nudity). See Clay 2012.  
Council of Europe study   Algorithms and Human Rights   17   human  right to freedom of ex pression  has been repeatedly  referred to ,22 their potential  for harming the freedom of information and freedom of expression of individuals, groups  and whole segments of societies  is now increasingly discussed .23 Concerns arise not only  with respect to the i ndividual right to freedom of expression but also with respect to the  inherent aim of Article 10 of creating an enabling environment for pluralist public debate  that is equally accessible and inclusive to all. Moreover, the privacy and data protection  concerns raised above can significantly impede on individuals’ ability to freely express  themselves.   Search engines act as crucial gatekeepers for human  being s who wish to seek, receive or  impart information. Content which is not indexed or ranked highly by an Internet search  engine  is less likely to reach a large audience  or to be seen at all . As a result, the use of  algorithms may lead to fragmentation of the public sphere and to the creation of “echo  chambers” that favour only certain types of news outlets , thereby enhancing  levels of   polarisation  in society which can seriously jeopardise social cohesion .24 A search  algorithm might also be biased towards certain types of content or content providers,  thereby risking affecting related values such as media plu ralism and diversity .25 This is  particularly the case in the context of  dominant online search engines  (Pasquale 2016) .  The algorithmic predictions of user preferences deployed by s ocial media platforms  guide  not only what advertisements individuals might see, but they also personalise search  results and dictate the way how social media feeds , including newsfeeds,  are arranged .                                              22 See, for instance, Council of Europe, Recommendation of the Committee of Ministers to member States on  the protection of human rights with regard to search engines, CM/Rec(2012)3, Adopted by the Committee of  Ministers on 4 April 2012 at the 1139th meeting of the Ministers’ Deputies, paragraph 1, available at  https://wcd.coe.int/ViewDoc.jsp?id=1929429  (last visited on 25 September 2017) , observing that s earch  engines “enable a worldwide public to seek, receive and impart information and ideas and other content in  particular to acquire knowledge, engage in debate and participate in democratic processes.”   23 See, for instance, the 2016 Report of the UN Special Rapporteur on the promotion and protection of the right  to freedom of opinion and expression , David Kaye, to the Thirty -second session of the Human R ights Council  (A/HRC/32/ 38), pointing out that “search engine algorithms dictate what users see and in what priority, and  they may be manipulated to restrict or prioritise content“.   24 See also Arstecnica, Roheeni Saxena, “The social media “echo chamber” is real”, available at  https://arstechnica.com/science/2017/03/the -social -media -echo-chamber -is-real/ (last visited on 25  September 2017) .  25 According to the UNESCO’s World Trends in Freedom of Expression and Media Development Publication,  internet technologies have enabled many more voices to be heard. While the lack of gender -disaggregated  statistics thus far prevents a better understanding of the gender -specific impacts of algorithms controlled  search tools on the exercise of the right to freedom of expression, it appears that regional and gender patterns  of communications are replicated also in this new volume of voices; see UNESCO’s World Trends in Freedom of  Expression and Media Development Publication at: http://www.unesco.org/new/en/world -media -trends  (last  visited on 25 September 2017) . 
Council of Europe study   Algorithms and Human Rights   18   Given the size of platforms such as Google or Facebook, their centrality for many  experience of the internet as a quasi -public sphere (York 201 0) and their abil ity to  massively amplify certain voices (Bucher 2012) , this is by no means a trivial matter.  On  the contrary, the personalisation of information that users receive bas ed on their  predicted preferences and interests can create “filter bubbles” and may substantially  compromise the freedom of expression, which includes the right to information.  While  filter bubbles and echo chambers are a plausible and therefor e a widely -discussed   concept, it should be noted  that the empirical evidence for their existence in Europe is  mixed  (Nguyen et al. 2014; Zuiderveen Borgesius et al. 2016) . Individuals  usually inform  themselves by using a repertoire of source s, not just via social media or i nternet  search es.   According to Article 10 of the ECHR, any measure that blocks access to content through  filtering or removal of content must be prescri bed by law, pursue one of the legitimate  aims foreseen in Article 10.2, and must be necessary in a democratic society. In line with  the jurisprudence of the  European Court of Human Rights , any restriction of the freedom  of expression must correspond to a “ pressing social need” and be proportionate to the  legitimate aim(s) pursued .26  However, content removal on social media platforms often takes place through semi automated or automated processes. Algorithms are widely used for content filtering and  content r emoval processes  (Urban, Karaganis, and Schofield 2016) , including on social  media platforms,  directly impacting on the freedom of expression  and raising rule of law   concerns (questions of legality, legitimacy and proportionality) . While large social media   platforms like Google or Facebook have frequently  claim ed that human being s remove all  content  (Buni and Chemaly 2016) , large parts of the process are automated (Wagner  2016b)  and based on s emi-automated  processes .  According to a report from the British                                              26 In Yildirim v. Turkey , 18 March 201 3, no 3111/10 , the European Court of Human Rights has emphasised that  “the dangers inherent in prior restraints are such that they call for the most careful scrutiny on the part of the  Court, (..) for news is a perishable commodity and to delay its publica tion, even for a short period, may well  deprive it of all its value and interest”.   Therefore blocking access to the internet or removal of online content  requires a legal framework, “ensuring both tight control over the scope of bans and effective judicia l review to  prevent any abuse of power (..) In that regard, the judicial review of such a measure, based on a weighing -up  of the competing interests at stake and designed to strike a balance between them, is inconceivable without a  framework establishing p recise and specific rules regarding the application of preventive restrictions on freedom  of expression”.  
Council of Europe study   Algorithms and Human Rights   19   Intelligence and Security Committee of Parliament , 27 various automated techniques exist  for identifying  content believed to break the terms of service  of the respective provider,  be it because of extremist content, child exploitation or illegal acts such as the incitement  to violence. These  techniques may also be used to disable or automatically suspend  user  account s (Rifkind 2014) . A particular challenge in this context is that intermediaries are  encouraged to remove this content volun tarily, without clear legal basis. This lack of a  legal basis for ‘voluntary’ automated content removal makes it even more difficult to  ensure that basic legal guarantees such as accountability, transparency or due process  are upheld  (Fernández Pérez 2017) .  In the US, the Obama administration has advocated for the use of automated  detection  and removal of extremist videos and images.28 Additionally, there have been proposals to  modify search algorit hms in order to “hide” websites that would incite and support  extremism. The automated filtering  mechanism for extremist videos has been  adopted by  Facebook and YouTube for videos. However, no information has been released about the  process or about the criteria adopted to establish which videos are ”extremist ” or show  “clearly illegal content ”’29 In the wake of reports from The Times of Lo ndon and The Wall  Street Journal that ads were appearing on YouTube videos that espoused “ extremism ”  and “ hate speech ”, YouTube reacted  with a tighter use of its algorithm operated to detect  “not advertiser -friendly” content, which has reportedly affected independent media  outlets, including comedians, political commentators and experts.30   Similar initiative s have been developed in Europe , where intermediary service providers ,  in response to public and political pressure,  have committed themselves to actively  counter online hate speech  through  automated techniques that detect and delete all  illegal content. While not disputing the necessit y to effectively confront hate speech, s uch  arrangements have been criticised for delegating law enforcement responsibilities from                                              27 See UK Intelligence and Security Committee of Parliament report,  Privacy and Security: A modern and  transparent legal framework, March 2015, availab le at: http://isc.independent.gov.uk/committee reports/special -reports  (last visited on 25 September 2017) .   28 See Article 19, “Algorithms and automated decision -making in the context of crime prevention”, 2 December  2016, available at: https://www.article19.org/resources.php/resou rce/38579/en/algorithms -and-automated decision -making -in-the-context -of-crime -prevention  (last visited on 25 September 2017) .   29 See Reuters, Joseph Menn, Dustin Volz, Exclusive: Google, “Facebook quietly move toward automatic  blocking of extremist videos” , available at: http://www.reuters.com/article/us -internet -extremism -video exclusive -idUSKCN0ZB00M  (last visited on 25 September 2017) .   30 See The New York Times, Amanda Hess, “ How YouTube’s Shifting Algorithms Hurt Independent Media”, 17  April 2017, available  at: https://www.nytimes.com/2017 /04/17/arts/youtube -broadcasters -algorithm ads.html?_r=0  (last visited on 25 September 2017) . 
Council of Europe study   Algorithms and Human Rights   20   state to private companies, for creating the risk of excessive interference with the right  to freedom of expression, and for their lack of compliance with the principles of legality,  proportionality, and due process.  Requiring intermediaries to restrict access to content  based on vague notions such as “extremism” obliges them to monitor all flows of  communication and data online  in order to be able to detect what may be illegal content.  It therefore goes against the es tablished principle that there should be no monitoring  obligation for intermediaries, which is enshrined in EU -law and in relevant Council of  Europe policy guidelin es.31 Due to the significant chilling effect that such monitoring has  on the freedom of expression, this principle is also reiterated in the draft  recommendation on the roles and responsibilities of internet intermediaries prepared by  the Council of Europe’ s Committee of Experts on Internet Intermediaries in September  2017.32   Moreover, by ordering the intermediary to decide itself what to remove as “extremist”  and what not, the public authority passes the choice  of tools and measures  onto a private  party, which can  then implement solution s (such as content removal or restriction)  that  the public authorities themselves  could not legally prescribe . Public -private partnerships  may thus allow public actors “t o impose regulations on expression that could fail to p ass  constitutional muster” (Mueller 2010:213)  in contravention of rule of law standards.  Moreover, these kinds of demands by public institutions of private actors lead to  overbroad and automated  monitoring and  filtering of content.    The Europol Internet Referral Unit had, one year after its launch in July 2015 , assessed  and processed 11.000 messages containing violent extremist content materials across 31  online platforms in eight languages , reportedly leading to the removal of 91.4% of the  total content from the platforms.33 Steps have reportedly been taken to automate  this  system  with the introduction of the Joint Referral Platform  announced in April 2016 .34                                               31 See Article 15 of Directive 2000/31/EC of 8 June 2000 (“Directive on electronic commerce”), and Principle 6  on limited liability of service providers for Intern et content  of the Council of Europe Declaration on freedom of  communication on the Internet of 28 May 2003.    32 See Draft Recommendation of the Committee of Ministers to Member States on the Roles and Responsibilities  of Internet Intermediaries, finalized b y the MSI -NET on 19 September 2017, at https://rm.coe.int/draft recommendation -on-internet -intermediaries -version -4/1680759e67 .   33 See Europol Internet Referral Unit One Year On, Press release, 22 July 2016, available at:  https://www.europol.europa.eu/newsroom/news/europol -internet -referral -unit-one-year (last visited on 25  September 2017) .  34 See EC Communication from the Commission to the European Parliament, The European Council And The  Council delivering on the European Agenda on Security to fight against terrorism and pave the way towards an   
Council of Europe study   Algorithms and Human Rights   21   While the imperative of acting decisively against the spread of hate messages  and the  incitement to racially -motivated offences is indisputable, s uch practices raise  considerable concerns related to foreseeability and legality of interferences with the  freedom of expression. Notably the data on extremist online content that Europol  is  processing refers not just to content that is illegal in Council of Europe Member States,  but also to material that violates the terms of service of an internet intermediary.  Moreover, in many situations extremist content or material inciting violence is difficult to  identify , even for a trained human being,  because of the complexity of disentangling  factors such as cultural context a nd humor . Algorithms are today not capable of  detecting irony or critical analysis. The filtering of speech to eliminate harmful content  through algorithms therefore faces a high risk of over -blocking and removing speech that  is not only harmless but can contribute positively to the public debate. According to the  European Court of Human Rights, Article 10 also protects shoc king, offensive or  disturbing content.35 Algorithmic blocking, filtering or removal of content  may thus have  a significant adverse impact on legitimate content. The already highly prevalent dilemma  of large amounts of legal content being removed because of the terms of service of  internet platforms  is further exacerbated by the pressure placed on them to actively filter  according to vague notions such as “extremist” , “hate speech”  or “clearly illegal content” .  According to the European Court of Human Rights,  any obligation to filter or remove  certain types of comments by users from online platforms puts an “excessive and  impracticable” burden on the operators and risks to oblige them to install a monitoring  system “capable of undermining the right to impart i nformation on the internet.” 36 The  Venice Commission has equally called for efforts to strengthen human rights safeguards                                                                                                                                       effective and genuine Security Union, available at: https:// ec.europa.eu/home affairs/sites/homeaffairs/files/what -we-do/policies/european -agenda -security/legislative documents/docs/20160420/communication_eas_progress_since_april_2015_en.pdf    (last visited on 25 September 2017) . See also Article 19, Algorithms and Automated   Decision -Making in the Context of Crime Prevention: A briefing paper, 2016.   35 As demonstrated not only in jurisprudence of domestic courts, but also in the case -law of the European Court  of Human Rights, the exercise of qualifying speech as (ill egal) hate speech is delicate. Several judgments of the  Court concerning the question whether certain speech could or should be qualified as criminal hate speech  resulted in divided votes, such as e.g. in I.A. v. Turkey , 13 September 2005, no 42571/98 ; Lindon,  Otchakovsky -Laurens and July v. France , 22 October 2007, no  21279/02 and no 36448/02 ; Féret v. Belgium ,  16 July 2009, no 15615/07 and Perinçek v. Switzerland , 15 October 2015, no 27510/08.  See also Vejdeland  and others v. Sweden , 9 February 2012, no 1813/07 .  36 Magyar Tartalomszolgáltatók Egyesülete and Index.hu Zrt v. Hungary , 2 February 2016, no 22947/13.  
Council of Europe study   Algorithms and Human Rights   22   and to avoid excessive burdens being placed on providers of electronic communication  networks and systems.37   Public concern in Europe and the U.S. has grown following the U.S. elections in 2016 with  respect to the dissemination of misinformation via fabricated, intentionally false and  misleading news (so -called ” fake news ”), including through automated techniques and on  social media platforms, thereby possibly having significant influence over democratic  decision -making processes (see also below 8.).38 As a result, there have been renewed  calls for traditional media responsibility standards to be applied to social media  platforms . Some scholars have likened Facebook to be acting as a “news editor [that]  has editorial responsibility for its trending topics” (Helberger and Trilling 2016) .  The  question follows, whether social media platform s, through their algorithms that rank and  curate third -party submissions, exert a form of editorial control traditionally performed  by media professionals and therefore engage  specific media responsibilities.39   4. FREEDOM OF ASSEMBLY AND ASSOCIATION   The internet and in particular social networking services are vital tool s for the exercise  and enjoyment of the right to freedom of assembly and association , offer ing great  possibilities for enhancing the potential for participation of individuals in political, social  and cultural life.40 The freedom of individuals to use internet platforms, such as social  media, to establish associations and to organise themselves for purposes of peaceful                                              37 See Joint Opinion of the Venice Commission, the Directorate of information society and action against crime  and of the Directorate of Human Rights (DHR) of the Directorate General of Human Rights and Rule of Law  (DGI) of the Council of Europe on the Draft Law n° 281 amending and completing Moldovan Legislation on the  so-called "Mandate of security", adopted by the Venice Commission at its 110th Plenary Session (Venice, 10 -11  March 2017), available at: http://www.venice.coe.int/webforms/documents/default.aspx?pdffile=CDL AD(2017)009 -e (last visited on 25 September 2017) .  38 See for example The Power of Big Data and Psychographics, available at:  https://www.youtube.com/watch?v=n8Dd5aVXLCc  (last visited on 25 September 2017)  or Das Magazin,  Hannes Grassegger und Mikael Krogerus, “Ich habe nur gezeigt, dass es die Bombe gibt“, no 48, 3 December  2016, available at https://www.dasmagazin.ch/2016/12/03/ich -habe-nur-gezeigt -dass-es-die-bombe -gibt/,  (last visited on 25 September 2017)  although the exact role of the techn iques used by Cambridge Analytica and  others during the U.S. elections is heavily disputed.   39 See also Reuters institute, Emma Goodman, “Editors vs algorithms: who do you want choosing your news?”,  available at: http://reutersinstitute.politics.ox.ac.uk/news/editors -vs-algorithms -who-do-you-want-choosing your-news (last visited on 25 September 2017) , and the Code of Conduct on countering i llegal hate speech  online, of 31 May 2016 between the EU and Facebook, Microsoft, Twitter and You Tube. See also The Guardian,  “2016: the year Facebook became the bad guy”, available at:  https://www.theguardian.com/technology/2016/dec/12/facebook -2016-problems -fake-news-censorship  (last  visited on 25 September 2017) .   40 See Recommendation CM/Rec(2012)4 of the Committee of Ministers to member States on the protection of  human rights with regard to social networking services.  
Council of Europe study   Algorithms and Human Rights   23   assembly , including protest,  in line with Article 11 of the ECHR has equally been  emphasised.41 Around the globe, social media and their algorithmically advanced  dissemination and networking potential have been suggested to play a promi nent role in  organising and motivating activists and protestors.42  In line with Article 11, any restriction to the right to freedom of peaceful assembly and to  freedom of association must be prescribed by law, pursue a legitimate aim and b e  necessary in a democratic society.   The operation of algorithms on social media platforms  and the vast amount of personally  identifiable information on individuals that is available may of course also be used to  track and identify human beings and may lead to the automatic sorting out of certain  individuals or groups from calls for assemblies, which could  have a significant negative  impact on the freedom of assembly . Profiling and crowd control of protesters does not  just take place on the internet, but  also extends to off -line data -based crowd control  methods. Theoretically, algorithms used to predict possible conflict and protest situations  could also be used as pre -emptive tool to prevent demonstrations or protests by  arresting certain individuals bef ore they even gather.43  5. EFFECTIVE REMEDY   Article 13 of the ECHR  stipulates  that everyone whose rights have been violated shall  have an effective remedy before a national authority.  The available remedy should be  effective in practice and in law.  States  must therefore ensure  that individuals have access  to judicial or other  procedures that can impartially decide on their claims concerning  violations of human rights online , including effective non -judicial mechanisms,  administrative or other means for seek ing remedy such as through national human rights   institutions. As primary responsible entity  for all rights contained in the ECHR , states  must take appropriate steps to protect against  human rights violations, including by                                              41 See Recommendation CM/Rec(2016)5 of the Committee of Ministers to member States on Internet fre edom  and Recommendation CM/Rec(2014)6 of the Committee of Ministers to member States on a Guide to human  rights for Internet users.   42 See, among others, Pablo Barberá and Megan Metzger, “Tweeting the Revolution: Social Media Use and the  #Euromaidan Protest s”, available at: http://www.huffingtonpost.com/pablo -barbera/tweeting -the-revolution s_b_4831104.html  ((last visited on 25 September 2017) . See also Zeyne p Tufekci, Twitter and Tear Gas: The  Power and Fragility of Networked Protest, Yale University Press, 2017.   43 See Tim de Chant, “The Inevitability of Predicting the Future”, available at:  http://www.pbs.org/wgbh/nova/next/tech/predicting -the-future/  (last visited on 25 September 2017) . 
Council of Europe study   Algorithms and Human Rights   24   privat e-sector actors , and must ensure  within their jurisdiction  that those  affected hav e  access to an effective remedy. Th is includes ensuring that private -sector actors respect  human rights throughout their  operations, in particular by establishing effective  complaint mechanisms that pr omptly remedy the grievances of individuals .  Automated decision -making processes lend themselves to particular challenges for  individuals’ ability to obtain effective remedy. These include the opaqueness of the  decision itself, its basis, and whether the i ndividuals have consented to the use of their  data in making this decision, or are even aware of the decision affecting them. The  difficulty in assigning responsibility for the decision also complicates individuals’  understanding of whom to turn to address  the decision. The nature of decisions being  made automatic, without or with little human input, and with a primacy placed on  efficiency rather than human -contextual thinking, means that there is an even larger  burden on the organisations employing such sy stems to provide affected individuals with  a way to obtain remedy.    The wide variety of sectors in which automated decision -making systems are employed  can have serious repercussions on human rights, whether related to health treatments,  job opportunities , predictive policing or otherwise, rendering the capability to obtain  effective remedy in each of these even more essential.   An increasing number of companies, especially larger ones, use algorithms and  automated data processing techniques for running the ir complaints procedures. In the  context of automated  content removal process es on social media platforms  (see above  3.), the use of algorithms  is particularly evident in the response s that different types of  content receive  and how content is prioriti sed, a process that is evidently automated. The  same is true for the threshold of user complaints that is required before a piece of  content is reviewed. There are strong suggestions that the complete response  system s of  internet platforms such as Facebook,  Google or Microsoft to user queries are automated  for many types of inquiries and complaints  (Wagner 2016b; Zhang, Stalla -Bourdillon, and  Gilbert 2016) . Often, many users will need to complain about a spe cific type of content  before an automated algorithm identifies it as relevant to be referr ed to a h uman  operator for content review . These operators are reported to be working often under 
Council of Europe study   Algorithms and Human Rights   25   considerable time pressure and with minimal instructions , in line with internal “deletion  rules”.44   The right to an effective remedy implies the right to a reasoned and individual decision .  Historically , all such decisions have been taken by human beings who, in the exercise of  their functions , based on comprehensive training  and in line with the applicable decision making processes , have been granted a margi n of discretion.  In principle, it is a judge ,  government minister  or administrative official ’s task to decide , in accordance with the  criteria and case-law developed by the Court,  how the balancing of individual rights, such  as the freedom of expression an d the protection from violence  or the protection of the  rights of others , shall be put into practice . The decision must be based on a careful  analysis of the specific context , taking into consideration the “chilling effect” that the   interference may entail and considering the proportionality of the interference . Today  however, it is increasingly algorithmic  data processing techniques  that are preparing and  influencing decision -making in complaints procedures .   In addition, serious co ncerns exist as to whether automatic response processes to  complaints constitute an effective remedy.  While the famous removal of a YouTube  video  on a European Parliament debate related to torture was reinstated after only few hours,  following a n MEP  compl aint, who even received a public apology from Google, there are  considerable doubts as to whether all complaints are treated with such attentiveness.45  Rather, algorithms often obscure access to a reasoned explanation as to why certain  steps were taken in a  particular case .    In all cases, the right to an effective remedy demands that access to an escalated  system of dispute resolution is provided. While the first step may be operated through  automated means, there must be a possibility to complain against t he outcome to a  higher internal review mechanism . If the complainant is not satisfied with the outcome,  he must have the possibility to challenge it through judicial remedy, in line with Article  13 of the European Convention.46  However  there is some suggestions that  a judicial  redress mechanism alone is insufficient and that there is a need for government                                               44 See Süddeutsche Zeitung, Till Krause and Hannes Grassegger, “Inside Facebook”, available at:  http://international.sueddeutsche.de/post/154513473995/inside -facebook  (last visited on 25 September 2017) .  45 See Marietje Schaake, “When You Tube took dow n my video”, available at:  https://www.marietjeschaake.eu/en/when -youtube -took-down -my-video  (last visited on 25 September 2017) .  46 See, among others, O’Keefe v. Ireland , 28 January 2014, no 35810/09.    
Council of Europe study   Algorithms and Human Rights   26   “supervision of collaborative negotiations bet ween consumers and corporations” (Loo,  2016) .  With respect to the right to privacy, automated techniques and algorithms fa cilitate  forms of secret surveillance and “data -veillance” that are impossible for the affected  individual to know about. The European Court of Human Rights has underlined that the  absence of notification at any point undermines the effectiveness of remedi es against  such measures .47   6. PROHIBITION OF DISCRIMINATION   Another key human right that is frequently cited in relation to the operation of algorithms  and other automated processing techniques is the right to enjoy all human rights and  fundamental freedoms without discrimination.    In terms of speed  and volume of data processed , algorithmic decision -making can have  considerable advantages over certain types of human decision -making. However,  algorithms may well have inbuilt biases  that may be hard to  detect and/or correct   (Sandvig et al. 2016) . This is particularl y the case when individual variables in big data  algorithms serve as ‘proxies’ for protected categories such as race, gender or age. An  algorithm may choose to discriminate against a group of users which correlates to 80%,  90%, 95%  or even 99% with a varia ble such as race, gender or age , without doing so  100% of the time.   Search algorithms and search engines by definition do not treat all information equally .  While processes used to select and index information may be applied consistently, the  search resul ts will typically be ranked according to perceived relevance. A ccordingly,  different items of information will receive different degrees of visibility depending on  which factors are taken into account by the ranking algorithm  (see also 3).48 As a result  of data aggregation and profiling, search  algorithms and search engines rank the  advertisement of smaller companies that are registered in less affluent neighbourhoods  lower than those of large entities , which may put them at a commercial disadvantage.  Search engines and search algorithms also do not treat all users equally . Different users                                              47 See Roman Zakharov v. Russia , 4 December 2015, no 47143/06.   48 The algorithm may also – deliberately or not – be impacted by a variety of external factors, which may relate  to business models, legal constraints (e.g. copyright)  or other contextual factors.  
Council of Europe study   Algorithms and Human Rights   27   may be presented with different results , on the basis of behavioural or other profiles ,  including personal risk profiles that may be developed for the purpose of insu rance or  credit scoring or more generally for differential pricing , i.e., offering different prices for  the same goods or services to different consumers based on their profile  (see above  2.).49   A biased algorithm that systematically discriminates one grou p in society , for example   based on their age, sexual  orientation,  race, gender or socio -economic standing, may  raise considerable concerns not just in terms of the access to rights of the individual  endusers or customer s affected by these decisions, but also for societ y as a whole.  Some  authors have even suggest ed that online services which use personalised rating systems  are inherently likely to lead to discriminatory practices (Rosenblat et al. 2016) . It can be  argued as a result that individuals should have the right to view an ‘unbiased’ and not  personally targeted version of their search results. This can be seen as a way for an  individual to exit their own ‘filter bubble’ and see an untargeted version of the search  content, social media timeline or other internet -based service or product that they are  using.  In the ory, algorithms could be useful tools to reduce bias in places where it is  common , such as in hiring processes.  Yet, experts have warned that automation and  machine learning have the potential to reinforce existing biases because, unlike humans,  algorithms  may be unequipped to consciously counteract learned biases. 50  One potentially helpful consideration to discern whether algorithms promote or prevent  discriminatory treatment is to refer to the legal distinction between direct and indirect  discrimination. Direct discrimination occurs where a decision -maker bases her decision  directly on criteria or factors which are regarded as unlawful ( such as race, ethnicity,  religion, gender, sexual orientation, age, or disability). Frequently these unlawful biases  are made sub -consciously and on the basis of information which is external to the  dataset which should form the basis of the decision -making (for example , an interviewer  noticing the age or racial origin of the person standing in fr ont of her). Arguably  algorithm -based systems are better at excluding such direct biases. Indirect                                              49 See also relevant provisions in the EU Regulation 2016/679 related to profiling and automated data  processing and the rights of the data subject.   50 See, for instance, The Guardian, “AI programs exhibit racial and gender bias es, research reveals”, available  at: https://www.theguardian.com/technology/2017/apr/13/ai -programs -exhibit -racist -and-sexist -biasesresearch -reveals  (last visited on 25 September 2017) ; and The Guardian, “How algorithms rule our working  lives”, available at: https://www.theguardian.com/science/2016/sep/01/how -algorithms -rule-our-working -lives  (last visited on 25 September 2017) . 
Council of Europe study   Algorithms and Human Rights   28   discrimination occurs where a certain characteristic or factor occurs more frequently in  the population groups against whom it is unlawful to discriminate ( such as a person with  a certain racial or ethnic background living in a certain geographical area; women having  fewer pensionable years because of career breaks). Since algorithmic decision -making  systems may be based on correlation between data sets and efficiency consideration s,  there is a danger that such systems perpetuate or exacerbate indirect discrimination  through stereotyping. Indirect discrimination is only present where differential treatment  cannot be justified.   When using algorithmic decision -making systems it is th erefore important to seek to  prevent unjustified differential treatments and to design systems accordingly. In  particular , differential treatment will be unjustified and unlawful where it relies on biased  data to generate a risk assessment . In that case , the decision itself is not directly but  indirectly discriminatory,  as it relies on data and information which may be , for instance ,  racially biased. An example for this is where the criminal system uses risk assessment  tools to decide whether a person shoul d be granted bail. This system generate s risk  profiles  that are  based on police data , such as the number or re-arrests for the same  offence.  The fact of  re-arrests, however, ma y be the consequence of direct discrimination  (racial bias) .51 If algorithmic dec ision-making systems are based on previous human  decisions , it is likely that the same biases which potentially undermine the human  decision -making are replicated  and multiplied in the algorithmic decision -making  systems, only that they are then more diffi cult to identify and correct.     7. SOCIAL RIGHTS AND ACCESS TO PUBLIC SERVICES   The workplace is another key area wher e automated decision -making has become  increasingly common in recent years. Algorithms may be involved in decisions on both  hiring and firing staff , staff organi sation and management , as well as the individual  evaluation s of employees . Automated feedback loops , sometimes linked to customer  input,  may decide over the performance evaluation of staff (Kocher and Hensel 2016) .                                              51 See Laurel Eckhouse, “Big data may be reinforcing racial bias in the criminal justice system”, available at:  https://www.washingtonpost.com/opinions/big -data-may-be-reinforcing -racial-bias-in-the-criminal -justice system/2017/02/10/d63de518 -ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73  (last  visited on 25 September 2017) ; and ProPublica, Angwin, Julia, Surya Mattu, and Lauren Kirchner, “Machine  Bias: There’s Softwar e Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks”,  2016, available at: https://www.propublica.org/article/machine -bias-risk-assessments -in-criminal -sentencing   (last visited on 25 September 2017) . 
Council of Europe study   Algorithms and Human Rights   29   These decision -making processes are by no means perfect when humans conduct  them.  Bias related to race  (Bertrand and Mu llainathan 2004)  class and gender  (Altonji and  Blank 1999; Goldin and Rouse 1997)  has been demonstrated  repeate dly in human  resources management practices and processes . With more and more companies moving  towards algorithmic recruitment methods (Rosenb lat, Kneese, and others 2014) ,  however, new concerns related to the lack of transparency in the decisions they make,  both in the hiring process and beyond , have been raised . Moreover many of these  automated decision -making processes are based on data received via internet  platforms .  Allowing the ‘wisdom of the crowd’ to make decisions about individuals ’ employment is  not only highly  questionable  from an ethical point of view , it also limits the ability of  workers to contest such decisions as they seem t o be an  ‘objective’ measures of their  performance  (Tufekci et al. 2015) .   As individual employment platforms are “ transforming  people into Human Computation ,”  (Irani 2015:227)  questions arise about wo rkers’ rights, employee self -determination and  how societies as a whole believe that human beings should be treated at the workplace.52  Notably the increased automation in the workplace also raises considerable challenges in  relation to privacy  rights  (Hendrickx and van Bever 2013)  of employees and how th ey  can be safeguarded in the workplace. As more and more systems are automated and  more and more data is collected at the workplace, employees ’ rights under Article 8 are  in danger even if they are not directly targeted by general  data collection measures  (see  above 2.) Finally, there are additional challenges related to the us e of algorithms by both  public and private sector organisations to monitor staff communications  or to conduct  internal “rankings” of employees that may not be part of the formal evaluation process  but possibly more decisive with respect to individual career opportunities . Such practices  are typically employed to ensure that staff  represent well either a company or a  bureaucracy and have evident implications for the freedom of expression of the  employees (Voorhoof and Humblet 2013)  and their human rights under Article 10 of the  Convention  (see above 3.).  Government agencies and services are increasingly automating their decision -making  with the use of algorithms  (van Haastert 2016) . While it is heavily debated whether such  systems can increase efficiency or not , what is evident is that the operation of such                                              52 See F. Dorssemont, K. Lörcher and I. Schömann (eds.), The European Convention of Human Rights and the  Employment Relation,  Hart Publishing, Oxford, 2013.  
Council of Europe study   Algorithms and Human Rights   30   systems pose s considerable questions for transparency and accountability of public  decision -making , which must be held to a hi gher standard than the private or non -profit  sector . At present t he public sector  in Europe  is employing automated  decision -making in  areas as diverse as social security, taxation, health care and the justice system  (van  Haastert 2016; Tufekci et al. 2015) . There is considerable danger of social sorting in  medical data  as algorithms can sort out specific citizen groups or human profiles , thereby  possibly preventing their access to social services. Another  example relates to the  practice of profiling the unemployed , which was analysed by researchers in an effort to  assess the social and political implications of algorithmic decision -making  associated with  social benefits  (Jędrzej Niklas, Karolina Sztandar -Sztanderska, and Katarzyna  Szymielewicz 2015) . This analysis identified several challenges  which are relevant also  for the us e of algorithms in other areas of public sector service delivery , such as non transparent and algorithmic rules being applied in the distribution of public services and  computational shortcomings triggering arbitrary deci sions, for instance, with respect to  receipt of social benefits.   8. THE RIGHT TO FREE ELECTIONS   The operation of algorithms and automated recommender systems  that may create ‘filter  bubbles’ - fully-automated echo chambers in which individuals only see pieces of  information that confirm their own opinions or match their profile (Bozdag 2013; Pariser  2011; Zuckerman 2013)  - can have momentous effects for democratic processes in  society. While the actual impact of ‘filter bubbles’ and targeted misinformation on the  formation of political opinion is difficult to determine  accura tely,53 fully-automated echo  chambers pose the danger of creating “ideological bubbles”  (O’Callaghan et al. 2015) ,  that may be re latively easy to enter  but hard to exit (Salamatian 2014) . This may have  crucial effects in particular in the context of elections.    While it has been argued s ince the advent of the internet  that online campaigning and  social media networks were l ikely to change the way in which po litics and elections were  run, it  is only  more recently that academic research has revealed the extent to which the                                              53 See Nguyen, Tien T., Pik -Mai Hui, F.Maxwell Harper, Loren Terveen, and Joseph A. Konstan. 2014. ‘Exploring  the Filter Bubble: The Effect of Using Recommender Systems on Content Diversity’. Pp. 677 –686 in Proceedings  of the 23rd International Conference on World Wide Web, W WW ’14. New York, NY, USA: ACM (available at  http://doi.acm.org/10.1145/2566486.2568012)  and Zuiderveen Borgesius, Frederik J. et al. 2016. ‘Should We  Worry About Filter Bubbles?’ Internet Policy Review. Journal on Internet Regulation 5(1). Retrieved 1  September 2016, available at: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2758126 . 
Council of Europe study   Algorithms and Human Rights   31   curation and manipulation of online content on social media platforms may ‘tip’ elections.  During U.S. el ections, r esearchers reportedly manipulated the Facebook platform  to  influence users voting behaviour by telling them how their friends had said they had  voted , without users ’ knowledge , and were able to convince a statistically significant  segment of the population to vote  in the congressional mid -term elections on 2  November 2010  (Bond et al. 201 2). 54 There are strong indications that since then  Facebook has been selling related political advertising services to political parties around  the world, with similar behaviour observed  during the UK  local elections in 2016 (Griffin  2016) . Whether Facebook and similar dominant  online platforms may (deliberately or  not) use their power to influence human voting  or not is less the point than the fact that  they – in principle – have the ability to influence elections.   Recent research suggests that elections may be won not by th e candidates with the best  political argument, but by those who use the most efficient technology to manipulate  voters , sometimes  emotionally and irrationally .55 While this may not be an altogether  new phenomenon, it  has certainly increased in scale and effect , leading to a shift in  paradigm that could jeopardise democracy itself . Data that is inconspicuously  amassed,  harvested and stored through algorithmic technologies has been likened to  the new  “currency of power”, as it can directly be employed for the micro -targeting of voters,  possibly with decisi ve effects on elections.  Indeed, less -known candidates may not have  the means to afford the most effective manipulation technologies  that help predict voter  preferences .56 While political advertising on TV is nowadays regulated and there are  impartiality requirements imposed on public broadcasters , no such equivalents exist for                                              54 In an experiment, Facebook researchers showed a graphic to some users in their news feed, indicating how  many of their friends had voted that day and providing a button to click that they had voted as well. Users who  were prompted with news of their frien ds’ voting turned out to be 0.39% more likely to vote than the others,  and their decision had a further effect on the voting behavior of their friends. The researchers concluded that  their single message on Facebook, strategically delivered, increased turn out directly by 60,000 voters, and  thanks to the ripple effect, ultimately caused an additional 340, -000 votes to be cast (amongst an overall 82  million) that day. See Jonathan Zittrain, Engineering an election, Harvard Law Review Forum Vol. 127, 335 –  339 (2014).   55 See also The Guardian, “The great British Brexit robberty: how our democracy was hijacked”, 7 May 2017,  available at: https:// www.theguardian.com/technology/2017/may/07/the -great-british -brexit -robbery hijacked -democracy  (last visited on 25 September 2017) , arguing that the Brexit referendum was decided in  the end by some 600,000 votes, just over 1% of the total of registered vot ers, which had been targeted by a  firm that “introduced mass data -harvesting to its psychological warfare techniques”, bringing together  “psychology, propaganda and technology in this powerful new way”.   56 Hannes Grassegger & Mikael Krogerus, “The Data That  Turned the World Upside Down”, available at:  https://motherboard.vice.com/en_us/article/mg9vvn/how -our-likes-helped -trump -win (last visited on 25  September 20 17). 
Council of Europe study   Algorithms and Human Rights   32   the use of algorithmic predictions of preferences and voter behavior that may have  equally if not more powerful an impact on voters.   In this context, the particular role played by so cial bots in shaping the political and public  debate leading up to elections has been di scussed in particular in the context of the 2016  US elections and the Brexit referendum. Social bots are algorithmically controlled  accounts that emulate the activity o f human users but operate at much higher pace  (e.g., automatically producing content or engaging in social interactions), while  successfully keeping their artificial identity undisclosed. Research  into the extent to which  the p resence of social media bots affect ed political discussion around the 2016 U.S.  Presidential election  suggests that it can negatively affect democratic political discussion  rather than improving it, which in turn can potentially alter public opinion and endanger  the integrity of the e lection  process.57 The right to free elections, as established by  Article 3 of Protocol 1 has been acknowledged by the European Court of Human Rights as  “fundamental principle in a truly democratic political regime.” Importantly , and  as noted  in the Feasibi lity study on the use of Internet in elections  by the Committee of Experts on  Media Pluralism and Transparency of Media Ownership (MSI -MED)  at the Council of  Europe, regulatory challenges related to elections are not due to the rise of  intermediaries but r ather a lack of adequate regulation. As the study n otes the  “most  fundamental, pernicious, and simultaneously difficult to detect implication of the shift to  social media is not the rising power of intermediaries but the inability of regulation to  level th e playing field for political contest and limit the role of money in elections ”.58   9. OTHER POSSIBLE IMPACTS   The above list of specific human rights that may be impacted through the use of  automated processing techniques and algorithms is not exhaustive. It rather aims to  project the most obviously implicated rights that are to a stronger or lesser degree  already  in the public discussion. Human rights and fundamental freedoms are universal,  indivisible, inter -dependent and inter -related . As a result, all human rights and  fundamental freedom are potentially impacted by the use of algorithmic technologies.                                              57 Bessi, Alessandro, and Emilio Ferrara. 2016. Social bots distort the 2016 U.S. Presidential election online  discussion, FIRST MONDAY , Volume 21, no 11, 7 November 2016, available at:  http://journals.uic.edu/ojs/index.php/fm/article/view/7090/5653   (last visited on 25 September 2017) .  58 See Feasibility Study on the use of internet in electoral campaigns (MSI -MED(2016)10rev (ONCE PUBLIC).  
Council of Europe study   Algorithms and Human Rights   33   Given its  limited scope, this study has not engaged in a discussion of the right to life in  the context of smart weapons and algorithmically operated drones , or in the context of  health and related research . It has further not explored the poss ible effects that the   systematis ation of views and opinions through algorithms may have on the right to hold  opinions an d on the right to freedom of th ought, conscience and religion.   Indeed, the increasing use of automation and algorithmic decision -making in all spheres  of pub lic and private life is threatening to disrupt the very concept of human rights as  protective shields against state interference . The traditional asymmetry of power and  information between state structures and human beings  is shifting towards an  asymmetry of power and information between operators of algorithms (who may be  public or private) and those who are acted upon and governed.    
Council of Europe study   Algorithms and Human Rights   34   IV. REGULATORY IMPLICATIONS  OF THE USE OF AUTOMA TED  DATA PROCESSING TECH NIQUES AND ALGORITHM S  There is growing concern at the political and public level globally regarding the increased  use of algorithms and automated processing techniques and their considerable impact on  the exercise of human rights. As a result, calls are being made to introduce tighter  control and regulati on.59   Already, there are numerous cases where governments and independent auditors engage  in some form of regulation of algorithmic development, usually before operation is  commenced. The software and data processing systems, including algorithms, used in  ‘slot machines’ in Australia and New Zealand must, by government regulation, be “fair,  secure and auditable” (Woolley et al. 2013) . Developers of such machines are required to  submit their algorithmic systems to regulators before they can be presented to  consumers. The Australian/New Zealand Gaming Machine National Standard in its most  recent revision 10.3 defines in extraordinary technical detail how such machines should  operate. For example the “Nominal Standard Deviation (NSD) of a game must be no  greater than 15” and “the hashing algorithm for the verification of gaming equipment  softw are, firmware and PSDs is the HMAC -SHA1 algorithm” .60 Gambling equipment in the  United Kingdom is controlled by a specific licensing regime and, at EU level, regulatory  technical standards have been adopted specifying the organisational requirements of  investment firms engaged in algorithmic trading.61 Section 28b of the German Federal  Law on Data Protection provides that there has to be a scientifically proven  mathematical -statistical process for the calculation of the probability of a specific  behaviour of an individual before such an algorithm can be used for making a decision  about a contract.62                                              59 See, for instance, the vote on 26  January 2016 in the French National Assembly for a new  Bill on digital  rights. The Bill includes provisions relating to algorithmic transparency and the duty of ‘loyalty’, or fairness, of  online platforms and algorithmic decision -making” (Rosnay 2016) .  60 The Australian/New Zealand Gaming Machine National Standard which is available at the following link:  https://publications.qld.gov.au/dataset/a -nz-gaming -machine -national -standards  (last visited on 25 September  2017) .  61 See http://ec.europa.eu/finance/securities/docs/isd/mifid/rts/160719 -rts-6_en.pdf  (last visited on 25  September 2017) .    62 See German Federal Law on Data Protection, promulgated on 14 January 2003 (Federal Law Gazette I p. 66),  and amended by Article 1 of the A ct of 14 August 2009 (Federal Law Gazette I p. 2814), available at:  https://www.gesetze -im-internet.de/bdsg_1990/__28b.html  (last visited on 25 September 2017) . 
Council of Europe study   Algorithms and Human Rights   35   Such licensing systems for algorithms that are used in certain sectors resemble the  quality control and assurance schemes employed in the production and manufacturi ng  industry. They are prepared by relevant experts who know and control the respective  quality standards in the given field. It is doubtful, however, to what extent such  regulatory methods can be exported to the multiple, evolving spheres of public and  private life in which automated data processing techniques and algorithms are operated.  The British Police Child Exploitation and Online Protection Centre demanded, for instance,  that their ‘Facebook button’ be provided by default to all Internet users (Wagner 2016b) .  While this attempt to pressure Facebook  into changing its default code on the British  Facebook website was unsuccessful, it suggests what kind of regulatory responses may  be expected if states seek to define the functioning of algorithms on large online  platforms.   Fundamental legal and ethical questions surround the legal personhood of automated  systems such as algorithms that cannot easily be resolved in this report. While not  wishing to exculpate those involved in development, programming and implementation of  autonomous systems, it must be ac knowledged that automation, vast data analysis and  adaptability and self -learning create considerable challenges for accountability of  algorithmic decisions. In February 16, 2017 European Parliament adopted resolution  calling European Commission to develop  legislative proposal for Civil Law Rules on  Robotics. Such proposal is expected to address, amongst other things,  general principles  concerning the development of robotics and artificial intelligence for civil use, ethical  principles, liability issues, in tellectual property rights and the flow of data, safety,  security and other issues.63   Historically, challenges related to automated data processing have been addressed  through data protection legislation. Today, relevant and innovative approaches such as  the introduction of a limited “right to explanation” (Goodman and Flaxman 2016;  Wachter et al. 2016)  and other rights of internet users are also the product of data  protection legislation. However, t here is a significant difference between the right to  privacy and data protection regulation, which is in the end still a governance mechanism  to safeguard privacy and personal data protection rights. Importantly, privacy, as the                                              63 See the European Parl iament resolution of 16 February 2017 with recommendations to the Commission on  Civil Law Rules on Robotics (2015/2103(INL)), available at:  http://www.europarl.europa.eu/sides/getDoc.do?pubRef= -//EP//TEXT+TA+P8 -TA-20170051+0+DOC+XML+V0//EN&language=EN#BKMD -11 (last visited on 25 September 2017) . 
Council of Europe study   Algorithms and Human Rights   36   exercise of other human ri ghts, requires effective enforcement. Some of the greatest  challenges in the area of data protection come from a lack of willingness to provide  sufficient resources to data protection authorities. While it is clear that the challenges  around discrimination  of content or the manipulation of elections go beyond privacy and  data protection and raise fundamental questions on a large set of issues, the expertise of  the data protection community may well be drawn from when attempting to identify  suitable regulato ry responses to algorithmic governance.   It has been suggested that “[t]echnologists think about trust and assurance for computer  systems a bit differently from policymakers, seeking strong formal guarantees or  trustworthy digital evidence that a system wo rks as it is intended to or complies with a  rule or policy objective rather than simple assurances that a piece of software acts in a  certain way” (Kroll et al. 2016) .  This in turn feeds into the wider debate on auditing of algorithms by which ‘zero  knowledge proofs’ could conceivably be generated by algorithms to demonstrate that  they conform to certain properties, without the individu al engaging in the proof being  able to see the actual algorithm (Kroll 201 6). Beyond zero knowledge proofs, new types  of technical accountability may be able to support common human notions of trust and  accountability. They could therefore be used in the future as supportive technological  approaches for establishing trust, tr ansparency, and accountability.   Some states have adopted strategies for the regulation of content by which they require  internet intermediaries to restrict content knowingly relying upon automated means  rather than the flagging by end users . This raises transparency, account ability and  human rights issues .  As attempts at regulation may not only in themselves raise human rights concerns but  may also be problematic in the sense that regulators may not have developed sufficiently  comprehensive expertise to for mulate standards that reflect not only the technological  and engineering perspectives but also legal and ethical considerations, efforts towards  promoting greater transparency and accountability surrounding the use of algorithms  
Council of Europe study   Algorithms and Human Rights   37   seem more appropriate initi al steps than direct regulation.64 Such standards would also  need to be combined with high -level technology neutral regulations.   While regulatory restraint is therefore warranted at this stage of the implementation of  algorithms and automated processing techniques, their implications for human rights and  ethical considerations must be carefully examined. In particular, the current a cademic  discourse has centred on concepts such as human autonomy and individual agency, both  related to the right to privacy and informational autonomy but not congruent with  privacy. Therefore, autonomy and agency should be considered separately. They ref er to  the human capability to set one’s own goals and the human capability to make decisions  and exercise discretion and as such may conflict with the use of algorithms and  automated processing techniques. This may mean that human rights may have to be  extended or reinterpreted to protect individual autonomy and agency.   1. TRANSPARENCY   Algorithms are often viewed as black boxes by both consumers and regulators alike  (Pasquale 2015) . Demands for more algorithmic transparency have thus been growing in  public and political debate,65 including government requests to companies regarding  algorithms that should be reviewed by independent auditors, regulators or the general   public (Diakopoulos 2015; Rosnay 2016)  before their implementation.66   Importantly , these challenges exist not just for professionals who develop algorithms but  also for other groups such as ‘data scientists’ who u se them. Different levels of  abstraction and complexity prompt distinct challenges of opacity and transparency. It has  been frequently argued that much of the usage of algorithms in machine learning takes  places without “understanding” causal relationships  (correlation instead of causation),  which may lead to bias and errors and raise concerns about data quality (O’Neil 2016) .                                              64 For further examples see Chapter 5 of Pasquale, Frank. 2015. The Black  Box Society: The Secret Algorithms  That Control Money and Information . Harvard University Press.   65 Angela Merkel, for instance, has called on major internet platforms to divulge information on their algorithms  as internet users had a right to know on what  basis the information they received via search engines was  channeled to them. See The Guardian, “Angela Merkel: internet search engines are 'distorting perception'”,  available at: https://www.theguardian.com/world/2016/oct/27/angela -merkel -internet -search -engines -aredistorting -our-perception  (last visited on 25 September 2017) .  66 See also Tufekci et al note: “a common ethical c oncern about algorithmic decision -making is the opaque  nature of many algorithms. When algorithms are employed to make straightforward decisions, such as in the  case of medical diagnostics or aviation, a lack of transparency raises important questions of a ccountability”  (Tufekci et al. 2015:11) .  
Council of Europe study   Algorithms and Human Rights   38   The challenge, however, relates also to the way human beings use, perceive and  interpret their resu lts. The belief that computer algorithms produce neutral unbiased  results (Chun 2006)  without any form of politics (Denardis 2008)  is at the heart of this  problem. Accordingly, it would be more helpful to ensure more critical engagement in  public debates about algorithms than to attem pt to change them.   The provision of entire algorithms or the underlying software code to the public is an  unlikely solution in this context, as private companies regard their algorithm as key  proprietary software that is protected.67 However, there may be a  possibility of  demanding that key subsets of information about the algorithms be provided to the  public, for example which variables are in use, which goals the algorithms are being  optimised for, the training data and average values and standard deviatio ns of the  results produced, or the amount and type of data being processed by the algorithm.   Key in this context is not the provision of all data imaginable, but rather the notion of  “effective transparency”. The underlying goal of increasing transparency must actually be  met by the data disclosed, which implies that the demand for ‘more data’ may  not always  be helpful but may, in the worst case, even serve to counteract the goal of enhancing  transparency.   Effective transparency of automated systems is complicated, however, by the frequent  changes in the algorithms that are used. Google, for examp le, changes its algorithm  hundreds of times per year (Tufekci et al. 2015) . There is also the danger of  manipulation and ‘gami ng’ of algorithms if they are made public. Moreover, machine  learning techniques complicate transparency to a point where provision of all of the  source codes of an algorithm may not even be sufficient. Instead, there is a need for an  actual explanation of  how the results of an algorithm were produced. Since algorithms  may actively obscure that a consequential decision is taken, transparency promotion  measures may also be targeted at the decision -making process itself, given that  algorithms cannot meaningfu lly be studied outside of their social and organisational  context.                                               67 In a decision of 28 January 2014, the German F ederal Supreme Court (Bundesgerichtshof) rejected a claim  for information concerning a credit agency’s algorithm as it was a protected business secret. It, however,  allowed a claim for information concerning the data used to calculate creditworthiness thro ugh the means of  the algorithm; see German Federal Supreme Court (Bundesgerichtshof), Judgment, 28 January 2014 Az. VI ZR  156/13, available at: https://openjur.de/u/677956.html . 
Council of Europe study   Algorithms and Human Rights   39   Transparency enhancement measures, finally, may not only facilitate scrutiny by the  public but also by independent experts, commissions or specialised agencies which, in  turn, may support efforts to promote compliance with consumer protection and human  rights standards.   2. ACCOUNTABILITY   Accountability is the principle that a person who is legally or politically responsible for  harm has to provide some form of justification or compensation. However someone can  only be accountable if they have a degree of control in the sense that they have  facilitated or caused the harm or are in a position to prevent or mitigate it. Legally,  accountability manifests itself through the concept of liability to provide a remedy (such  as damages). The law usually imposes liab ility on the person who is in a position to  prevent harm or mitigate a risk (for example through insurance). The allocation of  accountability for algorithmic decision -making is complicated by the fact that frequently it  is not clear who has the necessary d egree of control to be imputed with legal or political  accountability.   One aspect here is that the developer of algorithmic tools may not know their precise  future use and implementation. The person(s) implementing the algorithmic tools for  applications m ay, in turn, not fully understand how the algorithmic tools operate. Are  those developing and programming the algorithm to be held accountable? Some authors  have suggested that algorithmic accountability and regulation are impossible because the  programmer s themselves are unable to predict or fully understand how the algorithm  takes the decisions that it makes (Kroll 2016) . Another avenue to explore is whether  existing product liability regulation should be extended to include software? Or are rather  the public or private actors to be held accountable who purchase the algorith m and  introduce it into their services, even without understanding its operation?   The governance failure in the automobile emissions scandal also exemplifies the wider  challenge of enhancing accountability of algorithms across numerous different sectors.  Whether in the criminal justice, social media, healthcare, insurance or banking sector, to  name just a few examples, each area will need specific regulatory responses to ensure  greater transparency and accountability of automated data -processing and algorit hmic  decision -making systems. Algorithmic accountability must further be safeguarded  through due process and the rule of law. Effective redress mechanisms for individuals  whose rights were infringed by automated decision -making systems are also essential.  
Council of Europe study   Algorithms and Human Rights   40   Such an approach places a challenging duty on the operators of algorithms and  automated data processing techniques, whether public or private, to ensure basic  standards of human rights. These fundamental principles cannot be offset with  arguments of possib le greater efficiency of opaque technological systems (Wagner  2016a) . Similar issues arise in relation to private actors who employ algorithms and  automat ed data processing techniques in their operations, particularly when they are  market -dominant. Owing to the size and scale of their activities, they deliver services  with important public service value which, in turn, may also have an important impact on  the enjoyment of human rights.   The accountability of individuals or companies with respect to the algorithms they  implement depends very much on the nature of the algorithms and their outputs. In  some cases, if the outputs are defamatory, infringe copyright  or raise other legal  concerns, existing governance mechanisms ensure that these kinds of outputs are limited  (Staab, Stalla -Bourdillon , and Carmichael 2016) . However, such mechanisms typically  only regard the outputs of algorithms, but not the algorithms themselves. In fact, there  is a general lack of regulatory frameworks that ensure that algorithms, in the first place,  are programme d to produce results that uphold and protect fundamental values or basic  ethical and societal principles.   This touches upon fundamental ethical questions with respect to the operation of  automated data processing techniques and algorithms that are not add ressed in this  study. How can normative values be reflected in an automated system? Some of the  ethical discussions surrounding the self -driving car provide an insight into the complexity  of the challenge: how should the algorithm decide in the hypothetica l situation where a  likely accident may either threaten the life of a young child or the life of an elderly  person? Does the number of lives possibly at stake play a role? What are “right” or  “wrong” decisions in such a situation, and with what legal conse quence? Who is held  accountable in case a “wrong” decision is taken?   3. ETHICAL FRAMEWORKS AND IMPROVED RISK ASSESSMENT   Aside from direct regulatory mechanisms to influence the code of algorithms, indirect  mechanisms to influence algorithm codes could also  be considered. These address the  production process or the producers of algorithms and attempt to ensure that they are  aware of the legal challenges, ethical dilemmas and human rights concerns that arise  from automated data -processing and decision -making techniques. An instrument to 
Council of Europe study   Algorithms and Human Rights   41   achieve such goal s could consist of standardised professional ethics or forms of licensing  system for data engineers and algorithm designers similar to those that exist for  professions like doctors, lawyers or architects. Anoth er suggestion frequently made is  that existing mechanisms for the management and development processes  of software  could be improved (Spiekermann 2015) . This may particularly concern agile software  development techniques where modularity, temporality and capture pose considerable  challenges for privacy (Gürses and Hoboken 2017)  as well as other human rights  (Mannaro 2008) . As the use of algorithms in decision -making potentially prejudices the  rights of individuals, additional oversight mechanisms could contribute to ensuring that  the algorithm operates in a fair and sustainable manner.    In order to asses s and understand the human rights risks involved with operating  automated decision making systems, companies can exercise human right due diligence.  This can take the form of human rights impact assessments, investigating the concrete  and potential impacts  on individuals that the employment of these systems may have,  whether direct or indirect, and preventing or mitigating harms identified in these  assessments.   There are examples of emerging standards by industry associations such as the IEEE  (Institute of Electrical and Electronics Engineers) on algorithms, transparency, privacy,  bias and more broadly on ethical system design and the Internet Engineering Task Force  (IETF):   •         IEEE P7000: Model Process for Addressing Ethical Concerns During System Des ign  •         IEEE P7001: Transparency of Autonomous Systems   •         IEEE P7002: Data Privacy Process   •         IEEE P7003: Algorithmic Bias Considerations   •         IETF Research into Human Rights Protocol Considerations draft   Other examples of relev ant industry frameworks that could support greater levels of  human rights compliance include the FAT -ML (Fairness, Accountability, Transparency in  machine learning) principles for more accountable algorithms.68                                               68 See http://www.fatml.org/resources/principles -for-accountable -algorithms  (last visited on 25 September  2017) . 
Council of Europe study   Algorithms and Human Rights   42   That we see a frequent use of the word “ethic s” in connection with algorithms among  experts but also in the public debate may be an indicator for a tactical move by some  actors who want to avoid  strict regulation by pointing to non -formal normative concepts.  It may, however, also point to the need fo r deeper reflection about the interplay of  different types of norms and the role and responsibility of various actors in order to  shape the governance structure for algorithmic decision -making and “ethics” as a new  set of applicable meta -norms.     
Council of Europe study   Algorithms and Human Rights   43   V. MAIN F INDINGS AND CONCLUSI ONS   The notion of ‘algorithmic processing and decision -making’ is diversely interpreted and  understood in legal, technological, and social science circles, and again differently  amongst the public. In addition, the field is comparativel y new. Awareness of impacts for  the exercise of human rights and for broader societal development has grown only  recently and is yet to translate into a wider and inclusive public policy debate on possible  regulatory implications.   The authors of this stud y acknowledge that there is far too little information available to  make well -founded decisions on this topic and thus considerable additional research and  analysis is required, including with respect to the characteristics of human decision making process es. As decision -making processes by human beings are not necessarily  “better” than but simply different automated decision -making systems, different kinds or  bias, risk or error are likely to develop in automated decision -making. Thus it needs to be  openly dis cussed what criteria should be developed to measure the quality of automated decision making.   It is highly welcome that there is increasing research on these topics. However, academic  research on its own is insufficient. It is essential to ensure that mem bers of professional  (technological, engineering, legal, media, philosophical and ethical) communities engage  in discussions and debates that must also include the general public. In order to promote  active engagement of human beings and a lively public de bate about an issue that affects  all human beings and communities, adequate media and information literacy promotion  activities should be organised to facilitate the empowerment of the public to critically  understand and deal with the logic and operation o f algorithms. Notably, public entities  and governments must have access to sufficiently comprehensive information to properly  understand algorithmic decision -making systems that are already deeply embedded in  societies across the world. To provide just one  concrete example of this problem, the  automobile emissions scandal demonstrates what can happen when a small piece of  frequently used software is widely implemented without adequate independent regulatory  scrutiny. It is undesirable from a human rights pe rspective that there are powerful  publicly -relevant algorithmic systems that lack a meaningful form of public scrutiny. The  application of a human rights framework is crucial because it goes beyond just ensuring  transparency and accountability, as it ensur es that all rights are effectively considered in  automated decision -making systems such as algorithms. This is no simple task and will  require a combination of further developing industry standards which put human beings  and human rights at the centre of t he technology design process, and effective 
Council of Europe study   Algorithms and Human Rights   44   regulatory measures to ensure that when industry standards fail governments are able to  step in to promote and protect human rights.   Human beings have a right to effectively scrutinise the decisions made by publ ic  authorities. Issues related to algorithmic governance and/or regulation are public policy  prerogatives and should not be left to private actors alone. While these may engage in  voluntary measures to promote transparency and accountability within their o perations,  and while they have a duty of care towards their users and the responsibility to respect  human  rights, the task of devising comprehensive and effective mechanisms for ensuring  algorithmic accountability lies on the states. This is crucial not on ly because of the  important impact of automated data processing techniques and algorithms on the  exercise and enjoyment of human rights, but also because of their capacity to expand,  reinforce and redistribute power, authority and resources in society.   Importantly, there may be areas of societal and human interaction where algorithmic  decision -making systems are not appropriate. Automated data processing and decision making systems should not be relied upon heavily to promote societal development or  resolv e complex new challenges for future generations, as this is likely to do more harm  than good. Therefore it is critical to ensure that in key areas where automation is not  appropriate from a human rights perspective, it does not take place.   It is the view of the authors of this study that the public debate on the multiple human  rights dimensions of algorithms is lagging behind technological evolution and must be  strengthened rapidly to ensure that the human rights and interests of individuals are  effectivel y and sustainably safeguarded in line with the values laid down in the European  Convention and other international treaties. The use of algorithms and other automated  data processing techniques can potentially have positive and negative impacts on the  exercise and enjoyment of human rights. It must be the aim of policy makers to ensure  that these technologies are used in line with the principle of the “primacy of the human  being”,69 and that our increasingly technology -driven societies are designed - first a nd  foremost - with the effective exercise and enjoyment of the rights of all human beings in  mind.                                               69 See also Human rights in the robot age: Challenges arising from t he use of robotics, artificial intelligence, and  virtual and augmented reality , Report by the Rathenau Institut commissioned and funded by the Parliamentary  Assembly of the Council of Europe, adopted by PACE on 28 April 2017.  
Council of Europe study   Algorithms and Human Rights   45   In consequence, this study comes to the following conclusions :  1. Public entities and independent non -state actors should initiate and support  research that hel ps to better understand and respond to the human rights, ethical  and legal implications of algorithmic decision -making. Therefore, they should  support and engage with trans -disciplinary, problem -orientated and evidence based research, as well as the exchan ge of best practices.     2. Public entities should be held responsible for the decisions they take based on  algorithmic processes. The adoption of mechanisms should be encouraged that  enable redress for individuals that are negatively impacted by algorithmical ly  informed decisions. Human rights impact assessments should be conducted before  making use of algorithmic decision -making in all areas of public administration.     3. Technological development s should be monitored closely and  reviewed for  potential negative i mpacts , with particular attention paid to the use of algorithmic  processing techniques during elections and election campaigns. Effective  responses to such negative impacts could include experimental regulatory  approaches on how best to protect rights of others and guarantee regulatory  goals, provided they are accompanied with systematic monitoring of their effects.     4. Public awareness and discourse are crucially important. All available means should  be used to inform and engage the general public  so that us ers are empowered to  critically understand and deal with the logic and operation of algorithms . This can  include but is not limited to information and media literacy campaigns.  Institutions  using algorithmic processes should be encouraged to provide easily  accessible  explanations with respect to the procedures followed by the algorithms and to  how decisions are made. Industries that develop the analytical systems used in  algorithmic decision -making and data collection processes have a particular  responsibil ity to create awareness and understanding, including with respect to  the possible biases that may be induced by the design and use of algorithms.      5. Certification and auditing mechanisms for automated data processing techniques  such as algorithms should be  developed to ensure their compliance with human  rights. Public entities and non-state actors should encourage and promote the  further development of human rights by design and ethical -by-design approaches 
Council of Europe study   Algorithms and Human Rights   46   and the adoption of stronger risk -assessment appro aches in the development of  software.     6. States should not impose a general obligation on i nternet intermediaries to use  automated techniques to monitor information that they transmit , store or give  access to, as such monitoring infringes on users’ privacy a nd has a chilling effect  on the freedom of expression .    7. Public entities  should engage with their own sector -regulators (insurance, credit  reference agencies, banks, e -commerce and others) to develop specific standards  and guidelines to ensure that they are  able to respond to the challenges of the  use of automated decision -making through algorithms and taking into account the  interests of consumers and the general public.     8. Considering the complexity of the field, awareness of the general public –  important as it is – will not suffice. There is an evident need for additional  institution s, networks and spaces where different forms of algorithmic decision making  are analysed and assessed . All relevant stakeholders should engage in  such an endeavour.     9. The Counci l of Europe as the continent’s leading human rights organisation is the  appropriate venue to further explore the impacts on the effective exercise of  human rights of the increasing use of automated data processing and decision making systems  (in particular  algorithms) in public and private spheres. It should  continue its endeavours in this regard with a view to developing appropriate  standards -setting instruments for guidance to member states.                 
Council of Europe study   Algorithms and Human Rights   47   BIBLIOGRAPHY   Altonji, JG and RM Blank. 1999. ‘Race and Gender in the Labor Market’. Pp. 3143 –3259  in Handbook of labor economics. Elsevier B.V. Retrieved  (http://www.sciencedirect.com/science/article/pii/S15734463993003 90).  Andreessen, Marc. 2011. ‘Why Software Is Eating The World’. Wall Street Journal, August  20. Retrieved 1 September 2016  (http://www.wsj.com/articles/SB10001424053111903480904576512250915629460).   Angwin, Julia, Surya Mattu, and Lauren Kirchner. 2016. ‘Machine Bias: There’s Software  Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.’  ProPublica. Retrieved 31 August 2016 (https://www.propublica.org/article/machine -biasrisk-assessments -in-criminal -sentencing).   Bertrand, Marianne and Sendhil Mullainathan. 2004. ‘Are Emily and Greg More  Employable than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination’.  The American Economic Review 94(4):991 –1013.   Bond, Robert M. et al. 2012. ‘A 61 -Million -Person Experime nt in Social Influence and  Political Mobilization’. Nature 489(7415):295 –298.  Bozdag, Engin. 2013. ‘Bias in Algorithmic Filtering and Personalization’. Ethics and  Information Technology 15(3):209 –227.  Bucher, Taina. 2012. ‘Want to Be on the Top? Algorithmi c Power and the Threat of  Invisibility on Facebook’. New Media & Society 1461444812440159.   Bucher, Taina. 2016. ‘The Algorithmic Imaginary: Exploring the Ordinary Affects of  Facebook Algorithms’. Information, Communication & Society  1–15.  Buni, Catherine a nd Soraya Chemaly. 2016. ‘The Secret Rules of the Internet’. The  Verge . Retrieved 9 September 2016  (http://www.theverge.com/2016/4/13/11387934/internet -moderator -history -youtube facebook -reddit -censorship -free-speech).   Chun, Wendy Hui Kyong. 2006. Control and Freedom : Power and Paranoia in the Age of  Fiber Optics . Cambridge Mass.: MIT Press.   Clay, Kelly. 2012. ‘Is Microsoft Spying On SkyDrive Users?’ Forbes. Retrieved 31 August  2016 (http://www.forbes.com/sites/kellyclay/2012/07/19/is -microsoft -spying -onskydrive -users/).   Denardis, Laura. 2008. ‘Architecting Civil Liberties’. in Global Internet Governance  Academic Network Annual Meeting . Hyderabad (Andra Pradesh), India: GIGANET.  Retrieved (http://worldcat.org/oclc/619234880/viewonline).   Denardis, Laura. 20 12. ‘Hidden Levers of Internet Control’. Information, Communication  & Society  (September):37 –41.  Diakopoulos, Nicholas. 2015. ‘Algorithmic Accountability’. Digital Journalism  3(3):398 – 415.  Edwards, Lilian and Veale, Michael. 2017. ‘Slave to the Algorithm? Why a 'Right to an  Explanation' Is Probably Not the Remedy You Are Looking For ’, Duke Law & Technology  Review (Forthcoming), available at:   https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2972855.   Fernández Pérez, Maryant. 2017. ‘Parliamentarians Encour age Online Platforms to Censor  Legal Content’. EDRi. Retrieved 7 June 2017 (https://edri.org/parliamentarians encourage -online -platforms -to-censor -legal-content/).  
Council of Europe study   Algorithms and Human Rights   48   Gillespie, Tarleton. 2014. ‘The Relevance of Algorithms’. Pp. 167 –94 in Media  technologies: Essays on communication, materiality, and society , edited by T. Gillespie,  P. J. Boczkowski, and K. A. Foot. Cambridge Mass.: MIT Press.   Goldin, Claudia and Rouse, Cecilia. 1997. Orchestrating Impartiality: The Impact Of‘  blind’ auditions on Female Musicia ns. National bureau of economic research. Retrieved 9  September 2016 (http://www.nber.org/papers/w5903).   Goodman, Bryce and Seth Flaxman. 2016. ‘European Union Regulations on Algorithmic  Decision -Making and a Right to Explanation’. in 2016 ICML Workshop on  Human  Interpretability in Machine Learning . New York, NY: ArXiv e -prints.   Griffin, Andrew. 2016. ‘How Facebook Is Manipulating You to Vote’. The Independent .  Retrieved 31 August 2016 (http://www.independent.co.uk/life -style/gadgets -andtech/news/uk -electi ons-2016-how-facebook -is-manipulating -you-to-votea7015196.html).   Grindrod, Peter. 2014. Mathematical Underpinnings of Analytics: Theory and Applications  for Data Science in Customer -Facing Industries. Oxford: Oxford Univ. Press.   Gürses, Seda and Joris Hob oken. 2017. ‘Privacy After the Agile Turn’. in The Cambridge  Handbook of Consumer Privacy , edited by Selinger. Retrieved (https://osf.io/ufdvb/).   van Haastert, Hugo. 2016. ‘Government as a Platform: Public Values in the Age of Big  Data’. Oxford Internet In stitute.   Helberger, Natali and Damian Trilling. 2016. ‘Facebook Is a News Editor: The Real Issues  to Be Concerned about’. Media Policy Project . Retrieved 9 September 2016  (http://blogs.lse.ac.uk/mediapolicyproject/2016/05/26/facebook -is-a-news-editor -thereal-issues -to-be-concerned -about/).   Hendrickx, Frank and Aline van Bever. 2013. ‘Article 8 ECHR: Judicial Patterns of  Employment Privacy Protection’. Pp. 183 –208 in The European Convention on Human  Rights and the Employment Relation , edited by F. Dorssemon t, K. Lörcher, and I.  Schömann. Oxford: Hart Publishing.   Hildebrandt, Mireille and Serge Gutwirth. 2008. ‘General Introduction and Overview’. Pp.  1–13 in Profiling the European Citizen. Springer, Dordrecht. Retrieved 26 September  2017 (https://link.springe r.com/chapter/10.1007/978 -1-4020-6914-7_1).   Hoofnagle Chris Jay “Behavioural Advertising: The Offer You Cannot Refuse” (2012) 6  Harvard Policy & Law Review 273 -296.  Irani, L. 2015. ‘Difference and Dependence among Digital Workers: The Case of Amazon  Mechan ical Turk’. South Atlantic Quarterly  114(1):225 –234.  Jędrzej Niklas, Karolina Sztandar -Sztanderska, and Katarzyna Szymielewicz.  2015.Warsaw, Poland: Panoptykon Foundation. Retrieved  (https://en.panoptykon.org/articles/profiling -unemployed -poland -%E2%80%93 -report).   Kim H, Giacomin J and Macredie R (2014) A qualitative study of stakeholders’  perspectives on the social network service environment. International Journal of Human –  Computer Interaction 30(12): 965 –976.  Kitchin, R. and M. Dodge. 2011. Code/Space S oftware and Everyday Life .  Kocher, Eva and Isabell Hensel. 2016. ‘Herausforderungen Des Arbeitsrechts Durch  Digitale Plattformen – Ein Neuer Koordinationsmodus von Erwerbsarbeit’. Neue Zeitschrift  Für Arbeitsrecht  (16/2016):984 –89.  Kroll, Joshua A. et al. 2016. ‘Accountable Algorithms’. Retrieved 1 September 2016  (http://balkin.blogspot.co.at/2016/03/accountable -algorithms.html).  
Council of Europe study   Algorithms and Human Rights   49   Kroll, Joshua A. 2016. ‘Accountable Algorithms (A Provocation)’. Media Policy Project .  Retrieved 9 September 2016  (http://blogs.l se.ac.uk/mediapolicyproject/2016/02/10/accountable -algorithms -aprovocation/).   Lazer, David, Ryan Kennedy, Gary King, and Alessandro Vespignani. 2014. ‘The Parable  of Google Flu: Traps in Big Data Analysis’. Science 343(6176):1203 –5.  Lazer, David and Ryan Kennedy. 2015. What We Can Learn from the Epic Failure of  Google Flu Trends.   Loo, Van. 2016. The Corporation as Courthouse . Rochester, NY: Social Science Research  Network. Retrieved 7 June 2017 (https://papers.ssrn.com/abstract=2872096).   Mannaro, Katiuscia. 2008. ‘Adopting Agile Methodologies in Distributed Software  Development’. Universita` degli Studi di Cagliari, Cagliari. Italy. Retrieved  (http://le.uwpress.org/content/87/2/284.short).   McCarthy, Daniel R. 2011. ‘Open Networks and the Open Door:  American Foreign Policy  and the Narration of the Internet’. Foreign Policy Analysis  7(1):89 –111.  Mueller, Milton. 2010. Networks and States: The Global Politics of Internet Governance .  MIT Press.   Nguyen, Tien T., Pik -Mai Hui, F.Maxwell Harper, Loren Terve en, and Joseph A. Konstan.  2014. ‘Exploring the Filter Bubble: The Effect of Using Recommender Systems on Content  Diversity’. Pp. 677 –686 in Proceedings of the 23rd International Conference on World  Wide Web , WWW ’14 . New York, NY, USA: ACM. Retrieved  (http://doi.acm.org/10.1145/2566486.2568012).   Nikolaos Altreas et al “Predicting judicial decisions of the European Court of Human  Rights: a Natural Language Processing perspective” PeerJ Computer Science Open Access  (Published 24. October 2016)   O’Callaghan, D ., D. Greene, M. Conway, J. Carthy, and P. Cunningham. 2015. ‘Down the  (White) Rabbit Hole: The Extreme Right and Online Recommender Systems’. Social  Science Computer Review Social Science Computer Review  33(4):459 –78.  O’Neil, Cathy. 2016. Weapons of Math Destruction: How Big Data Increases Inequality  and Threatens Democracy . New York: Crown.   Pariser, Eli. 2011. The Filter Bubble: What the Internet Is Hiding from You . New York:  Penguin Press.   Pasquale, Frank. 2015. The Black Box Society: The Secret Algorithms That Control Money  and Information . Harvard University Press.   Pasquale, Frank A. 2016. Platform Neutrality: Enhancing Freedom of Expression in  Spheres of Private Power . Rochester, NY: Social Science Research Network. Retrieved 7  June 2017 (https ://papers.ssrn.com/abstract=2779270).   Perry, Walt L. 2013. Predictive Policing: The Role of Crime Forecasting in Law  Enforcement Operations . Rand Corporation. Retrieved 9 September 2016  (https://books.google.com/books?hl=en&lr=&id=ZdstAQAAQBAJ&oi=fnd&pg=PP 1&dq=P erry,+Walter,+and+Brian+McInnis.+2013.+Predictive+Policing:+The+Role+of+Crime+F orecasting+in+Law+Enforcement+Operations+Santa+Monica,+CA:+RAND.&ots=924yNa 6Vct&sig=N3HnEi1FBr9YyMXV77GsgPbovYc).   Rifkind, Malcolm. 2014. Report on the Intelligence Relati ng to the Murder of Fusilier Lee  Rigby .  Rosenblat, Alex, Tamara Kneese, and others. 2014. ‘Networked Employment  Discrimination’. Open Society Foundations’ Future of Work Commissioned Research 
Council of Europe study   Algorithms and Human Rights   50   Papers . Retrieved 9 September 2016  (http://papers.ssrn.com/sol3/ papers.cfm?abstract_id=2543507).   Rosenblat, Alex, Karen EC Levy, Solon Barocas, and Tim Hwang. 2016. ‘Discriminating  Tastes: Customer Ratings as Vehicles for Bias’. Retrieved 7 June 2017  (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2858946).   Rosnay , Mélanie Dulong de. 2016. ‘Algorithmic Transparency and Platform Loyalty or  Fairness in the French Digital Republic Bill’. Media Policy Project . Retrieved 1 September  2016 (http://blogs.lse.ac.uk/mediapolicyproject/2016/04/22/algorithmic -transparency and-platform -loyalty -or-fairness -in-the-french -digital -republic -bill/).   Rubinstein, Ira, Ronald D. Lee, and Paul M. Schwartz. 2008. Data Mining and Internet  Profiling: Emerging Regulatory and Technological Approaches . Rochester, NY: Social  Science Research Net work. Retrieved 9 September 2016  (http://papers.ssrn.com/abstract=1116728).   Salamatian, Kavé. 2014. ‘From Big Data to Banality of Evil’. Retrieved 9 September 2016  (https://www.oximity.com/article/Vortrag -Big-Data-und-Ethik-1).  Sandvig, Christian, Kevin Ha milton, Karrie Karahalios, and Cedric Langbort. 2016. ‘When  the Algorithm Itself Is a Racist: Diagnosing Ethical Harm in the Basic Components of  Software’. International Journal of Communication  10:19.   Schulz, Wolfgang and Kevin Dankert. 2016. ‘’Governance  by Things’ as a Challenge to  Regulation by Law’. Internet Policy Review  5(2).  Sills, Arthur J. 1970. ‘Automated Data Processing and the Issue of Privacy’. Seton Hall  Law Review  1.  Spiekermann, Sarah. 2015. Ethical IT Innovation: A Value -Based System Desig n  Approach . CRC Press.   Staab, Steffen, Sophie Stalla -Bourdillon, and Laura Carmichael. 2016. ‘Observing and  Recommending from a Social Web with Biases’. arXiv Preprint arXiv:1604.07180 .  Retrieved 9 September 2016 (http://arxiv.org/abs/1604.07180).   Tene, Om er and Jules Polonetsky. 2012. ‘To Track or “Do Not Track”: Advancing  Transparency and Individual Control in Online Behavioral Advertising’. Retrieved 9  September 2016 (http://conservancy.umn.edu/handle/11299/155947).   Toor, Amar. 2016. ‘Automated Systems F ight ISIS Propaganda, but at What Cost?’ The  Verge . Retrieved 9 September 2016  (http://www.theverge.com/2016/9/6/12811680/isis -propaganda -algorithm -facebook twitter -google).   Tufekci, Zeynep, Jillian C. York, Ben Wagner, and Frederike Kaltheuner. 2015. The Ethics  of Algorithms: From Radical Content to Self -Driving Cars . Berlin, Germany: European  University Viadrina. Retrieved (https://cihr.eu/publication -the-ethics -of-algorithms/).   Tufekci, Zeynep, Twitter and Tear Gas: The Power and Fragility of Networked Protest,  Yale University Press, 2017.   Tversky, Amos and Daniel Kahneman. 1974. ‘Judgment under Uncertainty: Heuristics  and Biases’. Science  185(4157):1124 –31.  Urban, Jennifer M., Joe Karaganis, and Brianna L. Schofield. 2016. ‘Notice and Takedown  in Everyd ay Practice’. Available at SSRN 2755628. Retrieved 28 October 2016  (http://papers.ssrn.com/sol3/Papers.cfm?abstract_id=2755628).   Voorhoof, Dirk and P. Humblet, eds. 2013. ‘The Right to Freedom of Expression in the  Workplace under Article 10 ECHR’. Pp. 183 –208 in The European Convention on Human  Rights and the Employment Relation . Oxford: Hart Publishing.  
Council of Europe study   Algorithms and Human Rights   51   Wachter, Sandra, Brent Mittelstadt, and Luciano Floridi. 2016. Why a Right to  Explanation of Automated Decision -Making Does Not Exist in the General Data P rotection  Regulation. Rochester, NY: Social Science Research Network. Retrieved 7 June 2017  (https://papers.ssrn.com/abstract=2903469).   Wagner, Ben. 2016a. ‘Efficiency vs. Accountability?’ Bureau de Helling. Retrieved 10  March 2017 (https://bureaudehelling .nl/artikel -tijdschrift/efficiency -vs-accountability).   Wagner, Ben. 2016b. Global Free Expression: Governing the Boundaries of Internet  Content. Cham, Switzerland: Springer International Publishing.   Williamson, Ben. 2016. ‘Computing Brains: Learning Algori thms and Neurocomputation in  the Smart City’. Information, Communication & Society 0(0):1 –19.  Winner, L. 1980. ‘Do Artifacts Have Politics?’ Daedalus.   Winner, L. 1986. ‘The Whale and the Reactor: A Search for Limits in an Age of High  Technology’.   Woolley, Richard, Charles Livingstone, Kevin Harrigan, and Angela Rintoul. 2013. ‘House  Edge: Hold Percentage and the Cost of EGM Gambling’. International Gambling Studies  13(3):388 –402.  York, Jillian C. 2010. ‘Policing Content in the Quasi -Public Sphere’. Boston, MA: Open Net  Initiative Bulletin. Berkman Center. Harvard University.   Zhang, Pei, Sophie Stalla -Bourdillon, and Lester Gilbert. 2016. ‘A Content -Linking Context Model For “notice -and-Take-Down” procedures’. Pp. 161 –65 in. ACM Press.  Retrieved 9 September 2 016 (http://dl.acm.org/citation.cfm?doid=2908131.2908171).   Zittrain, Jonathan, Engineering an election, Harvard Law Review Forum Vol. 127, 335 –  339 (2014).   Zuckerman, Ethan. 2013. Digital Cosmopolitans: Why We Think the Internet Connects  Us, Why It Doesn’ t, and How to Rewire It. W. W. Norton & Company.   Zuiderveen Borgesius, Frederik J. et al. 2016. ‘Should We Worry About Filter Bubbles?’  Internet Policy Review. Journal on Internet Regulation 5(1). Retrieved 1 September 2016  (http://papers.ssrn.com/sol3/pap ers.cfm?abstract_id=2758126).   
Council of Europe study   Algorithms and Human Rights   52   REFERENCES   I) Council of Europe’s instruments   European Convention on Human Rights (ETS no 5).   Automatic Processing of Personal Data Convention (ETS no 108).   Declaration of the Committee of Ministers on freedom of communica tion on the Internet of 28 May  2003.   Recommendation CM/Rec(2010)13 of the Committee of Ministers to member states on the  protection of individuals with regard to automatic processing of personal data in the context of  profiling, adopted on 23 November 2010 .  Recommendation CM/Rec(2012)3 of the Committee of Ministers to member States on the  protection of human rights with regard to search engines, adopted on 4 April 2012.   Recommendation CM/Rec(2012)4 of the Committee of Ministers to member States on the  prote ction of human rights with regard to social networking services, adopted on 4 April 2012.   Recommendation CM/Rec(2014)6 of the Committee of Ministers to member States on a Guide to  human rights for Internet users, adopted on 26 April 2014.   Recommendation CM /Rec(2016)5 of the Committee of Ministers to member States on Internet  freedom, adopted on 13 April 2016.   Guidelines on the Protection of Individuals with regard to the Processing of Personal Data in a  World of Big Data, 17 January 2017.   Draft Recommendat ion of the Committee of Ministers to Member States on the Roles and  Responsibilities of Internet Intermediaries, finalized by the MSI -NET on 19 September 2017,  available at https://rm.coe.int/draft -recommendation -on-internet -intermediaries -version 4/168075 9e67.   II) European Union Instruments   Directive 2000/31/EC of 8 June 2000 of the European Parliament and of the Council of 8 June 2000  on certain legal aspects of information society services, in particular electronic commerce, in the  Internal Market (“Directive on electronic commerce”).   Regulation (EU) 2016/679 on the protection of natural persons with regard to the processing of  personal data and on the free movement of such data.   Directive (EU) 2016/680 on the protection of natural persons with rega rd to the processing of  personal data by competent authorities for the purposes of the prevention, investigation, detection  or prosecution of criminal offences or the execution of criminal penalties, and on the free  movement of such data establish data pro tection safeguards.   European Parliament resolution of 16 February 2017 with recommendations to the Commission on  Civil Law Rules on Robotics (2015/2103(INL)), available at:  http://www.europarl.europa.eu/sides/getDoc.do?pubRef= -//EP//TEXT+TA+P8 -TA-20170051+0+DOC+XML+V0//EN&language=EN#BKMD -11.     
Council of Europe study   Algorithms and Human Rights   53   III) Newspapers and online articles   The great question of the 21st century: Whose black box do you trust?, available at:  https://www.linkedin.com/pulse/great -question -21st-century -whose -black-box-do-you-trust-tim-oreilly?trk=eml -b2_content_ecosystem_digest -hero-22null&midToken=AQGexvwxq0Q3iQ&fromEmail=fromEmail&ut=2S rYDZ8lkCS7o1 .  Article 19, “Algorithms and Automated Decision -Making in the Content of Crime Prevention:   A Briefing paper”, 2016.   Das Magazin, Hannes Grassegger und Mikael Krogerus, “Ich habe nur gezeigt, dass es die Bombe  gibt“, no 48, 3 December 2016, available at https://www.dasmagazin.ch/2016/12/03/ich -habenur-gezeigt -dass-es-die-bombe -gibt/.   Reuters institute, Emma Goodman, “Editors vs algorithms: who do you want choosing your news?”,  available at: http://reutersinstitute.politics.ox.ac.uk/news/editors -vs-algorithms -who-do-you-wantchoosing -your-news.   GCN, Kevin McCaney, “Prisons turn to analytics software for parole decisions”, 1 November 2013,  available at: https://gcn.com/articles/2013/11/01/prison -analytics -software.aspx .  Stanford news, New Stanford research finds computers are better judges of personality than  friends and family, available at: http://news.stanford. edu/2015/01/12/personality -computer knows -011215/ .   The Guardian, “The great British Brexit robberty: how our democracy was hijacked”, 7 May 2017,  available at: https://www.theguardian.com/technology/2017/may/07/the -great -british -brexit robbery -hijacked -democracy .  Roheeni Saxena, arstecnica, “The social media “echo chamber” is real”, available at:  https://arstechnica.com/science/2017/03/the -social -media -echo-chamber -is-real/.  Article 19, “Algorithms and automated decision -making in the context of crime prevention”,   2 December 2016, available at:     https://www.article19.org/resources.php/resource/38579/en/algorithms -and-automated -decision making -in-the-context -of-crime -prevention .   Joseph Menn, Dustin Volz, Reuters, Exclusive: Google, “Facebook quietly move toward automatic  blocking of extremist videos”, available at: http://www.reuters.com/article/us -internet -extremism video -exclusive -idUSKCN0ZB00M .  The Guardian, “2016: the year Facebook became the bad guy”, available at:  https://www.theguardian.com/technology/2016/dec/12/facebook -2016-problems -fake-newscensorship .  Pablo Barberá and Megan Metzger, “Tweeting the Revolution: Social Media Use and the  #Euromaidan Protests”, available at: http://www.huffingtonpost.com/pablo -barbera/tweeting -therevolution -s_b_4831104.html .  Tim de Chant, “The Inevitability of Predicting the Future”, available at:  http://www.pbs.org/wgbh/nova/next/tech/predicting -the-future/ .  Till Krause and Hannes Grassegger, Süddeutsche Zeitung, “Inside Facebook”, available at:  http://international.sueddeutsche.de/post/154513473995/inside -facebook .  Marietje Schaake, “When You Tube took down my video”, available at:  https://www.marietjeschaake.eu/en/when -youtube -took-down -my-video . 
Council of Europe study   Algorithms and Human Rights   54   The Guardian, “AI programs exhibit racial and gender biases, research reveals”, available at:  https://www.theguardian.com/technology/2017/apr/13/ai -programs -exhibit -racist -and-sexist biases -research -reveals .  The Guardian, “How algorithms rule our working lives”, available at:  https://www.theguardian.com/science/2016/sep/01/how -algorithms -rule-our-working -lives.  Laurel Eckhouse, “Big data may be reinforcing racial bias in the crim inal justice system”, available  at: https://www .washingtonpost.com/opinions/big -data-may-be-reinforcing -racial -bias-in-thecriminal -justice -system/2017/02/10/d63de518 -ee3a-11e6-9973c5efb7ccfb0d_story.html?utm_term=.720084735d73 .  ProPublica, “Machine Bias”, available at: https://www.propublica.org/article/machine -bias-riskassessments -in-criminal -sentencing .  Hannes Grassegger & Mikael Krogerus, “The Data That Turned the World Upside Down”, available  at: https://motherboard.vice.com/en_us/article/mg9vvn/how -our-likes-helped -trump -win.   Alessandro Bessi and Emilio Ferrara, “Social bots distort the 2016 U.S. P residential election online  discussion”, FIRST MONDAY, Volume 21, Number 11, 7 November 2016, available at:  http://journals.uic.edu/ojs/index.php/fm/article/view/7090/5653 .  The Guardian, “Angela Merkel: internet search engines are 'distorting perception'”, available at:  https://www.theguardian.com/wor ld/2016/oct/27/angela -merkel -internet -search -engines -aredistorting -our-perception .  IV) Miscellaneous   UNESCO’s World Trends in Freedom of Expression and Media Development Publication, 2014,  available at: http://www.unesco.org/new/en/world -media -trends .  UK Intelligence and Security Committee of Parliament report, Privacy and Security: A modern and  transparent legal framework, March 2015, availabl e at: http://isc.independent.gov.uk/committee reports/special -reports .  EC Communication From The Commission To The European Parliament, The European Council And  The Council del ivering on the European Agenda on Security to fight against terrorism and pave the  way towards an effective and genuine Security Union, available at: https://ec.europa.eu/home affairs/sites/homeaffairs/files/what -we-do/policies/european -agenda -security/legislative documents/docs/20160420/communication_eas _progress_since_april_2015_en.pdf .  2016 Report of the UN Special Rapporteur on the promotion and protection of the right to freedom  of opinion and expression , David Kaye, to the Thirty -second session of the Human Rights Council  (A/HRC/32/ 38).   Joint Opinion of the Venice Commission, the Directorate of information society and action against  crime and of the Directorate of Human Rights (DHR) of the Directorate General of Human Rights  and Rule of Law (DGI) of the Council of Europe on the Draft Law n° 281 amending and completing  Moldovan Legislation on the so -called "Mandate of security", adopted by the Venice Commission at  its 110th Plenary Session (Venice, 10 -11 March 2017)  http://www.venice.coe.int/webforms/documents/default.aspx?pdffile=CDL -AD(2017)009 -e.  U.N. Human Rights Council Resolution on the Right to Privacy in the Digital Age, U.N. Doc.  A/HRC/34/7, 23 Mar. 2017.   Human rights in the robot age : Challenges arising from the use of robotics, artificial intelligence,  and virtual and augmented reality , Report by the Rathenau Institut commissioned and funded by  the Parliamentary Assembly of the Council of Europe, adopted by PACE on 28 April 2017.   
      
       
