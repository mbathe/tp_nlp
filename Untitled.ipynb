{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8ea734f-c31a-4dc4-a360-f10a5528b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import spacy\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import textacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9a1f74b-d838-411b-baea-a23f494ffd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processing:\n",
    "    def __init__(self, log_file=None, corpus_dir=\"preprocessed\", corpus_file=\"corpus.txt\", mean_score = False, n_grams= 2, min_n_gram_freq =2):\n",
    "        self.log_file = log_file\n",
    "        self.corpus_dir = corpus_dir\n",
    "        self.tokens = set()\n",
    "        self.corpus = \"\"\n",
    "        self.corpus_file = corpus_file\n",
    "        self.tf_idf_dict = []\n",
    "        self.document_token = {}\n",
    "        self.number_of_tokens = 0\n",
    "        self.documents = []\n",
    "        self.vec = None\n",
    "        self.n_grams = n_grams\n",
    "        self.mean_score = mean_score\n",
    "        self.min_n_gram_freq = min_n_gram_freq\n",
    "\n",
    "    def process_corpus(self):\n",
    "        corpus_text = \"\"\n",
    "        for file in tqdm(glob.glob(os.path.join(os.getenv('TXT_FOLDER2'), \"*.txt\"))):\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                corpus_text += \" \"+f.read().strip().lower()\n",
    "\n",
    "        self.documents = sent_tokenize(corpus_text)\n",
    "        #self.corpus = corpus_text\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        document_count = len(self.documents)\n",
    "        token_doc_count = defaultdict(int)\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.tf_idf_dict = [{} for i in range(len(self.documents))]\n",
    "        for i, full_content in enumerate(tqdm(self.documents)):\n",
    "            content = re.sub(\n",
    "                r'(#\\S+|@\\S+|\\S*@\\S*\\s?|http\\S+|word01|word02|word03|[^A-Za-z0-9]\\'\\'|\\d+|<[^>]*>|[^A-Za-z0-9\\'\\- ]+)',\n",
    "                \"\",\n",
    "                full_content\n",
    "            )\n",
    "\n",
    "           \n",
    "            doc = nlp(content)\n",
    "            ngrams = list(textacy.extract.basics.ngrams(doc, self.n_grams, min_freq=self.min_n_gram_freq))\n",
    "            target_pos = {\"VERB\", \"ADJ\", \"NUM\", \"ADP\", \"PROPN\",\n",
    "                          \"PRON\", \"PUNCT\", \"SCONJ\", \"SYM\", \"ADV\", \"SPACE\", \"AUX\", \"CONJ\", \"SYM\", \"PUNCT\", \"SCONJ\"}\n",
    "            target_tags = {\"VB\", \"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\",\n",
    "                           \"CD\", \"PRP\", \"PRP$\", \"DT\", \"IN\", \"CC\", \"UH\", \"SYM\", \".\"}\n",
    "            tokens = {token.lemma_ for token in doc if len(\n",
    "                token.text) > 3 and token.text not in stop_words and token.pos_ not in target_pos and token.tag_ not in target_tags}\n",
    "            tokens.update(set([str(ngram) for ngram in  ngrams]))\n",
    "            self.tokens.update(tokens)\n",
    "            number_content_words = len(full_content.split())+len(set(ngrams))\n",
    "            for token in tokens:\n",
    "                token_doc_count[token] += 1\n",
    "                self.tf_idf_dict[i][token] = (full_content.count(\n",
    "                    token) / number_content_words if number_content_words > 1 else 0)\n",
    "\n",
    "        for i in tqdm(range(document_count)):\n",
    "            for token in self.tf_idf_dict[i]:\n",
    "                self.tf_idf_dict[i][token] *= np.log(\n",
    "                    document_count / (token_doc_count[token] + 1))\n",
    "                # self.vec[i, tokens_index[token]] += self.tf_idf_dict[i][token]\n",
    "    # .....\n",
    "    def get_key_words(self):\n",
    "        dictionnaire = dict(\n",
    "            zip(self.tokens, np.zeros(len(self.tokens))))\n",
    "        token_count_doc = dict(zip(self.tokens, np.ones(len(self.tokens))))\n",
    "        for i in tqdm(range(len(self.tf_idf_dict))):\n",
    "            for k in self.tf_idf_dict[i].keys():\n",
    "                dictionnaire[k] += self.tf_idf_dict[i][k]\n",
    "                if self.mean_score :\n",
    "                    token_count_doc[k] += 1\n",
    "        if self.mean_score:\n",
    "            for k in tqdm(list(token_count_doc.keys())):\n",
    "                dictionnaire[k] = dictionnaire[k]/token_count_doc[k]\n",
    "        dictionnaire = dict(sorted(dictionnaire.items(),\n",
    "                                   key=lambda item: item[1], reverse=True))\n",
    "        return dictionnaire\n",
    "\n",
    "    def compare_tokens_found(self):\n",
    "        # Lire le corpus\n",
    "        with open(self.corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            corpus_text = f.read().strip().lower()\n",
    "\n",
    "        r = Rake()\n",
    "\n",
    "        # To get keyword phrases ranked highest to lowest.\n",
    "        corpus_text = re.sub(\n",
    "            r'(#\\S+|@\\S+|\\S*@\\S*\\s?|http\\S+|word01|word02|word03|[^A-Za-z0-9]\\'\\'|\\d+|<[^>]*>|[^A-Za-z0-9\\' ]+)',\n",
    "            \"\",\n",
    "            corpus_text\n",
    "        )\n",
    "        r.extract_keywords_from_text(corpus_text)\n",
    "        return r.get_ranked_phrases_with_scores()\n",
    "\n",
    "    def tfidf_filter(self):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        for i, d in enumerate(tqdm(self.documents)):\n",
    "            tokens = word_tokenize(d)\n",
    "            tokens = (' ').join([t.lower() for t in tokens\n",
    "                                 if len(t) >= 3\n",
    "                                 and (t.isalpha() or t in r\"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\")\n",
    "                                 and t.lower() not in stopwords.words('english')\n",
    "                                 and \"http\" not in t.lower()\n",
    "                                 ])\n",
    "        X = vectorizer.fit_transform(tokens)\n",
    "        tfidf_values = np.array(X.mean(axis=0))[0]\n",
    "        median_tfidf = np.quantile(tfidf_values, 0.5)\n",
    "        mask = tfidf_values > median_tfidf\n",
    "        words_to_keep = vectorizer.get_feature_names_out()[mask]\n",
    "        # print(type(words_to_keep))\n",
    "        return words_to_keep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbe80c73-611d-4f78-93ee-71b0fdebd4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 33.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 32.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 29.7 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e694617d-937e-416b-8197-f559b195af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 570/570 [00:13<00:00, 41.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 511/511 [00:02<00:00, 201.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 511/511 [00:00<00:00, 170481.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 511/511 [00:00<00:00, 511402.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system', 'ethic', 'principle', 'intelligence', 'science', 'directive', 'decision', 'regulation', 'impact', 'approach', 'article', 'mind', 'risk', 'group', 'proposal', 'market', 'machine', 'government', 'justice', 'framework', 'society', 'telegraph', 'technology', 'expert', 'project', 'guideline', 'world', 'level', 'robotic', 'forum', 'challenge', 'autonomy', 'right', 'rule', 'machinery', 'question', 'service', 'state', 'process', 'philosophy', 'provider', 'scope', 'union', 'limitation', 'assessment', 'corea', 'recommendation', 'member', 'nature', 'sector', 'development', 'workshop', 'initiative', 'research', 'information', 'consequence', 'chair', 'purpose', 'term', 'set', 'obligation', 'policy', 'design', 'legislation', 'taddeo', 'luciano', 'practice', 'safety', 'document', 'requirement', 'cowl', 'objective', 'material', 'future', 'report', 'learning', 'introduction', 'culture', 'analysis', 'organization', 'issue', 'method', 'knowledge', 'basis', 'deciphering', 'dream', 'outcome', 'stakeholder', 'factor', 'context', 'paper', 'automation', 'protection', 'medium', 'force', 'decolonial', 'review', 'enforcement', 'statement', 'client', 'area', 'problem', 'art', 'result', 'way', 'model', 'reform', 'governance', 'audit', 'trail', 'contribution', 'policymaker', 'foundation', 'disclosure', 'evaluation', 'relationship', 'fairness', 'eaar', 'accountability', 'individual']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessing = Processing(\n",
    "log_file=\"\", corpus_dir=\"processed\", corpus_file=\"corpus.txt\")\n",
    "preprocessing.process_corpus()\n",
    "\n",
    "keys_words = preprocessing.get_key_words()\n",
    "if \"also\" in set(stopwords.words('english')):\n",
    "    print(\"aloso\" in set(stopwords.words('english')))\n",
    "# print(len(list(keys_words.keys())))\n",
    "print(list(keys_words.keys())[0:120])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c8ba9-542d-4d0a-b2cd-1218a289c8dc",
   "metadata": {},
   "source": [
    "### Implémentation de la méthode en divisant par le nombre de documents dans lequel il apparait,\n",
    "\n",
    "On constate que cette technique retourne des mots très rares. Cela peut s'expliquer par le fait qu'un mot apparaît dans très peu de documents. En calculant le ratio de la somme de son score TF-IDF par le nombre de documents dans lesquels il apparaît, on obtient un résultat très élevé. Ce mot est donc mis en évidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d281fc09-599d-4458-93b7-0fe7f719cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngcontent 4.542229002734101\n",
      "litmap 4.414530935901386\n",
      "paladyn 4.222301584150378\n",
      "endorse 4.041503537231809\n",
      "hendriks 4.040462550407026\n",
      "bage 3.984206023875873\n",
      "lesechos 3.984206023875873\n",
      "dglimage 3.8490509878398185\n",
      "chem 3.8490509878398185\n",
      "derputte 3.8490509878398185\n"
     ]
    }
   ],
   "source": [
    "for key in list(keys_words.keys())[:10]:\n",
    "    print(key,keys_words[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89937a7a-f459-4bea-b895-6487f29db042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system 2950.632441690472\n",
      "data 2165.4595208888486\n",
      "intelligence 2038.3984110036374\n",
      "right 1973.3538455389094\n",
      "decision 1578.9246841086167\n",
      "research 1564.7310221563682\n",
      "risk 1533.6798612161735\n",
      "information 1527.487334856244\n",
      "model 1477.6240036188656\n",
      "ethic 1431.0339169624265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key in list(keys_words.keys())[:10]:\n",
    "    print(key,keys_words[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e800a-ede8-4cd0-b04a-ed39f1774edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29ea79-8ec5-48bd-a597-aa191f406a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
