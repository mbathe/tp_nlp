nist special publication towards standard identifying managing bias artificial intelligence reva schwartz apostol vassilev kristen greene lori perine andrew burt patrick hall publication available free charge nist special publication reva schwartz national institute standards technology information technology laboratory apostol vassilev national institute standards technology information technology laboratory computer security division kristen greene national institute standards technology information technology laboratory information access division lori perine national institute standards technology information technology laboratory university maryland andrew burt patrick hall publication available free charge department commerce gina raimondo secretary national institute standards technology james olthoff performing functions duties secretary commerce standards technology director national institute standards technologytowards standard identifying managing bias artificial intelligence march certain commercial entities equipment materials may identified document order describe experimental procedure concept adequately identification intended imply recommendation endorsement national institute standards technology intended imply entities materials equipment necessarily best available purpose national institute standards technology special publication natl inst stand technol spec publ pages march coden publication available free charge executive summary individuals communities interact environment increasingly virtual often vulnerable commodification digital footprint concepts behavior ambiguous nature captured environment quantified used categorize sort recommend make decisions people lives many organizations seek utilize information responsible manner biases remain endemic across technology processes lead harmful impacts regardless intent harmful outcomes even inadvertent create significant challenges cultivating public trust artificial intelligence many approaches ensuring technology use every day safe secure factors specific require new perspectives systems often placed contexts impact whether impact helpful harmful fundamental question area trustworthy responsible harmful impacts stemming individual enterprise level able ripple broader society scale damage speed perpetrated applications extension large machine learning model across domains industries requires concerted effort fig challenge managing biascurrent attempts addressing harmful effects bias remain focused computational factors representativeness datasets fairness machine learning algorithms remedies vital mitigating bias work remains yet illustrated fig human systemic institutional societal factors significant sources bias well currently overlooked successfully meeting challenge require taking forms bias account means expanding perspective beyond machine learning pipeline recognize investigate technology created within impacts society trustworthy responsible whether given system biased fair ethical whether claimed many practices exist responsibly producing importance transparency datasets test evaluation validation verification tevv overstated human factors participatory design techniques approaches also important mitigating risks related bias however none practices individually concert panacea bias brings set pitfalls missing current remedies guidance broader socio perspective connects practices societal values experts area trustworthy responsible counsel successfully manage risks bias must operationalize values create new norms around built deployed document work national institute standards technology nist area bias based perspective intent document surface salient issues challenging area bias provide first step roadmap developing detailed guidance identifying managing bias specifically special publication describes stakes challenge bias artificial intelligence provides examples chip away public trust identifies three categories bias systemic statistical human describes contribute harms describes three broad challenges mitigating bias datasets testing evaluation human factors introduces preliminary guidance addressing bias neither new unique possible achieve zero risk bias system nist intends develop methods increasing assurance governance practice improvements identifying understanding measuring managing reducing bias reach goal techniques needed flexible applied across contexts regardless industry easily communicated different stakeholder groups contribute growth burgeoning topic area nist continue work measuring evaluating computational biases seeks create hub evaluating factors include development formal guidance standards supporting standards development activities workshops public comment periods draft documents ongoing discussion topics stakeholder community key words bias trustworthiness safety lifecycle development acknowledgments authors wish thank everyone responded call submitted comments draft version paper received comments suggested references essential improving paper future direction work also want thank many people assisted updating document including nist colleagues reviewers took time provide constructive feedback thank kyle fox insightful comments discussions invaluable input audience intended primary audience document includes individuals groups responsible designing developing deploying evaluating governing systems document informed motivated segments public experience potential harm inequities due bias systems affected biases newly introduced amplified systems background document result extensive literature review conversations experts areas bias fairness systems workshop public comments draft derived public comments integrated throughout document overview analysis themes public comments work publication include development formal guidance assessing managing risks bias series public workshops discuss topics stakeholder community build consensus trademark information trademarks registered trademarks belong respective organizations nist special publications national institute standards technology nist promotes innovation industrial competitiveness advancing measurement science standards technology ways enhance economic security improve quality life among broad range activities nist contributes research standards evaluations data required advance development use assurance trustworthy artificial intelligence information workshop see comments available information resources found nist bias webpage located information technology laboratory itl nist develops tests test methods reference data proof concept implementations technical analyses advance development productive use information technology itl responsibilities include development management administrative technical physical standards guidelines special publication focuses addressing managing risks associated bias design development use one series documents workshops related nist risk management framework rmf intended advance trustworthiness technologies documents rmf series publication provides reference information technical guidance terminology processes procedures test evaluation validation verification tevv practical nist may serve informative reference guidance remains voluntary content document reflects recommended practices document intended serve supersede existing regulations laws mandatory guidance term practice guide guide guidance like context paper informative reference intended voluntary use interpreted equal use term guidance legal regulatory document establish legal standard legal requirement defense law force effect law read document section lays purpose scope nist work bias section describes three categories bias may occur commission design development deployment technologies used generate predictions recommendations decisions use algorithmic decision systems systems may impact individuals communities create broader societal harms section describes challenge bias related three core areas datasets test evaluation validation verification human factors provides general guidance managing bias areas document uses terms technology system applications interchangeably terms related machine learning pipeline model algorithm also used document interchangeably depending context term system used may refer broader organizational social ecosystem within technology designed developed deployed used instead traditional use related computational hardware software important reading notes document includes series vignettes shown red callout boxes help exemplify bias reduce public trust interesting highlighted blue callout boxes important takeaways shown framed text terms displayed small caps text defined lossary clicking word shown small caps model takes reader directly definition term glossary one may click page number shown end definition return contents purpose scope bias context terminology characterizing bias contexts addressing bias categories bias bias contributes harms systems approach updated lifecycle bias challenges guidance counted datasets bias dataset challenges dataset guidance know right tevv considerations bias tevv challenges tevv guidance makes decisions make human factors bias human factors challenges human factors guidance manage provide oversight governance bias governance guidance conclusions glossary list figures fig challenge managing bias fig categories bias leaf node terms subcategory picture hyperlinked lossary clicking bring definition glossary return click current page number printed right glossary definition fig development lifecycle fig output system altered background content fig biases contribute harms fig design process iso fig design process systems purpose scope august fulfilling assignment executive order released plan federal engagement developing technical standards related tools based broad public private sector input plan recommended deeper consistent engagement standards help united states speed pace reliable robust trustworthy technology nist research continues along path focus measure evaluate enhance trustworthiness systems responsible practices designing developing deploying systems working community nist identified following technical characteristics needed cultivate trust systems accuracy explainability interpretability privacy reliability robustness safety security harmful biases mitigated controlled significant potential transformative technology also poses inherent risks since trust risk closely related nist work area trustworthy responsible centers around development voluntary risk management framework rmf unique challenges require deeper understanding risks differ domains nist rmf intended address risks design development use evaluation products services systems tasks recommendation diagnosis pattern recognition automated planning decisionmaking framework intended enable development use ways increase trustworthiness advance usefulness address potential harms nist leveraging approach creating maintaining actionable practice guides via rmf broadly adoptable risk management risk management seeks minimize anticipated emergent negative impacts systems including threats civil liberties rights one risks bias bias exists many forms omnipresent society become ingrained automated systems help make decisions lives bias always negative phenomenon certain biases exhibited models systems perpetuate amplify negative impacts individuals organizations society biases also indirectly reduce public trust shortage examples bias aspect technology use caused harm negatively impacted lives hiring health care criminal justice indeed many instances deployment technologies accompanied concerns whether societal biases perpetuated amplified public perspectives depending application americans likely unaware order fed reg interacting enabled technology however general view needs higher ethical standard forms technology mainly stems perceptions fears loss control privacy bias tightly associated concepts transparency fairness society much public assumptions underlying algorithms rarely transparent complex web code decisions went design development deployment rarely easily accessible understandable audiences nevertheless many people affected data used inputs technologies systems without consent apply college new apartment search internet individuals feel fairly judged applying jobs loans reduce public trust technology end user presented information online stigmatizes based race age gender accurately perceive identity causes harm consumers impacted price gouging practices resulting application even used make decisions directly affecting individual bias context terminology purposes publication term artificial intelligence refers large class systems receive signals environment take actions affect environment generating outputs content predictions recommendations classifications decisions influencing environments interact among outputs machine learning refers specifically field study gives computers ability learn without explicitly programmed computer programs utilize data learn apply patterns discern statistical relationships common approaches include limited regression random forests support vector machines artificial neural networks programs may may used make predictions future events programs also may used create input additional programs includes within scope holds great promise convenience automated classification discovery within large datasets come significant downsides individuals society amplification existing biases bias introduced purposefully inadvertently system emerge used application types bias purposeful beneficial example systems underlie applications often model implicit biases intent creating positive experiences online shopping identifying content interest proliferation recommender systems modeling predictive approaches also helped expose many negative social biases baked processes reduce public trust neither built deployed vacuum sealed societal realities discrimination unfair practices understanding system acknowledges processes used develop technology mathematical computational constructs approach takes account values behavior modeled datasets humans interact complex organizational factors commission design development ultimate deployment characterizing bias contexts addressing bias statistical context technical systems bias commonly understood treated statistical phenomenon bias effect deprives statistical result representativeness systematically distorting distinct random error may distort one occasion balances average international organization standardization iso defines bias generally degree reference value deviates truth context system said biased exhibits systematically inaccurate behavior statistical perspective sufficiently encompass communicate full spectrum risks posed bias systems legal context section developed response public comments stakeholder feedback noted discussion bias could divorced treatment bias legal system relates laws regulations addressing discrimination fairness especially areas consumer finance housing currently uniformly applied approach among regulators courts measuring impermissible bias areas impermissible discriminatory bias generally defined courts either consisting disparate treatment broadly defined decision treats individual less favorably similarly situated individuals protected characteristic race sex trait disparate impact broadly defined facially neutral policy practice disproportionately harms group based protected section presented legal guidance rather reminder developers deployers users must cognizant legal considerations work particularly regard bias testing section provides basic background understanding many ways bias treated federal laws relates disparate impact courts regulators utilized considered acceptable various statistical tests evaluate evidence disparate impact traditional methods statistical bias testing look differences predictions across protected classes race sex particular courts looked statistical significance testing assess whether challenged practice likely caused disparity result chance nondiscriminatory laws federal state even municipal levels focus preventing discrimination host areas see title vii civil rights act regarding discrimination basis sex religion race color national origin employment equal credit opportunity act focused broadly discrimination finance fair housing act focused discrimination housing americans disabilities act focused discrimination related disabilities among others federal agencies including equal employment opportunity commission federal trade commission department justice office federal contract compliance programs responsible enforcement interpretation laws analysis section intended serve fully comprehensive discussion law interpreted courts enforced regulatory agencies rather provide initial overview respectively uniform guidelines employment selection procedures ugesp state selection rate race sex ethnic group less eighty percent rate group important note however tests used measure bias applied uniformly within legal context particular federal circuit courts split whether require plaintiff demonstrate statistical practical significance make case disparate impact decisions expressly rejected practical significance tests recent years others continued endorse utility split illustrates legal context provides several examples bias fairness quantified adjudicated last several decades relevant standards still evolving also important note critical differences exist traditional disparate impact analyses described illegal discrimination relates people disabilities particularly americans disabilities act ada claims ada frequently construed screen rather disparate impact claims screen may occur individual disability performs poorly evaluation assessment otherwise unable meet employer job requirements disability individual loses job opportunity result addition ada prohibition denial reasonable accommodation example may require employer change processes procedures enable particular individual disability apply job perform job enjoy benefits privileges employment protections particularly important systems testing algorithm bias determining whether groups perform equally well may fail detect certain kinds bias likewise eliminating group discrepancies necessarily prevent screen need reasonable accommodation systems cognitive societal context teams involved system design development bring cognitive biases individual group process bias prevalent assumptions data used models developed system placed required systemic biases institutional level affect organizations teams structured controls decision making processes individual group heuristics biases throughout lifecycle described section decisions made end users downstream decision makers policy makers also impacted biases reflect limited points view lead biased outcomes biases impacting human decision making usually implicit unconscious therefore unable easily controlled mitigated assumption biases remedied human control awareness recipe success highest rate generally regarded federal enforcement agencies evidence adverse categories bias based previous academic work classify bias discussions thought leaders field possible identify three dominant categories bias threeway categorization helps expand understanding bias beyond computational realm defining describing systemic human biases present within build new approaches analyzing managing mitigating bias begin understand biases interact correspondingly fig presents three categories bias definitions terms found lossary list biases exhaustive constitutes prominent risks vulnerabilities consider designing developing deploying evaluating using auditing applications systemic systemic biases result procedures practices particular institutions operate ways result certain social groups advantaged favored others disadvantaged devalued need result conscious prejudice discrimination rather majority following existing rules norms institutional racism sexism common examples systemic bias occurs infrastructures daily living developed using universal design principles thus limiting hindering accessibility persons disabilities systemic bias also referred institutional historical bias biases present datasets used institutional norms practices processes across lifecycle broader culture society see ignette examples category bias often selected based either recently published papers topic seminal work within domain term associated multiple definitions identified relevant definition selected adapted references provided intended indicate specific endorsement assign originator credit systemic bias gender identification beyond personal identity human faces encode number conspicuous traits nonverbal expression indicators sexual attraction selection emotion facial recognition technology frt used many types applications including gender identification compares morphological distances faces classify human faces gender degree sexual dimorphism men women appears vary age ethnic group consequence accuracy frt gender identification vary respect age ethnic group prepubescent male faces frequently misclassified female older female faces progressively misclassified male studies highlighted human preferences sexually dimorphic faces may evolutionarily novel one study found differing levels facial sexual dimorphism samples taken countries located europe south america africa buolamwini gebru examined accuracy commercial technologies using skin types proxy ethnic group found lower accuracy particularly among female faces training data based limited sample group results lower accuracy categorizing members group degree sexual monomorphism dimorphism within group also affects accuracy additional biases occur due lack awareness multiplicity gender systemic bias human bias statistical computational biashistorical societal institutional selection sampling use interpretationprocessing group data generation detection ecological fallacy evaluation exclusion measurement popularity population representation simpson paradox temporal concept drift emergent content production data dredging feedback loop inherited error propagation model selection funding deployment sunk cost interpretation rashomon effect principle selective adherence streetlight effect annotator reporting human reporting presentation complacency consumer mode confusion cognitive anchoring availability heuristic confirmation effect implicit loss situational awareness user interaction fig categories bias leaf node terms subcategory picture hyperlinked lossary clicking bring definition glossary return click current page number printed right glossary definition statistical computational statistical computational biases stem errors result sample representative population biases arise systematic opposed random error occur absence prejudice partiality discriminatory intent systems biases present datasets algorithmic processes used development applications often arise algorithms trained one type data extrapolate beyond data error may due heterogeneous data representation complex data simpler mathematical representations wrong data algorithmic biases treatment outliers data cleaning imputation factors human human biases reflect systematic errors human thought based limited number heuristic principles predicting values simpler judgmental operations biases often implicit tend relate individual group perceives information automated output make decision fill missing unknown information biases omnipresent institutional group individual decision making processes across lifecycle use applications deployed wide variety human biases cognitive perceptual biases show domains unique human interactions rather fundamental part human mind entire field study centered around biases heuristics thinking behavioral economics example research investigates phenomena anchoring bias availability heuristic bias confirmation bias framing effects among many others noted heuristics adaptive mental shortcuts helpful allowing complexity reduction tasks judgement choice yet also lead cognitive biases human heuristics biases implicit simply increasing awareness bias ensure control focus broader examples human bias space bias contributes harms technology based tighter connections broader impacts society traditional software applications utilize often deployed across sectors contexts role replace humans human processes decisions example hiring technologies models underlie replace hiring processes implemented sector seeks automate recruiting employment pipeline yet models tend exhibit unexpectedly poor behavior deployed real world domains without constraints supplied human operators contradictions cause considerable concern large language models foundation models due considerable epistemic aleatoric uncertainty described section factors methods capturing poor performance harmful impacts results models currently imprecise values systems able model complex phenomena whether capable learning operating line societal values remains area considerable research concern systemic implicit biases racism forms discrimination inadvertently manifest data used training well institutional policies practices underlying commissioned developed deployed used human cognitive perceptual biases enter engineering modeling processes inability properly validate model performance leaves biases exposed deployment biases collide cognitive biases individuals interacting systems users experts loop decision makers teams develop deploy often inaccurate expectations technology used human oversight accomplish especially deployed outside original intent left unaddressed biases accompanying contextual factors combine complex pernicious mixture biases negatively impact individuals society amplifying reinforcing discrimination speed scale far beyond traditional discriminatory practices result implicit human institutional biases racism sexism ageism ableism systems approach likely due expectations based lack mature process governance organizations often default overly technical solutions bias issues yet mathematical computational approaches adequately capture societal impact systems limitations perspective addressing bias become evident systems increasingly expand lives reviewed literature suggests expansion many aspects public life requires extending view mainly technical perspective one sociotechnical nature considers within larger social system operates using approach bias makes possible evaluate dynamic systems bias understand impact conditions biases attenuated amplified adopting sociotechnical perspective enable broader understanding impacts key decisions happen throughout beyond whether technology even solution given task problem reframing factors datasets tevv participatory design practices sociotechnical lens means understanding functions society power impact society approach also enables analytic approaches take account needs individuals groups society computational technologies evolved increasing tendency believe technical solutions alone sufficient addressing complex problems may social political ecological economic ethical dimensions approach often termed technosolutionism assumes right code algorithm applied problem ignores minimizes relevance human organizational societal values behaviors inform design deployment use technology context systems promotes viewpoint narrow effectively address bias risks one control example used model risk management mitigate establish document review anticipated value system approaches emerging area identifying measurement techniques take factors consideration require broad set disciplines stakeholders identifying contextual requirements evaluating systems necessary developing scientifically supportable guidelines meet requirements core focus bias extends beyond computational algorithms models datasets upon built assumptions decisions made within processes used develop technology key factors well technology used interpreted deployed idea quantitative measures better objective observations known cnamara fallacy fallacy related concept technochauvinism center many issues related algorithmic bias traditional approaches attempt turn ambiguity context human subjectivity categorical observations objectively measurable quantities based numerical mathematical models representations process enables modeling also inadvertently creates new challenges systems representing complex human phenomena mathematical models comes cost disentangling context necessary understanding individual societal impact contributes fallacy objectivity science made great strides understanding limitations human cognition including humans perceive learn store visual aural textual information make decisions risk yet significant gaps remain thus mathematical attempt model human traits limited incomplete key challenge model causality predicting human interpretation model output without proper governance excising context flattening categories numerical constructs makes traceability difficult finding approaches tevv compensate limitations underlying modeling technology bringing back necessary context important area study updated lifecycle improving trust mitigating managing bias starts identifying structure presents within systems uses organizations design develop technology use lifecycle keep track processes ensure delivery functional necessarily identify harms manage document adapted lifecycle stakeholder intent enable designers developers evaluators deployers relate lifecycles utilized key guidance development approach centers excellence coe general services administration modernization coe organisation economic development organisation economic development another model lifecycle currently development joint technical committee international organization standardization iso international electrotechnical commission iec seeinformation technology artificial intelligence system life cycle processes development lifecycle processes bias categories effectively facilitate identification management academic literature best practice guidelines strongly encourage approach developing applications using lifecycle guidance organizations enable approach described section focuses participatory design methods design test developmentdeployment fig development lifecycleai lifecycles iterative begin stage planning problem specification background research identification data take place decisions include frame problem purpose component general notion problem requiring benefiting central decisions individuals groups makes individuals teams power control early decisions makes reflect systemic biases within organizational settings individual group heuristics limited points view systemic biases also reflected datasets selected within biases affect later stages decisions complex ways lead biased outcomes thedesign development stage typically starts analysis requirements available data based model designed selected compatibility analysis performed ensure potential sources bias identified plans mitigation put place model implementation progresses trained selected data effectiveness bias mitigation evaluated development organization periodically assess completeness bias identification processes well effectiveness mitigation finally end development stage deployment thorough assessment bias mitigation necessary ensure system stays within limits overall model specification must include identified sources bias implemented mitigation techniques related performance assessments model released deployment deployment stage system released used humans begin interact system performance system must monitored reassessed ensure proper function teams engage continuous monitoring detailed policies procedures handle system output behavior system retraining may necessary correct adverse events decommission may necessary since lifecycle iterative numerous opportunities technology development teams carry consultation ensure applications causing unintended effects harms specific guidance governing systems conditions subject section test evaluation stage continuous throughout entire development lifecycle organizations encouraged perform continuous testing evaluation system components features bias contribute harmful impacts example deployment model retrained new data specific context model deployer work model producer assess actual performance bias evaluation engagement encouraged ensure assessment balanced comprehensive deviations desired goals observed findings feed model stage ensure appropriate adjustments made data curation problem formulation proposed changes design model evaluated together new data requirements ensure compatibility identification potential new sources bias another round design implementation commences formulate corresponding requirements new model capabilities features additional datasets stage model developer perform continuous testing evaluation ensure bias mitigation maintains effectiveness new setting model optimized tested performance released deploying organization use documented model specifications test evaluate bias characteristics deployment specific context ideally evaluation performed together stakeholders ensure previously identified problems resolved everyone satisfaction accurate model necessarily one least harmful impact bias challenges guidance review literature various processes including public comments workshops listening sessions nist identified three broad areas present challenges addressing bias first challenge relates dataset factors availability representativeness societal biases second relates issues measurement metrics support testing evaluation validation verification tevv third area broadly comprises issues related human factors including societal historic biases within individuals organizations well challenges related implementing section outlines key challenges associated three areas along recommended guidance must noted tevv amount full application scientific method tevv engineering construct seeks detect remediate problems fashion scientific method compels holistic design thinking rigorous experimental design hypothesis generation hypothesis testing particular anecdotal evidence frequency bias incidents indicate solid experimental design techniques focus structured data collection selection minimization confirmation bias downplayed many projects construct validity particularly important system development development teams able demonstrate application measuring concept intends measure important stakeholders including development teams know evaluate scientific claims said bias mitigants governance processes outlined document show promise interestingly often borrowed practices outside core even technical guidance related improved experimental design rigorous application scientific method none panacea pitfalls nist plans work trustworthy responsible communities explore proposed mitigants governance processes build associated formal technical guidance coming years concert communities challenge bias complex many approaches mitigating challenge quick recommendations document include sampling potentially promising techniques approaches individually concert panacea bias brings strengths weaknesses counted datasets bias dataset challenges design development practices rely large scale datasets drive processes need lead researchers developers practitioners first data adapt questions accordingly creates culture focused datasets available accessible rather dataset might suitable result data used processes may fully representative populations phenomena modeled data collected differ significantly occurs real world example sampling bias occurs data collected responses online questionnaires scraped social media datasets result based samples neither randomized representative population users particular online platform datasets generalizable yet frequently used train applications deployed use broader contexts even though data representing certain societal groups may excluded systemic biases may also manifested form availability bias datasets readily available fully representative target population including proxy data used reused training data disadvantaged groups including indigenous populations women disabled people consistently underrepresented similarly datasets used natural language processing nlp often differ significantly applications lead discrimination systematic gaps performance issues arise due common practice reusing datasets practices datasets may become disconnected social contexts time periods creation scholars beginning examine ethical adverse impact implications using data collected specific time specific purpose uses originally intended decontextualizing data raises questions related privacy consent internal validity model results even datasets representative may still exhibit entrenched historical systemic biases improperly utilize protected attributes utilize culturally contextually unsuitable attributes developers sometimes exclude protected attributes associated social groups historically discriminated however remedy problem since information inadvertently inferred ways proxy latent variables latent variables gender inferred browsing history race inferred zip code models based variables still negatively impact individuals classes individuals thus proxies used development may poor fit concept characteristic seeking measured reveal unintended information persons groups also sensitivity related attributes inferences receive protection civil rights laws may enable discrimination inferred used model low income status alternately sufficient knowledge awareness context process phenomenon attributes collected use application may universally applicable modeling different social groups cultures analyzed using application example using past medical costs predict need future health interventions leads severe healthcare needs groups sufficient access health care african americans protected attributes host laws regulations established prohibit discrimination based grounds race sex age religious affiliation national origin disability status among others local laws apply protections across wide variety groups activities end users start interact system early design development decisions poorly incompletely specified based narrow perspectives exposed leaving process vulnerable additive statistical human biases designing compensate activity biases algorithmic models may built data active users likely creating downstream system activity reflect intended real user population resulting potentially harmful impacts one example considering ads jobs science technology engineering mathematics stem might seen often men due marketing algorithms optimize cost placement women intended audience ads never saw ignette details furthermore feedback loops result disparity amplification marginalized individuals groups less likely use system subsequent training data based frequent users example english speakers less likely use personal assistant people living transit deserts often dependent services experiences groups match intended purpose operation system dataset guidance key question must asked development deployment system datasets exist fit suitable purpose various applications domains tasks system developed deployed predictive behavior system determined data data also largely defines machine learning task question dataset fit suitability requires attention three factors statistical methods mitigating representation issues processes account context application deployed awareness interaction human factors technical system stages lifecycle datasets available set metrics demonstrating fairness many unable reduced concise mathematical definition statistical factors bias problems exacerbated variety statistical biases prevalent large scale datasets used modeling models deployed applications often settings uses harms perpetuated amplified major trend addressing bias focus balanced statistical representation datasets used modeling processes simple effective techniques class imbalance measures label imbalance measures analysis using statistical phenomena impson sparadox used detect bias datasets sometimes help mitigate numerous studies software libraries invoke data rebalancing processes causal models graphs may also used detect direct discrimination data generalized linear models require variables independent little multicollinearity residuals normally distributed homoscedastic furthermore common algorithmic techniques cost functions assume variables unimodal however data often heterogeneous multimodal especially populations disaggregated gender age race income thus important document communicate limitations applicability outputs whether model used benchmarking prediction classification many cases practitioners train models benchmark datasets use real data specific applications however may possible fully address mathematically imbalances representation heterogeneous nature heterogeneous datasets recent study highlighted serious errors commonly used benchmark dataset consequently model trained biased erroneous data may lead biased inaccurate predictions moreover training model one dataset using operate another requires special care account potential differences distributions datasets may exacerbate unfairness errors model accounting factors statistical methods indeed necessary sufficient addressing bias challenges associated datasets modeling processes intent making contextual concepts measurable context removed however difficult get back leading models learn inexact representations building codes designed based general principles designed incorporate specific geographic characteristics region must use datasets applications adapted take full spectrum factors context deployed word embeddings represent text data positions mathematical space representation allows arithmetic measurable comparisons performed words however text data simplified mathematical objects contextual information including homographs idioms fit neatly model may lost asked compute doctor father mother using arithmetic system might respond system answer due historical gender stereotypes professions due natural close association verb nurse mother scenarios even attempts made explicitly remove bias training data biases may still exist deep complex connections within text data attention factors system essential phases lifecycle importantly design development deployment design phase analysis provides insights social variations dynamics characteristics phenomenon help better frame questions analysis enable assessment dataset fit perspective development phase facilitates selection data sources attributes explicitly integrates impact assessment complement algorithmic accuracy studies shown possible mathematically address statistical bias dataset develop algorithm performs high accuracy yet produce outcomes harmful social class diametrically opposed intended purpose system need new ways measure impact systems current theme literature trustworthy responsible research community practice deploying uses systems applied task within social organizational context designed must approached caution especially settings sociotechnical analysis help determine use modification ethically technically feasible cases perspective implicates adopting processes include involving stakeholders examining cultural dynamics norms assessing societal impacts technologies perfectly accurate still contribute harmful outcomes interaction human factors datasets systemic institutional biases captured datasets used build models underlying applications biases compounded decisions assumptions made design development teams datasets use decisions affect gets counted get counted issue flattening societal behavioral factors within datasets problematic often overlooked problem exacerbated variety statistical biases prevalent large scale datasets used modeling human biases whether conditioned socially unconscious cognitive bias factors data selection curation preparation analysis processes person annotates training data example gesture recognition sentiment analysis may impart perception biases person chooses data sources variables leave take may way aligns held belief data typically needs cleaned way removing outliers spurious data missing data may imputed replacing missing values nearest neighbors extrapolated values removed entirely missing data may frequent marginalized populations furthermore compounding collection biases missing spurious data often random data analysis decisions cardinal treatment ordinal data data may lead biased estimator processes documenting potential sources human bias essential often overlooked elements characterizing model transparency explainability addition addressing bias fairness statistical factors analysis incorporating awareness documentation lifecycle helps define limitations ensure ethically socially appropriate uses perpetuate amplify harms see section thorough discussion challenges guidance related human factors bias know right tevv considerations bias tevv challenges delegating algorithms appealing systems produce consistent decisions compared humans however systems work vacuum operational context jurisdiction industry vertical system operates serves frame fairness goals even algorithm relies data training performance tuning turn assessed fairness metric therefore consider computational approaches mitigating bias must take consideration three components together algorithms data fairness metrics systems regularly model concepts partially observable capturable data without direct measures highly complex considerations development teams use proxies create many risks example criminality measurable index construct might created information arrests convictions used proxy variables predicting certain case whether certain individual likely repeat offender algorithmic hiring system might developed using input variables length time prior employment productivity number lost hours measurable proxies lieu directly measurable concept employment algorithm might also include predictor variable distance employment site might correlate employees quitting job due long commutes bad traffic however since distance employment site might disadvantage candidates certain neighborhoods length time prior employment might disadvantage candidates unable find stable transportation relate factors system contribute biased outcomes epistemic aleatoric uncertainty distinguishes two types predictive uncertainty epistemic aleatoric example models produced deep learning systems exhibit epistemic uncer tainty parameters computed model model parameters typically computed result nonconvex minimization appropriately chosen cost function well known mathematics formulation problem unique solution epistemic uncertainty reduced increasing amount representative training data fully eliminated impact behavior deep learning system deployment used data especially mismatch distributions real training data lead undesirable effects many system critical attributes robustness resilience including inducing harmful bias even convex problems multiple linear regression may suffer epistemic uncertainty decision variable included model another inherent type uncertainty associated machine learning aleatoric represents uncertainty inherent data uncertainty label assigning process training dataset aleatoric uncertainty irreducible part predictive uncertainty since two types uncertainties epistemic aleatoric highly changing context may blur difference even cause one turn thus characterization reducible irreducible absolute example datasets containing overlapping samples different attributes could embedded higher dimensions samples clearly separated thus reducing aleatoric uncertainty expense epistemic uncertainty model would likely overfit existing data larger space difficulty distinguishing epistemic aleatoric uncertainty models implicit mathematical representations data trained growth large language models large language model llms become dominant trend deep learning today expected continue grow importance although llms able achieve impressive advances performance number important tasks come significant risks could potentially undermine public trust technology llms create significant challenges epistemic aleatoric uncertainty relying large amounts uncurated web data increases aleatoric uncertainty indepth knowledge data statistical properties critically important detecting bias predictive output models identifying sources bias first step bias mitigation strategy epistemic uncertainty models availability large fast computing resources massive artificial neural networks becoming increasingly common particular language models consist parameter spaces trained hundreds gigabytes data training data often scraped internet sources commonly known gender racial cultural biases alternative approaches language datasets proposed mitigate harmful bias approach may introduce human biases selection datasets beyond systemic selection biases large language models also highlight epistemic uncertainty stochastic gradient descent accelerated methods methods used find set parameters minimize cost function associated model deep neural networks exhibit complicated nonlinearities result many potential local minima manifold may huge unknown number minima furthermore fit parameters computer memory often necessary use numbers introducing rounding error may undermine stability numerical methods result model may demonstrate unknown erratic behavior challenges reproducibility explainability quest fitting larger larger models existing finite computational resources llms rely techniques numerical representations models increase epistemic uncertainty deep learning models ignette early practice shown concerns use llms indeed valid preliminary experimental results showing llms exhibit significant bias reduce risks use llms future work area move towards efforts fully understand characterize behavior devise effective mitigation measures biases bring processes datasets exhibit numerous biases lead harmful impacts feed directly system level processes determine important model systems determine importance effectively categorize sort firehose data downstream recommendations decisions contextual information flattened unobservable phenomena quantified development indices use proxies use data attributes names like criminality hireability creditworthiness similar indicative experimental design problems give rise harmful bias software designers data scientists working design development often highly focused system performance optimization focus inadvertently source bias systems example model development selection modelers almost always select accurate models yet forde describe paper selecting models based solely accuracy necessarily best approach bias reduction furthermore choice model objective function upon model definition accuracy based reflect bias taking context consideration model selection lead biased results example disparities health care delivery relatedly systems designed use aggregated data groups make predictions individual practice initially meant remedy datasets lead biased outcomes bias known ecological fallacy occurs inference made individual based membership within group example predicting college performance risk based individual race unintentional weightings certain factors cause algorithmic results exacerbate reinforce societal inequities natural language processing nlp powerful computational approach allow machines meaningfully understand human spoken written languages powering activities algorithmic search speech translation even conversational text generation nlp able help communicate computer systems carry variety tasks set harms arise use nlp however become recent concern area trustworthy hovy prabhumoye describe five sources bias nlp potential ways counteract spurious correlations speed scope machine learning processes unfortunately expand development systems based questionable scientific underpinnings learn spurious correlations related human characteristics example german public radio outlet examined system purportedly assessed tone voice language gestures facial expressions create personality profile use hiring processes analysis showed system easily manipulated superficial changes inputs fig output system altered background candidates higher scores wore glasses bookshelf background diminishing claims system analyzed human expressions raising concerns shortcut learning indeed many systems attempt make inferences individuals based facial characteristics scientifically supportable propensity committing crimes even sexual orientation basis drawing conclusions emotional state facial characteristics ranges unscientific debunked theories emerging experimental studies presenting concerning challenges systems claim make judgements mechanizing human characteristics systems obfuscate significant uncertainty result harmful biases hiring systems claim glean information candidates audio video shown increase bias outcome decisions may present untenable bias mitigation prediction accuracy systems marketed making predictions based facial expressions often generate decisions based biased experimental design premises spurious patterns learned system shortcut learning cases illustrate risks associated using systems tasks like sentiment affect analysis along using systems infer spurious correlations broadly perpetuate biases across groups several instances scientifically unsound systems consequential sensitive areas built basis spurious correlations provide justification biased outcomes perspective broadens awareness risky computational approaches rise predictive analytics mechanism identifying patterns human behavior recent example process produce biased outcomes therefore used carefully applications highly effective identifying key insights data unable gleaned humans technology also often presented perceived way reduce human cognitive biases make decisions fair objective well defined constrained settings technologies result accurate fair outcomes however assumption systems objective especially high stakes decision making remains unclear categorizing unobservable behavior phenomena leads increased uncertainty system performance measuring whether patterns identified applications real result spurious correlations difficult adding challenge reality systems built placed within organizational settings along accompanying often unstated policies priorities used subject matter experts decision makers implicit heuristics biases fallacy objectivity often surround processes may create conditions technology capacity capabilities oversold see ignette example algorithmic effects algorithmic complexity vary greatly across models number parameters mathematically encode training data may one many one trillion simple models fewer parameters often used tend less expensive build explainable transparent easier implement however models exacerbate statistical biases restrictive assumptions training data often hold nuanced demographics furthermore designers must make decisions variables include exclude impart cognitive biases model complex models often used nonlinear multimodal data text images models may capture latent systemic bias ways difficult recognize predict expert systems another paradigm may encode cognitive perceptual biases knowledge accumulated practitioners system designed emulate validity ultimately systems demonstrate perform accurately know constitutes right answer validating performance difficult necessary endeavor system deployed public effective management mitigation bias many difficulties flaws arise system validation common challenge system testing lack ground truth noisy labeling annotation factors make difficult know accurate use proxy variables compounds difficulty since measured directly observable performing system tests optimal conditions conditions close deployed state another challenging design flaw system performance metrics also difficult generalize lead issues unintended use due challenges subject matter experts relied upon validation create oversee realistic possible validation processes also practice stratified mance evaluations system performance analyzed across segments training test data whether demographic segments otherwise basic consideration understanding system validity across population users validation deployment validation also means ensuring system used unintended ways eployment bias happens model used ways intended developers emergent bias happens model used unanticipated contexts developers algorithm used major cities assist coordinating housing homeless people began phasing several cities inappropriately used algorithm assessment tool rather presecreening tool designed another instance chicago police department decommissioned algorithm designed predict risk individual might involved future gun violence citing unintended use misapplication model uncommon deployment used system testing depending context institutional review may required carry type testing without system validation system could released technically flawed fails establish appropriate underlying mechanisms proper functioning system could deployed negligent manner based pseudoscience spurious correlations prey user generally exaggerate claims cases goal ensure applications reject development outright order prevent disappointment harm user well reputation provider systems may also run afoul existing legal frameworks proscribe unfair deceptive predatory practices udap type scenario may reinforce public distrust technology since untested technically flawed systems contribute bias harmful outcomes systems magic validation challenge systems stems accessibility hype physicist richard feynman referred practices superficially resemble science follow scientific method cargo cult science core tenet scientific method hypotheses testable experiments interpretable models falsifiable least verifiable commentators drawn similarities cargo cult science citing black box interpretability reproducibility problem processes machine learning libraries reduced costs cloud computing made affordable easier develop result development becoming increasingly democratized still remains largely neural networks bayesian inference require advanced mathematics understand unning effect cognitive bias person limited knowledge domain may vastly overestimate understanding domain federal trade commission act section even among experts technologies exacerbate confirmation bias particularly implicitly guided expected outcomes analysis examined hundreds algorithms identifying covid found effective danger enough tweaking hyperparameters across many candidate models one may appear highly accurate even measured standard performance datasets ata dredging also known statistical bias testing huge numbers hypotheses dataset may appear yield statistical significance even results statistically nonsignificant fig provides examples three categories bias systemic statistical computational human interact contribute harms within data processes used applications validation procedures determining performance systemic biasesstatistical computational biaseshuman biasessystemic biasesprocesses human factorstevvwho counted counted important know right issues latent variables underrepresentation marginalized groups automation inequalities underrepresentation determining utility function processes favor cultural bias objective function best individuals best group reinforcement inequalities groups impacted higher use predictive policing negatively impacted widespread adoption may change policies impact population based use sampling selection bias using proxy variables easier measure automation bias likert scale categorical ordinal cardinal nonlinear linear ecological fallacy minimizing norm general difficulty quantifying contextual phenomena lack adequate survivorship bias difficulty fairness observational bias streetlight effect availability bias anchoring mcnamara fallacy groupthink leads narrow choices rashomon effect leads subjective advocacy difficulty quantifying objectives may lead mcnamara fallacy confirmation bias automation bias fig biases contribute harms tevv guidance mitigate risks stemming epistemic aleatoric uncertainties model developers work closely organizations deploying teams work ensure periodic model updates test recalibrate model parameters updated representative datasets meet business objectives staying within desired performance targets acceptable levels bias bayesian inference perspective seen updating prior model help avoid issues may arise using stale priors organizations recommended employ appropriate governance procedures adequately capture need ensure negative impacts using technology algorithms meaningful assign bias model algorithm without contextual information specific tasks may used links model algorithm dataset trained tested see ignette contextual factors play role bias catchphrase bias bias widely used describe heavy dependence algorithmic behavior data example natural language processing context hate speech detection models use dialect markers toxicity predictors result bias minority groups another context algorithm designed deliver advertisements jobs stem resulted gender bias due younger women considered valuable subgroup expensive targets advertisements methods help reduce algorithmic bias another helpful construct understanding specific methods algorithmic mitigation bias many different machine learning tasks delineated surveyed recent studies considering approaches mitigating algorithmic bias specific task context recent literature categorizes debiasing methods one three categories transforming data underlying discrimination mitigated method used modeling pipeline allowed modify training data techniques modify algorithms order mitigate bias model training model training processes could incorporate changes objective cost function impose new optimization constraint typically performed help holdout dataset data used training model learned model treated black box predictions altered function phase function deduced performance black box model holdout dataset technique may useful adapting large language model dataset task interest limits algorithmic transparency eliminating bias automated appealing comes risks result discriminatory outcomes researchers investigated settings ads allocated algorithm found instances groups less likely see desirable ads setting field test performed intended promote job opportunities training stem stem career campaign motivated widespread concern shortage underrepresented groups stem sector particularly women assumption disseminating information stem careers women encouraging women enter field helps address problem however since women far likely make decisions household purchases valuable targets advertising creating pricing differentials displays result campaign men women viewed largest difference year old age group findings study help demonstrate difficulty evaluating algorithms preventing discrimination need lens challenge insufficient look bias algorithm alone relatedly according lambrecht one popular policy prescription focus algorithmic transparency algorithmic codes made public policies gaining increasing momentum example federal trade commission ftc launched new unit focused algorithmic transparency however algorithmic transparency would helped regulators foresee uneven outcomes reason examination algorithmic code would likely revealed algorithm focused minimizing costs advertisers without appropriate knowledge economic context minimization might affect distribution advertising transparency would particularly helpful transparency system mechanisms rarely direct bias mitigant explained transparency enables many critical governance functions transparency important mistaken fairness sectors economy equal credit opportunity court legal regulatory matters invoke legal doctrine disparate treatment debiasing efforts may less likely explicitly include postprocessing approaches instead rely alternative modeling approaches consumer supervision examination manual equal credit opportunity act ricci destefano finance employment litigation practice bias remediation debiasing pursued decades practitioners likely consider adjustments input variables model hyperparameters improve bias testing results outcomes demographic group membership necessary bias testing purposes often inferred using bayesian improved surname geocoding bisg process see modeling algorithms debiasing techniques rely demographic information postprocessing methods may pose higher risks regulated environments disparate treatment must avoided fairness metrics computational standpoint defining fairness metric requires developing formal mathematical model achieve desired predictive goals given dataset associated task numerous fairness metrics proposed literature much work determining fairness criteria involves supervised learning labeled data required tasks may readily available particularly true large language models sheer scale datasets used training prohibitive proper data labeling direct impact representativeness training data turn impact representativeness generated model might exacerbate discriminatory outcomes large language models adapted specific datasets tasks moreover even datasets representative may still exhibit biases improperly utilize protected attributes turn may lead discrimination proxies may used hiding protected attributes care taken avoid discrimination resulting badly chosen proxies even proxies used hide protected attributes may still reveal sensitive information individuals groups recent literature considers alternative learning tasks unsupervised learning reinforcement learning intermediate feedback provided model tries balance effects rewards several open questions still remain use representativeness synthetically generated data applications little data available emerging related line research use simulations evaluate impact machine learning systems incorporating elements system level dynamics feedback loops effects make fair decisions dynamic environments another challenge serious social ramifications measure fairness emergent class deployed generative models large language models computer vision systems deep fakes whose outputs free form text audio video academic research mathematical notions fairness blossomed recent years procedures testing fairness regulatory litigation settings employment consumer finance operational decades reached level maturity recent increase interest topic areas statistical tests applied determine whether automated system acting outside bounds applicable law analysis regression coefficients traditional statistical tests used show statistically significant difference system outcomes across demographic groups cases measurements differential validity also used ensure applicants employees receive roughly equal service systems employment system performance quality evaluated across demographic credible attempts bias mitigation maintain alignment acknowledged legal standards generally majority fairness metrics observational expressed using probability statements involving available random variables metrics classified many categories fairness unawareness individual fairness demographic parity disparate impact differential validity proxy discrimination equality opportunity etc however critically important lines inquiry answered observations alone moreover depending relationship protected attribute data certain observational definitions fairness increase discrimination hence research improve fairness metrics continues instance counterfactual fairness definition developed capture intuition decision fair towards individual actual world counterfactual individual belongs different demographic group simulations also used gain counterfactual information data would varied different data collection policy place algorithmic discrimination arise encoding spurious correlations noisy local dependencies systems training currently great focus causal tools formally incorporate effects hypothetical actions solve wide range fairness modeling problems causal methods widely available adopted minimizing number input variables ensuring strong correlation amongst logical relationship prediction target mitigation tactic proxy discrimination risks power cir deciding fairness metric adopt important recognize impossibility satisfying certain mathematical fairness constraints except highly constrained special cases example inherent incompatibility two conditions calibration balancing positive negative classes conditions satisfied simultaneously unless certain constraints mathematical fairness desiderata achieved simultaneously important note mitigated bias good performance achieved simultaneously plethora fairness metric definitions illustrates fairness reduced concise mathematical definition fairness dynamic social nature application context specific abstract universal statistical problem therefore important adopt approach fairness order realistic fairness definitions different contexts well datasets machine learning model development evaluation makes decisions make human factors bias human factors challenges algorithms evolved accuracy precision computational systems moved used purely decision explicit use control human automated decision making limited input humans computational decision support systems augment another typically human system making decisions comparatively algorithmic decision systems less human involvement system driver seat able produce outcomes little human involvement govern impact growth prevalence algorithmic decision systems helped drive decreased sense trust among public distrust exacerbated reality historical social biases data assumptions used algorithmic models generating automated decisions result algorithmic models higher probability producing amplifying unjust outcomes racial ethnic minorities areas criminal justice systemic biases embedded algorithmic models also exploited used weapon scale causing catastrophic harm organizations deploy models systems without assessing managing risks harm users jeopardize reputations deployment context use systems designed developed used specific real world settings often tested idealized scenarios deployed original intent idea impact assessment drift application repurposed used unforeseen ways settings contexts originally intended different deployment contexts means new set risks considered engaging broad set stakeholder communities may impacted deployment decision made build important consideration strongly recommended context use encompasses design perspective see subsequent section one major purpose significant benefit automated technology make sense information quickly consistently humans systems also often perceived way make public interest decisions fair reduce eliminate biased human decision making bring equitable society perspectives led deployment automated predictive modeling tools within trusted institutions settings hiring criminal justice settings automated decisions incorporate negative biases perpetuate harms quickly extensively systematically human societal biases algorithmic decision systems systems inextricably tied human social behavior datasets used processes decisions made build interactions humans provide insight oversight make systems actionable default assumption placing human systems ensure adverse events occur current perceptions role responsibility often implicit expectations level performance systems often based untested outdated hypotheses bulk academic literature available domain often relates humans working automated systems broad scale use systems deployed use subject matter experts scenario professionals expertise specific domain work conjunction automated system towards specific end consequential decision another individual depending purpose system expert may interact model rarely part design development system experts necessarily familiar data science computer science fields traditionally associated design development example systems deployed domain medicine experts physicians bring expertise data science data modeling engineering computational factors perception human expert otherwise effectively objectively oversee use algorithmic decision systems problematic assumption work needs done understand complex institutional societal structures systems developed placed humans carry significant cognitive biases heuristics operation systems exactly assist remains understudied area one challenge scenarios finding configuration enables system used way optimally leverages instead replaces subject matter expertise human difficult since subject matter experts developers often lack common vernacular contribute miscommunication misunderstood expectations capabilities sides system configurations complex even without aid highly advanced experts operators often placed system settings without explicit declarations governing authority specific task outcome promise approaches quantitative subject matter experts may inadvertently activate mcnamara fallacy leverage system take pressure often subjective processes presumed objectivity automation bias often referred automation complacency expert users may also subconsciously find ways leverage perceived objectivity cover even justification implicit biases inadvertently make decisions inaccurate harmful relatedly developer communities may subconsciously presume experts methods validated greater degree case kinds implicit individual group actions may create conditions indirectly encourage use technology quite ready use especially settings researchers recommend development teams work tighter conjunction subject matter experts practitioner end users turn must consider deliberate modest approach utilizing automated output practices intended serve form oversight systems accompanying results experts bring particular subject matter knowledge process necessarily trained govern use system played role developing current legal governance structures actively rely expert serve mechanism protecting society faulty mistaken dangerous algorithmic decisions fundamental assumption structures human overseer simply virtue human able provide adequate governance reality however frequently emphasized governance frameworks associate decisions posing less risk opposed fully automated decision making see example role general human intervention minimizing risks systems fda good machine learning practice medical device development guiding principles without significant procedural cultural support optimistic expectations humans able serve administrative capacity borne practice literature provides thorough review flaws human oversight policies general public challenge interpretable systems also factor consumer citizen use applications presumed trust improve public able interrogate engage systems transparent manner article public trust knowles richards state members public need trust individual ais need instead sanction authority provided suitably expert auditors trusted developing authority requires standard practices metrics norms perspective nist risk management framework help create standard practices metrics norms consensus community reliance various downstream professionals act governor automated processes complex societal systems viable approach human factors guidance impact assessments decision deploy technology function organizational incentives designed developed within set organizational norms policies one recent proposed approach ensuring technology developed ethical responsible manner algorithmic impact assessment identifying addressing potential biases important step assessment process currently momentum researchers include statements potential societal impacts submitting work journals conferences similar privacy impact assessments relied upon data protection privacy frameworks gauge respond data privacy risks impact assessments provide structure enables organizations frame risks algorithm deployment also accounting specifics use case engaging impact assessment also serve forcing mechanism nhtsa automated driving systems oluntary guidance military context even emphasis placed human intervention principles recommendations ethical use artificial intelligence department defense defense innovation board aiprinciples primary see also brig gen ret jean michel verney joint air space power conference organizations articulate risks generate documentation mitigation activities event associated misstep impact assessments apply beginning long iterative process goals outcomes change time overcome challenge nature impact assessments impact assessments must applied reasonable cadence used iterative evolving systems another concern impact assessments technology groups others assessed may undue influence building using assessment engagement practice technology development also complicated role power decision making within organizational structure consistent theme literature benefit engaging variety stakeholders maintaining diversity along social lines bias concern racial diversity gender diversity age diversity diversity physical ability kinds practices lead broadening perspectives turn thorough evaluation societal impacts technologybased applications using demographic traits organizational personnel identify problematic aspects within development culture practice sufficient may fair identifying downstream impacts may take time require involvement endusers practitioners subject matter experts interdisciplinary professionals law social science expertise matters stakeholders bring varied experiences bear core challenge identifying harmful outcomes context shifts within specific setting system deployed technology datasets seem one group may deemed disastrous others manner different user groups game certain applications may also obvious teams charged bringing technology market kinds impacts sometimes identified early testing stages usually specific contextual change time acquiring types resources risk associated impacts necessarily require huge allocation require deliberate planning guidance also place innovation approaching bias could improve practice factors part changing norms creating organizational risk culture teams improve capacity considering impact technology design develop communicating impacts broadly diversity equity inclusion without prioritizing diversity equity inclusion teams involved training deploying systems difficult move beyond focus system optimization address design considerations risks beyond narrow subset users consider example character limits impact languages cultures others cong recognition effect twitter increased character limit characters another example recent exercise social media company found used filter image content disfavored people white hair memes written scripts recent research shown developers similar demographic backgrounds make similar misjudgements ensuring individuals involved training testing deploying system diversity experience expertise backgrounds critical risk mitigant help organizations manage potential harms human heuristics biases lead examples implicit simply increasing awareness bias ensure control previously described section heuristics adaptive mental shortcuts often beneficial reduce complexity tasks judgement choice yet also lead cognitive biases concepts reasoning behind diversity equity inclusion workplace closely tied need broad engagement aspects lifecycle numerous studies touted benefits increased diversity equity inclusion workplace yet field noticeably lacks diversity extend benefits diversity equity inclusion users developers systems commentators experts recommend bias mitigation efforts multifaceted empowering diverse group individuals reflect range backgrounds perspectives expertise turn help broaden views system designers engineers particular diversity equity inclusion efforts help organizations better understand system likely impact wide variety users users might interact system practice potential harms benefits systems across users groups whether troubleshooting recourse channels described likely effective practice well system might impact broader populations beyond direct users system among others practice improvements taking lifecycle approach possible identify junctures guidance assurance governance processes assist business units data social scientists collaboratively integrate processes reduce bias without cumbersome blocking progress several technology companies developing utilizing guidance improve organizational decision making make practice development responsible implementing processes striving identify potential bias impacts algorithmic models one approach enumerate institutional assumptions developing algorithmic decision systems map assumptions expectations groups impacted requires deliberate community engagement cultural effective challenge practice seeks create environment technology developers actively challenge question steps modeling engineering help root statistical biases biases inherent human decision making requiring practitioners defend techniques within demographically professionally diverse setting incentivize new ways thinking stimulate improved practices help create change approaches individuals organizations configuration systems often deliberately placed settings counteract known subjectivity bias humans yet considerable questions remain optimally configure humans automation approach takes consideration broad set factors necessary especially context bias list relevant span fields human factors psychology organizational behavior interaction building bridges technology communities still necessary nist seeks develop formal guidance implement processes amplify perpetuate many human systemic computational biases degrade outcomes complex setting identifying system configurations necessary qualifications components result outcomes accurate trustworthy key focus system procedural transparency consistent finding literature systems need explainable interpretable proliferation tools datasheets model cards intended fill gap bias intersects transparency complex ways groups invent produce technology specific intentions use unlikely aware ways given application used repurposed deployed transparency tools especially helpful addressing problem unintended use even systems used intended significant individual differences humans interpret model output issue becomes particularly relevant deploying systems use subject matter experts less interested system works concerned whya system provided given output system designers take perceptual differences consideration lead misinterpretation output especially problematic settings coordinated guidance necessary ensure transparency tools effectively supporting professionals use indirectly contributing processes could amplify bias techniques flag factors datasets modeling processes produce biased outcomes cause noncompliance legal requirements intent flagging information somebody along lifecycle end user serve system check yet flagging information downstream users always result directly positive outcome fact create opposite developing guidance area require information settings human biases may amplify harmful outcomes humans work optimally complement system questions like related system design notably dependent setting aircraft systems public safety forensics manufacturing operator expert trained naive task recognition event detection forecasting reasoning keeping humans center design design hcd approach design development system technology aims improve ability users effectively efficiently use product hcd seeks improve user experience entire system involving aspects technology hardware design software design hcd methodology successfully applied myriad important domains nist authored several hcd handbooks tailored particular domains biometrics public safety hcd ongoing iterative process project teams design test continually refine system placing users core process humans needs drive process rather focus hcd works part development lifecycles including waterfall spiral agile models design hcd participatory design design key similarities highest level seek provide humans designs ultimately beneficial lives furthermore placing humans center approaches naturally lend deeper focus larger societal considerations fairness bias values ethics hcd works create usable products meet needs users turn reduces risk resulting system pose risks users result user harms fail hcd process illustrated fig designed meets user requirements evaluate design requirementsunderstand specify context useplan design process specify user requirements evaluate design requirementsiterate appropriatefig design process iso defined international organization standardization iso standard hcd involves explicit understanding users tasks context use involvement users throughout design development design driven refined evaluation iterative process whereby prototype designed tested modified addressing whole user experience design team including multidisciplinary skills perspectives based iso standard hcd methodology development systems could iteratively comprise following shown fig defining context use including operational environment user characteristics tasks social environment determining user organizational requirements including business requirements user requirements technical requirements developing design solution including system design user interface training materials conducting evaluation including usability conformance testing although components hcd critical context use key considerations systems dynamics conditions system used must considered front end project ensure design system meet needs users objectives organization larger societal needs system implemented environment use design organizatonal requirements users fig design process systemsa deep understanding contextual factors important throughout lifecycle context use simply involve users context use involves much broader view context organizational environment system developed including existing systems products operational environment system used larger societal environment system implemented example intended users systems may consistent reliable access fundamental internet technologies phenomenon widely described digital divide leading biases different communities access system similarly disabilities may experience difficulties interacting systems crucially difficulties often mitigated mathematical software approaches failure address important design issues may pose legal risks example employment related activities affecting persons recognized objects systems processes often designed individuals disabilities mind ensuring protections apply individual rather group level congress recognized means placing individual disabilities equal footing others may require individualized person disability may require reasonable accommodation different individual disability may require different accommodation accommodation disabilities heterogeneous even two individuals disability may need different accommodations employment context algorithm may screen particular individual therefore may violate americans disabilities act regardless whether broadly defined groups individuals disabilities tend assessed highly given algorithm growing number researchers pointed benefits approaches example ferrer note challenge could addressed approach consider technical dimensions complex social contexts systems deployed building public confidence greater democratic participation systems requires ongoing development explainable better interaction methods platforms tools public engagement increase critical public understanding research integrate hcd standard design development evaluation deployment processes today systems relatively recent chapter hcd handbook human factors ergonomics margetis state core concept hcd actively involving appropriate stakeholders process context means placing humans loop meaningful human control also active participation preparation learning phases hcai emerging area scholarship reconceptualizes hcd context providing design metaphors suggested governance structures develop reliable safe trustworthy systems schneiderman envisages hcai bridg ing gap ethics practice specific recommendations making successful technologies augment amplify empower enhance humans rather replace shift thinking could lead safer understandable manageable future hcai approach reduce prospects technologies calm fears unemployment diminish threats privacy security future also support human values respect human dignity raise appreciation human capacities bring creative discoveries manage provide oversight governance bias governance processes impact nearly every aspect managing bias reason essential view governance holistic implementation tier nature informing phase bias management process also important note governance simply focus technical artifacts systems alone also organizational processes cultural competencies directly impact individuals involved training deploying monitoring systems number components effective governance managing bias systems focus organizational measures culture governance guidance monitoring systems may perform differently expected deployed lead differential treatment individuals different groups key measure control risk deploy additional systems monitor potential bias issues alert proper personnel potential problems detected without monitoring place difficult know deployed system performance real world matches measurements conducted laboratory environment whether newly collected data match distribution training data key consideration success live monitoring bias collection data active user population especially data related user demographics age gender enable calculation assessment measures type data variety privacy implications may subject legal restrictions types data collected conditions recourse channels availability feedback channels allow system end users flag incorrect potentially harmful results seek recourse errors harms number legal frameworks prioritize ability users appeal override unfavorable decisions applied subset algorithmic systems deployed areas like consumer finance appeal override recourse often requires logical description questionable decision processes tightly connected system explainability interpretability though without criticism adverse action notices negative consumer credit decisions mandated equal credit opportunity act fair credit reporting act example explanation appeal additional appeal override processes could include options customers interact human instead system options avoid similar content future embedding processes technologies systems allows users appeal wrong decisions even suggestions also empowering technology development teams remediate potential incidents near inception point policies procedures context systems ensuring written policies procedures address key roles responsibilities processes stages model lifecycle critical managing detecting potential overall issues system procedures enable consistent development testing practices turn help ensure results systems repeatable related risks consistently mapped measured managed without policies management bias easily become subjective inconsistent across organizations exacerbate risks time rather minimize example irreconcilably different metrics used across systems policies may define key terms concepts related systems scope intended impact address use sensitive otherwise potentially risky data governors fed rsrv supervisory guidance model risk management letter apr detail standards experimental design data quality model training outline risks bias mapped measured according standards detail processes model testing validation detail process review legal risk functions set forth periodicity depth ongoing auditing review outline requirements change management detail plans related incident response systems event significant risks materialize deployment documentation clear documentation practices help systematically implement policies procedures standardizing organization bias management processes implemented recorded stage standardized documentation turn help ensure accountability described detail model documents contain interpretable descriptions system mechanisms enabling oversight personnel make informed decisions system potential perpetuate bias documentation also serves single repository important information supporting internal oversight systems related business processes also enhancing system maintenance serving valuable resource necessary corrective debugging model documentation especially important context accountability use documentation templates specific requirements enables practitioners walk workflows prescribed written policies procedures best practices omission key documentation elements indicate lack adherence written policies procedures part system developers testers model documentation templates also include contact information developers stakeholders act adding contact information document describing work product enable efficient oversight communications type practice also lead greater concern responsibility quality product turn impact bias management efforts within organization comptroller currency comptroller handbook model risk management accountability accountability plays critical role governance efforts governance without accountability practice unlikely effective ensuring specific team often specific individual chief model risk officer common large consumer finance organizations responsible bias management systems fundamental accountability individuals teams bear responsibility risks associated harms provides direct incentive mitigation put simply someone boss accountable bias issues accountable bias phenomenon promulgates practitioners accountability bias lie shoulders single individual accountability mandates also embedded within across various teams involved training deployment systems existing technical procedural frameworks accountability related include general governance procedures application system monitoring data quality measures computer security countermeasures nondiscrimination mechanisms among others fundamentally accountability requires clear assessment role system example systems may claimed result direct therefore pose less risks easily become overly relied upon users misused abused cases system would generate similar harms engaging directly model algorithmic audits used assess document crucial accountability considerations several notions audits commonly discussed responsible trustworthy communities audit may refer traditional internal audit function employed track issues model risk traditional model governance audit may refer structured principled application lessons learned financial audit practices systems alternatively audit may refer general documentation transparency approach audits effective accountability bias general risk mitigation mechanism indeed laws passed demand bias audits systems used employment however audits currently exist wide range forms varying levels quality consensus audits addressed future nist documents related risk management framework culture practice governance effective needs embedded throughout culture organization organizational culture practice defined variety ways central theme definitions emphasize beliefs norms values words behavior organization prioritizes practice even behavior codified written risk management culture practices powerful technique identifying biases across lifecycle system perspective governors fed rsrv supra note effective challenge principal effective challenge central component model risk management frameworks practice heavily relied financial sector mitigate algorithmic risk mandates important model design implementation decisions questioned experts authority stature make changes design culture effective challenge encourages actively challenging questioning steps development systems help raise issues bias materialize deployed systems organizational culture encourages serious questioning system designs likely identify problems turn harmful incidents relatedly individuals part development systems may knowledgeable potential harmful impacts technology build impact assessments exclusively developed teams due increased likelihood confirmation bias incentives may cause conflicts interest three lines defense culture difficult map measure directly one way encourage approach incentivize critical thinking review organizational procedural level model risk management frameworks example often systematically implemented three lines defense creates separate teams held accountable different aspects model lifecycle typically first line defense focuses model development second risk management third traditional approach may impractical smaller organizations ensuring culture effective challenge encouraged sustained help organizations anticipate therefore effectively mitigate risks bias materialize risk mitigation risk tiering incentive structures applications central cultural component effective risk management bias lies clear acknowledgment risk mitigation rather risk avoidance often effective factor managing risk mitigation mindset meaning clear acceptance incidents occur emphasizing practical detection mitigation help ensure risks bias quickly mitigated practice acknowledgement enables clear triaging risks enable organizations focus finite resources risks bias material therefore likely cause harm additional component effective organizational culture includes aligning pay promotion incentives across teams risk mitigation efforts participants risk mitigation superintendent fin inst canada model risk management institutions comm regulation european parliament council laying harmonised rules artificial intelligence artificial intelligence act amending certain union legislative acts proposed apr governors fed rsrv supra note three lines truly motivated use sound development approaches test rigorously audit information sharing described nist special publication sharing cyber threat information helps organizations improve security postures organizations identifying internal mechanisms teams share information bias incidents harmful impacts helps elevate importance risks provides information teams avoid past failed designs initial efforts already underway teams begin create norms tracking incidents potentially transform practices organizational culture improving awareness bias presents deployed related impacts enhance knowledge capabilities prevent incidents fostering culture information sharing also serve new area community engagement conclusions document provided broad overview complex challenge addressing managing risks associated bias clear developing detailed technical guidance address challenging area take time input diverse stakeholders within beyond groups design develop deploy applications including members communities may impacted deployment systems since neither built deployed vacuum approach system acknowledging systems associated bias extend beyond computational level bias introduced purposefully inadvertently emerge system used impacting society large perpetuating amplifying biased discriminatory outcomes adopting perspective brings new requirements many contextual nature processes comprise lifecycle important gain understanding computational statistical factors interact systemic human biases nist provided initial framing bias document including key context terminology highlights main challenges foundational directions future guidance information classified discussed document according three key areas dataset availability representativeness suitability contexts tevv considerations measurement metrics support testing evaluation human factors including societal historic biases within individuals organizations participatory approaches design loop practices identifying key requirements improving knowledge area necessary first step ensure broad input engagement consensus nist carry supporting standards development activities workshops public comment periods draft documents nist intends develop consensus guidance collaboration research community broad set stakeholders including directly impacted bias intent guidance specific assistance organizations commission design develop deploy use evaluate variety use cases providing entities clear explicit technically valid guidance nist intends improve state practice bias assure system trustworthiness glossary activity bias type selection bias occurs get training data active users rather less active inactive aleatoric uncertainty aleatoric uncertainty also known statistical uncertainty refers unknowns differ time run experiment refers variability outcome experiment due inherently random effects example machine learning context process may stochastic component reduced additional source information consequently even best model trained data able provide definite answer amplification bias arises distribution prediction outputs skewed comparison prior distribution prediction target anchoring bias cognitive bias influence particular reference point anchor people decisions often fully referred anchor set people adjust insufficiently anchor point arrive final answer decision makers biased towards initially presented value annotator reporting bias users rely automation heuristic replacement information seeking processing form individual bias often discussed group bias larger effects natural language processing models automation complacency humans automated systems skills attenuated spelling autocorrect spellcheckers availability heuristic also referred availability bias mental shortcut whereby people tend overweight comes easily quickly mind meaning easier available greater emphasis judgement behavioral bias systematic distortions user behavior across platforms contexts across users represented different datasets cognitive bias broad term referring generally systematic pattern deviation rational judgement large variety cognitive biases identified many decades research judgement adaptive mental shortcuts known heuristics concept drift use system outside planned domain application common cause performance gaps laboratory settings real world confirmation bias also called confirmatory bias cognitive bias people tend prefer information aligns confirms existing beliefs people exhibit confirmation bias search interpretation recall information famous wason selection task experiments participants repeatedly showed preference confirmation falsification tasked identifying underlying rule applied number triples shown overwhelmingly tested triples confirmed rather falsified hypothesized rule construct validity form validation seeks answer whether test measures intends measure consumer bias arises algorithm platform provides users new venue within express biases may occur either side party digital interaction content production bias arises structural lexical semantic syntactic differences contents generated users data dredging statistical bias testing huge numbers hypotheses dataset may appear yield statistical significance even results statistically nonsignificant data generation bias arises addition synthetic redundant data samples dataset deployment bias arises systems used decision aids humans since human intermediary may act predictions ways typically modeled system however still individuals using deployed system detection bias systematic differences groups outcomes determined may cause underestimation size effect effect cognitive bias tendency people low ability given area task overestimate ability typically measured comparing objective performance often called subjective ability objective ability respectively ecological fallacy occurs inference made individual based membership within group emergent bias use system outside planned domain application common cause performance gaps laboratory settings real world epistemic uncertainty epistemic uncertainty also known systematic uncertainty refers deficiencies lack knowledge information may methodology model built neglects certain effects particular data deliberately hidden error propagation arises applications built machine learning used generate inputs machine learning algorithms output biased way bias may inherited systems using output input learn models evaluation bias arises testing external benchmark populations equally represent various parts user population use performance metrics appropriate way model used exclusion bias specific groups user populations excluded testing subsequent analyses feedback loop bias effects may occur algorithm learns user behavior feeds behavior back model funding bias arises biased results reported order support satisfy funding agency financial supporter research study also individual researcher governance framework policies rules processes ensuring direction management accountability groupthink psychological phenomenon occurs people group tend make decisions based desire conform group fear dissenting group groupthink individuals often refrain expressing personal disagreement group hesitating voice opinions align group heuristics context human decision making often referred mental shortcuts term encompasses many methods may less fully rational optimal yet often sufficient approximate solution although heuristics reduce cognitive load aid people making decisions heuristics also result systematic errors cognitive biases historical bias referring biases encoded society time related distinct biases historical description interpretation analysis explanation history common example historical bias tendency view larger world western european view human reporting bias users rely automation heuristic replacement information seeking processing implicit bias unconscious belief attitude feeling association stereotype affect way humans process information make decisions take actions inherited bias arises applications built machine learning used generate inputs machine learning algorithms output biased way bias may inherited systems using output input learn models institutional bias contrast biases exhibited level individual persons institutional bias refers tendency exhibited level entire institutions practices norms result favoring disadvantaging certain social groups common examples include institutional racism institutional sexism interpretation bias form information processing bias occur users interpret algorithmic outputs according internalized biases views language model computational model trained using statistical methods find patterns written spoken language order predict classify words text speech linking bias arises network attributes obtained user connections activities interactions differ misrepresent true behavior users loss situational awareness bias automation leads humans unaware situation control system given back situation humans machines cooperate unprepared assume duties loss awareness automation taking care mcnamara fallacy belief quantitative information valuable information measurement bias arises features labels proxies desired quantities potentially leaving important factors introducing group noise leads differential performance mode confusion bias modal interfaces confuse human operators misunderstand mode system using taking actions correct different mode incorrect current situation cause many deadly accidents also source confusion everyday life model conceptual mathematical physical representation phenomenon observed system ideas events processes models used phenomenon often abstracted mathematical representation means characteristics represented mathematically may captured model model selection bias bias introduced using data select single seemingly best model large set models employing many predictor variables model selection bias also occurs explanatory variable weak relationship response variable popularity bias form selection bias occurs items popular exposed less popular items population bias systematic distortions demographics user characteristics population users represented dataset platform target population presentation bias biases arising information presented web via user interface due rating ranking output users biased interaction proxy variable stand another usually directly observable measurable variable ranking bias form anchoring bias idea results relevant important result clicks results rashomon effect principle refers differences perspective memory recall interpretation reporting event multiple persons witnesses representation bias arises due sampling subgroups causing trends estimated one population generalizable data collected new population selective adherence inclination selectively adopt algorithmic advice matches beliefs stereotypes simpson paradox statistical phenomenon marginal association two categorical variables qualitatively different partial association two variables controlling one variables example statistical association correlation detected two variables entire population disappears reverses population divided subgroups societal bias often referred social bias positive negative take number different forms typically characterized groups individuals based social identities demographic factors immutable physical characteristics societal social biases often stereotypes common examples societal social biases based concepts like race ethnicity gender sexual orientation socioeconomic status education societal bias often recognized discussed context nlp natural language processing models term used describe humans interact technology within broader societal context streetlight effect bias whereby people tend search easiest look sunk cost fallacy human tendency people opt continue endeavor behavior due previously spent invested resources money time effort regardless whether costs outweigh benefits example sunk cost fallacy could lead development teams organizations feel already invested much time money particular application must pursue market rather deciding end effort even face significant technical debt ethical debt survivorship bias tendency people focus items observations people survive make past selection process overlooking technochauvinism belief technology always solution temporal bias bias arises differences populations behaviors time uncertainty bias arises predictive algorithms favor groups better represented training data since less uncertainty associated predictions user interaction bias arises user imposes biases behavior interaction data output results etc references nist leadership plan federal engagement developing technical standards related tools national institute standards technology tech online available standards fedengagement plan ajunwa friedler scheidegger venkatasubramanian hiring algorithm predicting preventing disparate impact undefined online available barocas biega fish niklas stark design build deploy proceedings conference fairness accountability transparency ser fat new york usa association computing machinery online available bogen allhiring ways hiring algorithms introduce bias harvard business review online available dastin amazon scraps secret recruiting tool showed bias women reuters online available harlen schnuck objective biased online available dencik edwards mean solve problem discrimination hiring social technical legal perspectives automated hiring systems arxiv online available evans mathews new york regulator probes unitedhealth algorithm racial bias wall street journal online available fry hello world human age algorithms norton company gianfrancesco tamang yazdany schmajuk potential biases machine learning algorithms using electronic health record data jama intern med vol online available guo hao stanford vaccine algorithm left frontline doctors online available ledford millions black people affected racial bias healthcare algorithms nature vol number publisher nature publishing group online available maddox rumsfeld payne questions artificial intelligence health care jama vol online available obermeyer powers ogeli mullainathan dissecting racial bias algorithm used manage health populations science vol online available simonite algorithm blocked kidney transplants black patients wired wired online available singh ramamurthy understanding racial bias health using medical expenditure panel survey data stat arxiv online available cruz perils equity care big data elusive grasp health inequality big data society vol online available angwin larson mattu kirchner propublica machine bias software used across country predict future criminals biased propublica dormehl algorithms great also ruin lives wired online available goel shroff skeem slobogin accuracy equity jurisprudence criminal risk assessment ssrn journal online available brayne enter dragnet online available chouldechova fair prediction disparate impact study bias recidivism prediction instruments big data vol jun online available epic algorithms criminal justice system risk assessment tools electronic privacy information center epic tech online available hill flawed facial recognition leads arrest jail new jersey man online available johndrow lum algorithm removing sensitive information application recidivism prediction ann appl stat vol mar online available kamiran karim verwer goudriaan classifying socially sensitive data without discrimination analysis crime suspect dataset ieee international conference data mining workshops brussels belgium ieee online available kleinberg lakkaraju leskovec ludwig mullainathan human decisions machine predictions quarterly journal economics vol liptak sent prison software program secret algorithms new york times may online available wexler computer program keeps jail new york times online available state loomis aitken toreini carmichael coopamootoo elliott van moorsel establishing social licence financial technology reflections role private sector pursuing ethical data practices big data society vol online available bajorek oice recognition still significant race gender biases harvard business review section technology online available bary artificial intelligence could replace credit scores reshape get loans online available benjamin race technology abolitionist tools new jim code john wiley sons broussard artificial unintelligence ser mit press london england mit press apr boulamwini gebru gender shades intersectional accuracy disparities commercial gender classification proceedings machine learning research invisible women data bias world designed men abrams press dwork hardt pitassi reingold zemel fairness awareness arxiv online available eubanks automating inequality tools profile police punish poor martin press hardt price srebro equality opportunity supervised learning arxiv online available noble algorithms oppression search engines reinforce racism nyu press neil weapons math destruction big data increases inequality threatens democracy broadway books pandey caliskan iterative bias ridehailing measuring social bias dynamic pricing million rides jun arxiv online available redden harm data scientific american online available specia siri alexa reinforce gender bias finds new york times may online available west brookings survey finds worries impact jobs personal privacy concern fall behind china brookings tech furman haney home smart connected international conference interaction cham springer kerr barry kelleher expectations artificial intelligence performativity ethics implications communication governance big data society vol online available fast horvitz trends public perception artificial intelligence aaai conference artificial intelligence smith anderson automation everyday life pew research center tech ware records computers rights citizens rand corporation santa monica tech online available feathers major universities using race high impact predictor student success markup section news online available kirchner goldstein access denied faulty automated background checks freeze renters markup section locked online available ajunwa paradox automation intervention cardozo vol online available bogen ways hiring algorithms introduce bias harvard business review may section hiring online available bogen rieke help wanted examination hiring algorithms equity bias online available schellmann auditors testing hiring algorithms bias big questions remain online available bartlett morse stanton wallace discrimination fintech era national bureau economic research tech jun online available artificial intelligence affects financial consumers online available weber yurochkin botros markov black loans matter distributionally robust fairness fighting subgroup discrimination arxiv online available suresh guttag framework understanding sources harm throughout machine learning life cycle stat jun arxiv online available barocas hardt narayanan fairness machine learning russell norvig artificial intelligence modern approach online edition edition samuel studies machine learning using game checkers ibm journal research development vol milano taddeo floridi recommender systems ethical challenges society vol vries identity profiling algorithms world ambient intelligence ethics information technology vol richardson schultz crawford dirty data bad predictions civil rights violations impact police data predictive policing systems justice new york university law review vol elliott lowitz nfp cost poor credit washington urban institute bias problem credit scores help mit technology review june sarkesian singh hud new rule paves way rampant rithmic discrimination housing decisions new america oecd glossary statistical terms oecd online resource july iso statistics ocabulary symbols part general statistical terms terms used probability iso tech iso online available cowgill dell acqua deng hsu verma chaintreau biased programmers biased data field experiment operationalizing ethics econ arxiv online available barocas selbst big data disparate impact california law review vol publisher california law review online available design justice escape matrix domination journal design science jul online available elish barocas plasek ferryman social economic implications artificial intelligence technologies new york tech online available jacobs wallach measurement fairness proceedings acm conference fairness accountability transparency mar arxiv online available passi barocas problem formulation fairness proceedings conference fairness accountability transparency arxiv online available selbst boyd friedler venkatasubramanian vertesi fairness abstraction sociotechnical systems proceedings conference fairness accountability transparency fat atlanta usa acm press online available tversky kahneman judgment uncertainty heuristics biases science vol publisher american association advancement science online available caliskan bryson narayanan semantics derived automatically language corpora contain biases science vol apr online available danks london algorithmic bias autonomous systems proceedings international joint conference artificial intelligence melbourne australia international joint conferences artificial intelligence organization online available hellstr dignum bensch bias machine learning good arxiv online available information technology ocabulary international organization standardization geneva switzerland tech online available iso std information technology big data overview vocabulary international organization standardization geneva switzerland tech online available iso std mehrabi morstatter saxena lerman galstyan survey bias fairness machine learning arxiv online available mitchell artificial intelligence guide thinking human farrar straus giroux mitchell shadlen mirror mirror reflections quantitative fairness shira mitchell statistician dec online available mulligan kroll kohli wong thing called fairness disciplinary confusion realizing value technology proc acm interact vol cscw arxiv online available economic development recommendation council artificial inteliigence online available suresh guttag framework understanding unintended consequences machine learning stat arxiv online available chandler munday dictionary media communication oxford university press publication title dictionary media communication online available ngan grother ngan face recognition vendor test frvt performance automated gender classification algorithms department commerce national institute standards technology perrett lee oak rowland yoshikawa burt henzi castles akamatsu effects sexual dimorphism facial attractiveness nature vol scott clark josephson boyette cuthill fried gibson hewlett jamieson jankowiak human preferences sexually dimorphic faces may evolutionarily novel proceedings national academy sciences vol kleisner ture cek roberts havl valentova akoko leong apostol varella saribay patterns sexual dimorphism human faces vary across world scientific reports vol keyes misgendering machines implications automatic gender recognition proceedings acm interaction vol cscw online available organization scientific area committees forensic science osac preferred terms online available april kahneman slovic slovic tversky judgment uncertainty heuristics biases cambridge university press naureen alalawi role artificial intelligence recruitment process international conference decision aid sciences application dasa ieee rieke janardan hsu duarte essential work analyzing hiring technologies large hourly employers upturn july brown richardson hiring tools innovative recruitment expedited disability discrimination ctr democracy amour heller moldovan adlam alipanahi beutel chen deaton eisenstein hoffman hormozdiari houlsby hou jerfel karthikesalingam lucic mclean mincu mitani montanari nado natarajan nielson osborne raman ramasamy sayres schrouff seneviratne sequeira suresh veitch vladymyrov wang webster yadlowsky yun zhai sculley underspecification presents challenges credibility modern machine learning stat arxiv online available bommasani hudson adeli altman arora von arx bernstein bohg bosselut brunskill brynjolfsson buch card castellon chatterji chen creel davis demszky donahue doumbouya durmus ermon etchemendy ethayarajh finn gale gillespie goel goodman grossman guha hashimoto henderson hewitt hong hsu huang icard jain jurafsky kalluri karamcheti keeling khani khattab kohd krass krishna kuditipudi kumar ladhak lee lee leskovec levent malik manning mirchandani mitchell munyikwa nair narayan narayanan newman nie niebles nilforoshan nyarko ogut orr papadimitriou park piech portelance potts raghunathan reich ren rong roohani ruiz ryan sadigh sagawa santhanam shih srinivasan tamkin taori thomas tram wang wang xie yasunaga zaharia zhang zhang zhang zhang zheng zhou liang opportunities risks foundation models arxiv online available schiff ayesh musikanski havens ieee new standard assessing implications artificial intelligence ieee international conference systems man cybernetics smc oct online available birhane kalluri card agnew dotan bao values encoded machine learning research jun arxiv online available schmidt stephens introduction artificial intelligence solutions problems algorithmic discrimination arxiv preprint barabas doyle rubinovitz dinakar studying reorienting study algorithmic fairness around issues power proceedings conference fairness accountability transparency fish stark reflexive design fairness human values formal models arxiv online available bogen rieke help wanted examination hiring algortihims equity bias upturn tech online available robertson khoo song surveil predict human rights analysis algorithmic policing canada citizen lab int hum rts toronto slota fleischmann greenberg verma cummings shenefiel many hands make many fingers point challenges creating accountable soc online available mitchell potash barocas amour lum decisions fairness catalogue choices assumptions definitions annu rev stat appl vol mar arxiv online available green flaws policies requiring human oversight government algorithms ssrn journal online available green kak false comfort human oversight antidote online available boyarskaya olteanu crawford overcoming failures imagination infused system development deployment arxiv online available boyd crawford critical questions big data provocations cultural technological scholarly phenomenon information communication society vol jun online available ignazio klein data feminism mit press online available jacobs blodgett barocas daum wallach meaning measurement bias lessons natural language processing proceedings conference fairness accountability transparency ser fat new york usa association computing machinery online available moss metcalf high tech high risk tech ethics lessons pandemic response patterns vol online available washington kuo whose side ethics codes power responsibility social good proceedings conference fairness accountability transparency barcelona spain acm online available morozov save everything click folly technological solutionism public affairs aguera arcas mitchell todorov physiognomy new clothes may online available kroll outlining traceability principle operationalizing accountability computing systems proceedings acm conference fairness accountability transparency mar arxiv online available berk heidari jabbari joseph kearns morgenstern neel roth convex framework fair regression tromble data gone critical reflection academic digital research age social media society vol publisher sage publications online available cobham uncounted john wiley sons stone counting use numbers decide matters liveright publishing plank language nlp arxiv online available tan celis assessing social intersectional biases contextualized word representations stat arxiv online available paullada raji bender denton hanna data dis contents survey dataset development use machine learning research patterns vol online available abdollahpouri mansoury burke mobasher unfairness popularity bias recommendation arxiv online available bias web commun acm vol online available lambrecht tucker algorithmic bias empirical study apparent discrimination display stem career ads ssrn journal online available miceli posada yang studying machine learning data talk bias mean power arxiv online available simpson interpretation interaction contingency tables journal royal statistical society series methodological vol online available calmon wei ramamurthy varshney optimized data discrimination prevention kamiran calders data preprocessing techniques classification without discrimination knowledge information systems vol online available feldman friedler moeller scheidegger venkatasubramanian certifying removing disparate impact peng mathur narayanan mitigating dataset harms requires stewardship lessons papers arxiv online available bellamy dey hind hoffman houde kannan lohia martino mehta mojsilovic nagar ramamurthy richards saha sattigeri singh varshney zhang fairness extensible toolkit detecting understanding mitigating unwanted algorithmic bias arxiv online available northcutt athalye mueller pervasive label errors test sets destabilize machine learning benchmarks bolukbasi chang zou saligrama kalai quantifying reducing stereotypes word embeddings vol parasurama sedoc gendered language resumes implications algorithmic bias hiring arxiv online available paullada raji bender denton hanna data dis contents survey dataset development use machine learning research arxiv online available hoffmann fairness fails data algorithms limits antidiscrimination discourse information communication society vol olteanu castillo diaz kcman social data biases methodological pitfalls ethical boundaries front big data vol jul online available bond lang sad truth happiness scales journal political economy vol kahneman rosenfield gandhi blaser noise overcome high hidden cost inconsistent decision making harvard business review section decision making problem solving online available malik hierarchy limitations machine learning econ math stat arxiv online available friedman mccarthy employment law red flags use artificial intelligence hiring online available waegeman aleatoric epistemic uncertainty machine learning introduction concepts methods machine learning vol mar online available nesterov introductory lectures convex optimization ser applied optimization new york springer kochenderfer wheeler algorithms optimization ser mit press london england mit press mar breiman statistical modeling two cultures comments rejoinder author statistical science vol bommasani hudson adeli altman arora von arx bernstein bohg bosselut brunskill brynjolfsson buch card castellon chatterji chen creel davis demszky donahue doumbouya durmus ermon etchemendy ethayarajh feifei finn gale gillespie goel goodman grossman guha hashimoto henderson hewitt hong hsu huang icard jain jurafsky kalluri karamcheti keeling khani khattab koh krass krishna kuditipudi kumar ladhak lee lee leskovec levent malik manning mirchandani mitchell munyikwa nair narayan narayanan newman nie niebles nilforoshan nyarko ogut orr papadimitriou park piech portelance potts raghunathan reich ren rong roohani ruiz ryan sadigh sagawa santhanam shih srinivasan tamkin taori thomas tram wang wang xie yasunaga zaharia zhang zhang zhang zhang zheng zhou liang opportunities risks foundation models dosovitskiy beyer kolesnikov weissenborn zhai unterthiner dehghani minderer heigold gelly uszkoreit houlsby image worth words transformers image recognition scale arxiv vol bender gebru shmitchell dangers stochastic parrots language models big proceedings acm conference fairness accountability transparency ser facct new york usa association computing machinery online available wagner garcia jadidi strohmaier man wikipedia assessing gender inequality online encyclopedia proceedings international aaai conference web social media vol choromanska henaff mathieu arous lecun loss surfaces multilayer networks artificial intelligence statistics pmlr ieee ieee standard arithmetic ieee std novak numerical methods scientific computing definitive manual math geeks equal share press wolf miller grodzinsky seen coming comments microsoft tay experiment wider implications orbit journal vol rae borgeaud cai millican hoffmann song aslanides henderson ring young rutherford hennigan menick cassirer powell van den driessche hendricks rauh huang glaese welbl dathathri huang uesato mellor higgins creswell mcaleese elsen jayakumar buchatskaya budden sutherland simonyan paganini sifre martens kuncoro nematzadeh gribovskaya donato lazaridou mensch lespiau tsimpoukelli grigorev fritz sottiaux pajarskas pohlen gong toyama masson autume terzi mikulik babuschkin clark las casas guy jones bradbury johnson hechtman weidinger gabriel isaac lockhart osindero rimell dyer vinyals ayoub stanway bennett hassabis kavukcuoglu irving scaling language models methods analysis insights training gopher tamkin brundage clark ganguli understanding capabilities limitations societal impact large language models ansari ang soh refining deep generative models via discriminator gradient flow forde cooper littman model selection disparate impact deep learning applications apr arxiv online available blodgett barocas daum iii wallach language technology power critical survey bias nlp may arxiv online available neff nagy automation algorithms politics bots symbiotic agency case tay international journal communication vol number online available hovy prabhumoye five sources bias natural language processing language linguistics compass vol eprint online available geirhos jacobsen michaelis zemel brendel bethge wichmann shortcut learning deep neural networks nature machine intelligence vol hashemi hall retracted article criminal tendency detection facial images gender bias effect journal big data vol bbc facial recognition predict criminals sparks row bias bbc online news june levin new guess whether gay straight photograph guardian west bergstrom misinformation science proceedings national academy sciences vol miller searching gaydar blind spots study sexual orientation perception psychology sexuality vol barrett adolphs marsella martinez pollak emotional expressions reconsidered challenges inferring emotion human facial movements psychological science public interest vol booth hickman subburaj tay woo mello bias fairness multimodal machine learning case study automated video interviews proceedings international conference multimodal interaction montr canada acm online available narayanan recognize snake oil citp princeton davenport harris competing analytics boston harvard business review press kleinberg ludwig mullainathan sunstein algorithms discrimination detectors proc natl acad sci usa vol online available jarrahi newlands lee wolf kinder sutherland algorithmic management work context big data society vol brayne predict surveil new york oxford university press cowgill bias productivity humans algorithms theory evidence resume screening columbia business school columbia university thompson homeless enough housing san francisco algorithm decides nov online available office inspector general advisory concerning chicago police department predictive risk models city chicago tech hunter evans facebook emotional contagion experiment controversy research ethics vol wang kosinski deep neural networks accurate humans detecting sexual orientation facial images journal personality social psychology vol roberts driggs thorpe gilbey yeung ursprung avil rivero etmann mccague beer teng gkraniaklotsas rudd sala sch common pitfalls recommendations using machine learning detect prognosticate using chest radiographs scans nat mach intell vol wynants calster collins riley heinze schuit bonten dahly damen debray jong dhiman haller harhay henckaerts heus kammer kreuzberger lohmann luijken martin mclernon navarro reitsma sergeant shi skoetz smits snell sperrin spijker steyerberg takada tzoulaki kuijk bussel horst royen verbakel wallisch wilkinson wolff hooft moons smeden prediction models diagnosis prognosis systematic review critical appraisal bmj vol apr publisher british medical journal publishing group section research online available hutson artificial intelligence become alchemy american association advancement science dijkgraaf quanta magazine oct online available wynants van calster collins riley heinze schuit bonten dahly damen debray prediction models diagnosis prognosis systematic review critical appraisal bmj vol sap card gabriel choi smith risk racial bias hate speech detection proceedings annual meeting association computational linguistics florence italy association computational linguistics jul online available ntoutsi fafalios gadiraju iosifidis nejdl vidal ruggieri turini papadopoulos krasanakis kompatsiaris wagner karimi alani berendt kruegel heinze broelemann kasneci tiropanis staab bias systems introductory survey leveraging responsible counteract bias health care online available sowik bottou algorithmic bias data bias understanding relation distributionally robust optimization data curation stat jun arxiv online available varshney trustworthy machine learning chappaqua usa independently published cfpb using publicly available information proxy unidentified race ethnicity consumer financial protection bureau cfpb report gill hall montgomery schmidt responsible machine learning workflow focus interpretable models explanation discrimination testing information vol venkatasubramanian scheidegger friedler clauset fairness networks social capital information access interventions new york usa association computing machinery online available friedler scheidegger venkatasubramanian possibility fairness different value systems require different mechanisms fair decision making commun acm vol mar online available kusner loftus russell silva counterfactual fairness stat mar arxiv online available wang race proxy situational racism stereotypes depaul law review vol agan starr ban box criminal records racial discrimination field experiment quarterly journal economics vol online available fiscella fremont use geocoding surname analysis estimate race ethnicity health services research vol jabbari joseph kearns morgenstern roth fairness reinforcement learning arxiv online available amour srinivasan atwood baljekar sculley halpern fairness static deeper understanding long term fairness via simulation studies proceedings conference fairness accountability transparency ser fat new york usa association computing machinery online available brown mann ryder subbiah kaplan dhariwal neelakantan shyam sastry askell agarwal oss krueger henighan child ramesh ziegler winter hesse chen sigler litwin gray chess clark berner mccandlish radford sutskever amodei language models learners kilbertus parascandolo hardt janzing sch avoiding discrimination causal reasoning kleinberg mullainathan raghavan inherent fair determination risk scores rodolfa lamba ghani empirical observation negligible machine learning public policy nature machine intelligence vol richardson defining demystifying automated decision systems march forthcoming online available binns van kleek veale lyngs zhao shadbolt reducing human percentage perceptions justice algorithmic decisions montreal canada online available scaparrotti joint publication information operations citeseer raman alshebli waniek rahwan peng weaponizing disinformation bring city power grid plos one vol online available ferrara varol davis menczer flammini rise social bots commun acm vol jun online available phillips oxygen amplification better practices reporting extremists antagonists manipulators online data society may oxygen famplification busuioc processing algorithmic advice automation bias versus selective adherence mar arxiv online available dietvorst simmons massey algorithm aversion people erroneously avoid algorithms seeing err exp psychol gen vol overcoming algorithm aversion people use imperfect algorithms even slightly modify management science vol mar online available veale van kleek binns fairness accountability design needs algorithmic support public sector inproceedings chi conference human factors computing systems chi montreal canada acm press online available picard watkins rempal kerodal beyond algorithm pretrial reform risk assessment racial fairness center court innovation tech online available knowles richards sanction authority promoting public trust arxiv online available prunkl ashurst anderljung webb leike dafoe institutionalizing ethics broader impact requirements nature machine intelligence vol number publisher nature publishing group online available moss watkins singh elish metcalf assembling accountability algorithmic impact assessment public online available algorithmic impact assessment tool gov online resource apr kop impact assessment code conduct futurium may reisman schultz crawford whittaker algorithmic impact assessments practical framework public agency accountability apr selbst institutional view algorithmic impact assessments harvard journal law technology vol moss metcalf ethics owners publisher data society research institute online available crawford artificial intelligence white guy problem new york times jun online available rock grant diverse teams smarter rosen ihara giving characters express twitter blog knight twitter algorithm favors young thin females wired yee peradejordi sharing learnings first algorithmic bias bounty challenge twitter engineering blog herring diversity pay race gender business case diversity american sociological review vol ellemers rink diversity work groups current opinion psychology vol talke salomo kock top management team diversity strategic innovation orientation relationship consequences innovativeness performance journal product innovation management vol lorenzo reeves diversity drives financial performance harvard bus west whittaker crawford discriminating systems gender race power institute tech online available walsh fight bias machines people mit sloan mgmt build hire diverse team harvard bus hall gill cox responsible machine learning actionable strategies mitigating risks driving adoption sebastopol reilly media mitchell zaldivar barnes vasserman hutchinson spitzer raji gebru model cards model reporting proceedings conference fairness accountability transparency fat atlanta usa acm press online available gebru morgenstern vecchione wortman vaughan wallach daumee iii crawford datasheets datasets broniatowski psychological foundations explainability interpretability artificial intelligence nist tech gaube suresh raue merritt berkowitz lermer coughlin guttag colak ghassemi say susceptibility deployment clinical npj digit med vol bandiera abtest license type type nature research journals number primary atype research publisher nature publishing group subject term decision making human behaviour radiography subject term radiography online available zerilli knott maclaurin gavaghan algorithmic control problem minds machines vol nist usability biometrics ensuring successful biometric systems nist online resource june andbiome trics theofanos usability handbook public safety communications ensuring successful systems first responders nist handbook may iso ergonomics interaction part design interactive systems iso july ogels digital divides persist rural urban suburban america pew research center digital divide persists even americans lower incomes make gains tech adoption pew research center june ferrer van nuenen cot criado bias discrimination perspective ieee technol soc mag vol jun arxiv online available russell dewey tegmark research priorities robust beneficial artificial intelligence magazine vol online available margetis ntoa antona stephanidis design artificial intelligence john wiley sons ltd online available shneiderman london england oxford university press ejaz broken system credit reporting system fails consumers consumer reports june ammermann adverse action notice requirements ecoa fcra consumer compliance outlook smith using artificial intelligence algorithms ftc apr kroll huey barocas felten reidenberg robinson accountable algorithms university pennsylvania law review vol gao artificial intelligence accountability framework federal agencies entities gao june raji smart white mitchell gebru hutchinson theron barnes closing accountability gap defining framework internal algorithmic auditing proceedings conference fairness accountability transparency ser fat new york usa association computing machinery online available carrier brown taxonomy audit assurance assessment forhumanity taxonomy aiaudit assurance mulvaney nyc targets artificial intelligence bias hiring new law bloomberg law landers behrend auditing auditors framework evaluating fairness bias high stakes predictive models sull turconi sull comes culture company walk talk mit sloan mgmt july johnson badger waltermire snyder skorupka guide cyber threat information sharing mcgregor preventing repeated real world failures cataloging incidents incident database arxiv online available leino black fredrikson sen datta bias amplification stat arxiv online available misra zitnick mitchell girshick seeing human reporting bias visual classifiers noisy labels ieee conference computer vision pattern recognition cvpr las vegas usa ieee jun online available miller chang johnson terveen hecht blissfully happy ready fight varying interpretations emoji proceedings international conference web social media icwsm aaai press online available wason reasoning rule quarterly journal experimental psychology vol online available cronbach meehl construct validity psychological logical bulletin vol silva kenney algorithms platforms ethnic bias commun acm vol online available jeong kim park bennis kim communicationefficient machine learning federated distillation augmentation private data stat arxiv online available medicine catalogue bias mar online available kruger dunning unskilled unaware difficulties recognizing one incompetence lead inflated journal personality social psychology vol bias journal epidemiology community health vol online available lukacs burnham anderson model selection bias freedman paradox annals institute statistical mathematics vol olteanu castillo diaz kcman social data biases methodological pitfalls ethical boundaries frontiers big data vol online available lerman hogg leveraging position bias improve peer recommendation plos one vol jun publisher public library science online available kaplan conduct inquiry kaplan somerset transaction apr tufekci big questions social media big data representativeness validity methodological pitfalls goodman flaxman european union regulations algorithmic right explanation aimag vol arxiv online available
