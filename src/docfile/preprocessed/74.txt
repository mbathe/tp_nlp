defender rights comes law equalalgorithms preventing automated discrimination partnership cnil algorithms preventing automated discrimination algorithms preventing automated discrimination current global health crisis use digital tools increased diversified never resulting major debates digital tools often based algorithms although users always aware informed recourse algorithms basis public private new phenomenon automated calculation financial risk performed banks scoring involves combining various criteria drawn information provided loan applicants become widespread last decades yet noted conseil etat intensive use algorithms result computers new calculation power mass processing large amount data marks unprecedented turning point years use algorithms expanded private sector today processes found fields essential individuals access social policing running organisations hospitals access public services recruitment machine learning technologies taken rolled learning systems continue evolve striving perfection technological evolutions still progress undeniable sources progress individuals society allowing quicker reliable personalised results well new analyses many fields however data protection commissioncnil defender rights area expertise voiced concerns regarding impact algorithmic systems fundamental mindset defender rights acting partnership cnil hope highlighting considerable risk discrimination every one exposed exponential use algorithms aspects life topic long blind spot public debate must change conseil etat puissance publique plateformes numériques accompagner ubérisation documentation française see example state directorate digital information communications systems direction interministérielle numérique système communication etat dinsic guide des algorithmes publics national delegation combat fraud délégation nationale lutte contre fraude data mining une démarche pour améliorer ciblage des contrôles paris january soraya amrani mekki justice prédictive accès juge justice prédictive actes colloque february organised conseil etat cour cassation lawyers council bicentenary partnership psl university paris dalloz christine bargain marie beaurepaire dorothée prud homme recruter avec des algorithmes usages opportunités risques afmd cnil travaux sur système apb decision august comment permettre garder main rapport sur les enjeux éthiques des algorithmes artificielle december defender rights guide recruter avec des outils numériques sans discriminer published opinion december security stations report titled lutte contre fraude aux prestations sociales quel prix pour les droits des usagers september parcoursup decisions december january opinion october draft programming reform act justice opinion september draft act remarks algorithms preventing automated discrimination first glance algorithms sort categorise organise information eliminating prejudice bias specific human beings thus able ensure equal treatment expected applying criteria weighting regardless requester original sexual orientation example reality though technological magic mathematical neutrality algorithms designed humans using data mirror human practices bias introduced every stage development deployment systems intention initially governs algorithm development creation computer code executable code execution context execution completely intentional bias also result inclusion prohibited grounds discrimination algorithm reasons taken account justify criteria used algorithm specific cases state health insurance age bank loans place residence adjust premiums use considered proportionate legitimate however criteria gender origin constitute lawful criteria regardless context nevertheless discriminatory effects algorithms often based mechanisms less visible inclusion prohibited grounds discrimination data discriminatory mechanisms frequently based bias data selected used traditional algorithm fed learning algorithm learning phase one frequent biases based lack representativity data used example study explained facial recognition systems based learning found harder identify women people white women colour generating high error rate populations datasets model based characterised large predominance male white issue similar voice identification technologies designed women voices mind built therefore fed female data tested regard system work well data integrated algorithmic systems used teach machine learning system also biased mathematical result past oftendiscriminatory practices behaviour systemic discrimination present society barocas selbst andrew big data disparate impact california law review june vol october association française des stés financières see article testing scoring ranking revue trimestrielle droit civil cnil reconnaissance faciale pour débat hauteur des enjeux november according mit researcher joy buolamwini study error rates amazon rekognition software men women darker skinned men darker skinned women see hardesty larry study finds gender bias commercial systems mit news february internship focused voice recognition helicopter pilots woman put commands system work well representing serious safety issue tual diversité humaine est enjeu central pour développement intelligence artificielle monde algorithms discriminatory algorithms preventing automated discrimination eeoc conference big data workplace examining implications equal employment opportunity law october decision january operation national platform first year higher education parcoursup see kim discrimination work mary law review algorithms detect hate speech online biased black people august available employment data women less well represented tend occupy certain business sectors lower positions lower pay based data algorithm might deduce women productive men reach positions responsibility result algorithm used recruitment based biased data reproduce biases even exacerbate false neutrality algorithms true discriminatory effects use apparently neutral criteria criteria include prohibited grounds discrimination discriminatory effects highlighted defender rights parcoursup case university algorithms take account seemingly neutral criteria institution origin could indirectly result discriminating youths foreign origin given strong residential educational segregation observed particular often discriminatory effects caused combination several neutral criteria criteria data question may even seem far removed prohibited reasons correlation provides similar results would obtained protected characteristic applied learning algorithms many correlations make massive amounts data easily generate effects case belonging protected category encoded neutral data designed maximise ability find similar characteristics among massive amounts data programme recreates whole matching protected category applies specific processing order target advertising american supermarket company target developed predictive model identify pregnant clients based purchase habits concerning models could used discriminatory purposes could discriminatory effects algorithms may combine several sources bias ruin even best intentions several american studies recently demonstrated discriminatory nature main smart system models used automatically detect hate speech moderation purposes probability one message reported offensive hate speech system times higher among internet users biases come learning data panel data created humans first classed messages containing abusive language offensive hate speech biases also exacerbated technical limits system struggles identify nuances language put slang sarcastic statements context algorithms preventing automated discrimination discriminatory effects algorithms often measured researchers group level risk remaining completely invisible victims furthermore cognitive biases one human vary depending circumstances contingently translate discriminatory practices discriminatory biases integrated algorithm applied automatically could systematise discrimination significant risk reinforcing essentialization stereotypes algorithm predictive nature based behaviour homogenised characteristics groups people systems therefore could reinforce discrimination prejudices giving appearance objectivity discriminatory effects algorithm always identifiable individual level seemingly neutral algorithmic system may result discrimination protected social groups could translate example lesser access goods sought higher error rate produced system regard risk discrimination even greater social groups already victim major systemic discrimination society example women people disability immigrants integrating former discriminatory practices part dataset used learning phase bias smart systems tends increase rolled predpol software enables many police forces direct action rationalise activity identifying hot points higher risk offences committed order increase patrols model also takes accounts influence factors population density proximity bars means transport however predominance information places past offences crimes committed problematic united states countries police controls arrests places decide patrol target minorities certain areas much others based predpol suggestions police forces would mainly directed districts would observe new offences thereby feeding learning base new biased data algorithms could thus cause feedback loops stereotypes discrimination inequality mutually reinforce one another contribute towards crystallisation situations precisely regularly checking learning algorithm results ensured algorithm become discriminatory course successive encoding lastly added systems tend target control therefore stigmatise members alreadyunderprivileged dominated social groups several associations brought legal action dutch state algorithm developed ministry social affairs employment predict likelihood individual committing benefit tax fraud declared unlawful dunja mijatovic commissioner human rights safeguarding human rights era artificial intelligence commissioner human rights comment strasbourg july hiring algorithm predicting preventing disparate impact ifeoma ajunwa sorelle freidler carlos scheidegger suresh venkatasubramanian draft january virginia eubanks automating inequalities tools profiles police punish poor martin press january potentially massive discrimination algorithms preventing automated discrimination hearing government acknowledged algorithm targeted districts containing higher number social benefit recipients despite lack evidence districts showed higher benefit fraud open democracy welfare surveillance trial netherlands november hague court issued decision february acknowledging government breached right privacy family life set article echr ordered cease using algorithm judges based decision fact algorithm syri lacked transparency court address possible breach article gdpr bans automated cases cathy neil algorithmes bombe retardement les arènes usa cédric villani donner sens intelligence artificielle pour une stratégie nationale européenne report government march telecom paris tech algorithmes biais discrimination équité february aude bernheim flora vincent intelligence artificielle pas sans elles laboratoire égalité belin editions institut montaigne rapport algorithmes contrôle des biais svp march collective report ordered etalab mission ethique responsabilité des algorithmes publics ena class molière june international conference data protection privacy commissioners declaration ethics data protection october equinet regulating equal new role equality bodies meeting new challenges equality increased digitisation use artificial intelligence june right must effectively respected circumstances including decision involves recourse algorithm extensive use algorithms words cathy neil weapon math destruction regards equality nevertheless despite first alarm bells rang villani awareness slow emerge france algorithm designers like organisations buying using types systems demonstrate necessary vigilance avoid type invisible automated discrimination yet fairness principle poses notion users interests obligation person responsible algorithm like principle vigilance reflexivity involves regular methodical deliberative checks learning objects guide reflection reminded option part legal framework sets analysis grid identify situations unequal treatment order implement fundamental right using algorithms escape responsibilities cover ignorance technological incompetence opaque systems algorithmic biases must able identified corrected responsible discriminatory decisions result algorithmic processing must sanctionable highlighted existing literature lack transparency systems implemented data correlations enabled algorithms often entirely invisibly render protection offered law uncertain even ineffective thus one exercise right recourse one even aware victim discrimination result algorithm organisation using algorithm aware designer algorithm explain tool works one find whether algorithm discriminating given social group case breaches rights sanctioned work carried alongside european counterparts members equinet seminar algorithms bias combatting discrimination organised may partnership cnil recommendations algorithms preventing automated discrimination guidelines interdependent reinforce one another diversity fairness accountability transparency human agency oversight guideline example following deployment gdpr february european commission published white paper promoting approach based trust recommendations largely drawn works european expert group sarah myers west meredith whittaker kate crawford discriminating systems gender race power new york university april highlighted lack legal technical expertise need devise effective countermeasures guidelines published european commission april provide european experts first call mobilisation match stakes without reflection mobilisation public authorities significant risk france right able fulfil purpose protect population part mission combat discrimination promote equality defender rights therefore wishes raise awareness partnership cnil need mobilise today prevent correct discrimination awaiting mobilisation defender rights intends fully participate coming months following aims contribute towards launching structuring necessary collective reflection inform raise awareness amongst professionals social reality discrimination framework law still little known rarely considered data algorithm experts europe significant acculturation training issues data analysis professions often criticised lacking still unaware risks fundamental rights caused algorithms reciprocally professionals purchase use processes within organisations trained keep handle critical eye algorithms research develop studies measure methods prevent bias available studies analyses started show magnitude algorithm discriminatory bias still widely relate systems implemented united states order take account significantly regulated limited deployment algorithms europe specific demographic social contexts analyses must developed european union france public research organisations public procurement could support approaches experiments require real statistical expertise approach combining computer engineering understand handle issues economy measure potential discrimination law qualify discrimination exploring fair learning perspectives design algorithms meeting equality explainability objectives merely performance objectives another major research challenge algorithms preventing automated discrimination article code relations public administration collective report ordered etalab mission ethique responsabilité des algorithmes publics ena class molière june decision priority preliminary ruling issue constitutionality april judges considered limits imposed law exercise right access administrative documents justified legitimate interest grounds proportionate objective secrecy deliberations protecting independence educational teams authority decisions however makes one important reservation higher education establishment must account use terms article declaration criteria used applicable using algorithmic means processing study applications sent parcoursup french constitutional council commentary see also defender rights decision april operation parcoursup platform particularly lack transparency allocation procedure rejection request communication algorithmic procedures used association made french conseil etat june algorithms information transparency explainability requirements user right information one hand transparency explainability clear measure discrimination monitor systems ensure effectiveness right recourse however opacity systems secret nature obstacle discovery potential biases recourse obstacle troublesome algorithm result condition access fundamental rights public services gdpr provides first substantial solutions issues example reasons transparency article sets obligation providing meaningful information logic involved automated significant impact data subject furthermore crpa code relations public administration completed digital republic act specifies information must provided recipient individual decision regarding degree method contribution algorithmic processing process data processed source processing parameters weighting applied data subject combat discriminatory bias legal requirements information transparency explainability developed firstly requirements restricted algorithms involving personal data furthermore applied private public sector algorithms lastly different requirements based level automation decisions reviewed human intervention sometimes formally provided many algorithmic processing operations merely symbolic actually provide artificial protection exist transparency requirements respect third parties still insufficient noted constitutional council decision april third parties recipients individual decisions able access criteria used algorithm allow detect potential cases bias general information published algorithmic processing individual explanations regarding given decision must cases provided public users accessible intelligible language professionals involved algorithmic processes whether employees public servants must informed able understand tool general operation increase vigilance regards risk bias ensure effective control processing algorithms preventing automated discrimination aia part broader framework act public finance management directive automated entered force november test sets responsibilities federal institutions regards use automated systems administrative decisions see cnil website infographic algorithms dpias required repeats positions adopted european data protection committee white paper artificial intelligence european approach excellence trust european commission com final reco recommendation committee ministers member states human rights impacts algorithmic impact assessments anticipate algorithms discriminatory effects principle explainability identification potential bias seem clash black boxes many algorithms become secret regarding code revealed learning algorithm opaque issue monitoring effects systems must therefore resolved algorithm design phase learning phase canada audits including discrimination issues required public institutions since april federal government set platform aia algorithmic impact assessment assist administrations impact requirement could introduced france based data protection impact assessment dpia model already provided article gdpr prior analysis mandatory algorithms must include assessment risks rights freedoms individuals therefore already means anticipating discriminatory effects however expressly providing assessment biases part impact assessments rendering mandatory algorithmic processing would ensure effective compliance principle nondiscrimination addition prior assessment regular monitoring algorithms effects deployment required based model applied monitor medicines many questions still remain answers must clarified case methods means implemented must ensure respect fundamental rights freedoms face technological economic frenzy surrounding commonly referred artificial intelligence example suggested european commission adopt approach would increase level requirement monitoring based algorithm use expected audit accreditation procedures enough ensure rights respected prove discrimination set recommended council europe institutional regulatory framework algorithm standards according main defender rights continue reflection topic contribute reflection carried public decisionmakers notably partnership cnil also etalab cnnum cncdh academics participated seminar european network equinet perspective guaranteeing respect rights every individual particular right discriminated compass defender defender rights tsa paris cedex tel news
