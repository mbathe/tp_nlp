draft nist special publication proposal identifying managing bias artificial intelligence reva schwartz leann adam jonas elham tabassi draft publication available free charge draft nist special publication proposal identifying managing bias within artificial intelligence reva schwartz national institute standards technology information technology laboratory leann adam jonas parenthetic llc elham tabassi national institute standards technology information technology laboratory draft publication available free charge june department commerce gina raimondo secretary national institute standards technology james olthoff performing functions duties secretary commerce standards technology director national institute standards technology certain commercial entities equipment materials may identified document order describe experimental procedure concept adequately identification intended imply recommendation endorsement national institute standards technology intended imply entities materials equipment necessarily best available purpose national institute standards technology special publication draft natl inst stand technol spec publ draft pages june draft publication available free charge organizations encouraged review draft publication public comment period provide feedback nist public comment period june september national institute standards technology attn information technology laboratory bureau drive gaithersburg maryland email abstract nist contributes research standards evaluation data required advance development use trustworthy artificial intelligence address economic social national security challenges opportunities working community nist identified following technical characteristics needed cultivate trust systems accuracy explainability interpretability privacy reliability robustness safety security resilience harmful biases mitigated mitigation risk derived bias based products systems critical still insufficiently defined building block trustworthiness report proposes strategy managing bias describes types bias may found technologies systems proposal intended step towards consensus standards framework trustworthy responsible document also contains alphabetical glossary defines commonly occurring biases contributes fuller description understanding challenge harmful bias ways manage presence systems key words bias trustworthiness safety lifecycle development table contents table contents introduction challenge posed bias systems approach identifying managing bias artificial intelligence figure approach managing bias stage problem formulation decision making operational settings unknown impacts overselling tool capabilities performance practices example design development stage optimization context practices example deployment stage discriminatory impact intended context actual context contextual gaps lead performance gaps practical improvements figure example bias presentation three stages modeled lifecycle conclusion next steps appendices appendix glossary appendix collaborative work references iii acknowledgments authors wish thank many people assisted development document including nist colleagues many academic technical reviewers took time provide valuable feedback audience main audience document researchers practitioners field trustworthy responsible artificial intelligence researchers find document useful understanding view challenge bias initial step toward development standards risk framework building using trustworthy systems practitioners benefit gaining understanding bias use systems trademark information trademarks registered trademarks belong respective organizations note reviewers described throughout report one goal nist work trustworthy development risk management framework accompanying standards make necessary progress towards goal nist intends carry variety activities area core building blocks trustworthy accuracy explainability interpretability privacy reliability robustness safety security resilience mitigation harmful bias require concerted effort drawing upon experts within nist external stakeholders nist seeks additional collaborative feedback members research industry practitioner community throughout process interested parties encouraged please submit comments draft report types activities events would helpful via public comment process described page document also opportunities engaging discussions contributing development key practices tools manage bias please look announcements webinars call position papers request comment nist document introduction national institute standards technology nist promotes innovation industrial competitiveness advancing measurement science standards technology ways enhance economic security improve quality life among broad range activities nist contributes research standards evaluations data required advance development use assurance trustworthy artificial intelligence august fulfilling assignment executive nist released plan federal engagement developing technical standards related based broad public private sector input plan recommended deeper consistent engagement standards help united states speed pace reliable robust trustworthy technology nist research continues along path focus measure enhance trustworthiness systems working community nist identified following technical characteristics needed cultivate trust systems accuracy explainability interpretability privacy reliability robustness safety security resilience harmful biases mitigated paper proposal identifying managing bias artificial intelligence developed advance methods understand reduce harmful forms bias one series documents workshops pursuit framework trustworthy responsible significant potential transformative technology also poses inherent risks one risks bias specifically presence bias automated systems contribute harmful outcomes public lack trust managing bias critical still insufficiently developed building block trustworthiness international organization standardization iso defines bias statistical terms degree reference value deviates truth deviation truth either positive negative contribute harmful discriminatory outcomes even beneficial societal perspective bias often connected values viewed dual lens differential treatment disparate impact key legal terms related direct indirect discrimination respectively types bias negative many ways categorize manage bias report focuses biases present systems lead harmful societal outcomes harmful biases affect people lives variety settings causing disparate impact discriminatory unjust outcomes presumption bias present throughout systems challenge identifying measuring managing current approaches tend classify bias type statistical cognitive use case industrial sector hiring health care etc may able provide broad perspective required effectively managing bias phenomenon document attempts bridge gap proposes approach managing reducing impacts harmful across contexts intention leverage key locations within stages lifecycle optimally identifying managing bias nist develops framework standards area proposed approach starting point feedback activities related bias role trustworthy challenge posed bias systems proliferation modeling predictive approaches based machine learning techniques helped expose various social biases baked systems increasing evidence general public concerns risks society distrust manifest belief biases may automated within technologies perpetuate harms quickly extensively systematically human societal biases human decisions based automated predictive technology often made settings hiring criminal justice create harmful impacts amplify accelerate existing social inequities minimum perceptions inequities unlikely technology exhibiting zero risk developed managing reducing impacts harmful biases possible necessary public attitudes technology suggest often depending application americans unaware interacting enabled tech feel needs higher ethical standard forms technologies mainly stems perceptions fear loss control privacy certainly shortage examples bias aspect technology use caused harm negatively impacted people lives hiring health care criminal justice indeed many instances deployment technologies accompanied concerns whether societal biases perpetuated amplified since systems deployed across various contexts associated biases come use create harm ways proliferation bias list settings makes especially difficult develop overarching guidance mitigation techniques confounding factor especially difficult predict systems used current approach challenge bias tackle given use case particularly prevalent type bias resides strategy difficult scale unlikely achieve required building systems public trust instead viewing challenge bias within given context use case broader perspective strike problem bias might easiest manage within design development use systems specific conditional traits associated automation exacerbate distrust tools one major purpose significant benefit automated technology make sense information quickly consistently humans long two common assumptions rise use automation could make life easier purpose document term managing bias used refer approaches managing reducing mitigating bias also create conditions reduce eliminate biased human decision making bring equitable society two tenets led deployment automated predictive tools within trusted institutions settings help society achieve significant benefits convenience automated classification discovery within large datasets may come potentially significant downside tools proliferate across social systems increased interest identifying mitigating harmful impacts difficulty characterizing managing bias exemplified systems built model concepts partially observable capturable data without direct measures often highly complex considerations development teams often use proxies example criminality measurable index construct might created information past arrests age region employment suitability algorithm might rely time prior employment previous pay levels education level participation certain sports distance employment site might disadvantage candidates certain neighborhoods many challenges come common practice see thorough review one challenge rests reality decisions data use indices often made based available accessible rather might suitable difficult impossible utilize relatedly instead identifying specific questions interest first researchers developers practitioners may data adapt questions accordingly data also differ significantly collected occurs real world example responses online questionnaires specific sampling kinds people online therefore leaves many groups data representing certain societal groups may excluded training datasets used machine learning applications datasets used natural language processing often differ significantly applications lead discrimination systematic gaps performance even datasets reflective real world may still exhibit entrenched historical societal biases improperly utilize protected attributes federal laws regulations established prohibit discrimination based grounds gender age religion simply excluding explicit types attributes remedy problem however since inadvertently inferred ways example browsing history still produce negative outcomes individuals classes individuals proxies used development may poor fit concept characteristic seeking measured reveal unintended information persons groups additionally much public necessarily something directly interact systems algorithmic assumptions may transparent nevertheless many people affected used inputs technologies systems happen individual applies loan college new apartment historical training data measurement biases data used algorithmic models underlying types decisions biases may produce unjust outcomes racial ethnic minorities areas criminal justice hiring financial decisions another cause distrust may due entire class untested unreliable algorithms deployed settings often technology tested tested extensively deployment instead deployment may used testing technology example rush deploy systems covid pandemic turned methodologically flawed biased also examples literature describe technology based questionable concepts deceptive unproven practices lacking theoretical underpinnings broad consensus literature systems meant decision making predictive scenarios demonstrate validity reliability specific setting intended deployed hiring purposes risk assessments criminal justice system decisions based algorithms affect people lives significant ways appropriate expect protections place safeguard certain systems practices public cautious opinions toward might turn increasingly negative new technologies appear based approaches already contributed systematic societal harms summarize problem many reasons potential public distrust related bias systems include use datasets practices inherently biased historically contribute negative impacts automation based biases placed settings affect people lives little testing gatekeeping deployment technology either fully tested potentially oversold based questionable science causing harmful biased outcomes identifying working manage kinds bias mitigate concerns trustworthiness technologies systems effective approach likely need one segmented use case works across contexts improving trust systems advanced putting mechanisms place reduce harmful bias deployed systems technology mechanisms require features common vocabulary clear specific principles governance approaches strategies assurance part standards mechanisms associated performance measurements still need created adapted goal zero risk manage reduce bias way contributes equitable outcomes engender public trust challenges intertwined complex ways unlikely addressed singular focus one factor within specific use industry approach report authors sought capture common themes many ways bias defined categorized technology accomplished literature review discussions leaders field workshop bias evaluation prominent topics across broader research community work without precedent previous attempts define classify bias literature review consisted total articles books reports news publications variety perspectives survey literature identified list prominent biases present contributors societal harms list accompanying definitions presented alphabetical glossary appendix reviewed literature suggests expansion many aspects public life requires extending view mainly technical perspective one considers within social system operates taking social factors consideration necessary achieving trustworthy enable broader understanding impacts key decisions happen throughout beyond lifecycle whether technology even solution given task problem change perspective require working new stakeholders developing guidance effectively engaging social factors within technical perspective key factor area many ways institutions indirectly drive design use also practices may intend contribute inequality negative forms bias always complex social factors may overlooked especially since biases play ways may captured understood within one setting whether statistical societal bias continues challenge researchers technology developers seeking develop deploy trustworthy applications bias trust interrelate key societal question understanding paramount improving acceptance systems consistent finding literature notion trust improve public able interrogate systems engage transparent manner yet article public trust knowles richards state public need trust individual ais need instead sanction authority provided suitably expert auditors trusted creating authority requires standard practices metrics norms nist experience creating standards databases evaluating algorithms used biometric technologies since development privacy cybersecurity frameworks nist helped organizations manage risks digital environment series reports workshops intends contribute similar collaborative approach managing trustworthiness part broader stakeholder efforts identifying managing bias artificial intelligence improving trust mitigating managing bias starts identifying structure presents within systems uses propose approach derived information workshop see appendix document full bibliographic survey found lifecycle enable designers deployers better relate specific lifecycle processes types bias facilitate effective management organizations design develop technology use lifecycle keep track processes ensure delivery functional tools necessarily identify harms manage currently single global industrial lifecycle standard many versions used across multiple sectors regions range stages approach identifying managing bias proposed report adapted current versions consists three distinct stages presumed accompanying stakeholder groups approach starting point nist seeks feedback viability implementation technology devised defined elaborated design development technology constructed deployment technology used applied various individuals groups figure approach managing bias following provide key considerations examples highlight statistical biases present across various stages applications reflect interact many human cognitive societal biases inherent data modeling decision making practical processes associated use systems across sectors contexts following lifecycles utilized key guidance report centers excellence coe general services administration modernization coe organisation economic development organisation economic development another model lifecycle currently development joint technical committee international organization standardization iso international electrotechnical commission iec see stage problem formulation decision making products start stage planning problem specification background research identification quantification data take place decisions include frame problem purpose component general notion problem requiring benefitting technology solution since many downstream processes hinge decisions stage lot pressure get things central decisions individuals groups makes individuals teams power control early decisions makes reflect individual group heuristics limited points view affect later stages decisions complex ways lead biased outcomes key juncture guidance assurance governance processes assist business units data scientists collaboratively integrate processes reduce bias without cumbersome blocking progress operational settings unknown impacts current assumptions development often revolve around idea technological solutionism perception technology lead positive solutions perception often combined singular focus tool optimization odds operational scenarios increasing difficulty practitioners make sense tool output often high stakes settings seems like good idea given dataset utilized specific use case might perceived differently systems end users affected systems decisions obvious risk build decision tools settings already known discriminatory yet awareness conditions lead disparate impact negative outcomes always apparent easily overlooked production overselling tool capabilities performance whether unconscious unintentional often decisions made inadvertently lead harmful impact employed extremely negative societal ends addressing possibility optimistic potentially inflated expectations related systems risk management processes could fail communicate set reasonable limits related mitigating potential harms extreme cases tools apps fraudulent pseudoscientific prey user generally exaggerate claims goal ensure tools reject development outright order prevent disappointment harm user well reputation provider problems occur include poor problem framing basing technology spurious correlations approaches failing establish appropriate underlying causal mechanisms generally technically flawed cases often termed fire ready aim solution may mitigation rather rejection system way perceived underlying problem framed types scenarios may reinforce public distrust technology systems untested technically flawed also contribute bias technology designed use settings requires extensive testing demonstrate valid reliable performance practices currently momentum researchers include statements potential societal impacts submitting work journals conferences identifying addressing potential biases early problem formulation process important step process also complicated role power decision making consistent theme literature benefit engaging variety stakeholders maintaining diversity along social lines bias concern racial diversity gender diversity age diversity diversity physical ability kinds practices lead thorough evaluation broad societal impacts tools across three stages identifying downstream impacts may take time require involvement practitioners subject matter experts interdisciplinary professionals law social science expertise matters stakeholders bring varied experiences bear core challenge identifying harmful outcomes context shifts technology datasets seem one group may deemed disastrous others manner different user groups game certain applications tools may also obvious teams charged bringing technology market kinds impacts sometimes identified early testing stages usually specific contextual change time acquiring types resources risk associated impacts necessarily require huge allocation require deliberate planning guidance also place innovation approaching bias significantly contribute positive outcomes example many examples bias real world practices problem formulation stage may combined lack understanding downstream impacts example gender shades facial recognition evaluation project describes poor performance facial recognition systems trying detect face types gender skin type present training data example representation bias type sampling bias trends estimated one population inappropriately generalized data collected another population biased performance identified teams designed built facial recognition systems instead researchers evaluating systems performance different conditions stage kinds implicit decisions made constitutes valid face datasets selected additionally representation bias lead bigger problems biases later stages lifecycle issue referred error propagation eventually lead biased outcomes improving practices ensure inclusive representation help broaden larger teams perspectives considered relevant valid design development stage stage lifecycle modeling engineering validation take place stakeholders stage tend include software designers engineers data scientists carry risk management techniques form algorithmic auditing enhanced metrics validation evaluation optimization context software designers data scientists working design development often highly focused system performance optimization focus inadvertently source bias systems example model development selection modelers almost always select accurate models yet forde describe paper selecting models based solely accuracy necessarily best approach bias reduction taking context consideration model selection lead biased results populations example disparities health care delivery relatedly tools designed use aggregated data groups make predictions individual behavior practice initially meant remedy datasets lead biased outcomes type bias known ecological fallacy occurs inference made individual based membership within group example basing college admissions decisions individual race unintentional weightings certain factors cause algorithmic results exacerbate reinforce societal inequities surfacing inequities kind positive side effect algorithmic modeling enabling research community discover develop methods managing practices modeling tasks stage may become apparent algorithms biased contribute disparate impacts deployed cases technology taken production kind awareness remedy likely take place certain settings industries procedures clear lines accountability unfortunately tools deployed settings capturing wide array use cases scenarios particularly difficult also notable depending industry use case typically marketed easy solution necessarily require extensive support notion requires extensive monitoring belies reality easy use used extreme caution several technology companies developing utilizing guidance improve organizational decision making make practice development responsible implementing processes striving identify potential bias impacts algorithmic models example cultural effective challenge practice seeks create environment technology developers actively challenge question steps modeling engineering help root statistical biases biases inherent human decision making requiring practitioners defend techniques incentivize new ways thinking stimulate improved practices help create change approaches individuals organizations better identify mitigate organizational factors contribute bias experts also suggest use algorithmic tools specific use cases beyond use cases factor discussed section deployment additionally researchers also recommend development teams work tighter conjunction subject matter experts practitioner end users turn must consider deliberate modest approach utilizing tool output example one case biased outcome may manageable design development stage university admissions algorithm grade shown produce biased enrollment decisions incoming phd students without ground truth constitutes good fit construct developed using prior admission data put production model ended trained different job intended also known target leakage instead assessing student quality model learned previous admissions officer decisions another issue candidate quality truly known student matriculates case good example data hubris overstated claims arise big data analysis particularly problematic using data make causal claims inherently inductive method pattern recognition deployment stage stage users start interact developed technology sometimes create unintended uses stakeholders deployment often different types end users directly interact technology tools profession includes operators subject matter experts interpret output make support decisions discriminatory impact since many tools skip deployment specified expert end user marketed directly used general public intended uses given tool often quickly overcome reality additionally members public necessarily directly interact technology affected tool deployment individuals data used modeling sometimes without knowledge decisions affect lives based factors live work example algorithms used ride hailing apps learned landscape neighborhoods charged citizens live causing disparate impact kind systemic discriminatory pricing perpetuated citizens neighborhood without knowledge whether use app due fact live intended context actual context people start interact system early design development decisions poorly incompletely specified based narrow perspectives exposed leaves process vulnerable additive biases either statistical nature related human decision making behavior example designing compensate activity biases algorithmic models may built data active users likely creating downstream system activity reflect intended real user population basing system actions unrepresentative sample significant impact example considering stem ads might seen often men due marketing algorithms optimize cost placement women intended audience ads never saw deployment stage also offers interesting window perceptions uses differ based distance technology focus perceptions technology designed solve question market product innovate new area design development focus building testing operationalizing technology typically time market accuracy key criteria technology deployed used different settings different purposes see perceptions turn unintended use cases even distrust one case predictive analytics university admissions operators receiving end tool output ones sound warning biases although study based small number participants interviews admissions officials suggest believe validity risk scores thought scores depersonalized interactions students understand scores calculated kinds scenarios experts utilize rely upon automated results like college admissions example highly complex relatively understudied one key issue finding configuration enables system used way optimally leverages instead replaces user expertise often significant challenge since domain experts developers often lack common vernacular contribute miscommunication misunderstood capabilities promise quantitative approaches domain experts may tend offload method validation system end users may also subconsciously find ways leverage perceived objective results cover biases system side developer communities may presume method validation level actually present kinds loopholes create conditions operationalize technology quite ready use especially settings contextual gaps lead performance gaps distance technology also contribute different types performance gaps gaps intention gaps originally intended versus developed product deployed also gaps performance based intention gaps tool designed developed used specific setting tested use conditions clearer expectations intended performance tool deployed goes original intent idea impact assessment identified drift tool repurposed used unforeseen ways another important gap contributes bias relates differences interpretability requirements users developers previously discussed groups invent produce technology specific intentions use unlikely aware ways given tool repurposed individual differences humans interpret model output system designers take differences consideration contribute misinterpretation output differences combined societal biases found datasets human cognitive biases automation complacency particularly relevant deployment stage end users may unintentionally offload decisions automated tool cause significant negative impacts practical improvements one approach managing bias risks associated gaps described deployment monitoring auditing counterfactual fairness technique used researchers bridge gaps laboratory real world issue described individuals training data already equal opportunity algorithms enforcing remedy using grade algorithm example instead using previous admission decisions predictor model would consider seek compensate various social biases could impact student application happens capturing social biases make clear implicit prediction accuracy fairness unfair identifying standards practice implementing types risk management tools techniques focus future activities summary section described challenge bias proposed approach considering manage three stages modeled development lifecycle section also shows type bias manner presentation may differ bias occur across stages summarize help illustrate point figure shows exemplar bias could present within three stages figure example bias presentation three stages modeled lifecycle equal opportunity conclusion next steps identified many ways algorithms create conditions discriminatory decision making effort identify technical requirements cultivating trustworthy responsible report suggests approach managing bias approach intended foster discussion path forward collaborative development standards framework rather identifying tackling specific biases within cases report suggests need address nature bias associating applicable biases within specific stages modeled lifecycle effective management mitigation nist interested obtaining feedback broader community proposed approach via public comment series public events broader research community practitioners users many valuable insights recommendations offer managing mitigating bias identifying techniques include framework seeks promote trustworthiness responsibility requires approach actively representative includes broad set disciplines stakeholders allow interested parties move forward guidance effective implementable accurate realistic fit purpose potential increase public trust advance development use beneficial technologies systems end report concludes bias neither new unique goal zero risk rather identifying understanding measuring managing reducing bias standards guides needed terminology measurement evaluation bias bias reduction techniques needed flexible applied across contexts regardless industry nist plans develop framework trustworthy responsible participation broad set stakeholders ensure standards practices reflect viewpoints traditionally included development nist collaboratively develop additional guidance assurance governance practice improvements well techniques enhancing communication among different stakeholder groups make necessary progress towards goal trustworthy responsible nist intends act hub broader community interest collaboratively engage experts stakeholders address challenges end nist host variety activities area core building blocks trustworthy accuracy explainability interpretability privacy reliability robustness safety security resilience bias appendices appendix glossary table presents glossary definition term accompanying reference goal contribution glossary aggregate terms common usage relevance bias definitions selected based either recently published papers bias community seminal work area term associated multiple definitions bias identified relevant definition selected adapted references provided intended indicate specific endorsement assign originator credit table bias terminology table lists definitions accompanying references select biases bias type definition activity bias type selection bias occurs get training data active users rather less active inactive amplification bias arises distribution prediction outputs skewed comparison prior distribution prediction target annotator bias human reporting bias users rely automation heuristic replacement information seeking processing automation complacency humans automated systems skills attenuated spelling autocorrect spellcheckers behavioral bias systematic distortions user behavior across platforms contexts across users represented different datasets cognitive bias systematic errors human thought based limited number heuristic principles predicting values simpler judgmental operations concept drift emergent bias use system outside planned domain application common cause performance gaps laboratory settings real world consumer bias arises algorithm platform provides users new venue within express biases may occur either side party digital interaction content production bias arises structural lexical semantic syntactic differences contents generated users data generation bias arises addition synthetic redundant data samples dataset deployment bias arises systems used decision aids humans since human intermediary may act predictions ways typically modeled system detection bias systematic differences groups outcomes determined may cause underestimation size effect evaluation bias arises testing external benchmark populations equally represent various parts user population use performance metrics appropriate way model used exclusion bias specific groups user populations excluded testing subsequent analyses feedback loop bias effects may occur algorithm learns user behavior feeds behavior back model funding bias arises biased results reported order support satisfy funding agency financial supporter research study historical bias arises models trained past potentially biased decisions inherited bias error propagation arises tools built machine learning used generate inputs machine learning algorithms output tool biased way bias may inherited systems using output input learn models institutional bias systemic bias tendency procedures practices particular institutions operate ways result certain social groups advantaged favored others disadvantaged devalued need result conscious prejudice discrimination rather majority simply following existing rules norms institutional racism institutional sexism common examples interpretation bias form information processing bias occur users interpret algorithmic outputs according internalized biases views linking bias arises network attributes obtained user connections activities interactions differ misrepresent true behavior users loss situational awareness bias automation leads humans unaware situation control system given back situation humans machines cooperate unprepared assume duties loss awareness automation taking care measurement bias arises features labels proxies desired quantities potentially leaving important factors introducing group noise leads differential performance mode confusion bias modal interfaces confuse human operators misunderstand mode system using taking actions correct different mode incorrect current situation cause many deadly accidents also source confusion everyday life popularity bias form selection bias occurs items popular exposed less popular items population bias arises statistics demographics user characteristics differ original target population user population represented actual dataset platform presentation bias biases arising information presented web via user interface due rating ranking output users biased interaction ranking bias idea results relevant important result clicks results sampling bias representation bias arises due sampling subgroups causing trends estimated one population generalizable data collected new population selection bias bias results using nonrandomly selected samples estimate behavioral relationships ordinary specification bias arises missing data problem selective adherence inclination selectively adopt algorithmic advice matches beliefs stereotypes societal bias ascribed attributes social groups largely determined social context arise adaptable byproduct human cognition statistical bias systematic tendency estimates measurements true values note statistical biases arise systematic opposed random error note statistical bias occur absence prejudice partiality discriminatory intent temporal bias bias arises differences populations behaviors time training data bias biases arise algorithms trained one type data extrapolate beyond data uncertainty bias epistemic uncertainty arises predictive algorithms favor groups better represented training data since less uncertainty associated predictions user interaction bias arises user imposes biases behavior interaction data output results etc appendix collaborative work report based series collaborative events including literature review input leaders field ongoing discussions workshop broad evaluation significant themes across community interest detailed information events described literature review nist implemented broad review materials shared pieces focused bias within technologies use artificial intelligence review incorporated content described bias societal perspective existing technologies development processes factors influence development implementation adaptation ensure perspectives literature identified across variety publication types including journals popular news media books organizational reports conference proceedings presentations across publications literature review topics represent wide range stakeholder perspectives challenges current future implementations workshop bias recognizing lack consensus regarding several fundamental concepts identifying understanding bias nist convened virtual workshop august experts researchers stakeholders variety organizations sectors whose work focuses topic workshop consisted panel discussions data algorithmic bias followed five contemporaneous breakout sessions notes workshop organizers facilitators scribes reviewed key takeaways themes workshop participants suggested forums workshops like one held august important maintaining awareness alignment current challenges future solutions participants also referred term nature challenge key takeaways included described throughout report references abdollahpouri mansoury burke mobasher unfairness popularity bias recommendation aguera arcas mitchell todorov physiognomy new clothes medium aitken toreini carmichael coopamootoo elliott van moorsel establishing social licence financial technology reflections role private sector pursuing ethical data practices big data society ajunwa paradox automation intervention cardozo rev ajunwa friedler scheidegger venkatasubramanian hiring algorithm predicting preventing disparate impact ssrn electric journal busuioc processing algorithmic advice automation bias versus selective adherence angwin larson mattu kirchner propublica machine bias software used across country predict future criminals biased propublica bias web commun acm bailey put away machine learning hammer criminality nail wired bajorek voice recognition still significant race gender biases harvard business review barocas biega fish niklas stark design build deploy proceedings conference fairness accountability transparency association computing machinery new york usa barocas selbst big data disparate impact california law review bartlett morse stanton wallace discrimination fintech era national bureau economic research bary artificial intelligence could replace credit scores reshape get loans market watch benjamin race technology abolitionist tools new jim code john wiley sons bogen ways hiring algorithms introduce bias harvard business review bogen rieke help wanted examination hiring algorithms equity bias upturn boyarskaya olteanu crawford overcoming failures imagination infused system development deployment boyd crawford critical questions big data provocations cultural technological scholarly phenomenon information communication society brayne enter dragnet logic magazine broniatowski psychological foundations explainability interpretability artificial intelligence nist broussard artificial unintelligence computers misunderstand world mit press brown gaertner blackwell handbook social psychology intergroup processes john wiley sons buolamwini gebru gender shades intersectional accuracy disparities commercial gender classiﬁcation proceedings machine learning research burke texas stop using controversial algorithm evaluate applicants inside higher inside higher caliskan bryson narayanan semantics derived automatically language corpora contain biases science centre medicine catalogue bias catalog bias chandler munday dictionary media communication oxford university press chouldechova fair prediction disparate impact study bias recidivism prediction instruments big data coalition critical technology abolish techtoprisonpipeline design justice escape matrix domination journal design science crawford artificial intelligence white guy problem new york times crawford time regulate interprets human emotions nature criado perez invisible women data bias world designed men abrams press danks london algorithmic bias autonomous systems proceedings international joint conference artificial intelligence international joint conferences artificial intelligence organization melbourne australia dastin amazon scraps secret recruiting tool showed bias women reuters bias journal epidemiology community health dietvorst simmons massey algorithm aversion people erroneously avoid algorithms seeing err exp psychol dietvorst simmons massey overcoming algorithm aversion people use imperfect algorithms even slightly modify management science ignazio klein data feminism mit press dormehl algorithms great also ruin lives wired dwork hardt pitassi reingold zemel fairness awareness elish barocas plasek ferryman social economic implications artificial intelligence technologies new york epic algorithms criminal justice system risk assessment tools electronic privacy information center eubanks automating inequality tools profile police punish poor martin press evans mathews new york regulator probes unitedhealth algorithm racial bias wall street journal fast horvitz trends public perception artificial intelligence proceedings aaai conference artificial intelligence association advancement artificial intelligence feathers major universities using race high impact predictor student success markup markup fish stark reflexive design fairness human values formal models forde cooper littman model selection disparate impact deep learning applications friedman mccarthy employment law red flags use artificial intelligence hiring american bar association fry hello world human age algorithms norton company furman haney home smart connected international conference interaction springer cham gaffney matias caveat emptor computational social science missing data reddit corpus plos one gianfrancesco tamang yazdany schmajuk potential biases machine learning algorithms using electronic health record data jama intern med goel shroff skeem slobogin accuracy equity jurisprudence criminal risk assessment ssrn journal goodman flaxman european union regulations algorithmic right explanation aimag grother ngan hanaoka face recognition vendor test part demographic effects national institute standards technology gaithersburg guo hao stanford vaccine algorithm left frontline doctors mit technology review hall gill cox responsible machine learning actionable strategies mitigating risks driving adoption reilly media sebastopol hardt price srebro equality opportunity supervised learning harlen schnuck objective biased bayerischer randfunk heckman sample selection bias specification error econometrica hellström dignum bensch bias machine learning good artificial intelligence affects financial consumers brookings hill flawed facial recognition leads arrest jail new jersey man new york times iso statistics vocabulary symbols part general statistical terms terms used probability information technology vocabulary international organization standardization geneva switzerland information technology big data overview vocabulary international organization standardization geneva switzerland modernization coe coe guide ethics general services administration jacobs blodgett barocas daumé wallach meaning measurement bias lessons natural language processing proceedings conference fairness accountability transparency association computing machinery new york usa jacobs wallach measurement fairness proceedings acm conference fairness accountability transparency jeong kim park bennis kim device machine learning federated distillation augmentation private data stat johndrow lum algorithm removing sensitive information application recidivism prediction ann appl stat kamiran karim verwer goudriaan classifying socially sensitive data without discrimination analysis crime suspect dataset ieee international conference data mining workshops ieee brussels belgium kerr barry kelleher expectations artificial intelligence performativity ethics implications communication governance big data society kirchner goldstein access denied faulty automated background checks freeze renters markup markup kleinberg lakkaraju leskovec ludwig mullainathan human decisions machine predictions quarterly journal economics klempin grant ramos practitioner perspectives use predictive analytics targeted advising college students community college research center knowles richards sanction authority promoting public trust proceedings acm conference fairness accountability transparency association computing machinery new york usa kusner loftus russell silva counterfactual fairness stat lambrecht tucker algorithmic bias empirical study apparent based discrimination display stem career ads ssrn journal ledford millions black people affected racial bias algorithms nature lee yee toward data digital health communication research theory matters age big data front commun leino black fredrikson sen datta bias amplification stat lerman hogg leveraging position bias improve peer recommendation plos one liptak sent prison software programʼs secret algorithms new york times maddox rumsfeld payne questions artificial intelligence health care jama malik hierarchy limitations machine learning econ math stat mcgraw figueroa shepardson bonett architectural risk analysis machine learning systems toward secure machine learning berryville institute machine learning clarke county mehrabi morstatter saxena lerman galstyan survey bias fairness machine learning miller chang johnson terveen hecht blissfully happy ready fight varying interpretations emoji proceedings international conference web social media icwsm aaai press misra zitnick mitchell girshick seeing human reporting bias visual classifiers noisy labels ieee conference computer vision pattern recognition cvpr ieee las vegas usa mitchell artificial intelligence guide thinking human farrar straus giroux mitchell shadlen mirror mirror reflections quantitative fairness shira mitchell statistician shira mitchell statistician moss metcalf ethics owners data society moss metcalf high tech high risk tech ethics lessons pandemic response patterns mulligan kroll kohli wong thing called fairness disciplinary confusion realizing value technology proc acm interact nist framework improving critical infrastructure cybersecurity version national institute standards technology gaithersburg nist leadership plan federal engagement developing technical standards related tools national institute standards technology nist nist privacy framework tool improving privacy enterprise risk management version national institute standards technology gaithersburg noble algorithms oppression search engines reinforce racism nyu press obermeyer powers vogeli mullainathan dissecting racial bias algorithm used manage health populations science olteanu castillo diaz kıcıman social data biases methodological pitfalls ethical boundaries front big data neil weapons math destruction big data increases inequality threatens democracy broadway books organisation economic development recommendation council artificial intelligence oecd legal instruments organization scientific area committees forensic science osac preferred terms osac preferred terms pandey caliskan iterative bias ridehailing measuring social bias dynamic pricing million rides passi barocas problem formulation fairness proceedings conference fairness accountability transparency pfeffer mayer morstatter tampering twitter sample api epj data sci picard watkins rempal kerodal beyond algorithm pretrial reform risk assessment racial fairness center court innovation picchi job hunters face new hurdle impressing cbs news plank language nlp prunkl ashurst anderljung webb leike dafoe institutionalizing ethics broader impact requirements nature machine intelligence raghavan barocas challenges mitigating bias algorithmic hiring brookings redden harm data scientific american roberts driggs thorpe gilbey yeung ursprung etmann mccague beer teng rudd sala schönlieb common pitfalls recommendations using machine learning detect prognosticate using chest radiographs scans nature machine intelligence dencik edwards mean solve problem discrimination hiring social technical legal perspectives automated hiring systems schellmann auditors testing hiring algorithms bias big questions remain mit technology review selbst boyd friedler venkatasubramanian vertesi fairness abstraction sociotechnical systems proceedings conference fairness accountability transparency fat acm press atlanta usa silva kenney algorithms platforms ethnic bias commun acm simonite algorithm blocked kidney transplants black patients wired singh ramamurthy understanding racial bias health using medical expenditure panel survey data stat sipior considerations development use response international journal information management smith anderson automation everyday life pew research center specia siri alexa reinforce gender bias finds new york times suresh guttag framework understanding unintended consequences machine learning stat tan celis assessing social intersectional biases contextualized word representations stat tobin matsakis china home growing market dubious emotion recognition rest world tromble data gone critical reflection academic digital research age social media society tufekci big questions social media big data representativeness validity methodological pitfalls tversky kahneman judgment uncertainty heuristics biases science ware records computers rights citizens rand corporation santa monica washington kuo whose side ethics codes power responsibility social good proceedings conference fairness accountability transparency acm barcelona spain waters miikkulainen grade machine learning support graduate admissions aimag weber yurochkin botros markov black loans matter distributionally robust fairness fighting subgroup discrimination west brookings survey finds worries impact jobs personal privacy concern fall behind china brookings west brookings survey finds divided views artificial intelligence warfare support rises adversaries developing brookings west allen turning point brookings wexler computer program keeps jail new york times wynants calster collins riley heinze schuit bonten dahly damen debray jong vos dhiman haller harhay henckaerts heus kammer kreuzberger lohmann luijken martin mclernon navarro reitsma sergeant shi skoetz smits snell sperrin spijker steyerberg takada tzoulaki kuijk bussel horst royen verbakel wallisch wilkinson wolff hooft moons smeden prediction models diagnosis prognosis systematic review critical appraisal bmj state loomis
