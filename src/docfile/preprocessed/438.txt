auditing achieving intersectional fairness classification problems giulio quantumblack mckinsey company london united kingdom fairness oliinyk quantumblack mckinsey company london united kingdom waton quantumblack mckinsey company london united kingdom ines marušić quantumblack mckinsey company london united kingdom georgatzis quantumblack mckinsey company london united kingdom abstract machine learning algorithms extensively used make increasingly consequential decisions people achieving optimal predictive performance longer focus particularly important consideration fairness respect race gender sensitive attribute paper studies intersectional fairness intersections multiple sensitive attributes considered prior research mainly focused fairness respect single sensitive attribute intersectional fairness comparatively less studied despite critical importance safety modern machine learning systems present comprehensive framework auditing achieving intersectional fairness classification problems define suite metrics assess intersectional fairness data model outputs extending known fairness metrics propose methods robustly estimating even intersectional subgroups underrepresented furthermore develop techniques mitigate detected intersectional bias classification model techniques rely assumptions regarding underlying model preserve predictive performance guaranteed level fairness finally give guidance practical implementation showing proposed methods perform dataset introduction fairness growing topic field machine learning models built determine events loan approvals parole decisions thus critical models discriminate individuals basis race gender sensitive attribute learning replicate exacerbate biases inherent society much algorithmic fairness literature thus far focused fairness respect individual sensitive attribute work consider fairness anintersection sensitive attributes focus ensuring fairness groups defined multiple sensitive attributes example black women instead black people women ensuring intersectional fairness critical safe deployment modern machine learning systems stark example intersectional bias deployed systems discovered buolamwini gebru showed several commercially available gender university warwick department statistics work done intern systems facial image data substantial intersectional accuracy disparities considering gender race represented via fitzpatrick skin type women misclassified group accuracy drop compared men buolamwini gebru emphasize need investigating intersectional error rates noting gender skin type alone paint full picture regarding distribution misclassifications hart notes medical data randomized control trials often biased favor white men therefore model trained data may exacerbate existing healthcare inequalities study increased risk maternal death among ethnic minority women ameh van den broek note limited data specifically black ethnic minority women born emphasized need reliable statistics understand scale problem contributions present comprehensive framework auditing achieving intersectional fairness consisting three pillars metrics measuring intersectional fairness datasets model outputs methods robustly estimating metrics iii methods ensuring intersectional fairness classification problems first define metrics measuring intersectional fairness datasets model outputs extending fairness metrics case intersectionalities work builds directly upon concept fairness introduced foulds specifically extend definition differential fairness statistical parity elift impact ratio metrics data equal opportunity equalized odds metrics model outputs enables practitioners assess intersectional fairness multiple mutually exclusive lenses second propose techniques robustly measure intersectional fairness techniques address concerns marginalized intersectional subgroups even underrepresented available datasets due biases importantly provide theoretical guarantees demonstrate performance estimators qualitatively experimentally synthetic dataset third develop algorithms mitigate detected intersectional bias binary classification model methodologies threshold risk scores randomize predictions separately intersection sensitive attributes combining jun extending work hardt methods maximize predictive performance whilst guaranteeing intersectional fairness furthermore formulation allows practitioner simultaneously focus multiple fairness metrics thus allowing control multiple facets model bias provide implementation details demonstrate utility methods experimentally adult income prediction problem paper structure discuss related work section define intersectional fairness metrics section proving theoretical properties section presenting methods robustly estimating section section frame optimization problem aims preserve good predictive performance ensuring intersectional fairness introduce formulations sections binary score predictors respectively demonstrate utility methods experimentally synthetic dataset adult dataset section section conclude suggest future work proofs notes reproducibility results presented supplementary material running example throughout paper consider practical application auditing mitigating bias using census adult dataset uci repository aim predict whether individual income greater using attributes dataset contains multiple sensitive attributes paper focus following three age gender race related work one fairness definition suitable use cases application domains indeed different fairness metrics proposed mutually incompatible constitutes appropriate fairness metric depends application societal context regulatory requirements one broadly divide existing fairness metrics group individual ones group fairness partitions population groups according sensitive attributes aims ensure similar treatment respect fixed statistical measure individual fairness seeks individuals similar features treated similarly regardless sensitive attributes assessing group fairness dataset model output becomes much challenging considering multiple sensitive attributes number generated subgroups grows exponentially number attributes considered making difficult inspect every subgroup fairness due computational well data sparsity issues first challenge therefore come fairness metrics accommodate large number intersectional subgroups work builds directly upon fairness metric introduced foulds metric satisfies important desiderata overlooked metrics considers multiple sensitive attributes protects subgroups defined intersections individual sensitive attributes black women women respectively iii safeguards minority groups aims rectifying systematic differences subgroups foulds demonstrate fairness also satisfies important properties providing privacy economical generalization guarantees also extend original definition handle confounders propose deep neural network classifiers handle intersectional fairness foulds focus mainly enabling subtle understanding unfairness single sensitive attribute whereas present multiple metrics allow nuanced analysis intersectional discrimination foulds propose pointwise estimate intersectional bias found unstable practice illustrated example later work foulds use elegant hierarchical approach probabilistic models overcome issue instability provide uncertainty estimates formulation however requires careful tuning computationally demanding proposed ones several methods proposed handling intersectional bias either make use algorithms based visual analytic tools intersectional bias detection chung suggest method find underperforming subgroups dataset divided granular groups considering features subgroup statistically significant loss found contrast lakkaraju use approximate explanations describe subgroup outcomes well detecting discriminatory bias another line research focused achieving fairer models three possible points intervention mitigate unwanted bias machine learning pipeline training data learning algorithm predicted outputs associated three classes bias mitigation algorithms postprocessing methods transform data remove bias extract representations contain information related sensitive attributes methods modify model construction mechanism take fairness account methods transform output model order decrease discriminatory bias kearns propose demonstrate performance training algorithm mitigates intersectional bias imposing fairness constraints protected subgroups work generalization oracle efficient algorithm agarwal case infinitely many protected subgroups contrast develop novel method postprocessing methods popular practical applications interfere training process thus suitable runtime environments addition methods model agnostic privacy preserving require access model features sensitive attributes work hardt aims ensure equal opportunity two subgroups population defined single binary sensitive attribute achieve randomly flipping predictions order mitigate discriminatory bias propose another approach treating model predictions differently depending subgroup membership combine approaches expand case intersectional fairness metrics intersectional fairness section introduce fairness metrics handle intersections multiple sensitive attributes metrics applied assess fairness either data model outputs robustly estimating practice due subgroup underrepresentation indeed minority groups may even severely underrepresented dataset compared true representation general population one cause bias data collection practices defining metrics section section present three approaches robustly estimating intersectional impact ratio approach applied intersectional fairness metric notation letpbe number different sensitive attributes denote apdisjoint sets sensitive attributes represent gender forth space intersections denoted therefore specific element particular combination attributes woman black italian suppose access finite dataset nobservations denoted xirepresents individual features including sensitive attributes binary outcome interpret positive outcome negative otherwise denoting ythe random variable describing true outcomes furthermore let sbe discrete random variable support brevity denote probability mass function µsis probability individual sensitive attributes analogously denote probability given individual positive outcome finally also denote probability individual sensitive attributes shas positive outcome make explicit assumptions distribution yorsbut shall assume given classifier denote prediction theithindividual ˆythe corresponding random variable describing predicted outcomes importantly make assumptions model constructed regard black box definitions metrics introduce intersectional fairness metrics datasets model outputs metrics based fairness framework foulds metrics introduced paper seen relaxations fairness metrics single sensitive attribute motivated fact number intersections grows exponentially sensitive attributes table define fairness metrics assess intersectional bias data table defines metrics assess intersectional bias model outputs exception fairness statistical parity introduced foulds intersectional fairness definitions metrics table knowledge novel contributions prove theoretical properties theorem although restrict analysis fairness metrics binary outcomes easily extendedtable fairness metrics data fairness metric intersectional definition elift impact ratio slift table fairness metrics model fairness metric intersectional definition statistical parity demographic parity tpr parity equal opportunity fpr parity equalized fairness satisfied tpr fpr parity categorical case simply requiring hold possible outcomes metrics parameterized note achieving perfect fairness respect given metric moreover fairness allows compare bias two different models particular assume two models achieve fairness quantity exp interpreted multiplicative one model bias respect phenomenon known bias amplification let apply metrics running adult dataset example focusing two sensitive attributes gender race income distribution population differ across race gender subgroups elift ratio would close ϵwould close would like collect representative sample intersection satisfies requirements census data see high income rate white men whilst black women ϵvalue elift driven subgroup largest absolute difference log proportion high incomes base rate entire population case subgroup gender race women performance metric intersectional false positive rate fpr parity fair model similar fprs predicting high income individuals white men black women say well combinations sensitive attributes key desideratum intersectional fairness metric intersectional fairness imply fairness respect individual sensitive attributes arbitrary subsets thereof theorem proves indeed case fairness satisfied also satisfied considered possible combination theorem ack fairness satisfied metrics tables space intersections fairness also satisfied space metric robust estimation metrics tackle problem auditing discriminatory bias access finite dataset particular interested case combinations sensitive attributes may underrepresented data often case datasets usually due historical societal biases first make clear mean auditing intersectional fairness explore three different methodologies achieve smoothed empirical estimation fairness metrics directly computed data bootstrap estimation measure uncertainty empirical estimates iii bayesian estimation provide credible intervals byestimating level intersectional bias mean computing minimum value chosen intersectional fairness conditions one tables hold simplicity exposition focus impact ratio reasoning readily applied metrics per table estimating level impact ratio bias means computing practical applications often interest also check attributes big values computing rmay appear straightforward could letϵi max however values usually unknown estimating data values challenging instances particular combination attributes smay present dataset moreover previously mentioned minority subgroups may even severely underrepresented dataset compared true representation general population making problem even harder example adult dataset training set contains individuals white people leaves hundreds people smallest minority groups might also low rates high income splitting dataset additional sensitive attributes produce subgroups consisting high earners methods recognize subgroups fewer individuals produce noisier estimates quantify uncertainty smoothed empirical estimation simple approach directly estimate data proposed foulds particular set sis empirical count occurrences individuals sensitive attributes sand positive outcome dataset nsis total number individuals attributes introduce smoothing parameters smay small due data sparsity note equation represents expected posterior value model prior parametersα final estimate ϵis ˆϵi max estimation procedure requires computing possible combinations attributes leading computational complexity general hard tune parameters αand βproperly large values either αorβwill introduce additional bias small values βwill solve data sparsity problem therefore procedure robust ˆϵi rwill generally biased uncertainty quantification provided nevertheless prove proposition dataset size grows smoothed empirical estimator converges true value regardless chosen smoothing parameters although result holds practice one would choose set zero smoothing desired proposition smoothed empirical estimate ϵfor fairness metric consistent bootstrap estimation propose bootstrap estimation procedure provide confidence intervals estimate ˆϵi generate bdifferent datasets sampling replacement nobservations original dataset bootstrap sample obtain estimate bas equation final estimate ˆϵi ris obtained averaging samples empirical confidence intervals easily constructed computational complexity practice also observe computational overhead due construction bdatasets notice generated datasets may contain instances specific attributes producing undefined values smoothing parameters βare set zero bayesian estimation motivated form equation propose bayesian approach considering likelihood setting conjugate prior beta posterior therefore tractable given use monte carlo simulation techniques get estimate particular simulate mvalues posterior use compute estimate ras equation computational complexity averaging soconstructed sample gives final estimate moreover procedure promptly provides credible intervals finally note simulated values always greater zero need resort smoothing prior parameters βcan chosen using domain knowledge set close zero suggest prior information follows proposition estimator also consistent proposition bayesian estimate ϵfor fairness metric consistent classifier model defined section different metrics assessing intersectional fairness model outputs section present postprocessing methods mitigate detected intersectional bias classification model argue possible best way ensure fairness collect representative data retrain model nevertheless commonly case historical data conscious unconscious bias present available training new classifier may impractical due cost time constraints moreover practice often access outputs trained classifier knowledge predictions made either model hard interpret access model motivates need develop techniques model agnostic indeed make assumptions model training mechanism require access outputs sensitive attributes refer binary predictor outputs score predictor outputs propose framework allow practitioner make model accuracy fairness let return running example data loan applications model trained adult dataset without likely slightly higher overall performance one driven majority subgroup dataset imbalanced model may incorrectly deny loans often black women white men indicating intersectional bias depending desired notion fairness proposed ensure model balanced performance across subgroups gives proportion loans every subgroup construct derived predictor improved fairness respect one chosen metrics particular combining approaches hardt propose class derived predictors able handle classifiers returning either binary predictions scores section presents general framework construction derived predictors explore compute binary score predictor sections respectively crucially value derived predictor depends given prediction individual combination sensitive attributes definition predictor random variable whose distribution depends solely classifier predictions intersection sensitive attributes aim construct derived predictor transforming predictions given classifier achieves better fairness terms one fairness metric model returns binary predictions resort randomization randomly flipping predictions hand model returns scores also threshold scores retrieve binary prediction combine two approaches following definition definition randomized thresholding derived predictor given classifier returning predictions randomized thresholding derived predictor rtdp bernoulli random variable iis indicator function tuning interpret equation follows given individual predicted score ˆyand combination sensitive attributes first construct binary prediction thresholding τsand specific probability accommodate possibility reverse keep particular sis probability flipping would negative prediction sis probability keeping positive prediction note definition covers also case model binary predictor explore case detail section consequential applications randomization may desired permissible due legal requirements case definition allows construct deterministic derived predictor setting formulation optimization problem construct rtdp solving optimization problem order assess performance model introduce loss function given true outcomes returns cost making prediction following approach hardt without loss generality assume making correct predictions contribute loss indeed either bonus penalty desired correct predictions incorporated changing values andl therefore minimizing expected loss function preserves good predictive performance take slightly different approach aim maximize utility function defined advantage approach requires tuning constant cthat interpreted cost making positive prediction prove approach special case framework propose proposition maximizing immediate utility function constant equivalent minimizing setting one control level bias model selecting desired value ϵfor chosen one intersectional metrics table consider two possible approaches find unknown parameters minimizing expected loss subject selected fairness metric satisfied chosen adding penalty term expected loss values parameters satisfy required fairness constraint two approaches principle equivalent practical implementations may differ different numerical optimization routines need used instance one established fairness guideline rule statistical parity corresponding requiring fairness statistical parity hold theorem either consider constraint parameter space optimization problem consider minimizing fortappropriately large note fairness metric table considered constraint instance section show achieve better equalized odds intersectional fairness show proposition expected loss rewritten weighted sum false positive rate false negative rate model weights depend proposition minimizing equivalent minimizing binary predictor predictor returns solely binary predictions set aand tune probabilities sand sto construct derived predictor find unknown parameters minimize expected loss subject required fairness constraint proposition shows optimization problem efficiently solved via linear programming proposition minimizing equation variables subject constraints fairness metrics table threshold linear programming problem conclude case binary predictor rtdp computed polynomial time unknown constant base ratesµs model metrics fpr fnr estimated data via techniques introduced section score predictor focus generic setting model outputs form scores high scores indicate high probability positive outcome assume knowledge scores computed treat underlying model black box construct rtdp optimize probabilities sand thresholds τsfor corresponding total optimize although observe overfitting experiments section applications may necessary use add regularization terms reduce degrees freedom imposing explore detail deterministic scenario section case thresholds probabilities optimized discussed section deterministic randomization desired construct rtdp fixing case particular interest randomization may undesirable applications instance assessing judicial decisions carefully tune thresholds drive predictive performance model figure illustrates constrained optimization routine explanatory purposes consider intersections sensitive attributes model performance differs across intersectional subgroups apparent roc curves subgroup note value τsuniquely determines point oneach curve chosen level fairness determines region around roc curve roc curves must also lie therefore optimal thresholds must intersection compact spaces practice points roc curve observed optimum found exhaustive search alternatively roc curves may estimated data note fairness constraints strict admissible solution may always return positive negative predictions using randomization focus constructing rtdp finding optimal thresholds τsand probabilities first investigate whether applying randomization deteriorates model performance intuitively case given model performs reasonably well every intersection attributes formalized proposition show randomization improve predictive accuracy model performance metrics within certain bounds proposition given score predictor solving minτs rtdp definition equivalent setting solving minτse even randomization worsens predictive performance may still improve intersectional fairness find optimal thresholdsτsand probabilities first consider simple approach name sequential first find optimal thresholds τswhen fairness constraints imposed applying thresholds convert scores ˆyto binary predictions find optimal probabilities sthat achieve desired fairness constraints via linear programming roc curve admissible region intersection roc curve admissible region intersection roc curve admissible region intersection figure example deterministic equal opportunity intersections sensitive attributes selected level ϵdetermines admissible regions table overview proposed optimization approaches using rtdp definition scenario method existance binary outcomes ˆyi thresholding possible optimize rtdp randomization choosing sby using proposition minimise equation values fairness constraint randomization appropriate regulatory reasons optimize rtdp deterministically choosing thresholds τsadmissible region may trivial solutions fairness constraints strict randomization thresholding sequential approach optimize rtdp first selecting thresholds without fairness constraints choosing sby using proposition find solution given fairness constraint guarantee return global optimum randomization thresholding overall approach optimize rtdp jointly thresholds τsand randomly flipping probabilities sguaranteed find solution given fairness constraint guarantee return global optimum proposition procedure may return acceptable result case hand guarantee return global optimum different approach refer overall postprocessing solve following optimization problem minτsf optimal cost function value found solving optimization problem variables fixed section although may seem adding extra layer complexity note values efficiently computed via linear programming general since model metrics estimated finite dataset piecewise constant function therefore optimization routines unlikely succeed gradient objective function defined zero points discuss supplementary material details optimizer use discuss viable approaches conclusion summarize approaches table experiments perform following experiments comprehensively evaluate methods auditing achieving intersectional fairness section apply techniques section estimate level intersectional fairness synthetic dataset purposefully constructed one subgroup underrepresented common scenario practice due societal data collection biases section estimate level intersectional fairness trained classifier mitigate detected intersectional bias using techniques section consider intersectional fairness sensitive attributes underrepresented subgroup synthetic dataset contains two sensitive attributes one binary one possible values intersectional subgroups one denoted sparse corresponding dataset details dataset generation mechanism supplementary material concreteness focus intersectional fairness impact ratio true value ris known equal first show figure estimates behave size dataset increases analyze confidence intervals applicable consistent theoretical guarantees propositions methods converge true value dataset size grows furthermore smaller dataset sizes confidence intervals provided bootstrap method generally wider ones obtained via bayesian approach surprising estimate ris particularly unstable instances subgroup replicated one bootstrapped datasets case driven chosen values smoothing parameters second approximate mean squared error mse three estimators shown figure bayesian estimate performs better considered dataset sizes small dataset sizes bootstrap estimate performs slightly worse empirical estimate illustrating one get biased estimates rif one intersectional subgroup experiment poorly represented bootstrapped dataset overall observe smoothed empirical estimator requires considerably less computational effort two methods however unlike bootstrap bayesian estimates provide insight reliable estimate moreover bayesian estimation general faster bootstrap posterior parameters need computed computational overhead observed adult income prediction return running example focusing three sensitive attributes gender age race treat age binned gender binary sensitive attributes race five values treat model black box details experiment configuration supplementary material first audit intersectional fairness dataset model outputs compare performances different techniques auditing intersectional fairness figure shows minimum values ϵsuch fairness satisfied different intersectional metrics data classifier outputs results indicate unfairness across different metrics fairness fpr parity worst note confidence intervals bayesian procedure generally wider due model performing poorly subgroups leading high variance estimates achieving intersectional fairness table predictive performance given binary predictor models adult training set gender age race sensitive attributes fairness constraints fairness constraint given binary predictoroptimal score modelrandomization onlydeterministic sequential overall tpr fpr expected loss function dataset empirical bootstrap bayesian true value figure comparison different estimators intersectional impact ratio synthetic datasets increasing size vertical bars represent confidence intervals bootstrap bayesian estimation bootstrapped datasets monte carlo samples drawn respectively dataset mse empirical bootstrap bayesian figure comparison estimator mse synthetic datasets increasing size mse estimated generating different datasets equal base rates focus mitigating detected intersectional bias first consider scenario access binary elift impact ratio stat parity tpr parity fpr parity fairness value empirical bootstrap bayesianfigure estimates fairness data model outputs metrics adult training set gender age race considered sensitive attributes vertical bars represent confidence intervals predictions assume knowledge underlying model possible technique randomization section focus improving equalized odds intersectional fairness metric set ambitious aim reducing bias amplification multiplicative factor amounts reaching fairness fpr parity equal want deteriorate tpr parity score impose constraint fairness equalized odds less calculate optimal probabilities changing predictions refer model randomization next consider scenario prediction scores available rtdp achieves best predictive performance obtained fairness constraints imposed section model henceforth referred optimal score model represents baseline assessing whether imposing fairness constraints deteriorates predictive performance significantly aim achieve level fairness equalized odds access scores construct following three models deterministic optimize thresholds sequential consider optimal score model apply randomization top original binary predictoroptimal score modelrandom onlydeterministic sequential overall model value empirical bootstrap bayesian constraintfigure estimate fairness equalized odds original models using adult training set gender age race considered sensitive attributes constraint set overall simultaneously optimize thresholds probabilities figure shows level fairness equalized odds achieved different techniques note models achieve desired fairness constraint according smoothed empirical estimator required value also contained confidence intervals produced bootstrap bayesian estimators table reports models predictive performances note almost loss performance randomization used top given binary predictor indeed case experiment found model performance better randomization small underrepresented intersection model produced incorrect predictions often correct ones illustrates utility model assessing quality original model optimal score model best predictive performance reach desired fairness constraint hand deterministic model reaches fairness constraint expected loss significantly greater models observe sequential overall models perform similarly close optimal score model conclusion future work intersectional fairness crucial safe deployment modern machine learning systems yet algorithmic fairness literature thus far focused fairness respect single sensitive attribute present comprehensive framework auditing achieving intersectional fairness fairness intersections multiple sensitive attributes considered first propose metrics assess intersectional fairness data model outputs second propose methods robustly estimate metrics smoothed empirical bootstrap bayesian estimation using methods assess confidence theestimates rapidly evaluate subgroups misrepresented data discriminated model third propose postprocessing techniques transform output given binary classifier achieve intersectional fairness respect chosen metric implemented proposed auditing methods adult dataset many remaining open problems area including defining intersectional fairness metrics calibration refining estimation procedures thereof weighting bootstrap samples differently tuning prior parameters bayesian estimators taking hierarchical approach techniques improved introducing regularization term avoid overfitting smoothing cost functions modifying optimization procedure although focused post processing research preand techniques achieve intersectional fairness also carried another direction future work develop techniques regression categorical classification problems acknowledgements thank imran ahmed anil choudhary philip pilgerstorfer stavros tsalides helpful comments discussions would also like thank anonymous referees valuable feedback helped improve paper references alekh agarwal alina beygelzimer miroslav dudik john langford hanna wallach reductions approach fair classification proceedings international conference machine learning charles anawo ameh nynke van den broek increased risk maternal death among ethnic minority women obstetrician gynaecologist julia angwin jeff larson surya mattu lauren kirchner machine bias software used across country predict future criminals biased blacks propublica joy buolamwini timnit gebru gender shades intersectional accuracy disparities commercial gender classification proceedings conference fairness accountability transparency richard byrd peihuang jorge nocedal ciyou zhu limited memory algorithm bound constrained optimization siam journal scientific computing ángel alexander cabrera epperson fred hohman minsuk kahng jamie morgenstern duen horng chau fairvis visual analytics discovering intersectional bias machine learning arxiv preprint yeounoh chung tim kraska neoklis polyzotis kihyun tae steven euijong whang automated data slicing model validation big data integration approach ieee transactions knowledge data engineering sam emma pierson avi feller sharad goel aziz huq algorithmic decision making cost fairness proceedings acm sigkdd international conference knowledge discovery data mining elliot creager david madras jacobsen marissa weis kevin swersky toniann pitassi richard zemel flexibly fair representation learning disentanglement proceedings international conference machine learning dheeru dua casey graff uci machine learning repository cynthia dwork moritz hardt toniann pitassi omer reingold richard zemel fairness awareness proceedings innovations theoretical computer science conference equal employment opportunity commission civil service commission department labor department justice uniform guidelines employee selection procedure federal register john forrest ted ralphs stefan vigerske louhafer bjarni kristjansson jpfasano edwinstraver miles lubin haroldo gambini santos rlougee matthew saltzman version james foulds rashidul islam kamrun keya shimei pan bayesian modeling intersectional fairness variance bias arxiv preprint james foulds rashidul islam kamrun naher keya shimei pan intersectional definition fairness arxiv preprint moritz hardt eric price nati srebro equality opportunity supervised learning advances neural information processing systems robert david hart july white male artificial intelligence use healthcare could dangerous quartz july tim head mechcoder gilles louppe iaroslav shcherbatyi fcharras vinícius cmmalone christopher schröder nuno campos todd young stefano cereda thomas fan kejia shi justus schwabedal carlosdanielcsantos mikhail pak somanyusernamestaken fred callaway loãŕc estãĺve lilian besson mehdi cherti karlson pfannschmidt fabian linzberger christophe cauet anna gut andreas mueller alexander fabisch ursula michael kim omer reingold guy rothblum multicalibration calibration masses proceedings international conference machine learning minna kotkin diversity discrimination look complex bias william mary law rev faisal kamiran toon calders data preprocessing techniques classification without discrimination knowledge information systems toshihiro kamishima shotaro akaho hideki asoh jun sakuma classifier prejudice remover regularizer joint european conference machine learning knowledge discovery databases springer narendra karmarkar new algorithm linear programming proceedings sixteenth annual acm symposium theory computing michael kearns seth neel aaron roth zhiwei steven preventing fairness gerrymandering auditing learning subgroup fairness proceedings international conference machine learning michael kearns seth neel aaron roth zhiwei steven empirical study rich subgroup fairness machine learning proceedings conference fairness accountability transparency michael kim amirata ghorbani james zou multiaccuracy blackbox fairness classification proceedings conference ethics society jon kleinberg inherent algorithmic fairness abstracts acm international conference measurement modeling computer systems dieter kraft software package sequential quadratic programming himabindu lakkaraju ece kamar rich caruana eric horvitz identifying unknown unknowns open world representations policies guided exploration proceedings aaai conference artificial intelligence edward manoukian modern concepts theorems mathematical statistics arvind narayanan translation tutorial fairness definitions politics conference fairness accountability transparency dino pedreshi salvatore ruggieri franco turini data mining proceedings acm sigkdd international conference knowledge discovery data mining geoff pleiss manish raghavan felix jon kleinberg kilian weinberger fairness calibration proceedings international conference neural information processing systems blake woodworth suriya gunasekar mesrob ohannessian nathan srebro learning predictors proceedings conference learning theory brian zhang blake lemoine margaret mitchell mitigating unwanted biases adversarial learning proceedings conference ethics society jieyu zhao tianlu wang mark yatskar vicente ordonez chang men also like shopping reducing gender bias amplification using constraints arxiv preprint provide proofs experiment configuration appendix proofs section proof theorem theorem foulds proves result case fairness statistical parity proof based following reformulation original definition lemma max min proving max max min min analogous reformulation holds definitions fairness impact ratio tpr parity fpr parity therefore desired result holds metrics reproducing proof theorem foulds definition fairness elift metric reformulated max equation follows max max desired proof proposition prove result impact ratio similar reasoning applied prove consistency fairness metrics introduced tables assume access dataset containing nobservations make dependency nexplicit using superscript prove ˆϵn rconverges probability defined equation recall sdenotes number occurrences dataset individuals attributes sand positive outcome nsis number individuals attribute define following estimators andµs ˆµn ˆµn respectively two estimators consistent strong law large numbers apply slutsky theorem show ˆµn slutsky theorem follows ˆµn ˆµn finally continuous mapping theorem conclude ˆϵn ris consistent estimator proof proposition expected value posterior distribution given equation variance therefore posterior distribution converges adirac delta concentrated proof proposition showed probability central limit theorem implies monte carlo procedure yields consistent estimates proofs section proof proposition consider therefore proposition maxe proof proposition recall assumed implies follows mine desired proof proposition denote fpr individuals attribute sof given model ˆfpr fnr ˆfnr follows sˆfpr ˆfnr therefore linear combination variables sand proposition minimizing equation equivalent minimizing therefore objective function indeed linear remains show optimization constraints also linear consider instance using statistical parity fairness constraint law total probability follows already shown sand sare linear variables optimized conclusion holds equal opportunity fpr parity considered constraints therefore also equalized odds indeed require fairness constraint multiple fairness definitions hold simultaneously one possibly different value proof proposition following steps proof proposition first notice expected loss function marginalizes suffices prove result solving min arbitrary brevity denote although explicitly stated sand sare functions variables therefore min min min assumptions equation follows minimize desired quantity must set desired configuration experiments reproducibility provide configuration details experiments synthetic dataset section dataset generation consider set consisting binary sensitive attribute consisting different sensitive attribute possible values therefore space encompasses intersections sensitive attributes fix true base rates follows true value rcan exactly computed parameter configuration number bootstrapped datasets size equal original one smoothing parameters avoid divisions zero using bayesian estimation generate carlo samples consider prior approximate estimators mean squared error mse generate different datasets increasing size true base rates equation dataset estimate using techniques section adult income prediction section dataset preparation adult income prediction dataset publicly available already split training set consisting observations test set data points removed training set individuals originally netherlands represented test set represent age binned binary categorical variable indicating individuals gender considered binary attribute adult dataset race encoded dataset different categories purpose experiment since dataset contains instances categories eskimos american indians encode together label also standardized continuous variables created dummy variables categorical ones model built classifier returning scores via extreme gradient kept default parameters except setting boosting iterations built model returning binary predictions applying fixed threshold equal intersectional fairness estimation parameters choose smoothing parameters avoid division zero using empirical bootstrap estimators prior parameters beta distribution set parameters implementation set loss function gives equal weights false positive false negative predictions applied different optimization routines depending method linear programming using branch cut solver overall constrained optimization using sequential quadratic programming deterministic sequential unconstrained optimization using two different approaches first uses algorithm approximates gradient information therefore make use smoothing technique proposed previous paragraph second uses bayesian optimizer approximates objective function gaussian process thus deal functions rely gradient information xgboost python package version extra material experiments adult income prediction section table predictive performance given binary predictor models adult test set gender age race sensitive attributes fairness constraints fairness constraint given binary predictoroptimal score modelrandomization onlydeterministic sequential overall tpr fpr expected loss function table probabilities flipping original predictions model constructed binary classifier trained adult training set gender age considered sensitive attributes probability unreported combinations sensitive attributes equal model prediction income female female black female age female black female age male female age original binary predictoroptimal score modelrandom onlydeterministic sequential overall model value empirical bootstrap bayesian constraint figure estimate fairness equalized odds across original models results based adult test set gender age race considered sensitive attributes constraint set
