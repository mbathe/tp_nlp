recommendations updating national artificial intelligence research development strategic plan disclaimer stanford institute artificial intelligence hai nonpartisan research institute representing range voices views expressed white paper reflect views authors march white paper stanford institute artificial intelligence daniel jennifer king russell wald daniel zhang white paper recommendations updating national artificial intelligence research development strategic plan contributors principal authors daniel william benjamin scott luna scott professor law professor political science senior fellow stanford institute economic policy research stanford university directs regulation evaluation governance lab reglab stanford faculty fellow center advanced study behavioral sciences associate director stanford institute humancentered artificial intelligence hai received yale law school harvard university clerked judge stephen williams court appeals district columbia circuit jennifer king privacy data policy fellow stanford institute artificial intelligence hai completed doctorate information management systems information science university california berkeley school information prior joining hai director consumer privacy center internet society stanford law school russell wald director policy stanford institute artificial intelligence hai responsible leading team advances stanford hai engagement governments civil society organizations coauthor building national research resource blueprint national research cloud comprehensive study national research cloud date wald previously held various government relations roles stanford university term member council foreign relations visiting fellow national security institute george mason university partner truman national security project wald graduate ucla daniel zhang policy research manager stanford institute artificial intelligence hai role leads development index program annual report aims measure evaluate rapid rate advancement daniel also works policy team hai develop manage several policy research programs building bridge technology research policy communities previously worked global talent flows security risks center security emerging technology daniel holds security studies georgetown university politics international affairs furman university acknowledgments thank justin sherman benjamin contribution white paper jeanina casusi joe hinman nancy king shana lynch stacy pe√±a michi turner help preparing publication recommendations updating national artificial intelligence resear development strategic plan stanford institute artificial intelligence hai fers following submission consideration response request information rfi white house fice science technology update national artificial intelligence research development strategic plan submission recommends strategy boost budgets particularly infrastructure support investments strategy increase support interdisciplinary multidisciplinary research collaboration expands beyond exclusively technical research strategy please refer stanford hai letter submitted january response white house fice science technology proposal bill rights safeguards american public powerful technologies strategy develop appropriate acquisition strategies update existing procurement regulations respond procurement acquisition challenges federal government strategy expand government data access academic researchers train models develop frameworks government agencies evaluate datasets applications tandem strategy establish mechanism evaluate models within exact context intended use ensure safe deployment well designate nist collaboration federal agencies benchmark models institutional contexts strategy update immigration policies attract talent technical fields well develop federal programs hire talent civil servants technical capacity institutional knowledge strategy strengthen partnerships academic institutions build framework ecosystem drive development forward strategy make investments resear recommendation boost budgets particularly infrastructure support investments commitment sustained federal research development funding critical advance united states leadership global innovation federal government increase investment basic research strengthen research critical fields including healthcare education finance underpin economic stability robust growth investment reflect multidisciplinary approach focused advancing basic applied research governance supporting research infrastructure collaboration current federal funding however meet needs field public budget requested federal agencies participating networking information technology research development nitrd program national artificial intelligence initiative represents increase percent spent contrast national security commission artificial intelligence nscai recommended final report increase public funding compounding levels doubling annually reach billion per year federal high basic research funding address challenges innovation ecosystem currently facing united states example high cost compute lack access critical data hindering forts academic researchers engage federal government lagging behind private sector development federal standards technical ethical sorely needed public investment infrastructure strengthen supporting variety federal initiatives including national artificial intelligence research resource nairr aims expand access critical resources educational tools spur innovation economic prosperity another example initiative multilateral research institute mairi recommended nscai report would facilitate joint forts develop technologies advance responsible learning better societies allow allies pool talents schmidt final biden administration launches national artificial intelligence research resource task force white house united states government june david freeman engstrom daniel catherine sharkey mariano government algorithm artificial intelligence federal administrative agencies stanford law school february daniel jennifer king russell wald christopher wan building national research resource blueprint national research cloud stanford institute artificial intelligence october benjamin jones lawrence summers calculation social returns innovation nat bureau econ research working paper eric schmidt final report national security commission artificial intelligence march networking information technology program national artificial intelligence initiative office supplement president budget december strategy develop effective methods collaboration recommendation increase support interdisciplinary multidisciplinary research collaboration expands beyond exclusively technical research intentionally building trustworthy unbiased supportive human flourishing crucial ensuring successful development deployment key part fort requires interdisciplinary multidisciplinary approach involving collaboration variety fields develop hardware software understand design people behaviors expectations interacting dif ferent institutional contexts establish policies regulations determine human responsibilities well required domain knowledge various applications approach calls collaboration among multiple disciplines harnessing potential rapidly growing capabilities addressing impact existing structural inequalities biases rely voices computer scientists engineers alone yet current policies necessarily match need national science foundation program algorithmic fairness instance calls interdisciplinary perspectives stating program supports conduct fundamental computer science research requiring bring computer science expertise producing algorithms understanding concept fairness requires knowledge expertise outside computer science field incorporate social legal contexts systems deployed federal government expand support multidisciplinary research collaboration include critical academic fields social sciences law ethics prominent voice providing necessary frameworks understanding today future strategy understand addr ess ethical legal societal implications respect recommendations strategy please refer stanford hai letter submitted january response white house fice science technology proposal bill rights safeguards american public powerful technologies strategy ensur safety security systems recommendation develop appropriate acquisition strategies update existing procurement regulations respond procurement acquisition challenges federal government public sector rely heavily contracting procurement external vendors build technical capacity research shows almost half identified use cases federal agencies use came external sources coming private commercial michele elam rob reich stanford hai artificial intelligence bill rights stanford institute artificial intelligence january andrew selbst fairness abstraction sociotechnical systems proceedings conference fairness accountability transparency january nsf program fairness artificial intelligence collaboration amazon fai national science foundation sources via procurement process compared internal sourcing systems may accountable uses procured government raise several concerns terms trustworthiness transparency safety federal acquisition regulation example provides strong protection vendors protections obscure certain information inputs tools use tools operate behind trade secrecy claims turn prevents appropriate analysis audit testing ensure fairness use moreover protections may also create uncertainty acquisition aimed nature systems makes clear distinction rights software rights data systems particularly machine learning integrate customer software new data generated process training current procurement policies suf ficiently address rights data resulting systems distributed constraints conditions federal government develop appropriate acquisition strategies update existing procurement regulations help address public sector challenges evaluating monitoring using systems specific examples include developing clear standards call disclosure data information design operation contractors algorithms requirements ensure contractors adhere ethical standards testing infrastructures allow iterative testing evaluation strategy develop shar public datasets envir onments training testing recommendation expand government data access academic researchers train models develop frameworks government agencies evaluate datasets applications tandem publicly available resources development still work done promote open collaborative use training testing data environments example access data resources suf ficient training systems increasingly limited lar private companies turn direct resources toward developing applications focus private profit instead public interest lar platforms unequaled access data development smaller actors may legitimately lack financial resources invest building training data scratch incentivized mine public sphere data violating individual privacy expectations creating privacy risks individuals society lar time jathan sadowski data capital datafication accumulation extraction big data society january building national research engstrom government lavi dor cary coglianese procurement governance ieee transactions technology society laura gerhardt mark headd digital service delivery love modular contracting april ken farber kristine lam ellery taylor ethics operations current federal policy advanced technology academic research center october deirdre mulligan kenneth bamberger procurement policy administrative process machine learning berkeley technology law journal federal acquisition regulation part patents data copyrights general services administration accessed march engstrom government significant barriers interagency external researchers access rich portfolio public sector data employment healthcare education starting point executive branch use nairr opportunity make better quality government data available research community cost federal government weigh considerations privacy security fairness begin develop frameworks evaluating datasets applications tandem informed important developments foundations policymaking act national secure data service considering lack standardized framework test evaluate models safety security government pursue creating testing environment purpose especially systems strategy measur evaluate technologies thr ough standards benchmarks recommendations establish mechanism evaluate models within exact context intended use ensure safe deployment designate nist collaboration federal agencies benchmark models institutional contexts understanding true accuracy accuracy systems deployment specific contexts dif ferent industries dif ferent subpopulation groups crucial federal government capture capabilities technology ensure safe deployment many current performance evaluations comprehensively assess systems would perform context object recognition systems example often evaluated lar benchmark datasets validate performance datasets limited coverage estern contexts temporally bounded capturing parts real world gap observed language models systems amplify human bias discriminate minority users performance degrades given text told accuracy systems one domain automatically translate uses domains changing context significantly impact performance kawin ethayarajh dan jurafsky utility eye user critique nlp leaderboards proceedings conference empirical methods natural language processing emnlp thomas manzini black criminal caucasian police detecting removing multiclass bias word embeddings proceedings conference north raji everything whole wide world inioluwa deborah raji everything whole wide world benchmark november note recommend government agencies withhold certain data allow sufficient testing example nist face recognition vendor test frvt challenge provide training images tests seek mimic operational reality algorithms almost always shipped used without training adaptation customer read face recognition vendor test asked questions faqs national institute standards technology november avrim blum moritz hardt ladder reliable leaderboard machine learning competitions february creating testbed federal government see tina huang creating testbed government institute progress january building national research resource amy hara carla medalia data sharing federal statistical system impediments possibilities annals american academy political social science white house consider proposal char national institute standards technology nist collaboration federal agencies regulatory oversight products food drug administration fda consumer financial protection bureau cfpb national highway traffic safety administration nhtsa develop improved benchmarking protocols benchmarks explicitly address incorporate institutional contexts systems developed commercial settings deployed border control nist also consider measure fectiveness deploying settings enabling technologies cameras microphones computing hardware factors may vary instance one field experiment earlier generation predictive policing algorithms found models worked well lab perform well field failing reduce crime used context however perspective grounded ethical frameworks privacy design focus exclusively identifying technical benchmarks requirement suggests potentially dif ferent set roles expertise nist past strategy better understand national workfor needs recommendations update immigration policies attract talent technical fields develop federal programs hire talent civil servants technical capacity institutional knowledge future leadership hinges country necessary process hiring well ability attract retain talent already exists country needs individuals equipped skills build systems also know ask right questions systems pose risks individuals society break law every expert technical background understanding technology essential designing implementing accountable initiatives talent could also useful tool accountability helping design maintain transparent auditable responsible systems well engaging stakeholders ensure trustworthiness systems could include expediting hiring process certain kinds talent updating immigration policies attract retain see problems hiring young tech talent jack corrigan government struggle hire young tech talent worse thought nextgov december engstrom government daniel emily black maneesh agrawala evaluating facial recognition technology protocol performance assessment new domains stanford institute artificial intelligence november priscillia hunt jessica saunders john hollywood evaluation shreveport predictive policing experiment national institute justice fda see artificial intelligence machine learning medical devices food drug administration fda september cfpb see patrice alexander ficklin tom pahl paul watkins innovation spotlight providing adverse action notices using models consumer financial protection bureau july nhtsa see automated vehicles safety national highway traffic safety administration accessed march technical talent expanding opportunities permanent residency technical degrees well opportunities entrepreneurs improving public sector capacity essential accountability oversight ensure systems built deployed ways promote rather degrade public interest currently federal government faces numerous challenges hiring talent including competing salaries benefits competing shorter hiring timelines enticing applicants face onerous often confusing federal hiring process versus often faster easier one industry civil society white house work well congress industry partners identify biggest challenges attracting talent government ways potentially resolve several agencies initiated forts fice personnel management opm established classification information technology positions ease hiring burden federal government competitive service direct hire appointing authority several stem positions agencies critical hiring needs general services administration gsa launched fellowship aimed placing software engineers data scientists others technical skills federal agencies finally bears mentioning government take view developing future talent expertise investing technical skills development primary secondary education educational initiatives strategy expand partnerships accelerate advances recommendation strengthen partnerships academic institutions build framework ecosystem drive development forward federal workforce date still lack appropriate training use fectively public operations dischar regulatory responsibilities government business council accenture survey found percent federal employee respondents worry lack technical support user training public deployment evaluation customs border protection cbp facial recognition program used air exit government accountability fice gao found agents ground received little kristen vaughan britaini carroll michael gavin federal workers ready thrive age accenture february chris kuang introducing digital corps new path public service technologists general services administration august margaret weichert delegation appointing authority positions office personnel management april joan timoney building federal civil service century challenge attracting great talent government service summit science engineering workforce meeting summary national library medicine house version america competes act one possible starting point see bioeconomy research development act america competes act house representatives committee rules february tina huang zachary arnold immigration policy global competition talent center security emerging technology june training use feature system undermine agencies trying carry missions individuals using systems without appropriate training also create exacerbate threats privacy civil liberties even safety supporting education research university environment help address government talent problems help fill talent pipeline public sector kind collaboration scientific technological areas also fuel innovation example world war department veteran affairs collaborated academic institutions specifically medical centers meet increasing medical needs returning veterans collaboration academic medicine helped revolutionize healthcare spurred innovation healthcare many levels including instance invention pacemakers scan prototypes lead authors proudly submit response behalf colleagues stanford institute artificial intelligence hai daniel william benjamin scott luna scott professor law stanford university faculty associate director stanford institute artificial intelligence hai russell wald director policy stanford institute artificial intelligence hai jennifer king privacy data policy fellow stanford institute artificial intelligence hai daniel zhang policy research manager stanford institute artificial intelligence hai rob marek actions needed help better identify agency inventions government accountability office april anniversary academic mission department veterans affairs january adam hoffman cbp tsa taking steps implement programs cbp address privacy system performance issues government accountability office september
