nistir four principles explainable artiﬁcial intelligence jonathon phillips carina hahn peter fontana amy yates kristen greene david broniatowski mark przybocki publication available free charge nistir four principles explainable artiﬁcial intelligence jonathon phillips carina hahn peter fontana amy yates kristen greene information access division information technology laboratory david broniatowski information technology laboratory mark przybocki information access division information technology laboratory publication available free charge september department commerce gina raimondo secretary national institute standards technology james olthoff performing functions duties secretary commerce standards technology director national institute standards technology certain commercial entities equipment materials may identiﬁed document order describe experimental procedure concept adequately identiﬁcation intended imply recommendation endorsement national institute standards technology intended imply entities materials equipment necessarily best available purpose national institute standards technology interagency internal report natl inst stand technol interag intern pages september publication available free charge abstract introduce four principles explainable artiﬁcial intelligence comprise fundamental properties explainable systems propose explainable systems deliver accompanying evidence reasons outcomes processes provide explanations understandable individual users provide explanations correctly reﬂect system process generating output system operates conditions designed reaches sufﬁcient conﬁdence output termed four principles explanation meaningful explanation accuracy knowledge limits respectively signiﬁcant stakeholder engagement four principles developed encompass multidisciplinary nature explainable including ﬁelds computer science engineering psychology explanations exist different users require different types explanations present ﬁve categories explanation summarize theories explainable give overview algorithms ﬁeld cover major classes explainable algorithms baseline comparison assess well explanations provided people follow four principles assessment provides insights challenges designing explainable systems key words artiﬁcial intelligence explainable explainability trustworthy publication available free charge executive summary space vast complicated continually evolving advances computing power datasets algorithms explored developed use wide variety application spaces variety potential users associated risks community pursuing explainability one many desirable characteristics trustworthy systems working community nist identiﬁed additional technical characteristics cultivate trust addition explainability interpretability among system characteristics proposed support system trustworthiness accuracy privacy reliability robustness safety security resilience mitigation harmful bias transparency fairness accountability explainability system characteristics interact various stages lifecycle critically important work focuses solely principles explainable systems paper introduce four principles believe comprise fundamental properties explainable systems principles explainable informed engagement larger community nist public workshop public comment period recognize systems may require explanations however systems intended required explainable propose systems adhere following four principles explanation system delivers contains accompanying evidence reason outputs processes meaningful system provides explanations understandable intended consumer explanation accuracy explanation correctly reﬂects reason generating output accurately reﬂects system process knowledge limits system operates conditions designed reaches sufﬁcient conﬁdence output work recognize importance explanations well importance explanation purpose style example developers designers may different explanation needs policy makers end users therefore explanation requested delivered may differ depending users four principles heavily inﬂuenced considering system interaction human recipient information requirements given situation task hand consumer inﬂuence type explanation deemed appropriate situation situations include limited regulator legal requirements quality control system customer relations four principles explainable systems intended capture broad set motivations reasons perspectives principles allow deﬁning contextual factors consider explanation pave way forward measuring explanation quality publication available free charge imagine given complexity space principles beneﬁt additional reﬁnement community input time fully acknowledge numerous factors inﬂuence trustworthiness beyond explainability work principles explainable systems part much larger nist trustworthy data standards evaluation validation necessary measurements nist metrology institute deﬁning initial principles explainable systems acts roadmap future measurement evaluation activities agency goals activities prioritized informed statutory mandates white house directions needs expressed industry federal agencies global research community current work one step much larger space imagine work continue evolve progress time much like larger ﬁeld program fact sheet iii publication available free charge table contents introduction four principles explainable explanation meaningful explanation accuracy knowledge limits summary purposes styles explanations risk management explainable overview principles literature overview explainable algorithms models explanations local explanations global explanations adversarial attacks explainability evaluating explainable algorithms evaluating meaningfulness evaluating explanation accuracy humans comparison group explainable explanation meaningful explanation accuracy knowledge limits discussion conclusions references list figures fig illustration four principles explainable artiﬁcial intelligence arrows indicate system explainable must provide explanation remaining three principles fundamental properties explanations fig illustration elements explanation styles publication available free charge introduction father one authors diagnosed cancer went speak oncologist oncologist described state cancer went strategies options treatment oncologist answered father questions explained role treatment author father felt partner control father trusted treatment received meaningful understandable explanation process doctor bedside manner father medical arts changed possessing good bedside manner become rigueur artiﬁcial intelligence systems contribute diagnosis could support good bedside manners explaining recommendations physicians medical diagnoses one example systems contribute decisions impact person life examples systems evaluate loan applications recommend jail sentences nature decisions spurred drive create algorithms methods techniques accompany outputs systems explanations drive motivated part laws regulations state decisions including automated systems must provide information reasoning behind also motivated desire create trustworthy explainable one several properties characterize trust systems properties include resiliency reliability bias accountability usually terms deﬁned isolation part set principles pillars deﬁnitions vary author focus norms society expects systems follow based calls explainable systems assumed failure articulate rationale answer affect level trust users grant system suspicions system biased unfair raise concerns harm individuals society may slow societal acceptance adoption technology increased call explanations ﬁeld needs principled method characterizes good explanation system first characterization needs humans consume second need understandable people third explanations correctly reﬂect system process generating output foster conﬁdence explanations system indicate operating outside designed conditions core concepts good explanation basis four principles explainable although principles may affect methods algorithms operate meet explainable goals focus concepts algorithmic methods computations also principles pertain system usage deployment rather present four principles organized around humans consume explanations provide structure begin measuring components explanations quality goodness accuracy limitations measure explanations structured way fair credit reporting act fcra european union general data protection regulation gdpr article publication available free charge essential ﬁeld make progress toward concrete deﬁnitions explanation quality measured serve guide future research directions ﬁeld four principles support foundation explainable measurement policy considerations safety acceptance society aspects technology present discuss principles section adopt expansive view explanations characterize space section outline risks introduced explainable especially introduced principles met section put current work context provide review current explainable methods evaluation metrics existing principles explainable sections finally review existing literature assess extent humans meet principles introduce section performance expectations vary humans machines although contexts differing expectations may may appropriate baseline could compared needed four principles explainable present four fundamental principles explainable systems principles heavily inﬂuenced considering system interaction human recipient information requirements given situation task hand consumer inﬂuence type explanation deemed appropriate situation situations include limited regulator legal requirements quality control system customer relations four principles intended capture broad set motivations reasons perspectives principles apply systems produce explanations support full range techniques machine learning ones delve principles document operationally deﬁne three key terms explanation output process explanation evidence support reasoning related system output process deﬁne output system outcome action taken machine system performing task output system differs task loan application output decision approved denied recommendation system output could list recommended movies grammar checking system output grammatical errors recommended corrections classiﬁcation system could object identiﬁer spam detector automated driving could navigation process refers procedures design system workﬂow underlie system includes documentation system information data used system development data stored related knowledge system brieﬂy four principles explainable explanation system delivers contains accompanying evidence reason outputs processes publication available free charge meaningful system provides explanations understandable intended consumer explanation accuracy explanation correctly reﬂects reason generating output accurately reﬂects system process knowledge limits system operates conditions designed reaches sufﬁcient conﬁdence output deﬁned put context detail figure shows principles indicates system considered explainable must ﬁrst explanation contain accompanying evidence accessed fig illustration four principles explainable artiﬁcial intelligence arrows indicate system explainable must provide explanation remaining three principles fundamental properties explanations explanation theexplanation principle states system considered explainable supplies evidence support reasoning related outcome process system explanation principle independent whether explanation correct informative intelligible principle impose metric quality explanations factors components meaningful explanation accuracy principles explanations practice vary according given system scenario means large range ways explanation executed embedded system accommodate large range applications adopt deliberately broad deﬁnition explanation meaningful system fulﬁlls meaningful principle intended recipient understands system explanation commonalities across explanations make publication available free charge meaningful example stating system didbehave certain way understandable describing behave certain way many factors contribute individual people consider good explanation therefore developers need consider intended audience several factors inﬂuence information people ﬁnd important relevant useful include person prior knowledge experiences overall psychological differences people moreover consider meaningful change time gain experience task system different groups people also different desires system explanations groups may deﬁned broadly according role relationship system example developers system likely different desires explanation compared addition audience considered meaningful vary according explanation purpose different scenarios needs drive important useful given context meeting meaningful principle accomplished understanding audience needs level expertise relevancy question query hand provide detailed discussion purposes section measuring meaningful principle area ongoing work section challenge develop measurement protocols adapt different audiences rather viewing burden argue awareness appreciation explanation context support ability measure quality explanations scoping factors therefore bound possibilities execute explanation meaningful way explanation accuracy together explanation meaningful principles call system produce explanations intelligible intended audience two principles require explanation correctly reﬂects system process generating output theexplanation accuracy principle imposes veracity system explanations explanation accuracy distinct concept decision accuracy decision accuracy refers whether system judgment correct incorrect regardless system decision accuracy corresponding explanation may may accurately describe system came conclusion action researchers developed standard measures algorithm system accuracy established decision accuracy metrics exist researchers process developing performance metrics explanation accuracy section review current work subject additionally explanation accuracy needs account level detail explanation audiences purposes simple explanations sufﬁce given reasoning might succinctly focus critical point provide high level reasoning without extensive detail simple explanations could lack nuances necessary completely characterize algorithm process generating output however publication available free charge nuances may meaningful certain audiences experts system similar humans approach explaining complex topics professor neuroscience may explain new ﬁnding extensive technical details colleague ﬁnding likely distilled changed presenting undergraduate student order present pertinent higher level details professor may explain ﬁnding differently untrained friends parents together highlights point explanation accuracy meaningfulness interact detailed explanation may accurately reﬂect system processing sacriﬁce useful accessible certain audiences likewise brief simple explanation may highly understandable would fully characterize system given considerations principle allows ﬂexibility explanation accuracy metrics knowledge limits previous principles implicitly assume system operating within scope design knowledge boundaries knowledge limits principle states systems identify cases designed approved operate cases answers reliable identifying declaring knowledge limits practice safeguards answers judgment provided may inappropriate principle increase trust system preventing misleading dangerous unjust outputs two ways system reach exceed knowledge limits one way operation query system outside domain example system built classify bird species user may input image apple system could return answer indicate could ﬁnd birds input image therefore system provide answer answer explanation second way conﬁdence likely answer may low depending internal conﬁdence threshold revisit example bird classiﬁcation system input image bird may blurry determine species case system may recognize image bird image low quality example output may found bird image image quality low identify summary given wide range needs applications explainable systems system may considered explainable better able meet principles generate one type explanation metrics used evaluate accuracy explanation may universal absolute body ongoing work currently seeks develop validate explainable methods overview efforts provided sections four principles serve guidance consider whether explanation meets user needs ﬁeld explainable area active research understanding systems use vary ﬁeld grows new knowledge data therefore publication available free charge principles serve way guide think needs system principles provide basis approaching new challenges questions purposes styles explanations illustrate broad range explanations characterize explanations two properties purpose style purpose reason person requests explanation question explanation intends answer style describes explanation delivered audience strongly inﬂuence purpose explanation information provides information vary according different groups people role system system builder may want explanations related debugging models evaluating training data regulators may inquire system meets stated regulatory requirements explanation purpose turn inﬂuence style figure visualize three elements style level detail degree interaction human machine format attributes exhaustive explanations take many forms however highlight elements ones closely related meeting four principles therefore considering lay groundwork producing explanations expound upon detail thelevel detail depicted range sparse extensive sparse mean amount information provided brief limited lacking detail example sparse explanation might explanation decision made alert system system processes slowed extensive explanation may contain detailed information system provide large amount information report relevant system information understand process place degree interaction three categories declarative explanations interaction interaction declarative explanation systems provides explanation interaction describes current explainable methods section example loan application system may always output rationale acceptance rejection object classiﬁer may output saliency map model card may contain information system declarative explanation based default query object classiﬁer produce decision human alter question asked barring change system produce something different higher degree interaction interaction explanation determined based query question input system example may graphical output depending factors person wishes visualize may allow explanation consumer ability probe submit different queries deﬁne category deepest interaction level interaction models conversation people person probe machine probe back ask clarifying questions provide new avenues exploration publication available free charge fig illustration elements explanation styles publication available free charge example system may probe user additional details propose alternate questions knowledge interactions yet exist developing future research direction explanation format includes visual graphical verbal auditory visual alerts examples graphical formats include outputs data analyses saliency maps verbal formats include written outputs reports well auditory outputs speech visual verbal formats carry assumption audience expecting attending explanation another form explanation capture unaware audience attention siren light system produce different alarms light ﬂashing patterns light colors explanation alert audience example speciﬁc siren pitch pattern could indicate something system status may need attention style elements need considered produce explanation purpose meet four principles cases may call simple declarative explanation appropriate style optimize meaningful sometimes case weather emergency tornado area current weather alert national weather service tornado warning take action operates alert simple explanation alert take action simple explanation tornado depending metric explanation may considered highly accurate minimal level detail include tornado warning declared explanation however example brevity appropriate ensure understood wide audience enable swift action although minimal additional information alert may helpful address noncompliance responding weather alerts effects different scenario debugging system explanation could include information internal steps system could lengthy contain language audience may need time effort examine explanation decide next actions details user preferred format would helpful interactions could become important different purposes styles considerations illustrate range types explanations points need ﬂexibility addressing scope systems require explanations circumstances explanation provided differ four principles encourage adapting different styles appropriate explanations easier achieve others designers need consider tradeoffs accomplishing different goals risk management explainable risk deﬁned effect uncertainty objectives includes negative outcomes threats well positive outcomes opportunities risk management process used deﬁne assess mitigate risk explainable mitigate publication available free charge risks artiﬁcial intelligence assessing measuring predicting risk model system explanations used test vulnerabilities alternatively explainable introduce risks adversarial attacks discussed section section focuses latter managing potential risks introduced explainable explainable system contain potential risks threats opportunities degree stakeholders prepared accept general risk goals called stakeholders risk appetite many risk management strategies share common components identifying analyzing responding monitoring reviewing explainable risk management strategy need factor four principles anexplanation ﬁrst principle necessary explainable explanation introduces risks positive negative potential negative outcome explanation exposure proprietary details single explanation may expose inner workings system however multiple explanations either multiple independent queries interaction could expose intellectual property connected many explanations must end user access understanding system scope explanation may impact number include explanations describe limit system knowledge however explanations also potential positive outcomes user better understand system could lead improvements increased trust system explanations may also necessary compliance regulations fair credit reporting act fcra article european union general data protection regulation gdpr explanations need meaningful audience second principle introduces risks meaningful explanation give deeper insight system may expose intellectual property system vulnerabilities exposing inner workings explanation meaningful hand jeopardy ignored recognized explanation order useful explanation needs meaningful must also accurate third principle relevant potential risk commonly known model risk potential negative outcomes derived invalid misapplied model stated federal reserve system one two main sources type risk underlying errors model causing erroneous outputs inaccurate explanation lead misinterpretation misunderstanding system works arrived outcome negative risk end user systems also used one part larger system might bias human face recognition human face examiner could receive information algorithm parts face useful accurate explanation help examiner accurately assess pair faces inaccurate explanation could lead wrong decision judicial system algorithms used decision defendant may arrested inaccurate explanation algorithm publication available free charge arrived outcome could result miscarriage justice accurate explanation help create society main source model risk using model incorrectly beyond knowledge limits explanations describe knowledge limits system fourth principle provide assurance model operating scope nurture conﬁdence describing limits system potentially expose inner workings system especially combined information collected explanations examining potential risks software exposure different contexts categories levels risk end users internal organizations developers management strategy different end users include external customers explainable introduces new threats system however also introduces new opportunities well whether outcome threat opportunity sometimes depends audience risk management considers possibilities factors assessing risk two components often assessed likelihood risk impact outcome general need develop risk management framework request information information risk management see overview principles literature theories properties explainable discussed different perspectives commonalities differences across points view lipton divides explainable techniques two broad categories transparent interpretability lipton deﬁnes transparent explanation reﬂecting degree system came output subclass simulatability requires person grasp entire model implies explanations reﬂect inner workings system explanations often elucidate precisely model works may nonetheless confer useful information practitioners end users machine example bird cardinal similar cardinals training set rudin argues assumed interpretability must sacriﬁced accuracy recommend high stakes decisions one avoid model unless one prove interpretable model exist level accuracy note refer remainder document rudin builds previous work presenting ﬁve principles ten grand challenges interpretable machine learning mueller reviews basic concepts explainable systems based concepts describe scorecard date accessed publication available free charge present set design principles psychological perspective broniatowski makes case interpretability explainability distinct requirements machine learning systems resulting analysis implies system output tailored different types users wachter argue explanations need meet explanation accuracy property claim counterfactual explanations sufﬁcient counterfactual explanation prediction describes smallest change feature values changes prediction predeﬁned output arrived platform minutes earlier would caught train counterfactual explanations necessarily reveal inner workings system property allows counterfactual explanations protect intellectual property gilpin deﬁnes set concepts explainable similar meaningful explanation accuracy principles current work gilpin propose explanations allow interpretability completeness addition state must obscure key limitations system kim address critical question measuring explanations meaningful users consumers present framework science measure efﬁciency explanations paper discusses factors required begin testing interpretability explainable systems paper highlights gap principles concept creating metrics evaluation methods information commissioner ofﬁce alan turing institute lays principles follow explainable principles transparent accountable consider context operating reﬂect impact system individuals affected well wider society addition discussing principles discuss different things explanation including explanations explanations rationale responsibility made decisions explanation data design steps maximize fairness safety impact use system barredo arrieta discuss various terms used different sources describe explainability interpretability understandability comprehensibility interpretability explainability transparency discuss terms different yet tied together weller discusses types transparency address different classes users consumers explanations similar explanation accuracy principle paper introduces faithfulness explanation broadly beneﬁcial society provided explanations given faithful sense accurately convey true understanding without hiding important details notion faithful hard characterize precisely similar spirit instructions sometimes given courts tell truth whole truth nothing across viewpoints exist commonalities disagreement similar publication available free charge four principles commonalities include concepts distinguish existence explanation meaningful accurate complete although disagreements remain perspectives provide guidance development explainable systems key disagreement philosophies relative importance explanation meaningfulness accuracy disagreements highlight difﬁculty balancing multiple principles simultaneously context application community user requirements speciﬁc task drive importance principle overview explainable algorithms researchers developed different algorithms explain systems following sources organize explanations two broad categories models explanations models algorithm model representation algorithm directly read interpreted human case model explanation explanations explanations often generated software tools describe explain model algorithm give idea algorithm works explanations often used algorithms without inner knowledge algorithm works provided queried outputs chosen inputs rather mention different explanation subtypes different explanations available highlight examples categorizations refer reader various surveys explainable models models models explanations explain entire model globally walking input model simulation input model provide local explanation decision common models include decision trees regression models including logistic regression work producing many interpretable models improve accuracy basic decision trees basic regression models many cases models include decision lists decision sets prototypes representative samples class kim feature combination rules completely classify sets inputs kuhn bayesian rule lists additive decision trees improved variants decision trees models sources state tradeoff models less accurate models making model exact meaningful humans however rudin rudin radin disagree arguing necessarily many cases interpretable models used without loss decision accuracy publication available free charge explanations explanations grouped two kinds local explanations global explanations local explanation explains subset decisions explanation aglobal explanation produces model approximates model cases global explanation also provide local explanations simulating speciﬁc inputs provide local explanations individual inputs simple examples consider logistic regression could either model approximation opaque model regression coefﬁcients provide global explanation explain inputs however one plug input weights use weights explain output algorithm discuss explanations following subsections describing local explanations section global explanations section local explanations local explanations explain subset inputs common type local explanation explanation provides explanation algorithm output decision single input point one local explanation algorithm lime local interpretable modelagnostic explainer lime takes decision querying nearby points builds interpretable model represents local decision uses model provide explanations default model chosen logistic regression images lime breaks image superpixels queries model random search space varies superpixels omitted replaced black color user choice another local explanation algorithm shap shapley additive explanations shap provides importance input regression problem converting scenario coalitional game game theory producing shapley values game shap treats features players features value default value strategies system output payoff forming coalitional game input see ferguson information shapley values coalitional games another common local explanation counterfactual counterfactual explanation saying input new input instead system would made different explanations although often multiple widelydiffering instances counterfactuals counterfactual explanation often provides single instance hope instance similar possible input exception system makes different decision however systems produce multiple counterfactual instances single explanation ustun develop counterfactual explanation logistic linear regression models counterfactuals represented amounts speciﬁc features change another popular type local explanations problems image data saliency publication available free charge pixels saliency pixels color pixel depending much pixel contributes classiﬁcation decision one ﬁrst saliency algorithms class activation maps cam popular saliency pixel algorithm enhanced cam generalized cam explain convolutional network additional local explanation koh liang takes decision produces estimate inﬂuence training data point particular decision another additional local explanation individual conditional expectation ice ice curve shows marginal effect change one feature instance data global explanations global explanations produce explanations entire algorithm often involves producing global model algorithm system one global explanation partial dependence plots pdps partial dependence plots shows marginal change predicted response feature value speciﬁc data column component changes pdps useful determining relationship feature response linear complex deep neural networks one global algorithm tca testing concept activation vectors tca wishes explain neural network way representing neural network state linear weighting concepts called concept activation vectors tca applied explain image classiﬁcation algorithms learning including color see colors inﬂuenced image classiﬁer decisions two visualizations used provide global explanations partial dependence plots pdps individual conditional expectation ice partial dependence plot shows marginal change predicted response feature value speciﬁc data column component changes pdps useful determining relationship feature response linear complex ice curves show marginal effect change one feature instance data ice curves useful check relationship visualized pcp across ice curves help identify potential interactions prototypes representative samples class also sometimes used global explanation neural network addition sometimes model mentioned section another way produce global explanations summarize local explanations taken variety inputs variant lime uses submodular pick choose relevant local lime explanations summary explanations another way try approximate model learning global model system decision set summary counterfactual rules publication available free charge adversarial attacks explainability explanation accuracy principle important component explanations sometimes explanation percent explanation accuracy exploited adversaries manipulate classiﬁer output small perturbations input hide biases system first lakkaraju bastani observes even explanation mimic predictions insufﬁcient explanation accuracy systems produce explanations may mislead users approach generate misleading explanations demonstrated slack producing scaffolding around given classiﬁer matches classiﬁcation input data instances changes outputs small perturbations input points obfuscate global system behavior queried locally means system anticipating explained tool lime gives similar instances training set instances inputs system develop alternative protocol decide instances differ classify trials training test sets mislead explainer anticipating trials system might asked classify another similar approach demonstrated aivodji fairwash model taking model produce ensemble interpretable models approximate original model much fairer hide unfairness original model another example slightly perturbing model manipulate explanations demonstrated dimanov ability developers cover unfairness models one several vulnerabilities explainable discussed hall kindermans shows many saliency pixel explanations lack input invariance meaning small change input greatly change output attribution relevant pixels evaluating explainable algorithms sections summarizes art evaluating explainable algorithms paper separate evaluation explainable algorithms according principle evaluated explanation principle principle covered section overview explainable algorithms section reviews current explanation methods section review current methods measuring explanation meaningfulness principle explanation accuracy principle knowledge limited work developing evaluating algorithms knowledge limits principle result discuss evaluating knowledge limits section evaluating meaningfulness one way measure meaningfulness explanation involves measuring human simulatability essentially ability person understand machine learning model extent would able take input data model understand parameters model would able produce prediction publication available free charge model reasonable amount time ability simulate model would reﬂect high degree understanding typically measured models way measure complexity model several studies put human simulatability test lage lage measured accuracy humans results response time taken human poll subjective difﬁculty simulating model hase bansal discusses two kinds human simulatability forward simulation human predicts system output given input counterfactual simulation human given input output must predict output system would give input changed particular way evaluating explanations evaluated forward counterfactual simulation measuring change user accuracy relative different explanations measured accuracy humans simulating different logistic regression models housing prices slack conducted whatif simulatability evaluation user receives input explanation user asked simulate model new input slightly perturbed given input new input mirrors counterfactual another strategy evaluate meaningfulness ask humans complete task using provided system output input measuring human time taken decision accuracy task also asking humans predict believe house prices addition asking model predict house price humans disagree models step kim harnessed power examples model bayesian case model bcm learned prototypes different cooking recipes humans provided ingredients prototype measured well able classify recipe lai tan tested meaningfulness deception detection task task determine hotel reviews genuine deceptive human accuracy deception detection compared provided review presented explanations machine comparison enables comparing human decision accuracy without machine lakkaraju evaluated interpretability different complexity decision sets asking humans view explanations make decisions measuring accuracy response time mac aodha evaluated explanation comparing human accuracy humans trained systems provide explanations compared trained systems provide explanation schmidt biessmann recruited users complete tasks given without system explanations measures user total time taken decision accuracy anderson studied two techniques explaining actions reinforcement learning agents people trained tested multiple explanation conditions explanation two explanations separately explanations overall humans accurate combining techniques saliency maps bars meaningfulness measured subjective ratings well hoffman discussed variety criteria good explanations provide explanation publication available free charge tion scale holzinger developed system causability scale scs compare explanations part evaluation human simulatability lage also asked humans subjectively rate difﬁculty simulating model rajagopal conducted experiments asking users evaluate different properties explanations metrics size complexity model sometimes used measures model interpretability lakkaraju measured interpretability model asking users information provided sufﬁcient make conclusions poursabzisangdeh compared two types models test enabled participants closely simulate model actual predictions found less information less transparent model could enable better transparent model perhaps due information overload lage measured effect complexity human simulatability idea different levels types complexity affect transparency less types lakkaraju asked humans made decisions provided explanation help measured quickly accurately made decisions narayanan compared different types output complexity affected human performance bhatt designed complexity metric quantify feature importance explanations evaluating explanation accuracy explanation accuracy closely related work ﬁdelity several studies evaluated explanation ﬁdelity one way tested simulate models using system output ground truth evaluating explanations using machine learning metric lakkaraju followed strategy also checked instance one explanation every instance explained explanation model second method mohseni proposed humans evaluate explanations apply sanity checks evaluate explanation accuracy third method asked system explain variety inputs many cases inputs adaptive new inputs slightly changed versions previous inputs based provided explanation experiments measure change output relative change input importance changed features explanation samek evaluated quality explanation accuracy saliency pixels gradually deleted important pixels measured much classiﬁcation score changes idea pixels important inﬂuence decision accuracy deleted system less likely classify image original class hooker tested whether systems performed worse important features removed applied strategy removed important pixels retrained systems measured decision accuracy retrained systems yeh developed inﬁdelity measure evaluate explanation accuracy alvarez melis jaakkola evaluated explanation accuracy faithfulness removing model higher order features measuring drop classiﬁcation probability also measured explanation accuracy adding white noise publication available free charge inputs measuring much explanation changes adebayo evaluated explanation accuracy saliency pixel explanations deep neural networks measuring amount explanation changed relative trained models differed sixt evaluated quality saliency pixels randomizing middle convolutional layers comparing saliency pixels also compared saliency pixels labels actual labels random evaluated explanation accuracy adding deleting image pixels deemed relevant explanation compared system scores new images bhatt evaluated explanation accuracy feature importance explanations checking sensitivity meaning similar inputs similar feature importance explanations faithfulness meaning change explanations correlate change inputs quality counterfactual explanations tested wachter counterfactual explanation answer minimum amount input would need change system change decision input therefore tested far away counterfactual original data point humans comparison group explainable considering performance humans systems fairly signiﬁcant differences opinion regarding performance expectations argue hold machines much higher standard humans others believe sufﬁcient machines simply good humans cascade interesting difﬁcult questions arise overarching philosophical divide much better machines humans way must better measure good regardless one falls particular philosophical debate nonetheless helpful consider human performance baseline section describe human respect extent humans explanations line four principles independent humans operating alone also make high stakes decisions expectation explainable example physicians judges lawyers forensic scientists often expected provide rationale judgments proffered explanations adhere four principles focused strictly human explanations judgments decisions arrive conclusion choice external events sky blue event occur external events accompanied explanations helpful human reasoning formulating predictions consistent desire explainable however outlined follows explanations judgments decisions conclusions largely unreliable humans comparison group explainable inform development benchmark metrics explainable systems lead better understanding dynamics collaboration publication available free charge explanation principle states system considered explainable provides explanation section focus whether humans produce explanations judgments decisions whether beneﬁcial decision makers section discuss whether human explanations meaningful section discuss accuracy explanations humans able produce variety explanation types however producing verbal explanations interfere decision reasoning processes thought one gains expertise underlying processes become automatic outside conscious awareness therefore difﬁcult explain verbally produces similar tension exists desire high accuracy often thought come reductions explainability however generally processes occur limited conscious awareness harmed requiring decision expressed explicitly example comes lie detection lie detection based explicitly judging whether person telling truth lie typically inaccurate however judgments provided via implicit categorization tasks therefore bypassing explicit judgment lie detection accuracy improved suggests lie detection may nonconscious process interrupted forced made conscious one together ﬁndings suggest assessments humans may accurate left automatic implicit compared requiring explicit judgment explanation human judgments decision making oftentimes operate closedbox interfering process deleterious accuracy decision meaningful meet principle system provides explanations intelligible understandable intended audience focused ability humans interpret another human arrived conclusion consider mean whether audience reaches conclusion intended person providing explanation whether audience agrees conclusion based explanation one analogous case explainable interaction forensic scientist explaining forensic evidence laypeople members jury currently gaps ways forensic scientists report results understanding results laypeople see edmond jackson reviews jackson extensively studied types evidence presented juries ability juries understand evidence found types explanations forensic scientists misleading prone confusion therefore meet internal criteria challenge ﬁeld learning improve publication available free charge explanations proposed solutions always consistent outcomes complications producing meaningful explanations others include people expecting different explanation types depending question hand context driving formation opinions individual differences considered satisfactory explanation therefore considered meaningful varies context across people explanation accuracy principle states system explanation correctly reﬂects reasons generating certain output accurately reﬂects process humans analogous explanation one decision processes truly reﬂecting mental processes behind decision section focused aspect evaluation quality coherence explanation falls outside scope principle type introspection related explanation accuracy although people often report reasoning decisions reliably reﬂect accurate meaningful introspection coined introspection illusion term indicate information gained looking inward one mental contents based mistaken notions value people fabricate reasons decisions even thought immutable personally held opinions fact people conscious reasoning able verbalized seem always occur expressed decision instead evidence suggests people make decision apply reasons decisions fact neuroscience perspective neural markers decision occur seconds person conscious awareness ﬁnding suggests decision making processes begin long conscious awareness people largely unaware inability introspect accurately documented studies choice blindness people accurately recall prior decisions despite inaccurate recollection participants provide reasons making selections never fact made studies involve memory participants also shown unaware ways evaluate perceptual judgments example people inaccurate reporting facial features use determine someone identity based deﬁnition explanation accuracy ﬁndings support idea humans reliably meet criteria case algorithms human decision accuracy explanation accuracy distinct numerous tasks humans highly accurate verbalize decision process knowledge limits principle states system operates conditions designed reaches sufﬁcient conﬁdence output actions principle narrowed broad ﬁeld metacognition thinking one thinking publication available free charge focused whether humans correctly assess ability accuracy whether know report know answer several ways test whether people evaluate knowledge limits one method ask participants predict well believe performed perform task relative others percentile scores fall relative another way test awareness knowledge limits obtain measure response conﬁdence higher conﬁdence indicating person believes greater likelihood correct demonstrated well known effect people inaccurately estimate ability relative others similar ﬁnding people including experts generally predict accuracy ability well asked explicitly estimate performance however recent replication effect face perception showed although people reliably predict accuracy ability estimates varied accordingly task difﬁculty suggests although exact value predicted performance percentile relative others predicted percent correct may erroneous people modulate direction predicted performance appropriately knowing task less difﬁcult variety judgments decisions people often know made errors even absence feedback use eyewitness testimony relevant example although conﬁdence accuracy repeatedly shown weakly related person conﬁdence predict accuracy absence contamination interrogation process extended time event time recollection therefore human shortcomings assessing knowledge limits similar producing explanations asked explicitly produce explanation explanations interfere automatic processes gained expertise often accurately reﬂect true cognitive processes likewise outlined section people asked explicitly predict estimate ability level relative others often inaccurate however asked assess conﬁdence given decision explicit judgment people gauge accuracy levels chance suggests people insight knowledge limits although insight limited weak cases discussion conclusions introduced four principles encapsulate fundamental elements explainable systems principles provide framework address different components explainable system four principles system produce explanation explanation meaningful humans explanation reﬂects system processes accurately system expresses knowledge limits principles derive strength system follows four system provides explanation understandable accurate outside knowledge limits reduced value publication available free charge fact may impact users acceptance system outcomes different approaches philosophies developing evaluating explainable computer science approaches tackle problem explainable variety computational graphical techniques perspectives may lead promising breakthroughs blossoming ﬁeld puts humans forefront considering effectiveness explanations human factors behind effectiveness four principles provide multidisciplinary framework explore type humanmachine interaction practical needs system inﬂuence principles addressed dismissed needs mind community ultimately adapt apply four principles capture wide scope applications focus explainable advance capability systems produce quality explanation addressed whether humans meet set principles set forth showed humans demonstrate limited ability meet principles outlined provides benchmark compare systems reﬂection societal expectations recent regulations imposed degree accountability systems via requirement explainable advances made explainable may ﬁnd certain parts systems better able meet societal expectations goals compared humans understanding explainability system human collaboration opens door pursue implementations incorporate strengths potentially improving explainability beyond capability either human system isolation paper focused limited set human factors related explainable decisions much learned studied regarding interaction humans explainable machines although beyond scope current paper considering interface humans understanding general principles drive human reasoning decision making may prove highly informative ﬁeld explainable humans general tendencies preferring simpler general explanations however described earlier individual differences explanations considered high quality context decision type decision made inﬂuence well humans make decisions isolation factors without conscious awareness people incorporate irrelevant information variety decisions ﬁrst impressions personality trait judgments jury decisions even provided identical information context person biases way information presented inﬂuences decisions considering human factors within context explainable begun succeed explainable community needs study interface humans systems collaborations shown highly effective terms accuracy may similar breakthroughs explainability collaborations principles deﬁned provide guidance philosophy driving explainable toward safer world giving users deeper publication available free charge standing system output meaningful accurate explanations empower users apply information adapt behavior appeal decisions developers auditors explanations equip ability improve maintain deploy systems appropriate explainable contributes safe operation trust multiple facets complex systems common framework deﬁnitions four principles facilitate evolution explainable methods necessary complex systems acknowledgments authors thank harold booth john libert reva schwartz rachael sexton brian stanton craig watson jesse zhang insightful comments discussions thank everyone responded call submitted comments draft version paper thank panelists participants nist explainable workshop discussions insight thought provoking commentary essential shaping improving paper future directions work references general data protection regulation gdpr amina adadi mohammed berrada peeking inside survey explainable artiﬁcial intelligence xai ieee access issn doi julius adebayo justin gilmer michael muelly ian goodfellow moritz hardt kim sanity checks saliency maps bengio wallach larochelle grauman garnett editors advances neural information processing systems pages curran associates url pdf gael aglin siegfried nijssen pierre schaus learning optimal decision trees using caching search url ulrich aivodji hiromi arai olivier fortineau gambs satoshi hara alain tapp fairwashing risk rationalization international conference machine learning pages may url issn section machine learning david alvarez melis tommi jaakkola towards robust interpretability neural networks bengio wallach larochelle grauman garnett editors advances neural information processing systems pages curran associates url publication available free charge andrew anderson jonathan dodge amrita sadarangani zoe juozapaitis evan newman jed irvine souti chattopadhyay matthew olson alan fern margaret burnett mental models mere mortals explanations reinforcement learning acm trans interact intell may issn doi url julia angwin jeff larson surya mattu lauren kirchner machine bias software used across country predict future criminals biased blacks propublica may url alejandro barredo arrieta natalia javier del ser adrien bennetot siham tabik alberto barbado salvador garcia sergio daniel molina richard benjamins raja chatila francisco herrera explainable artiﬁcial intelligence xai concepts taxonomies opportunities challenges toward responsible information fusion june issn doi url marianne bertrand sendhil mullainathan emily greg employable lakisha jamal field experiment labor market discrimination american economic review doi dimitris bertsimas jack dunn optimal classiﬁcation trees machine learning july issn doi url umang bhatt jos moura adrian weller evaluating aggregating model explanations volume pages july doi url issn umang bhatt alice xiang shubham sharma adrian weller ankur taly yunhan jia joydeep ghosh ruchir puri jos moura peter eckersley explainable machine learning deployment proceedings conference fairness accountability transparency pages markus bindemann janice attard robert johnston perceived ability actual recognition accuracy unfamiliar famous faces cogent psychology issn doi url anna bobak viktoria mileva peter hancock facing facts naive participants moderate insight face recognition face perception abilities quarterly journal experimental psychology page issn doi charles bond bella depaulo accuracy deception judgments characterizations deception personality social psychology review publication available free charge david broniatowski psychological foundations explainability interpretability artiﬁcial intelligence nistir national institute standards technology david broniatowski valerie reyna formal model theroy variations framing effects allais paradox decision wash doi rich caruana scott lundberg marco tulio ribeiro harsha nori samuel jenkins intelligible explainable machine learning best practices practical challenges proceedings acm sigkdd international conference knowledge discovery data mining pages association computing machinery new york usa august isbn url chen lin cynthia rudin shaposhnik wang wang explainable model credit risk performance technical report url chi two approaches study experts characteristics ericsson charness feltovich hoffman editors cambridge handbook expertise expert performance chapter pages cambridge university press cambridge doi chief finalcial ofﬁcers council performance improvement council playbook enterprise risk management federal government technical report july url deng dong socher imagenet largescale hierarchical image database ieee conference computer vision pattern recognition cvpr botty dimanov umang bhatt mateja jamnik adrian weller trust learning models conceal unfairness multiple explanation methods european conference artiﬁcial intelligence jennifer doleac luke stein visible hand race online market outcomes economic journal doi url finale kim towards rigorous science interpretable machine learning arxiv preprint ˇsilovi hlupi explainable artiﬁcial intelligence survey international convention information communication technology electronics microelectronics mipro pages may doi gary edmond alice towler bethany growns gianni ribeiro bryan found david white kaye ballantyne rachel searston matthew thompson jason tangen richard kemp kristy martire thinking forensics cognitive science forensic practitioners science justice issn publication available free charge doi url everingham eslami van gool williams winn zisserman pascal visual object classes challenge retrospective international journal computer vision january marte fallshore jonathan schooler verbal vulnerability perceptual expertise journal experimental psychology learning memory cognition issn doi board governors federal reserve system guidance model risk management supervision regulation letters federal reserve system april url tom ferguson game theory second edition url heather flowe joyce humphries examination criminal face bias random sample police lineups applied cognitive psychology issn doi leilani gilpin david bau ben yuan ayesha bajwa michael specter lalana kagal explaining explanations overview interpretability machine learning proceedings ieee international conference data science advanced analytics dsaa pages doi google inc facets url google llc explanations whitepaper pages united states government accountability ofﬁce report committee oversight government reform house representatives technical report december url riccardo guidotti anna monreale salvatore ruggieri franco turini fosca giannotti dino pedreschi survey methods explaining black box models acm computing surveys csur august issn doi url lars hall petter johansson betty sverker sikstr ese deutgen magic marketplace choice blindness taste jam smell tea cognition issn doi url lars hall petter johansson thomas strandberg lifting veil morality choice blindness attitude reversals survey plos one issn doi patrick hall navdeep gill nicholas schmidt proposed guidelines responsible use explainable machine learning nigel harvey conﬁdence judgment trends cognitive sciences issn doi publication available free charge peter hase mohit bansal evaluating explainable algorithmic explanations help users predict model behavior proceedings annual meeting association computational linguistics pages online july association computational linguistics doi url michael hind explaining explainable xrds april issn doi url robert hoffman shane mueller gary klein jordan litman metrics explainable challenges prospects february url arxiv andreas holzinger andr carrington heimo measuring quality explanations system causability scale scs intelligenz june issn doi url sara hooker dumitru erhan kindermans kim benchmark interpretability methods deep neural networks wallach larochelle beygelzimer dntextquotesingle alch fox garnett editors advances neural information processing systems volume pages curran associates url ying connor parde matthew hill naureen mahmood alice toole first impressions personality traits body shapes psychological science issn doi ibm research trusting accessed july url information commissioner ofﬁce alan turing institute explaining decisions made url jackson kaye neumann ranadive reyna communicating results forensic science examinations technical report natalie japkowicz mohak shah evaluating learning algorithms classiﬁcation perspective cambridge university press june url petter johansson lars hall sverker sikstr andreas olsson failure detect mismatches intention outcome simple decision task science issn doi saul kassin itiel dror jeff kukucka forensic conﬁrmation bias problems perspectives proposed solutions journal applied research publication available free charge memory cognition issn doi url frank keil explanation understanding annual review psychology doi url kim cynthia rudin julie shah bayesian case model generative approach reasoning prototype classiﬁcation ghahramani welling cortes lawrence weinberger editors advances neural information processing systems pages curran associates url pdf kindermans sara hooker julius adebayo maximilian alber kristof sven dumitru erhan kim reliability saliency methods wojciech samek montavon andrea vedaldi lars kai hansen editors explainable interpreting explaining visualizing deep learning lecture notes computer science pages springer international publishing cham isbn doi url pang wei koh percy liang understanding predictions via inﬂuence functions proceedings international conference machine learning volume icml pages sydney nsw australia joshua kroll joanna huey solon barocas edward felton joel reidenberg david robinson harlan accountable algorithms university pennsylvania law review pages justin kruger david dunning unskilled unaware difﬁculties recognizing one incompetence lead inﬂated journal personality social psychology richard kuhn raghu kacker lei dimitris simos combinatorial methods explainable iwct march url library catalog jeff kukucka saul kassin patricia zapf itiel dror cognitive bias blindness global survey forensic science examiners journal applied research memory cognition issn doi url chan lee brooks geoffrey norman using comprehensive feature lists bias medical diagnosis journal experimental psychology learning memory cognition issn doi kestutis kveraga avniel ghuman moshe bar prediction publication available free charge cognitive brain brain cognition doi isaac lage emily chen jeffrey menaka narayanan kim sam gershman finale evaluation explanation stat august url arxiv isaac lage emily chen jeffrey menaka narayanan kim samuel gershman finale human evaluation models built aaai conference human computation crowdsourcing october url vivian lai chenhao tan human predictions explanations predictions machine learning models case study deception detection november doi url hima lakkaraju julius adebayo sameer singh explaining maching learning predictions challenges opportunities december url himabindu lakkaraju osbert bastani fool manipulating user trust via misleading black box explanations proceedings conference ethics society aies page new york usa association computing machinery isbn doi url himabindu lakkaraju cynthia rudin learning interpretable treatment regimes artiﬁcial intelligence statistics pages april url himabindu lakkaraju stephen bach jure leskovec interpretable decision sets joint framework description prediction proceedings acm sigkdd international conference knowledge discovery data mining kdd pages new york usa association computing machinery isbn doi url san francisco california usa himabindu lakkaraju ece kamar rich caruana jure leskovec faithful customizable explanations black box models proceedings conference ethics society aies pages new york usa association computing machinery isbn doi url honolulu usa jared leclerc susan joslyn cry wolf effect decision making risk analysis benjamin letham cynthia rudin tyler mccormick david madigan interpretable classiﬁers using rules bayesian analysis building better stroke publication available free charge prediction model annals applied statistics september issn doi url oscar hao liu chaofan chen cynthia rudin deep learning casebased reasoning prototypes neural network explains predictions aaai conference artiﬁcial intelligence april url brian lim anind dey daniel avrahami explanations improve intelligibility intelligent systems conference human factors computing systems proceedings pages doi jimmy lin chudi zhong diane cynthia rudin margo seltzer generalized scalable optimal sparse decision trees proceedings international conference machine learning volume url pantelis linardatos vasilis papastefanopoulos sotiris kotsiantis explainable review machine learning interpretability methods entropy january doi url number publisher multidisciplinary digital publishing institute zachary lipton mythos model interpretability communications acm tania lombrozo structure function explanations trends cognitive sciences october issn doi url jos marcio luna efstathios gennatas lyle ungar eric eaton eric diffenderfer shane jensen charles simone jerome friedman timothy solberg gilmer valdes building accurate decision trees additive tree proceedings national academy sciences october scott lundberg lee uniﬁed approach interpreting model predictions guyon luxburg bengio wallach fergus vishwanathan garnett editors advances neural information processing systems pages curran associates url oisin mac aodha shihan yuxin chen pietro perona yisong yue teaching categories human learners visual explanations february url tim miller explanation artiﬁcial intelligence insights social sciences artiﬁcial intelligence february issn doi url publication available free charge smitha milli ludwig schmidt anca dragan moritz hardt model reconstruction model explanations url margaret mitchell simone andrew zaldivar parker barnes lucy vasserman ben hutchinson elena spitzer inioluwa deborah raji timnit gebru model cards model reporting proceedings conference fairness accountability transparency fat pages new york usa january association computing machinery isbn doi url sina mohseni niloofar zarei eric ragan multidisciplinary survey framework design evaluation explainable systems august url arxiv christoph molnar interpretable machine learning christoph molnar interpretable machine learning christophmolnar online edition edition april url shane mueller robert hoffman william clancey abigail emrey gary klein explanation systems literature synopsis key ideas publications bibliography explainable february url arxiv shane mueller elizabeth veinott robert hoffman gary klein lamia alam tauseef mamun william clancey principles explanation humanai systems arxiv preprint url menaka narayanan emily chen jeffrey kim sam gershman finale humans understand explanations machine learning systems evaluation explanation february url arxiv richard nisbett timothy decamp wilson michael kruger lee ross amos indeed nancy bellows dorwin cartwright alvin goldman sharon gurwitz ronald lemley harvey london hazel markus telling know verbal reports mental processes psychological review nist standards inclusivity effort team guidance nist staff using inclusive language documentary standards nist interagency internal report stuart oskamp overconﬁdence judgments journal consulting psychology issn doi phillips moon rizvi rauss feret evaluation methodology algorithms ieee trans pami october phillips scruggs toole flynn bowyer schott sharpe frvt ice results ieee trans pami jonathon phillips amy yates ying carina hahn eilidh noyes kelsey publication available free charge jackson jacqueline cavazos jeckeln rajeev ranjan swami sankaranarayanan face recognition accuracy forensic examiners superrecognizers face recognition algorithms proceedings national academy sciences phillips bowyer flynn liu scruggs iris challenge evaluation second ieee international conference biometrics theory applications systems pohl editor cognitive illusions handbook fallacies biases thinking judgement memory psychology press forough daniel goldstein jake hofman jennifer wortman vaughan hanna wallach manipulating measuring model interpretability november url arxiv emily pronin introspection illusion advances experimental social psychology pages elsevier mark przybocki alvin martin audrey nist speaker recognition evaluations utilizing mixer ieee transactions audio speech language processing zhongang saeed khorram fuxin visualizing deep networks optimizing integrated gradients proceedings aaai conference artiﬁcial intelligence april issn doi url arxiv dheeraj rajagopal vidhisha balachandran eduard hovy yulia tsvetkov selfexplain architecture neural text classiﬁers march url arxiv kaivalya rawal himabindu lakkaraju beyond individualized recourse interpretable interactive summaries actionable recourses advances neural information processing systems volume pages curran associates url marco tulio ribeiro sameer singh carlos guestrin trust explaining predictions classiﬁer kdd proceedings acm sigkdd conference knowledge discovery data mining san francisco usa august acm url allyson rice jonathon phillips alice toole role face body unfamiliar person identiﬁcation applied cognitive psychology john roach microsoft responsible machine learning capabilities build trust publication available free charge systems developers say accessed july url marko ˇsikonja marko bohanec explanations prediction models jianlong zhou fang chen editors human machine learning visible explainable trustworthy transparent interaction series pages springer international publishing cham isbn doi url cynthia rudin stop explaining black box machine learning models high stakes decisions use interpretable models instead nature machine intelligence cynthia rudin joanna radin using black box models need lesson explainable competition harvard data science review cynthia rudin chaofan chen zhi chen haiyang huang lesia semenova chudi zhong interpretable machine learning fundamental principles grand challenges arxiv preprint url seyed omid sadjadi timoth kheyrkhah audrey tong craig greenberg douglas reynolds elliot singer lisa mason jaime nist speaker recognition evaluation interspeech pages wojciech samek alexander binder montavon sebastian lapuschkin evaluating visualization deep neural network learned ieee transactions neural networks learning systems november issn doi conference name ieee transactions neural networks learning systems philipp schmidt felix biessmann quantifying interpretability trust machine learning systems stat january url arxiv jonathan schooler tonya verbal overshadowing visual memories things better left unsaid cognitive psychology issn doi jonathan schooler stellan ohlsson kevin brooks thoughts beyond words language overshadows insight journal experimental psychology general issn doi reva schwartz leann adam jonas elham tabassi proposal identifying managing bias artiﬁcial intelligence draft nist special publication national institute standards technology ramprasaath selvaraju michael cogswell abhishek das ramakrishna vedantam devi parikh dhruv batra visual explanations deep networks via localization proceedings ieee international publication available free charge conference computer vision pages keng siau weiyu wang building trust artiﬁcial intelligence machine learning robotics cutter business technology journal issn leon sixt maximilian granz tim landgraf explanations lie many modiﬁed attributions fail december url dylan slack sorelle friedler carlos scheidegger chitradeep dutta roy assessing local interpretability machine learning models stat august url arxiv dylan slack sophie hilgard emily jia sameer singh himabindu lakkaraju fooling lime shap adversarial attacks post hoc explanation methods proceedings conference ethics society aies page new york usa association computing machinery isbn doi url chun siong soon marcel brass hans jochen heinze john dylan haynes unconscious determinants free decisions human brain nature neuroscience issn doi siegfried ludwig sporer steven penrod read brian cutler choosing conﬁdence accuracy relation eyewitness identiﬁcation studies psychological bulletin issn doi brian stanton theodore jensen trust artiﬁcial intelligence draft nistir national institute standards technology kevin stine stephen quinn gregory witte robert gardner integrating cybersecurity enterprise risk management erm technical report nist internal interagency report nistir national institute standards technology october url leanne ten brinke dayna stimson dana carney evidence unconscious lie detection psychological science leanne ten brinke kathleen ohs dana carney ordinary people detect deception trends cognitive sciences issn doi url royal society explainable basics policy brieﬁng url alexander todorov face value irresistible inﬂuence ﬁrst impressions princeton university press alexander todorov anesu mandisodza amir goren crystal hall inferences competence faces predict election outcomes science new york publication available free charge issn doi url ehsan toreini mhairi aitken kovila coopamootoo karen elliot carlos gonzalezzelaya aad van moorsel relationship trust trustworthy machine learning technologies conference fairness accountability transparency fat barcelona spain doi alice towler david white richard kemp evaluating feature comparison strategy forensic face identiﬁcation journal experimental psychology applied amos tversky daniel kahneman framing decisions psychology choice science doi amos tversky eldar shaﬁr disjunction effect choice uncertainty psychological science issn doi berk ustun alexander spangher yang liu actionable recourse linear classiﬁcation proceedings conference fairness accountability transparency fat pages new york usa association computing machinery isbn doi url atlanta usa ellen oorhees system explanations cautionary tale acm chi workshop operationalizing perspectives explainable sandra wachter brent mittelstadt chris russell counterfactual explanations without opening black box automated decisions gdpr harv adrian weller transparency motivations challenges explainable interpreting explaining visualizing deep learning pages springer james wexler mahima pushkarna tolga bolukbasi martin wattenberg fernanda jimbo wilson tool interactive probing machine learning models ieee transactions visualization computer graphics doi timothy wilson yoav unseen mind science issn doi timothy wilson johnathan schooler thinking much introspection reduce quality preferences decisions journal personality social psychology issn doi john wixted laura mickes ronald fisher rethinking reliability eyewitness memory perspectives psychological science issn doi allison woodruff sarah fox steven jeff warshaw qualitative exploration perceptions algorithmic fairness conference human factors computing systems proceedings doi publication available free charge yeh hsieh arun suggala david inouye pradeep ravikumar ﬁdelity sensitivity explanations pages url nick yeung christopher summerﬁeld metacognition human decisionmaking conﬁdence error monitoring philosophical transactions royal society biological sciences issn doi qingyuan zhao trevor hastie causal interpretations models journal business economic statistics june issn doi url bolei zhou aditya khosla agata lapedriza aude oliva antonio torralba learning deep features discriminative localization proceedings ieee conference computer vision pattern recognition pages jianlong zhou amir gandomi fang chen andreas holzinger evaluating quality machine learning explanations survey methods metrics electronics january doi url number publisher multidisciplinary digital publishing institute xingchen zhou rob jenkins effects face perception cognition january issn doi luisa zintgraf taco cohen tameem adel max welling visualizing deep neural network decisions prediction difference analysis february url publication available free charge
