fra focus bigdata discrimination decision makinghelping make fundamental rights reality everyone european union fra focus contentswe live world big data technological developments area machine learning artificial intelligence changed way live decisions processes concerning everyday life increasingly automated based data affects fundamental rights various ways intersection rights technological developments warrants closer examination prompting fundamental rights agency research theme focus paper specifically deals discrimination fundamental rights area particularly affected technological developments algorithms used decision making potential discrimination individuals principle enshrined article charter fundamental rights european union needs taken account applying algorithms everyday life paper explains discrimination occur suggesting possible solutions overall aim contribute understanding challenges encountered increasingly important field data fundamental rights implications decision making predictions algorithms machine learning omputers learning discriminate etecting avoiding discrimination ossible ways forward addressing fundamental rights big data references bigdata discrimination decision making big data fundamental rights implications past decades technological advancements changed way live organise lives changes inherently connected proliferation use big data big data generally refers technological opments related data collection storage sis applications often characterised increased volume velocity variety data produced three typically refers limited data big data comes variety sources including social media data website metadata inter net things iot contributes big data behavioural location data smartphones fitness tracking devices addition transaction data business world form part big data providing information payments administrative increased availability data led improved technologies ana lysing using data example area machine learning artificial intelligence arguably big data enhance lives instance health sector personalised diagnosis medicine lead better care however negative fundamental rights impli cations big data technologies recently acknowledged public authorities international organisations use new technologies algorithms including machine learning affects several fundamental rights include limited right fair trial prohibition discrimination privacy free dom expression right effective rem edy outlined council europe report addition european parliament adopted resolution highlighting need action area big data fundamental rights resolution european liament uses strongest language refer ring threat discrimination use algorithms serves underpin particu lar focus paper additionally terms increased variability respect consistency data time veracity respect accuracy data quality complexity terms link multiple datasets added list characteristics big data callegaro yang callegaro yang seitz uropean parliament council europe european data protection supervisor edps white house council europe european parliament fra reports highlighted fundamen tal rights challenges posed technologies built big data includes fra reporting oversight surveillance national ligence authorities requested european parliament addition fra published reports use biometric related data large databases starting present paper fra explores implications big data regarding fundamental rights focusing first instance discrimination key area legal competence also area fra undertaken extensive research date european parliament resolutions big data artificial intelligence use big data commercial purposes public sector european parliament calls european commission member states data protection authorities identify take possible measures minimise algorith mic discrimination bias develop strong common ethical framework transpar ent processing personal data automated decision may guide data usage ongoing enforcement union law comes use big data law enforce ment purposes parliament warns maxi mum caution required order prevent unlawful discrimination targeting certain individ uals groups people defined reference race colour ethnic social origin genetic features language gender expression identity sexual ori entation residence status health membership national minority often subject ethnic profiling intense law enforcement policing well individuals happen defined particular characteristics moreover european parliament high lighted need ethical principles concern ing development robotics artificial intel ligence civil use points guiding ethical framework based principles values enshrined article treaty european union charter fundamental rights human dignity equality justice equity non informed consent private family life data protection among principles european parliament european parliament fra focus discrimination direct indirect discrimination use algorithms using big data increasingly considered one pressing challenges use new technologies paper addresses selected fundamental rights implications related big data focusing threat discrimination using big data support decision making previously decisions processes taken little support computers nowadays increase use sophisticated tech niques statistical data analysis facilitate tasks however lead discrimination example may include automated selection candidates job interviews based predicted productivity another example use risk scores assessing credit worthiness individu als applying loans furthermore course trial use risk scores decision process sentencing lead discrimination principle non iscrimination embedded law article charter fundamen tal rights prohibits discrimination based sev eral grounds including sex race colour ethnic social origin genetic features language religion belief political opinion membership national minority property birth disability age sexual orientation nationality tion related attributes pro cessed data connected individual definition makes personal data protected data protection legal frame work particularly important due grow ing availability processing large amounts data also include information potentially indi cating one characteristics individuals level general data protection regulation gdpr addresses new technologi cal developments including potential dis crimination fundamental rights implications using algorithms decision making include consid erations addressed gdpr also beyond issues raised highlight potentially prob lematic nature use data decision mak ing however use data inform decisions also considered positive development poten tially allows objective informed deci sions comparison decisions take account available data also potential limit discriminatory treatment based human decision making derived existing limits data data analysis need taken account decisions supported data potentially better decisions without empirical support algorithms turn used identify systematic bias poten tially discriminatory processes therefore big data also presents opportunities assessing rights compliance decision making predictions algorithms machine learning increased availability use data deci sions increasingly facilitated times even completely taken using called predictive modelling methods often referred use algorithms using data predict incidents behaviour major part develop ments related machine learning classic example using algorithms based data sis tool people experience every day spam filter algorithm learned level certainty identify whether email spam block basic understanding algorithms support decisions essential allow experts prac titioners fields enter discussion increase awareness technical literacy furthermore important able identify ask right questions potential prob lems arise using algorithms particularly comes discrimination creating algorithms make predictions may involve different methods use train ing data find calculations predict cer tain outcome accurately example set several thousands emails identified either spam spam used identify charac teristics define differences two general data protection regulation bigdata discrimination decision making emails example characteristics may include specific words combinations words within emails process rules identifying spam established many differ ent calculations algorithms used best performing one selected cal culation categorise cases correctly critically important note output algorithms always based probability means uncertainty attached clas sifications made see daily lives methods work quite well infallible sometimes spam passes email inbox false negatives errone ously identified spam less frequently fully legitimate email might suppressed filter false positive rate true positives rate true negatives trade two rates commonly used assess classification problem detect ing spam several rates indicators available analysed assessment well algorithm works computers learning discriminate using data algorithms prediction con siderably facilitate decisions allows revelation patterns otherwise identified however algorithm contribute discriminatory decision making specified legislation discrimination illegal respect several conditions related employment access social services education also forbidden indirect apparently neutral provision criterion practice would put persons racial eth nic origin particular disadvantage compared persons unless provision rion practice objectively justified legit imate aim means achieving aim appropriate similarly article police directive directive explicitly prohibits profiling results tion basis special categories personal data race ethnic origin sexual orientation council directive july implementing principle equal treatment persons irrespective racial ethnic origin see also fra fra algorithm term algorithm widely used context big data machine learning algorithm sequence commands computer transform input output example list per sons sorted according age computer takes ages people list input produces new ranking list output area machine learning several algorithms used could also referred ways calculating desired predictions use data many algorithms statistical methods based regression methods widely used statisti cal techniques calculating influence set data selected outcome example consider calculating average influence drinking alcohol life expectancy using existing data aver age amount alcohol person drinks compared life expectancy based calculations life expectancy calculated predicted persons simply taking consideration amount alcohol person drinks assuming correlation exists algorithm used depends way data presented whether numerical textual data goal calculation prediction explanation grouping cases machine learning often several algorithms tested see one best performance predicting outcome creation algorithms prediction complex process involves many decisions made sev eral people variously involved process therefore refer rules followed computer also process collecting preparing analysing data human process includes several stages involving decisions developers managers statistical method part process developing final rules used prediction classification decisions according racial equality directive dis crimination occurs one person treated less favour ably another would treated comparable situation grounds racial ethnic origin fra focus opinion religious whereas term discrimination absent previous data protection directive need prevent discrimination result automated decision making emphasised new gdpr automated decision making including profiling significant effects data subject bidden according article gdpr ject specific exceptions order ensure fair transparent processing respect data subject controller use appropriate mathematical statistical procedures profiling implement technical organisational measures appropriate ensure particular factors result inaccuracies personal data corrected risk errors minimised secure personal data manner takes account potential risks involved interests rights data subject prevents inter alia discriminatory effects natural persons general data protection regulation recital academics practitioners increasingly researching ways detect repair algorithms potentially discriminate individuals certain groups basis particular attrib utes example sex ethnic pens predicted outcome particular group systematically different groups therefore one group consistently treated dif ferently others example cases member ethnic minority lower chance invited job interview algo rithm trained based data par ticular group performs worse worse comes groups result may invited job interview occur data used train algorithm include information regarding protected characteristics gender ethnicity religion furthermore proxy information sometimes included data may include height person correlates gender postcode indirectly indicate ethnic origin cases segre gated areas cities directly person country birth unequal outcomes differen tial treatment especially relating proxy informa tion need assessed see amount discrimination directive protection natural persons regard processing personal data competent authorities purposes prevention investigation detection prosecution criminal offences execution criminal penalties free movement data repealing council framework decision increasing literature discrimination algorithms see example zliobaite clusters kamiran žliobaitė calders sandvig discrimination might based differences outcomes groups choice data used might neutral data used building algorithm biased group systematic differences due way data collected algorithm replicate human bias selecting learn discriminate group particularly dangerous assumed machine ing procedure results objective without taking account potential problems underlying data data biased several rea sons including subjective choices made selecting collecting preparing data give real example automated description images trained based thousands images described humans however humans neutrally describe images namely baby white skin colour described baby baby black skin colour described black baby biased data assigned additional attributes certain group objectively either cases described including colour none biased information included training data used development algorithms used resultant predictions therefore addition problematic use data rein forcing bias groups low quality data lead poor predictions discrimination data either poorly selected data incomplete incorrect poorly selected data might include unrepresentative data allow generalising groups example algorithm created using data certain group job applicants predictions another group might hold quality data potential bias particularly important age big data data generated often quickly internet without quality control tics often referred garbage garbage even well methods predictions function effectively using low quality sense data quality checks appropriate documentation data meta data essential high quality data analysis use algorithms decision making miltenburg white house example found speech recognition work well women men difference might come algorithms trained using datasets include men women see article issue example number databases used field border management asylum incorrect poor quality data highlighted area needs addressed see fra report data interoperability fra fra report biometrics fundamental rights fra bigdata discrimination decision making detecting avoiding discrimination auditing algorithms detecting discrimination algorithms easy task detecting forms discrimination general difficult algorithms used modern applications machine learning artificial intel ligence increasingly complex consequence results difficult almost impossible interpret terms information data influences predictions way related fact huge amounts data used pre dict certain outcomes example frequently algorithms based neural networks work hidden layers relationships combinations different characteristics data makes difficult assess whether person discriminated grounds gender ethnic origin religious belief grounds however predictive algorithm fed information different groups finds differ ence according information may potentially provide output discriminates despite complexity algorithms need audited show lawful words must shown process data way leads discrimination context term auditing comes discrimination testing situation testing real situations example experiment two identical fictional job appli cations sent employers group membership interest ethnic origin varies often simply name applicant changed using names typically indicating ethnic way experimental situation created differences call rates job interview directly interpreted discrimination similar approaches also used detecting discrimination use algorithms example recent advances research algorithmic discrimi nation suggested different ways detect dis crimination internet platforms algorithms audited obtaining full access computer ware code used algorithms could evaluated technical expert however might always straightforward discrimination directly encoded syn tax computer software methods could include creation profiles platforms randomised testers sent repeatedly see outcome differs according characteristics see example bertrand mullainathan could influence discrimination however tests undertaken real data platforms could lead legal problems service question harmed example overloading server requests hence tests created way avoid problems one example test used detect well gender clas sification algorithms work different groups using preselected images results study showed gender recognition works considera bly less well darker skinned females compared white males additionally methods extract informa tion data contribute come algorithm way checked information protected grounds ethnic ori gin important information certain characteristics needs extracted algorithm understand differences comes results algorithm example difference income explains person offered loan might however group membership makes difference deci sion loan might discrimination easiest way detect discrimination full transparency granted sense code data used building algorithm accessible auditor however even case always straightforward results might appear discriminatory closer look shows example group may appear treated differently aggregate level breaking results explainable differences shows discrimination exists avoid violating fundamental rights crucial automated tools used making decisions people lives transparent however data sandvig buolamwini gebru see example blog issue wachter mittelstadt russel kamiran žliobaitė calders related one cases related admissions studies university california berkeley shown overall women much less likely admitted men however splitting admission rates type studies shown different types studies women actually slightly higher chance admitted studies completely result occur certain circumstances related different magnitude applications per type studies differing admission rates bickel one explanation women tended apply competitive studies compared men fra focus creating algorithm often available people outside company institution creating algorithm due issues copyright competition businesses ers data easily make case either methods detect discrimination without access dataset need developed allowed discrimination testing ways auditing algorithms process pro tects business secrets need put place example external expert auditors access data code similar financial auditor checks regard consideration could given establishing public bodies right power investigate use predictive systems used make decisions affect peo ple one example study bias algorithms carried journalists investigated racial bias risk scores used crimi nal justice system analysed correctional offender management profiling alternative sanc tions compas scores including risk scores recid evaluate well risk scores work often legitimate reasons example law enforcement purposes data algorithms often publicly available moreover user algorithms might data use consequently able share data applications third parties using twitter data sandvig tutt study including accompanying documentation available propublica webpage report cited angwin journalists obtained dataset criminal histo ries period two years broward county florida analysed actual recidivism com parison risk score based analysis reported white defendants often mislabelled low risk compared black defend ants risk score likely falsely flag black defendants high risk potentially indicated racially algorithms however company researchers contested result pointing alternative analyses showing model make difference looking risk scores group separately cussion demonstrates complex prove algorithm discriminates interpretation results takes place comparatively advanced level sta tistical analysis findings fully conclusive due absence standards evaluation challenge create fair algorithms pres ence different outcomes interpretation situation complicated absence case law field last point relates problem fully clear differential treatment groups flores legal principle right meaningful information full transparency processing might pos sible due number legitimate reasons example issues may include intellectual property rights issues national security ever controllers make sure auto mated decision process explained clearly simply individuals particular involv ing decision processes based profiling materialises gdpr intro duction right explanation states data subjects provided meaningful information logic involved well significance envisaged consequences processing data subject euro pean parliament clarified understanding concept stating include information data used training rithms allow individuals understand monitor decisions affecting however concrete implementation right remains seen related mainly auto mated decision making emphasised sev eral researchers delimitation scope feasibility explanation provided evident notably wording meaningful information raises several questions information deemed mean ingful information cover ration ale behind technique technique point would benefit clarification example european data protection board edpb established gdpr general data protection regulation recital art european parliament see wachter mittelstadt floridi godman flaxman tarran selbst powles bigdata discrimination decision making constitutes discrimination united states case hiring practices ruled unlawful even decision explicitly deter mined based race barriers accessing job test directly related job requirements disproportionately put black applicants case certain level dif ferential treatment reached deemed constitute discrimination time united states supreme court acknowledged general rule level differen tial treatment seen discriminatory instead situation application needs assessed rately detailed analysis way group dis criminated conducted taking propor tionality account respect impact certain criteria different groups example specific job requirement completed training particular country may genuine occupational requirement carrying job requirement puts certain groups disad vantage immigrants potential algorithm sorts job applications use information determining eligibility job data protection impact assessment dpia algorithms may complex character istics influence outcome might easily identifiable article gdpr made data protection impact assessment dpia datory processing particular using new technologies likely result high risk rights freedoms natural persons dpias complex procedures may require combination technical legal sociological knowledge institutional academic actors developed standards guidance help data scientists development implemen tation standards developed industry associations context guid ance minimum requirements inserted dpia established regional level would likely enhance effectivity dpias using dpias could way controllers increase accountability make sure outset applications discriminatory see instance template smart grid smart metering systems developed european commission guidance privacy design dpia developed information commissioner office see instance detailed roadmap developed spiekermann referred council europe study human rights dimensions automated data processing techniques council europe supreme court griggs duke power feldmann discrimination outset repairing algorithms research discussion needed develop methods ensure algorithms discriminating rectify algorithms found discriminatory several ways assess whether group treated differently presence different outcomes different groups one group likely eligible job inherent trade creating fair predictions accounts mathe matically proven possible overall risk scores different groups balanced prediction model unless groups performing differently certain outcome example women may fare worse scale used hiring people using income infor mation proxy past performance work algorithms discriminate particular group needs assessment implications using different performance measurements proxies however pre cisely balanced predictions genders would possible limits ability make fair predictions groups reality fare ently certain addition situations potential bias discrim ination easily solved simply exclud ing information protected group dataset exclude information gender ethnic origin could additional informa tion may related membership pro tected group example given post code could indicate ethnic origin mentioned ways address detect indirect informa tion protected attributes result indirect discrimination used repair algorithms example testing whether pro tected characteristics individuals pre dicted well information included dataset however research topic begun needs progress develop standardised methods detecting example overall accuracy one group likely hired controlling explanatory factors true positive rate algorithm often selects people one group among selected true negative rate algorithm often rejects people one group among rejected different ways assess fairness discussion measuring bias different fairness criteria see chouldechova kleinberg accessible description see blog focusing quantitative issues alder fra focus discrimination one fundamental chal lenge data analysis question whether information protected group available data used create predictions data protection design default mere exclusion information protected groups sufficient avoid discrimina tion non data processing may strengthened implementa tion appropriate technical organisational measures designed integrate necessary safeguards processing order protect rights data subjects pointed edps data pro tection authorities ensuring data protec tion embedded early conception phase algorithm several positive consequences minimises risks potential tory results increases trust data subject data controller led programmers focus develop privacy enhancing technologies pets similar meth ods could conceptualised integrated development data decision mak ing ensure non sense design approach would work con junction risk assessment also takes potential discrimination account time consideration could given whether data protection design default also leads situation information could used detect poten tial discriminatory practices withheld data analysis protecting data subject clarification needed data sensitive personal characteristics employed example data may used aggregate data statistical purposes identify poten tial discrimination practices selection persons interviewing general data protection regulation edps ico guide data protection privacy design enisa privacy enhancing technologies evolution state art march availability information protected characteristics methods developed far identify poten tial discrimination make use information protected group information ethnic origin dataset availability information pro tected groups differs considerably across countries data collection approaches example infor mation race ethnicity frequently collected united states united kingdom however many member states move away data collection ethnicity countries data collection information ethnic origin forbidden introduced countries due abuse administra tive data included ethnic origin religion second world means collecting data ethnic origin protected attribute data protection becomes particularly important includes explicit consent data subject well con sideration data collection pub lic interest explicit legal obligation col lect data general rule gdpr established processing sensitive data prohib ited listed ten exceptional cases processing data may allowed include limited obtention data subject explicit consent protection vital interests protection public interest area public health contrast number member states united kingdom movement favour collecting data ethnicity started purpose able detect discrimination strong arguments use ethnic identifiers data collection order able detect discriminatory treat ment outcomes fact including infor mation needed detect discrimination algorithms also needed correct discussion area also related interplay right privacy right fair processing personal data future research assess extent proxies information highly correlated protected characteristics citizenship simon patrick general data protection regulation chopin farkas germaine zliobaite clusters hoboken bigdata discrimination decision making ethnic origin height gender used instead direct information protected characteristics discussion extends grounds well increased amount information datasets including potentially thou sands different characteristics collected mind assess ment types characteristics profiled algorithms purpose prediction consid ering list protected characteristics article charter fundamental rights lot infor mation easily relate grounds example protection could apply political opinions facebook twitter posts impor tant note use algorithms profil ing example using social media data easily predict relate special categories forbidden article gdpr example combining likes social media data used quite accurately determine per son sexual orientation ethnic origin feedback loops efficient decision making algorithms used daily decision making shape people lives applied longer period time replicate decision making decisions based algorithms shape future data algorithm learn words decisions based data exists feedback loop might problematic algorithm fair balanced however models based biased data algorithms discrimination replicated perpetuated potentially even reinforced discriminatory predictions create allegedly neutral basis future data fore basis development algo rithms used future could even lead con tinuously increasing discrimination due efficient decisions taken basis data puts certain groups disadvantage consequently makes difficult members disadvantaged groups obtain fairer opportu exacerbated automated deci sions created algorithms example statistical learning procedures efficient law enforcement adapt policing surveillance practices based historical assumed neutral data lead unequal comes even underlying levels criminality among certain sub negative effect criminalising spe cific sub problem using algo rithms future assessments based results algorithms example hiring algorithms receive feedback people hired predictive policing algorithms observe crime neighbourhoods patrol means feedback loops algorithms need assessed detail identify whether neces sary repair algorithms avoid discrimination ultimately produce accurate results possible ways forward addressing fundamental rights big data use data research evidence policies decision making gained popularity past decades trend also needs embrace use big data new current developments area big data call action several levels society given impact use big data across multiple spheres focus paper highlighted poten tial threats fundamental rights focusing dis crimination building algorithms based big data support decisions use data advanced methods prediction improve way make decisions long basis decisions fully understood neg atively affect fundamental rights efficiency see examples explanation given article data protection working party also needs assessed basic fundamental values equal treatment since decision making based predictive models related methods relatively new need safeguards area needs considered comparison production sell ing food drugs highly regulated due dangers related unregulated use ing different reasons needs regulation white house neil osoba osonde welser william ensign see example discussion kleinberg menon williamson comparison taken tutt fra focus two areas however use algorithms making decisions building automated processes may significant impact people lives makes important consider need review impact existing legislation oversight mechanisms place would help prevent possible negative consequences people resulting use algorithms poten tially lessons could learned existing fields highly regulated medicine consideration could given oversight algorithms used different purposes might undertaken practice reflected fra opinions related oversight area surveillance attached agency report subject effective oversight would require detailed assessment experts would need provided law mem ber independent oversight experts dif ferent subject fields including technical fields crucial safeguarding legality data applications one way potentially ensuring effec tive accountability setting dedicated bod ies exclusive mandate provide oversight big data technologies similar role data protection authorities dpas developing guidance use algorithms calls strong collaboration statisticians lawyers social scientists computer scientists mathematicians subject area experts obtain clarity presence discrimination fundamental rights breaches sum field warrants careful analysis gdpr include safeguards address non discriminatory automated decision making processing data data science experts agree big data technologies currently held accountable need oversight urgent real parency key element achieve effective reme dies complex algo rithmic decision making large datasets part elements legal practitioners review gdpr increased powers data protection authorities euro pean union making competent hear com plaints conduct investigations promote aware ness however practical terms progress required ensure use algorithms compliance law developing using algorithms decision making lawfulness needs assessed data fra pew research center see fra fra principles provide guidance come legal decisions concerning use algorithms however needs consid ered fundamental rights approach encompasses analysis legality based data protection time goes beyond stated fra opinion proposed regulation european travel information authorisation sys tem etias defining risks threat dis crimination use algorithms neg ligible without testing algorithms showing application risk assessments sary proportionate result discriminatory profiling decision making may certain application compliance mind following examples move towards fundamental rights compliance development use algorithms transparent possible opening scrutiny algorithms built supports development tools allows others detect therefore rectify erro neous applications fundamental rights impact assess ments identify potential biases abuses application output algorithms include among others assessment potential discrimination relation dif ferent grounds gender age ethnic ori gin religion sexual political orientation time impact assessments could assess potential discrimination bias using proxy information addresses respect protected grounds area discrimination quality data given amount data generated used remains challenge assess quality data collected used building algorithms however essential collect metadata information data make quality assessments correctness generalisability data sure way algorithm built operates may meaningfully explained would help facilitate access remedies people challenge data decisions also relates principle transparency challenge understanding mathemati cal background statistical method algo rithm prevent general description process rationale behind calcula tions feeding decision making notably data used create algorithm fra bigdata discrimination decision making alder auditing models indirect influence proceedings ieee interna tional conference data mining icdm angwin machine bias software used across country predict future crim inals biased blacks data protection working party guidelines automated individual ing profiling purposes regulation bertrand mullainathan emily greg employable lakisha jamal field experiment labor market discrimination nber working paper series working paper bickel sex bias graduate admis sions data berkeley science vol buolamwini gebru gender shades intersectional accuracy disparities commercial gender classification proceedings machine learning research vol conference fairness accountability transparency callegaro yang role veys era big data vannette krosnick eds palgrave handbook sur vey research chouldechova fair prediction dis parate impact study bias recidivism predic tion instruments february chopin farkas germaine ethnic origin disability data collection europe comparing discrimination migration policy group open society foundations council europe guidelines protec tion individuals regard processing personal data world big data january council europe study human rights dimensions automated data processing techniques particular algorithms possible regulatory implications committee experts internet intermediaries msi european data protection supervisor edps edps opinion coherent enforcement funda mental rights age big data opinion meeting challenges big data opinion november edps opinion european data protec tion supervisor promoting trust informa tion society fostering data protection pri vacy march ensign runaway feedback loops predictive policing proceedings machine learn ing research vol conference fair ness accountability transparency european parliament fundamental rights implications big data european parliament civil law rules robotics feldmann certifying removing disparate impact proceeding kdd proceed ings acm sigkdd international confer ence knowledge discovery data mining flores false positives false nega tives false analyses rejoinder machine bias software used across country predict future criminals biased blacks federal probation vol fra european union agency fundamental rights towards effective policing understanding preventing discriminatory ethnic profiling guide luxembourg publications office european union publications office fra handbook european tion law luxembourg publications office fra surveillance intelligence services fundamental rights safeguards remedies luxembourg publications fundamental rights interop erability information systems borders security luxembourg publications office fra impact fundamental rights proposed regulation european travel information authorisation system etias fra opinion vienna watchful eyes biometrics systems fundamental rights luxembourg publications office fra focus flaxman regulations algorithmic right explanation van hoboken collection use privacy regulation forward looking comparison european frameworks personal data processing van der sloot broeders jvers eds exploring boundaries big data netherlands scientific council government pol icy information accountability foundation effective data protection governance project improving operational efficiency regulatory certainty digital age july information commissioner office big data artificial intelligence machine learning data pro tection march kamiran žliobaitė calders quan tifying explainable discrimination removing gal discrimination automated decision making knowledge information systems kleinberg forthcoming inherent trade fair determination risk scores proceedings innovations theoretical computer science itcs kleinberg human decisions machine predictions nber working paper menon aditya williamson robert cost fairness binary classification proceedings machine learning research vol conference fairness accountability transparency march miltenburg stereotyping bias dataset proceedings workshop multimodal corpora computer vision processing may slovenia mittelstadt allo taddeo wachter floridi ethics algorithms mapping debate big data society neil weapons math destruction new york osonde welser william intelligence image risks bias errors artificial intelligence santa monica rand corporation pew research center pros cons algorithm age simon ethnic statistics data protection council europe countries bourg council auditing algorithms research methods detecting discrimination internet platforms paper presented data discrimination converting critical concerns pro ductive inquiry may seattle usa seitz big data pharmaceutical sec tor vermeulen lievens eds data protection privacy pressure selbst powles meaningful infor mation right explanation international data privacy law vol spiekermann ethical innovation system design approach crc right explanation decisions significance magazine april white house big data report algorithmic systems opportunity civil rights tutt fda algorithms administrative law review vol mittelstadt floridi right explanation automated deci exist general data pro tection regulation international data privacy law wachter mittelstadt russel counterfactual explanations without opening black box automated decisions gdpr last revised dec zliobaite clusters using sensitive personal data may necessary avoiding dis crimination data decision models artificial intelligence law fra european union agency fundamental rights schwarzenbergplatz vienna austria tel fax info european union agency fundamental rights print isbn pdf isbn print pdf information following fra publications offer information relevant topic paper handbook european data protection law edition watchful eyes biometrics systems fundamental rights fundamental rights interoperability information systems borders security surveillance intelligence services fundamental rights safeguards remedies volume field perspectives legal update surveillance intelligence services fundamental rights safeguards remedies european union mapping member states legal frameworks impact fundamental rights proposed regulation european travel information authorisation system etias interoperability fundamental rights implications handbook european law edition
