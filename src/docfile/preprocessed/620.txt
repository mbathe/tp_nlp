review bias algorithmic november centre data ethics centre data ethics innovationpreface board centre data ethics innovation executive summary part introduction background scope issue part sector reviews recruitment financial services policing local government part iii addressing challenges enabling fair innovation regulatory environment transparency public sector next steps future challenges acknowledgements bias algorithmic contentsreview bias algorithmic contents centre data ethics fairness highly prized human value societies individuals flourish need held together practices institutions regarded fair means fair much debated throughout history rarely recent months issues global black lives matter movement levelling regional inequalities within many complex questions fairness raised pandemic kept fairness equality centre public debate inequality unfairness complex causes bias decisions organisations make individuals often key aspect impact efforts address unfair bias often either gone unmeasured painfully slow take effect however currently going period change use data automation existed sectors many years currently expanding rapidly due explosion volumes available data increasing sophistication accessibility machine learning algorithms data gives powerful weapon see bias occurring measure whether efforts combat effective organisation hard data differences treats people build insight driving differences seek address however data also make things worse new forms surfaced numerous examples algorithms entrenched amplified historic biases even created new forms bias unfairness active steps anticipate risks measure outcomes required avoid concern algorithmic bias starting point policy review began work issue concern growing relatively small number people publish report issue exploded mainstream attention context exam results strong narrative algorithms inherently problematic highlights urgent need world better using algorithms right way promote fairness undermine algorithms like technology work people true sectors especially key public sector state making decisions individuals individual often elsewhere society may reasonably conclude justice requires processes designed human judgment intervene needed achieve fair reasonable outcomes person informed individual evidence work progressed become clear separate question algorithmic bias question biased broadly approach take tackling biased algorithms recruitment example must form part consistent way understand tackle discrimination recruitment generally core theme report opportunity adopt rigorous proactive approach identifying mitigating bias key areas life policing social services finance recruitment good use data enable organisations shine light existing practices identify driving bias ethical obligation act wherever risk bias causing harm instead make fairer better choices risk growing algorithms datasets feed become increasingly complex organisations often find challenging build skills capacity understand bias determine appropriate means addressing world cohort people needed skills navigate data gives powerful weapon see bias occurring measure whether efforts combat effective organisation hard data differences treats people build insight driving differences seek address review bias algorithmic contents centre data ethics techniques expose bias ethical legal considerations inform best responses organisations may able create internally others want able call external experts advise senior organisations need engage understanding inherent introducing algorithm expect demand sufficient explainability algorithm works make informed decisions balance risks opportunities deploy process regulators industry bodies need work together wider society agree best practice within industry establish appropriate regulatory standards bias discrimination harmful context specific forms take precise mechanisms needed root vary greatly contexts recommend clear standards anticipating monitoring bias auditing algorithms addressing problems overarching principles details standards need determined within sector use case hope cdei play key role supporting organisations regulators government getting right lastly society whole need engaged process world many different concepts fairness introduce complex algorithms systems range definitions multiplies rapidly definitions often contradictory formula deciding correct technical expertise needed navigate choices fundamental decisions fair left data scientists alone decisions truly legitimate society agrees accepts report sets organisations might tackle challenge transparency key helping organisations build maintain public trust clear understandable nervousness use consequences algorithms exacerbated events summer open algorithms used checks balances place best way deal organisational leaders need clear retain accountability decisions made organisations regardless whether algorithm team humans making decisions basis report set key next steps government regulators support organisations get use algorithms right whilst ensuring ecosystem set support good ethical innovation recommendations designed produce step change behaviour organisations making life changing decisions basis data however limited regardless whether used complex algorithms traditional methods enabling data used drive better fairer trusted challenge countries face around world taking lead area strong legal traditions centres expertise help address bias inequalities within borders also across globe board centre data ethics innovationreview bias algorithmic preface executive summary centre data ethics biases whether conscious unconscious problem many processes review considers impact increasing use algorithmic tools bias steps required manage risks opportunities better use data offers enhance fairness focused use algorithms significant decisions individuals looking across four sectors recruitment financial services policing local government making recommendations aim help build right systems algorithms improve rather worsen well established risk algorithmic systems lead biased decisions perhaps largest underlying cause encoding existing human biases algorithmic systems evidence far less clear whether algorithmic tools carry less risk bias previous human processes indeed reasons think better use data role making decisions fairer done appropriate care changing processes make decisions individuals always proceed caution important recognise algorithms everything aspects human judgement including ability sensitive flexible unique circumstances individual remain crucial using data algorithms innovative ways enable organisations understand inequalities reduce bias aspects also circumstances using algorithms make decisions seen unfair failing consider individual circumstances depriving personal agency directly focus kind unfairness report note argument also apply human individual subject decision role contributing decision history date design deployment algorithmic tools good enough numerous examples worldwide introduction algorithms persisting amplifying historical biases introducing new ones must better making fair unbiased decisions good individuals involved good business society successful sustainable innovation dependent building maintaining public trust polling undertaken review suggested prior august controversy exam results people aware algorithmic systems used support decisions disagreeing principle suggestion fair accurate algorithm helping make decisions october found awareness risen slightly disagreement principle suggest step change public attitudes clearly still long way build trust algorithmic systems obvious starting point ensure algorithms trustworthy use algorithms complex area widely varying approaches levels maturity across different organisations sectors ultimately many steps needed challenge bias contextspecific work identified number concrete steps industry regulators government take support ethical innovation across wide range use cases report guidance manual considers guidance support regulation incentives needed create right conditions fair innovation flourish crucial take broad view whole decisionmaking process considering different ways bias enter system might impact fairness issue simply whether algorithm biased whether overall processes biased looking algorithms isolation fully address important consider bias algorithmic decisionmaking context systems even human differing views fair society developed range standards common practices manage issues legal frameworks support organisations level understanding constitutes appropriate level due care fairness challenge make sure translate understanding across algorithmic world apply consistent bar fairness whether decisions made humans algorithms combination two must ensure decisions scrutinised explained challenged current laws frameworks lose effectiveness indeed made effective time significant growth happening data availability use algorithmic across many sectors window opportunity get right ensure changes serve promote equality entrench existing bias algorithmic executive summary centre data ethics reviews four sectors studied part report different maturity levels use algorithmic issues face sectorspecific found common challenges span sectors beyond recruitment saw sector experiencing rapid growth use algorithmic tools stages recruitment process also one relatively mature collecting data monitor outcomes human bias traditional recruitment well evidenced therefore potential tools improve matters standardising processes using data inform areas discretion human biases creep however also found clear consistent understanding well lacking leading risk algorithmic technologies entrench inequalities guidance needed ensure tools unintentionally discriminate groups people particularly trained historic current employment data organisations must particularly mindful ensure meeting appropriate legislative responsibilities around automated reasonable adjustments candidates disabilities innovation space real potential making recruitment fairer however given potential risks scrutiny tools work used impact different groups required along higher clearer standards good governance ensure ethical legal risks anticipated managed financial services saw much mature sector long used data support finance relies making accurate predictions peoples behaviours example likely repay debts however specific groups historically underrepresented financial system risk historic biases could entrenched algorithmic systems found financial service organisations ranged highly innovative risk averse use new algorithmic approaches keen test systems bias mixed views approaches regarding done particularly evident around collection use protected characteristic data therefore organisations ability monitor outcomes main focus within financial services credit scoring decisions made individuals traditional banks work found key obstacles innovation sector included data availability quality source data ethically available techniques sufficient explainability risk averse culture parts given impacts financial crisis difficulty gauging consumer wider public acceptance regulatory picture clearer financial services sectors looked financial conduct authority fca main regulator showing leadership prioritising work understand impact opportunities innovative uses data sector use data sources could enable population groups historically found difficult access credit due lower availability data traditional sources gain better access future time data complex algorithms could increase potential introduction indirect bias via proxy well ability detect mitigate adoption algorithmic public sector generally early stage policing found tools currently operation varied picture across different police forces usage approaches managing ethical risks notable government reviews issue bias policing important context considering risks opportunities around use technology sector found potential algorithms support introduces new issues around balance security privacy fairness clear requirement strong democratic oversight police forces access digital material ever expected use data identify connections manage future risks million funding police technology programmes announced january demonstrates government drive innovation clearer national leadership needed though strong momentum data ethics policing national level picture fragmented multiple governance regulatory actors single body fully empowered resourced take ownership use data analytics tools policing carries significant risk without sufficient care processes lead review bias algorithmic executive summary centre data ethics biased particular groups systematically unfair many scenarios tools helpful still important balance struck automated application professional judgement discretion given sensitivities area sufficient care taken internally consider issues also critical police forces transparent tools used maintain public trust local government found increased use data inform across wide range services whilst tools still early phase deployment increasing demand sophisticated predictive technologies support efficient targeted services bringing together multiple data sources representing existing data new forms technologies guide providing contextualised picture individual needs beyond decisions individuals tools help predict map future service demands ensure sufficient sustainable resourcing delivering important services however technologies also come significant risks evidence shown certain people likely overrepresented data held local authorities lead biases predictions interventions related problem occurs number people within subgroup small data used make generalisations result disproportionately high error rates amongst minority groups tools present genuine opportunities local government however tools considered silver bullet funding challenges cases additional investment required realise potential moreover found data infrastructure data quality significant barriers developing deploying tools effectively responsibly investment area needed developing advanced systems recommendations regulators government recommendations report identified following recommendations specific individual sectors details given sector chapters recruitment recommendation equality human rights commission update guidance application equality act recruitment reflect issues associated use algorithms collaboration consumer industry bodies recommendation information commissioner office work industry understand current guidance consistently applied consider updates guidance employment practices code greater promotion existing guidance action appropriate policing recommendation home office define clear roles responsibilities national policing bodies regards data analytics ensure access appropriate expertise empowered set guidance standards first step home office ensure work underway national police chiefs council policing stakeholders develop guidance ensure ethical oversight data analytics tools appropriately supported local government recommendation government develop national guidance support local authorities legally ethically procure develop algorithmic tools areas significant decisions made individuals consider compliance guidance bias algorithmic executive summary centre data ethics challenges found underlying challenges across four sectors indeed sectors algorithmic happening part iii report focus understanding challenges ecosystem got addressing key next steps organisations regulators government main areas considered enablers needed organisations building deploying algorithmic tools help fair way see chapter regulatory levers formal informal needed incentivise organisations create level playing field ethical innovation see chapter public sector major developer user technology show leadership area transparency see chapter inherent links areas creating right incentives succeed right enablers place help organisations act fairly conversely little incentive organisations invest tools approaches fair insufficient clarity expected norms want system fair accountable one preserves protects improves fairness decisions made use algorithms want address obstacles organisations may face innovate ethically ensure increased levels accountability decisions society identify respond bias algorithmic decisionmaking processes considered existing landscape standards laws area whether sufficient increasingly society realise vision need clear mechanisms safe access data test bias organisations able make judgements based data bias skilled industry third parties provide support assurance regulators equipped oversee support sectors remits fair innovation found many organisations aware risks algorithmic bias unsure address bias practice universal formulation rule tell algorithm fair organisations need identify fairness objectives want achieve plan sector bodies regulators standards bodies government key role setting clear guidelines appropriate different contexts getting right essential avoiding bad practice giving clarity enables good innovation however organisations need clear accountability getting right whether algorithm structured human process used make decision change organisation accountability improving diversity across range roles involved development deployment algorithmic tools important part protecting bias government industry efforts improve must continue need show results data needed monitor outcomes identify bias data protected characteristics available often enough one reason incorrect belief data protection law prevents collection usage data indeed number lawful bases data protection legislation using protected special characteristic data monitoring addressing discrimination genuine challenges collecting data innovative thinking needed area example around potential trusted third party intermediaries machine learning community developed multiple techniques measure mitigate algorithmic bias organisations encouraged deploy methods address bias discrimination however little guidance choose right methods embed development operational processes bias mitigation treated purely technical issue requires careful consideration wider policy operational legal contexts insufficient legal clarity concerning novel techniques area many used legitimately care needed ensure application techniques cross unlawful positive bias algorithmic executive summary centre data ethics good anticipatory governance crucial many high profile cases algorithmic bias could anticipated careful evaluation mitigation potential risks organisations need make sure right capabilities structures place ensure happens algorithms introduced processes life well requires understanding empathy expectations affected decisions often achieved right engagement groups given complexity area expect see growing role expert professional services supporting organisations although ecosystem needs develop already plenty organisations get right data protection impact assessments equality impact assessments help structuring thinking documenting steps government recommendation government continue support invest programmes facilitate greater diversity within technology sector building current programmes developing new initiatives gaps recommendation government work relevant regulators provide clear guidance collection use protected characteristic data outcome monitoring processes encourage use guidance data address current historic bias key sectors recommendation government office national statistics ons open secure research service broadly wider variety organisations use evaluation bias inequality across greater range activities recommendation government support creation development public private partnerships especially focused identification reduction biases issues specific groups office national statistics ons government statistical service work partnerships regulators promote harmonised principles data collection use private sector via shared data standards development recommendations regulators recommendation sector regulators industry bodies help create oversight technical guidance responsible bias detection mitigation individual sectors adding detail existing guidance data protection new guidance equality act guidance organisation leaders boards responsible governance organisations deploying using algorithmic tools support significant decisions individuals ensure leaders place accountability understanding capabilities limits tools considering carefully whether individuals fairly treated process tool forms part making conscious decision appropriate levels human involvement process putting structures place gather data monitor outcomes fairness understanding legal obligations carried appropriate impact assessments especially applies public sector citizens often choice whether use service decisions made individuals often review bias algorithmic executive summary centre data ethics regulatory environment clear industry norms good proportionate regulation key addressing risks algorithmic bias promoting level playing field ethical innovation thrive increased use algorithmic presents genuinely new challenges regulation brings question whether existing legislation regulatory approaches address challenges sufficiently well currently limited case law statutory guidance directly addressing discrimination algorithmic ecosystems guidance support different maturity levels different sectors though limited amount case law recent judgement court appeal relation usage live facial recognition technology south wales police seems likely significant one grounds successful appeal south wales police failed adequately consider whether trial could discriminatory impact specifically take reasonable steps establish whether facial recognition software contained biases related race sex court found meet obligations public sector equality duty even though evidence specific algorithm biased suggests general duty public sector organisations take reasonable steps consider potential impact equality upfront detect algorithmic bias ongoing basis current regulatory landscape algorithmic decisionmaking consists equality human rights commission ehrc information commissioner office ico sector regulators stage believe need new specialised regulator primary legislation address algorithmic bias however algorithmic bias means overlap discrimination law data protection law sector regulations becoming increasingly important see overlap playing number contexts including discussions around use protected characteristics data measure mitigate algorithmic bias lawful use bias mitigation techniques identifying new forms bias beyond existing protected characteristics first step resolving challenges clarify interpretation law stands particularly equality act give certainty organisations deploying algorithms ensure existing individual rights eroded wider equality duties met however use algorithmic grows foresee future need look legislation kept consideration guidance developed case law evolves existing regulators need adapt enforcement algorithmic provide guidance regulated bodies maintain demonstrate compliance algorithmic age regulators require new capabilities enable respond effectively challenges algorithmic larger regulators greater digital remit may able grow capabilities others need external support many regulators working hard ico shown leadership area starting build skills base address new challenges convening regulators consider issues arising deeper collaboration across regulatory ecosystem likely needed future outside formal regulatory environment increasing awareness within private sector demand broader ecosystem industry standards professional services help organisations address algorithmic bias number reasons highly specialised skill organisations able support important consistency problem addressed regulatory standards sectors may require independent audit systems elements ecosystem might licenced auditors qualification standards individuals necessary skills audit bias likely form part broader approach audit might also cover issues robustness explainability government regulators industry bodies private industry play important roles growing ecosystem organisations better equipped make fair regulators need adapt enforcement algorithmic provide guidance regulated bodies maintain demonstrate compliance algorithmic age review bias algorithmic executive summary centre data ethics recommendations government recommendation government issue guidance clarifies equality act responsibilities organisations using algorithmic include guidance collection protected characteristics data measure bias lawfulness technical bias mitigation techniques recommendation development guidance implementation government assess whether provides sufficient clarity organisations meeting obligations leaves sufficient scope organisations take actions mitigate algorithmic bias government consider new regulations amendments equality act address recommendations regulators recommendation ehrc ensure capacity capability investigate algorithmic discrimination may include ehrc reprioritising resources area ehrc supporting regulators address algorithmic discrimination sector additional technical support ehrc recommendation regulators consider algorithmic discrimination supervision enforcement activities part responsibilities public sector equality duty recommendation regulators develop compliance enforcement tools address algorithmic bias impact assessments audit standards certification regulatory sandboxes recommendation regulators coordinate compliance enforcement efforts address algorithmic bias aligning standards tools possible could include jointly issued guidance collaboration regulatory sandboxes joint investigations public sector transparency making decisions individuals core responsibility many parts public sector increasing recognition opportunities offered use data algorithms use technology never reduce real perceived accountability public institutions citizens fact offers opportunities improve accountability transparency especially algorithms significant effects significant decisions individuals range transparency measures already exist around current public sector processes proactive sharing information decisions made reactive rights citizens request information decisions made government shown leadership setting guidance usage public sector including focus techniques explainability transparency however needed make transparency public sector use algorithmic norm window opportunity ensure get right adoption starts increase sometimes hard individual government departments public sector organisations first transparent strong central drive needed development delivery algorithmic decisionmaking tool often include one suppliers whether acting technology suppliers business process outsourcing providers ultimate accountability fair always sits public body limited maturity consistency contractual mechanisms place responsibilities right place supply chain procurement processes updated line wider transparency commitments ensure standards lost along supply bias algorithmic executive summary centre data ethics recommendations government recommendation government place mandatory transparency obligation public sector organisations using algorithms significant influence significant decisions affecting individuals government conduct project scope obligation precisely pilot approach implement require proactive publication information decision use algorithm made type algorithm used overall process steps taken ensure fair treatment individuals recommendation cabinet office crown commercial service update model contracts framework agreements public sector procurement incorporate set minimum standards around ethical use particular focus expected levels transparency explainability ongoing testing steps future challenges review considered complex rapidly evolving field plenty across industry regulators government manage risks maximise benefits algorithmic next steps fall within cdei remit happy support industry regulators government taking forward practical delivery work address issues identified future challenges may arise outside specific activities noting complexity range work needed across multiple sectors see key need national leadership coordination ensure continued focus pace addressing challenges across sectors rapidly moving area level coordination monitoring needed assess organisations building using algorithmic tools responding challenges highlighted report proposed new guidance regulators government government clear wants coordination sit number possible locations example central government directly specific regulator cdei review concluded significant scope address risks posed bias algorithmic within law stands succeed clear possibility future legislation may required encourage organisations respond challenge innovate responsibly think implications individuals society large bias algorithmic executive summary centre data ethics introduction background scope cdei review focus approach issue introduction use algorithms bias discrimination equality fairness applying ethical principles background scopechapter centre data ethics adoption technology affects every aspect society use creating opportunities well new ethical challenges centre data ethics innovation cdei independent expert committee led board specialists set tasked government investigate advise maximise benefits technologies goal create conditions ethical innovation thrive environment public confident values reflected way datadriven technology developed deployed trust decisions informed algorithms fair risks posed innovation identified addressed information cdei found october budget chancellor announced would investigate potential bias decisions made algorithms review formed key part work programme though completion delayed onset final report cdei review includes set formal recommendations government government tasked draw expertise perspectives stakeholders across society provide recommendations address issue also provide advice regulators industry aiming support responsible innovation help build strong trustworthy system governance government committed consider respond publicly goal create conditions ethical innovation thrive environment public confident values reflected way technology developed cdei review review bias algorithmic background scope led focus reas algorithms potential make inform decision directly affects individual human opposed entities companies centre data ethics innovation use algorithms increasing across multiple sectors society bias algorithmic decisionmaking broad topic review prioritised types decisions potential bias seems represent significant imminent ethical risk led focus areas algorithms potential make inform decision directly affects individual human opposed entities companies significance decisions course varies typically focused areas individual decisions could considerable impact person life decisions significant sense data protection act extent algorithmic used likely soon different sectors decisions made supported algorithms wider ethical issues use artificial intelligence changes ethical risk algorithmic world compared analogue circumstances decisions biased see chapter discussion means rather forms unfairness arbitrariness unreasonableness scope broad cover possible areas algorithmic bias issue example cdei review online targeting published earlier year highlighted risk harm bias targeting within online platforms decisions individually small example targeting advert recommending content user overall impact bias across many small decisions still problematic review touch issues fell outside core focus significant decisions worth highlighting main work review carried number highly relevant events mid pandemic black lives matter awarding exam results without exams less widespread attention specific relevance judgement court appeal bridges south wales police considered links issues review able treat full focus cdei review online targeting final report recommendations note roger taylor chair cdei board also chair ofqual english exams regulator following controversy around august exam results roger stepped away involvement changes made final version review cdei direct role assessing ofqual approach time writing understand number regulators looking issues raised bias algorithmic background scope centre data ethics sector approach ethical questions relation bias algorithmic vary depending context sector chose four initial areas focus illustrate range issues recruitment financial services policing local government rationale choosing sectors set introduction part themes work carried four sectors well engagement across government civil society academia interested parties sectors able identify themes issues opportunities went beyond individual sectors set three key questions interim sought address basis data organisations regulators access data require adequately identify mitigate bias tools techniques statistical technical solutions available required future identify mitigate bias represent best practice governance responsible governing auditing assuring algorithmic systems questions guided review made recommendations appropriate recommendations focus heavily opportunities address questions others across multiple sectors ethical questions relation bias algorithmic vary depending context sector therefore chose four initial areas focus illustrate range issues recruitment financial services policing local approach bias algorithmic background scope centre data ethics innovation evidence base final report informed variety work including landscape summary led professor michael rovatsos university edinburgh assessed current academic policy open call evidence received responses wide cross section academic institutions individuals civil society industry public series interviews companies financial services recruitment sectors developing using algorithmic tools work behavioural insights team attitudes use algorithms personal commissioned research royal united services institute rusi data analytics policing england contracted work faculty technical bias mitigation representative polling public attitudes number issues raised report conducted deltapoll part cdei ongoing public engagement work meetings variety stakeholders including regulators industry groups civil society organisations academics government departments well research understand existing technical policy landscape evidence base final report informed variety work including open call received responses wide cross section academic institutions individuals civil society industry public sector cdei landscape summary bias algorithmic cdei call evidence summary responses review bias algorithmic behavioural insights team perceptions fairness algorithms proxy information financial services royal united services institute briefing paper data analytics algorithmic bias policing royal united services institute occasional paper data analytics algorithms policing england wales main report produced contract bias algorithmic background scope issue chapter centre data ethics innovation algorithms structured processes long used aid human recent developments machine learning techniques exponential growth data allowed sophisticated complex algorithmic decisions corresponding growth usage algorithm supported across many areas society growth accompanied significant concerns bias use algorithms cause systematic skew results unfair outcomes clear evidence algorithmic bias occur whether entrenching previous human biases introducing new ones forms bias constitute discrimination equality act namely bias leads unfair treatment based certain protected characteristics also kinds algorithmic bias still lead unfair outcomes multiple concepts fairness incompatible many ambiguous human decisions often accept ambiguity allow human judgement consider complex reasons decision contrast algorithms unambiguous fairness much absence bias fair decisions need also reasonable consider equality implications respect circumstances personal agency individuals despite concerns black box algorithms ways algorithms transparent human decisions unlike human possible reliably test algorithm responds changes parts input opportunities deploy algorithmic transparently enable identification mitigation systematic bias ways challenging humans human developers users algorithms must decide concepts fairness apply context ensure algorithms deliver fair outcomes fairness unawareness often enough prevent bias ignoring protected characteristics insufficient prevent algorithmic bias prevent organisations identifying addressing bias need address algorithmic bias goes beyond regulatory requirements equality data protection law also critical innovation algorithms used way fair seen public issue summary fairness unawareness often enough prevent bias review bias algorithmic issue centre data ethics always flawed shaped individual societal biases often unconscious years society identified ways improving often building processes structures encourage make decisions fairer objective way agreed social norms equality legislation however new technology introducing new complexities growing use algorithms raised concerns around bias fairness even context challenges new commission racial equality found british medical school guilty algorithmic discrimination inviting applicants computer program used determined biased women applicants growth area driven availability volume often personal data used train machine learning models inputs decisions well cheaper easier availability computing power innovations tools techniques usage algorithmic tools grows complexity understanding risks therefore crucial ensure tools positive impact improve decisionmaking algorithms different related vulnerabilities human processes able explain statistically less able explain human terms consistent humans less able take nuanced contextual factors account highly scalable efficient consequently capable consistently applying errors large populations also act obscure accountabilities liabilities individual people organisations making fair introduction lowry stella macpherson gordon blot profession british medical journal different related vulnerabilities human processes able explain statistically less able explain human terms review bias algorithmic issue centre data ethics innovation simple terms algorithm structured process using structured processes aid human much older computation time tools approaches available deploy become sophisticated many organisations responsible making large numbers structured decisions example whether individual qualifies welfare benefits payment whether bank offer customer loan make processes scalable consistent giving staff processes rules follow initial computerisation decisions took similar path humans designing structured processes algorithms followed computer handling technology reached point specifics processes always explicitly manually designed machine learning tools often seek find patterns data without requiring developer specify factors use exactly link formalising relationships extracting information could useful make decisions results tools simple intuitive humans understand interpret also highly complex sectors credit scoring insurance long history using statistical techniques inform design automated processes based historical data ecosystem evolved helps manage potential risks example credit reference agencies offer customers ability see credit history offer guidance factors affect credit scoring cases range regulations govern factors use algorithms sectors credit scoring insurance long history using statistical techniques inform design automated processes based historical bias algorithmic issue centre data ethics seeing application much wider range scenarios number drivers increase including exponential growth amount data held organisations makes processes amenable approaches improvements availability cost computing power skills increased focus cost saving driven fiscal constraints public sector competition disruptive new entrants many private sector markets advances machine learning techniques especially deep neural networks rapidly brought many problems previously inaccessible computers routine everyday use image speech recognition simple terms algorithm set instructions designed perform specific task algorithmic word applied two different contexts machine learning algorithm takes data input create model process something happens continually new data gathered algorithm also used describe structured process making decision whether followed human computer possibly incorporating machine learning model usage usually clear context review focused mainly processes involving machine learning algorithms although content also relevant structured decisionmaking processes note hard definition exactly statistical techniques algorithms constitute novel machine learning observed many recent developments associated applying existing statistical techniques widely new sectors novel techniques interpret algorithmic include process algorithm makes meaningfully assists decision includes sometimes referred algorithmicallyassisted review focused mainly decisions individual people figure following page shows example machine learning algorithm used within decisionmaking process bank making decision whether offer loan individual seen advances machine learning techniques especially deep neural networks rapidly brought many problems previously inaccessible computers routine everyday use example image speech recognition review bias algorithmic issue centre data ethics innovation important emphasise algorithms often represent complete process may elements human judgement exceptions treated outside usual process opportunities appeal reconsideration fact significant decisions appropriate provision human review usually required comply data protection law even algorithm deployed process humans decide objectives trying meet data available output therefore critical consider algorithmic aspect whole process sits around human intervention processes vary cases may absent entirely fully automated systems ultimately aim avoid bias algorithmic aspects process process whole achieves fair decisionmaking human human figure data algorithms come together support bias algorithmic issue centre data ethics algorithmic grows scale increasing concerns raised around risks bias bias precise meaning statistics referring systematic skew results output correct average respect overall population sampled however general usage review bias used refer output skewed skewed way unfair see discussion unfair might mean context bias enter algorithmic systems number ways including historical bias data model built tested operated could introduce bias may previously biased human decisionmaking due societal historical inequalities example company current workforce predominantly male algorithm may reinforce whether imbalance originally caused biased recruitment processes historical factors criminal record part result likely arrested compared someone else history behaviour arrests algorithm constructed assess risk reoffending risk reflecting true likelihood reoffending instead reflects biased likelihood caught reoffending data selection bias data collected selected could mean representative example recording particular groups could mean algorithm less accurate people gave skewed picture particular groups main cause widely reported problems accuracy facial recognition algorithms across different ethnic groups attempts address focusing ensuring better balance training algorithmic design bias may also design algorithm leads introduction bias example cdei review online noted examples algorithms placing job advertisements online designed optimise engagement given cost leading adverts frequently targeted men women costly advertise human oversight widely considered good thing algorithms making decisions mitigates risk purely algorithmic processes apply human judgement deal unfamiliar situations however depending humans interpret use outputs algorithm also risk bias process human applies conscious unconscious biases final decision also risk bias amplified time feedback loops models incrementally retrained new data generated either fully partly via use earlier versions model decisionmaking example model predicting crime rates based historical arrest data used prioritise police resources arrests high risk areas could increase reinforcing imbalance cdei landscape discusses issue detail bias see example ibm initiative around cdei review online targeting final report recommendations cdei landscape summaries commissioned centre data ethics innovation review online noted examples algorithms placing job advertisements online designed optimise engagement given cost leading adverts frequently targeted men women costly advertise review bias algorithmic issue centre data ethics report use word discrimination sense defined equality act meaning unfavourable treatment basis protected equality act makes unlawful discriminate someone basis certain protected characteristics example age race sex disability public functions employment provision goods services choice characteristics recognition used treat people unfairly past society deemed unfairness unacceptable many albeit concerns algorithmic bias relate situations bias may lead discrimination sense set equality act equality act defines two main categories direct discrimination person treated less favourably another protected characteristic indirect discrimination wider policy practice even applies everyone disadvantages group people share protected characteristic legitimate reason discrimination direct interpretation law algorithmic process seems relatively clear algorithmic model explicitly leads someone treated less favourably basis protected characteristic would unlawful specific exceptions case direct discrimination basis age discrimination could lawful proportionate means proportionate aim services targeted particular age range limited positive actions favour increased use technology created new possibilities indirect discrimination example model might consider individual postcode protected characteristic correlation postcode race model used process perhaps financial services policing could principle cause indirect racial discrimination whether case depends judgement extent selection methods proportionate means achieving legitimate example insurer might able provide good reasons postcode relevant risk factor type insurance level clarity acceptable practice varies sector reflecting part maturity using data complex ways algorithmic spreads use cases sectors clear norms need established indeed ability algorithms deduce protected characteristics certainty proxies continues improve could even argued examples could potentially cross direct discrimination equality avoidance confusion place neutral meaning often used machine learning scientific literature discriminate use distinguish equality act equality act note addition discrimination equality act also forbids victimisation harassment places requirement organisations make reasonable adjustments people disabilities see section details equality human rights commission words terms used equality act increased use technology created new possibilities indirect discrimination review bias algorithmic issue centre data ethics bias beyond discrimination discrimination narrower concept bias protected characteristics included law due historical evidence systematic unfair treatment individuals also experience unfair treatment basis characteristics protected always grey areas individuals experience systematic unfair bias basis characteristics protected example accent hairstyle education cases may considered indirect discrimination connected protected characteristics cases may reflect unfair biases protected discrimination increased use algorithms may exacerbate difficulty introduction algorithms encode existing biases algorithms trained existing decisions reinforce amplify existing unfair bias whether basis protected characteristics algorithmic also beyond amplifying existing biases creating new biases may unfair though difficult address discrimination law machine learning algorithms find new statistical relationships without necessarily considering whether basis relationships fair apply systematically large numbers individual decisions note public sector bodies scotland must address inequalities fairer scotland always grey areas individuals experience systematic unfair bias basis characteristics protected example accent hairstyle education review bias algorithmic issue centre data ethics defined bias including element unfairness highlights challenges defining mean fairness complex long debated topic notions fairness neither universal unambiguous often inconsistent one another human systems possible leave degree ambiguity fairness defined humans may make decisions complex reasons always able articulate full reasoning making decision even pros cons allows good consider specific individual circumstances human understanding reasons circumstances might conform typical patterns especially important critical decisions policing social services decisions often need made basis limited uncertain information wider circumstances beyond scope specific decision need taken account hard imagine automated decisions could ever fully replace human judgement cases human decisions also open conscious unconscious biases well variations competence concentration levels mood specific decisions made algorithms contrast unambiguous want model comply definition fairness must tell explicitly definition significant challenge depends context sometimes meaning fairness clearly defined take extreme example chess playing achieves fairness following rules game often though existing rules processes require human exercise discretion judgement account data difficult include model context around decision readily quantified existing processes must fully understood context order decide whether algorithmic likely appropriate example police officers charged enforcing criminal law often necessary officers apply discretion whether breach letter law warrants action broadly good thing discretion also allows individual personal biases whether conscious unconscious affect decisions even cases fairness precisely defined still challenging capture relevant aspects fairness mathematical definition fact mathematical definitions demonstrate model conform possible fairness definitions time humans must choose notions fairness appropriate particular algorithm need willing upfront model built process designed general data protection regulation gdpr data protection act contain requirement organisations use personal data way fair legislation elaborate meaning fairness ico guides organisations general fairness means handle personal data ways people would reasonably expect use ways unjustified adverse effects note discussion section wider notion gdpr attempt define word fair interpreted fairness even cases fairness precisely defined still challenging capture relevant aspects fairness mathematical definition ico guide general data protection regulation gdpr principles review bias algorithmic issue centre data ethics innovation fairness notions fair whether human algorithmic typically gathered two broad categories procedural fairness concerned fair treatment people equal treatment within process decision made might include example defining objective set criteria decisions enabling individuals understand challenge decisions outcome fairness concerned decisions made measuring average outcomes process assessing compare expected baseline concept fair outcome means course highly subjective multiple different definitions outcome fairness definitions complementary none alone capture notions fairness fair process may still produce unfair results vice versa depending perspective even within outcome fairness many mutually incompatible definitions fair outcome consider example bank making decision whether applicant eligible given loan role applicant sex decision two possible definitions outcome fairness example probability getting loan men women probability getting loan men women earn income taken individually either might seem like acceptable definition fair incompatible real world sex income independent gender pay gap meaning average men earn given gap mathematically impossible achieve example means exhaustive highlighting possible conflicting definitions made large collection possible definitions identified machine learning human often accept ambiguity around type issue determining algorithmic process fair able explicitly determine notion fairness trying optimise human judgement call whether variable case salary acting proxy protected characteristic case sex seen reasonable proportionate context investigated public reactions similar example work behavioural insights team see detail chapter determining algorithmic decisionmaking process fair able explicitly determine notion fairness trying optimise ons gender pay gap comprehensive review different possibilities given example mehrabi ninareh morstatter fred saxena nripsuta lerman kristina galstyan aram survey bias fairness machine learning chouldechova alexandra roth aaron frontiers fairness machine learning review bias algorithmic issue centre data ethics fairness even agree constitutes fairness always clear respond conflicting views value fairness definitions arise application process intended fair produces outcomes regarded unfair explained several ways example differences outcomes evidence process fair principle good reason differences average ability men women particular job differences outcomes male female applicants may evidence process biased failing accurately identify able correcting process fairer efficient differences outcomes consequence past injustices example particular set previous experience might regarded necessary requirement role might common among certain backgrounds due past differences access employment educational opportunities sometimes might appropriate employer flexible requirements enable get benefits diverse workforce perhaps bearing cost additional training sometimes may possible individual employer resolve recruitment especially highly specialist roles first argument implies greater outcome fairness consistent accurate fair second argues different groups ought treated differently correct historical wrongs argument associated quota regimes possible reach general opinion argument correct highly dependent context also possible explanations processes based human judgement rarely possible fully separate causes differences outcomes human recruiters may believe accurately assessing capabilities outcomes seem skewed always possible determine extent fact reflects bias methods assessing handle human world variety techniques example steps ensure fairness recruitment process might include training interviewers recognise challenge individual unconscious biases policies composition interview panels designing assessment processes score candidates objective criteria applying formal informal quotas though quota based protected characteristics would usually unlawful training interviewers recognise challenge individual unconscious biases one technique used ensure fairness recruitment process review bias algorithmic issue algorithms different increased use complex algorithmic approaches introduces number new challenges opportunities need conscious decisions fairness systems organisations need address issues point model built rather relying human interpret guidance appropriately algorithm apply common sense basis humans able balance things implicitly machines optimise without balance asked explainability systems allow degree explainability factors causing variation outcomes systems different groups assess whether regarded fair example possible examine directly degree relevant characteristics acting proxy characteristics causing differences outcomes different groups recruitment process included requirements length service qualification would possible see whether example length service generally lower women due career breaks causing imbalance extent possible depends complexity algorithm used dynamic algorithms drawing large datasets may allow precise attribution extent outcome process individual woman attributable particular characteristic association gender however possible assess degree time period different characteristics influencing recruitment decisions correlate characteristics time term black box often used describe situations variety different reasons explanation decision unobtainable include commercial issues organisation understand details algorithm supplier considers intellectual property technical reasons machine learning techniques less accessible easy human explanation individual decisions information commissioner office alan turing institute recently published detailed joint advice organisations overcome challenges provide level explanation scale impact potential breadth impact algorithm links market dynamics many algorithmic software tools developed platforms sold across many companies therefore possible example individuals applying multiple jobs could rejected sift algorithm perhaps sold large number companies recruiting skill sets industry algorithm reasons irrelevant actual performance basis set characteristics protected feels much like systematic discrimination group individuals equality act provides obvious protection algorithmic inevitably increase time aim ensure happens way acts challenge bias increase fairness promote equality rather entrenching existing problems recommendations review targeted making happen systems allow degree explainability recruitment process included requirements length service qualification would possible see whether example length service generally lower women due career breaks causing imbalance ico explaining decisions made centre data ethics innovation bias algorithmic issue centre data ethics innovation study exam results august due governments across decided cancel school examinations summer find alternative approach awarding grades four nations attempted implement similar processes deliver combining teacher assessments statistical moderation process attempted achieve similar distribution grades previous years approaches changed response public concerns significant criticism individual fairness concerns grades biased fairness interpreted case number different notions fairness consider fairness year groups achieve similar distribution grades previous future year groups group fairness different schools attempt standardise teacher assessed grades given different levels grading different schools fair individual students different schools group fairness discrimination avoid exacerbating differences outcomes correlated protected characteristics particularly sex race include addressing systematic bias results based inequality opportunity seen outside mandate exam body avoid bias based status fair process allocating grades individual students allocating grade seen fair representation individual capabilities efforts main work review complete prior release summer exam results clear links issues raised contents review including issues public trust transparency governance fairness interpreted case detailed example ofqual exam regulator england set details approach also provided statement parliament reflections process bias algorithmic issue centre data ethics way decisions made potential biases subject impact decisions individuals highly context dependent unlikely forms bias entirely eliminated also true human important understand status quo prior introduction technology given scenario decisions may need made kinds degrees bias tolerable certain contexts ethical questions vary depending sector want help create conditions ethical innovation using technology thrive therefore essential ensure approach grounded robust ethical principles government along countries signed oecd principles artificial provide good starting point considering approach dealing bias benefit people planet driving inclusive growth sustainable development many potential advantages algorithmic tools used appropriately potential efficiency accuracy predictions also opportunity tools support good reducing human error combating existing bias designed correctly offer objective alternative supplement human subjective interpretation core review wider purpose cdei identify collectively ensure opportunities outweigh risks systems designed way respects rule law human rights democratic values diversity include appropriate safeguards example enabling human intervention necessary ensure fair society principle sets core terms mean fairness algorithmic decisionmaking process cover number aspects throughout review focus review significant decisions means largely considering decisions algorithm forms part overall process hence level direct human oversight individual decisions however consideration always needed whether role human remains meaningful human understand algorithm limitations sufficiently well exercise oversight effectively organisational environment working within empower risk human biases could reintroduced oversight chapter consider ability existing legal regulatory structures ensure fairness area especially data protection equality legislation need evolve adapt algorithmic world applying ethical principles oecd principles artificial intelligence course many sets ethical principles frameworks variety organisations including various organisations consultancies council europe government along countries signed oecd principles artificial review bias algorithmic issue centre data ethics transparency responsible disclosure around systems ensure people understand outcomes challenge work identified variable levels transparency usage algorithms variety recent reviews called increased levels transparency across public sector clear work needed achieve level transparency consistent way across economy especially public sector many highest stakes decisions made discuss achieved chapter systems must function robust secure safe way throughout life cycles potential risks continually assessed managed chapter identify approaches taken mitigate risk bias development lifecycle algorithmic system suggest action government take support development teams taking fair approach organisations individuals developing deploying operating systems held accountable proper functioning line principles use algorithmic tools within decisions significant impact individuals society raising requirement clear lines accountability use impact decisions made humans large organisations generally consider possible get right every time instead expect organisations appropriate structures policies procedures anticipate address potential bias offer redress occurs set clear governance processes lines accountability decisions organisations introducing algorithms decisions previously purely made humans looking achieve least equivalent standards fairness accountability transparency many cases look better defining equivalence always easy course may occasions standards achieved different way algorithmic world discuss issue detail part iii report issues important remember interested output algorithm overall process sits around organisations existing accountability processes standards use algorithms needs sit within existing accountability processes ensure used intentionally effectively therefore organisation accountable outcome traditional human decisionmaking must decide far mitigate bias govern approach decisions require value judgements competing values humans often trusted make without explicitly state much weight put different considerations algorithms different programmed make according rules decisions interrogated made explicit issues important remember interested output algorithm overall process sits around review bias algorithmic issue centre data ethics oecd principles clearly high level take far making difficult ethical balances individual decisionmaking systems work review suggests algorithmic continues grow scale ambitious aiming avoid new bias use opportunity address historical unfairness organisations responsible using algorithms require specific guidance principles apply circumstances principles often discussed detail sector sections however start outline rules thumb guide organisations using algorithms support significant processes history shows processes biased often unintentionally want make fairer decisions using data measure best approach certainly assuming bias process highly unreliable approach data shows historical patterns bias mean algorithms considered bias addressed evidence data help inform approach algorithms designed mitigate bias may part solution algorithm introduced replace human decision system bias mitigation strategy designed result fairer outcomes reduction unwarranted differences groups important test outputs algorithms assess fairness key measure fairness algorithm impact whole decision process cases resolving fairness issues may possible outside actual process addressing wider systemic issues society putting human loop way addressing concern unforgiving nature algorithms bring perspectives contextual information available algorithm also introduce human bias system humans loop monitoring fairness whole decision process also needed responsibility whole process humans loop need understand machine learning model works limitations making great enough extent make informed judgements whether performing effectively opportunity putting human loop way addressing concern unforgiving nature algorithms also introduce human bias system review bias algorithmic issue centre data ethics innovation sector reviews recruitment background findings financial services background findings policing background findings local government background findings centre data ethics innovation sector reviews ethical questions relation bias algorithmic vary depending context sector therefore chose four initial areas focus illustrate range issues recruitment financial services policing local government sectors following common involve making decisions scale individuals involve significant potential impacts individuals lives growing interest use algorithmic tools sectors including involving machine learning particular evidence historic bias within sectors leading risks perpetuated introduction algorithms course sectors could considered chosen representative sample across public private sector judged risk bias acute specific cases part review focus issues reach number recommendations specific individual sectors sector studies inform findings recommendations part bias algorithmic sector reviews recruitment chapter centre data ethics centre data ethics innovation summary overview findings use algorithms recruitment increased recent years stages recruitment process trends suggest tools become widespread meaning clear guidance robust regulatory framework essential developed responsibly tools potential improve recruitment standardising processes removing discretion human biases creep however using historical data human biases highly likely replicated rigorous testing new technologies necessary ensure platforms unintentionally discriminate groups people way collect demographic data applicants use data monitor model performs currently little standardised guidance testing meaning companies largely algorithmic recruitment currently governed primarily equality act data protection act however found cases confusion regarding organisations enact legislative responsibilities recommendations regulators recommendation equality human rights commission ehrc update guidance application equality act recruitment reflect issues associated use algorithms collaboration consumer industry bodies recommendation information commissioner office work industry understand current guidance consistently applied consider updates guidance employment practices code greater promotion existing guidance action industry organisations carry equality impact assessments understand models perform candidates different protected characteristics including intersectional analysis multiple protected characteristics future cdei work cdei consider work relevant organisations assist developing guidance applying equality act algorithms bias algorithmic recruitment centre data ethics shortlist interview employ significant effects lives individuals society certain groups disadvantaged either directly indirectly recruitment process social inequalities broadened embedded existence human bias traditional recruitment famous study found orchestral players kept behind screen audition significant increase number women research found candidates ethnic minority backgrounds send many applications white candidates receive positive even concerning fact little historical improvement figures last recruitment also considered barrier employment people range factors affinity biases recruiters tend prefer people similar informal processes recruit candidates already known organisation amplify biases people believe technology could play role helping standardise processes make internet also meant candidates able apply much larger number jobs thus creating new problem organisations needing review hundreds sometimes thousands applications factors led increase new tools promising greater efficiency standardisation objectivity consistent upwards trend adoption around functions international companies using however important distinguish new technologies algorithmic whilst new technology increasingly applied across board recruitment research focused tools utilise algorithmic systems training data predict candidate future concerns potential negative impacts algorithmic also concerns effectiveness technologies able predict good job performance given relative inflexibility systems challenge conducting thorough assessment using automated processes scale purpose report focus bias rather effectiveness approached work work recruitment sector began call evidence landscape summary evidence gathering provided broad overview challenges opportunities presented using algorithmic tools hiring addition research conducted series interviews broad range software providers recruiters conversations focused providers currently test mitigate bias tools also spoke range relevant organisations individuals including think tanks academics government departments regulators civil society range factors form affinity biases including recruiters tend prefer people similar background correll shelley stephen benard gender racial bias hiring memorandum report university pennsylvania gender action portal orchestrating impartiality impact blind auditions female musicians nuffield college oxford centre social investigation new csi research reveals high levels job discrimination faced ethnic minorities britain applied pandemic racism failure data implicit bias systemic discrimiation trades union congress disability employment pay gaps chartered institute personnel development head hiring behavioural science recruitment personnel today recruitment algorithms infected biases expert warns mit technology review emotion researchers say overblown claims give work bad name bias algorithmic recruitment centre data ethics tools created used every stage recruitment process many stages recruitment process algorithms increasingly used starting sourcing applicants via targeting online screening interview selection phases datadriven tools sold efficient accurate objective way assisting recruiting decisions organisations may use different providers stages recruitment process increasing options integrate different types tools figure examples algorithmic tools used sourcing screening interview selection stages recruitment process comprehensive analysis different tools associated risks see bogen miranda aaron rieke help wanted examination hiring algorithms equity upturn cdei recent review online targeting covers detail findings tools sold efficient accurate objective way assisting recruiting bias algorithmic recruitment centre data ethics trained historic data carry significant risks bias many ways bias introduced recruiting process using datadriven technology decisions data collected variables collect variables weighted data algorithm trained impact vary depending context however one theme arises consistently risk training algorithms biased historical data high profile cases biased recruiting algorithms include trained using historical data current past employees within organisation used try predict performance future similar systems used video interviewing software existing employees prospective applicants undertake assessment assessed correlated line performance model trained data understand traits people considered high performers without rigorous testing kinds predictive systems pull characteristics relevance job performance rather descriptive features correlate current employees example one company developed predictive model trained company data found name jared key indicator successful example machine learning process picked explicit bias others often subtle still damaging high profile case amazon application system trained existing employees never made past development phase testing showed women cvs consistently rated pattern detection type likely identify various factors correspond protected characteristics development goes unchecked essential organisations interrogate models identify proxies risk indirectly discriminating protected groups another way bias arise dataset limited respect candidates certain characteristics example training set company never hired woman algorithm would far less accurate respect female candidates type bias arises imbalance easily replicated across demographic groups industry therefore careful datasets used develop systems respect biases arising historical prejudice also unbalanced data whilst companies spoke evaluated models check patterns detected correlate protected characteristics little guidance standards companies meet difficult evaluate robustness detail found section challenges limitations bias mitigation approaches bbc news amazon scrapped sexist tool hirevue train validate build hirevue assessments models quartz companies hook hiring algorithms biased bbc news amazon scrapped sexist tool detailed report challenges gaps related auditing recruitment see report institute future work artificial intelligence hiring assessing impacts equality one company developed predictive model trained company data found name jared key indicator successful applicant example process picked explicit bias review bias algorithmic recruitment centre data ethics recruiting tool providers largely tend follow international standards currently guidance discrimination within recruitment sits equality human rights commission oversee compliance equality act employment statutory code setting fair recruitment looks like equality act also provide detailed guidance employers interpret apply equality however currently guidance equality act extends algorithmic recruitment means providers recruiting tools largely often base systems equality law jurisdictions especially high profile legal cases area research found companies test tools internally independently validate results led researchers civil society groups calling greater transparency around bias testing recruiting algorithms way assuring public appropriate steps taken minimise risk seeing companies publish information tools validated tested however researchers civil society groups believe gone far enough calling recruiting algorithms independently discussion regulatory landscape auditing found chapter regulators recommendation equality human rights commission update guidance application equality act recruitment reflect issues associated use algorithms collaboration relevant industry consumer bodies cdei happy support work would helpful see equality human rights commission equality law means employer recruit someone work related guidance documents javier dencik lina edwards lilian mean solve problem discrimination hiring javier dencik lina edwards lilian mean solve problem discrimination hiring hirevue train validate build hirevue assessments models sciencedaily hiring algorithms fair opaque tell study finds javier dencik lina edwards lilian mean solve problem discrimination hiring providers recruiting tools largely often base systems equality law jurisdictions especially bias algorithmic recruitment collecting demographic data monitoring purposes increasingly widespread helps test models biases proxies way sure model directly indirectly discriminating protected group check requires necessary data practice collecting data protected characteristics becoming increasingly common recruitment part wider drive monitor improve recruiting groups allows vendors recruiting organisations test models proxies monitor rate groups across recruitment process compared sectors studied recruitment advanced practice collecting equality data monitoring purposes found interviews standard practice collect data provide applicants disclaimers highlighting data used part process one challenge raised interviews applicants may want provide data part job application within rights withhold consider issue detail section conclude clearer national guidance needed support organisations organisations also encouraged monitor overlap people multiple protected characteristics may picked monitoring reviews data lens form intersectional analysis essential ensuring people missed result multiple protected advice employers industry organisations carry equality impact assessments understand models perform candidates different protected characteristics including intersectional analysis multiple protected characteristics specific guidance setting minimum level allowed applicants protected groups recruitment process could considered discriminatory known rule introduced mechanism adjudicate whether recruitment process considered disparate impact certain groups found research many third party software providers use standards tools offer feature part platforms assess proportion applicants moving process however rule part law meaningful test whether system might lead discrimination law therefore important ehrc provide guidance equality act applies see chapters discussion area challenge raised interviews applicants may want provide data part job application within rights withhold crenshaw kimberlé demarginalizing intersection race sex black feminist critique antidiscrimination doctrine feminist theory antiracist university chicago legal forum volume issue equal employment opportunity commission questions answers eeoc final rule disparate impact reasonable factors age age discrimination employment act bias algorithmic recruitment centre data ethics many tools developed used fairness mind although frequently cited reason adopting technologies efficiency found genuine desire use tools make processes fairer historically decisions hire made referrals unconscious biases recruiters also often relevant demographic data applicants know whether fair criteria applying looking candidates many companies developing tools want provide less biased assessments candidates standardising processes using accurate assessment potential candidates succeed job example one provider offers machine learning software redacts parts cvs associated protected characteristics assessing application make fairer judgment others try equalise playing field developing games assess core skills rather relying cvs place weight markers like educational institutions innovation space real potential making recruitment less biased developed deployed however risks wrong significant tools incorporating replicating biases larger scale given potential risks need scrutiny tools work used impact different groups needs done ensure tools support reasonable adjustments need alternative routes available one area particular concern certain tools may work disabilities often identifies patterns related defined norm however disabilities often require bespoke arrangements requirements likely differ majority may lead indirect example someone speech impediment may disadvantage assessed video interview someone particular cognitive disability may perform well gamified recruitment exercise way reasonable adjustments made interviews companies consider algorithmic recruitment process takes factors account meeting obligations equality act organisations start building inclusive design processes include explicit steps considering certain tools may impact disabilities may include increasing number people disabilities hired development design teams offering candidates disabilities option humanassessed alternative route appropriate worth noting reports found recruitment could improve experience disabled applicants reducing biases however likely vary depending tools wide spectrum barriers progression faced candidates disabilities one size fits approach unlikely successful due focus review bias less concerned research accuracy tools involved clearly important question tools ineffective also arguably unethical however sits outside scope report financial times design eliminates disability bias wired stop hiring bias could bad news disabled people start building inclusive design processes include explicit steps considering certain tools may impact disabilities review bias algorithmic recruitment centre data ethics innovation automated rejections governed data protection legislation compliance guidance seems mixed algorithmic tools recruitment designed assist people decisionmaking however fully automate elements process appears particularly common around automated rejections candidates application stage meet certain requirements fully automated regulated article general data protection regulation gdpr overseen information commissioner office ico ico set requirement operationalised automated guidance states organisations giving individuals information processing introducing simple ways request human intervention challenge decision carrying regular checks make sure systems working intended clear organisations screening many thousands candidates make provisions second suggestions indeed often common practice large scale sifts carried either algorithmic methods research suggested guidance rarely applied way outlined particularly introducing ways request human intervention review therefore think would value ico working employers understand guidance detailed guidance set employment practices interpreted applied consider ensure greater consistency application law individuals sufficiently able exercise rights data protection recommendations regulators recommendation information commissioner office work industry understand current guidance consistently applied consider updates guidance employment practices code greater promotion existing guidance action appropriate clearly would helpful ehrc ico coordinate work ensure organisations clarity data protection equality law requirements interact may even wish consider joint guidance addressing recommendations topics may relevant overlaps include levels transparency auditing recording recommended improve standards practice ensure legal compliance cdei happy support collaboration ico guide general data protection regulation gdpr rights related automated decision making including profiling ico employment practices code automated regulated article general data protection regulation gdpr overseen information commissioner office ico review bias algorithmic recruitment financial services chapter centre data ethics innovation services summary overview findings financial services organisations long used data support range highly innovative risk averse use new algorithmic approaches example comes credit scoring decisions banks using logistic regression models rather advanced machine learning algorithms mixed views approaches amongst financial organisations collection use protected characteristics data affects ability organisations check bias explainability models used financial services particular decisions key organisations regulators identify mitigate discriminatory outcomes fostering customer trust use algorithms regulatory picture clearer financial services sectors looked financial conduct authority fca lead regulator conducting work understand impact opportunities innovative uses data sector future cdei work cdei observer financial conduct authority bank england public private forum explore means support safe adoption machine learning artificial intelligence within financial services regulatory picture clearer financial services sectors looked review bias algorithmic financial services centre data ethics background creating flows interpersonal bits automation london stock exchange economy society carter mwaura ram trehan jones barriers ethnic minority women enterprise existing evidence policy tensions unsettled questions international small business journal case credit scoring credit reference agency data tends back six years lenders generally look last years provide mitigation discriminatory lending practices decades ago mit technology review easy way make lending fairer women trouble illegal financial services organisations make decisions significant impact lives amount credit offered price insurance premium set algorithms long used sector recent technological advances seen application machine learning techniques inform given historical use algorithms financial services industry embrace advanced technology make better decisions products offer customers however decisions made context environment financial resources spread evenly different groups example established evidence documenting inequalities experienced ethnic minorities women accessing credit either business owners individuals though generally attributed wider societal structural inequalities rather direct actions financial organisations rely making accurate predictions peoples behaviours example likely repay debts like mortgages specific individuals groups historically underrepresented financial system risk historic biases could entrenched algorithmic theory using data better algorithms could help make predictions accurate example incorporating data sources could enable groups historically found difficult access credit paucity data traditional sources gain better access future time complex algorithms could increase potential indirect bias via proxy become less able understand algorithm reaching output assumptions making individual case study difficulty assessing credit discrimination new york department financial services investigated goldman sachs potential credit discrimination gender came web entrepreneur david heinemeier hansson tweeted apple card goldman manages given credit limit times extended wife though two filed joint tax returns better credit score goldman sachs response consider gender determining creditworthiness would illegal therefore way could discriminate basis gender full facts around case yet public regulatory investigation ongoing nonetheless evidence considering gender could help mitigate gender bias least test algorithm better understand whether biased example brings key challenge financial organisations terms testing bias explore later chapter web entrepreneur david heinemeier hansson tweeted apple card goldman manages given credit limit times extended wife though two filed joint tax returns better credit score bias algorithmic financial services centre data ethics financial times banking reality behind hype survey sent almost firms total responses received financial conduct authority bank england machine learning financial services machine learning maturity different business areas financial services surveyed fca bank landscape despite plenty anecdotal evidence previously lack structured evidence adoption machine learning financial services financial times global survey banks provided evidence cautious approach taken however bank england fca conducted joint survey use financial services first systematic survey kind survey found algorithms increasingly used financial services two thirds reporting use form average firm using applications two business also found development entering mature stages deployment particular banking insurance sectors survey focused algorithms financial services rather algorithms key difference human explicitly programme algorithm instead computer programmes fit model recognise patterns data many algorithms constitute incremental rather fundamental change statistical methods used financial services also provide flexibility constrained linear relationships often imposed traditional economic financial analysis often make better predictions traditional models find patterns large amounts data increasingly diverse sources key uses algorithms financial services inform functions risk management compliance include identifying third parties trying damage customers bank fraud identity theft money laundering area algorithms find highest extent application due ability connect large datasets pattern detection however algorithms also increasingly applied areas credit scoring applications used granting access credit products credit cards loans mortgages review bias algorithmic financial services centre data ethics ibid bank england global economy machine learning market intelligence conference mckinsey company adoption advances foundational barriers remain financial conduct authority financial services public private forum underlying methodology behind different applications varies traditional methods logistic regression random forest models advanced machine learning natural language processing varying reports widely advanced tools used example fca bank england report highlights many cases development entered advanced stages deployment particular banking insurance quote seen real acceleration last five years machine learning deep learning widely adopted financial services see slowing leading credit reference agency fca bank england identified following potential benefits increased use algorithmic decisionmaking financial services improved customer choice services accurate pricing increased access credit households smes substantially lower cross border transaction costs improved diversity resilience however obstacles adoption algorithmic financial services organisations report mainly internal firms rather stemming regulation range lack data accessibility legacy systems challenges integrating existing business fca lead sector regulator financial services regulates financial services firms financial markets prudential regulator firms fca bank england recently jointly announced would establishing financial services artificial intelligence forum aippf forum set recognition work needed better understand pursuit algorithmic increasing data availability driving change financial markets consumer engagement wide range views need gathered potential areas principles guidance good practice examples could support safe adoption technologies bank england identified following potential benefits increased use algorithmic financial services improved customer choice services pricing increased access credit households smes substantially lower cross border transaction costs improved diversity resilience review bias algorithmic financial services centre data ethics findings cdei snapshot paper personal insurance term coined early days computing describe concept nonsense input data produces nonsense output bank england speech james proudman main focus within financial services credit scoring decisions made individuals traditional banks look detail algorithms used fintech companies insurance industry incorporate key trends findings areas review also separately conducted short piece research personal order understand key opportunities risks regards use algorithms financial sector conducted semistructured interviews financial services organisations predominantly traditional banks credit reference agencies also ran online experiment behavioural insights team understand people perceptions use algorithms credit scoring fair view use data could act proxy sex ethnicity particularly newer forms data social media informing algorithms whole financial organisations interviewed range innovative risk averse regards models building data sources drawing however agreed key obstacles innovation sector follows data availability quality source data ethically available techniques sufficient explainability risk averse culture parts given impacts financial crisis difficulty gauging consumer wider public acceptance algorithms mainly trained using historical data financial organisations hesitant incorporate newer interviews organisations argued financial data would biased due fact past mainly men participated financial system one could also see another risk fact fewer training datasets minority communities might result reduced performance investment advice algorithms communities quote key challenge posed output model good quality data fed rubbish rubbish problem underpinned huge expansion availability sources data amount data used grows scale managing problem james proudman bank england whole financial organisations train algorithms historical data amount data bank credit reference agency disposal varies know interview one major banks use data location transactions made along data share companies identify existing credit relationships banks consumers case credit reference agency spoke models built historical data trained variety public sources including applications made credit market electoral registry public data filing bankruptcy data provided clients algorithms mainly trained using historical data financial organisations hesitant incorporate newer nontraditional bias algorithmic financial services centre data ethics hurley adebayo credit scoring era big yale journal law technology volume issue experian credit reports include rent payments first time validation techniques including detecting errors risks companies spoke using social media data sceptical value behavioural information turnover returned items rental data terms using forms data phenomenon describes move credit scoring algorithms using data individual credit history drawing additional data individual example rent repayment history wider social network companies spoke using social media data sceptical value example credit reference agency major bank interviewed explored using social media data years ago decided believe would sufficiently improve accuracy algorithm justify use use data sources could enable population groups historically found difficult access credit due less data traditional sources gain better access future example interview credit reference agency spoke customers referred thin files little data available could source financial exclusion approach customers ensure decisions subjected manual review order address problem thin files experian added rent repayments credit reports million tenants intention making easier renters access finance data could improve inclusiveness improve representativeness datasets data complex algorithms could also increase potential introduction indirect bias via proxy well ability detect mitigate although general standard collect protected characteristics data financial organisations developing approaches testing algorithms bias common practice avoid using data protected characteristics proxies characteristics inputs algorithms likely unlawful discriminatory however understanding distribution protected characteristics among individuals affected decision necessary identify biased outcomes example difficult establish existence gender pay gap company without knowing whether employee man woman tension need create algorithms blind protected characteristics also checking bias characteristics creates challenge organisations seeking use data responsibly means whilst organisations lengths ensure breaking law discriminatory ability test outcomes decisions affect different population groups limited lack demographic data instead organisations test model accuracy validation ensuring sufficient human oversight process way managing bias development model review bias algorithmic financial services centre data ethics innovation human judgement involves interpreting direction models going variable fit pattern rejecting transforming approach requires significant oversight ensure fair operation effectively mitigate study london fintech company spoke london fintech company uses supervised order predict whether people able repay personal loans detect fraud line legislation include protected characteristics models check bias adopt fairness unawareness involving ongoing monitoring human judgement ongoing monitoring includes checking sufficiency across model performance business optimisation building test models counteract model human judgement involves interpreting direction models going variable fit pattern rejecting transforming approach requires significant oversight ensure fair operation effectively mitigate organisations hold protected characteristic data use models example major bank interviewed access sex age postcode data customers test bias basis sex age moreover banks advise parameters consider strongly correlate protected characteristics usually removed models given defined threshold bias imposed fca standards body organisations manage risks around algorithmic bias using judgement managing data quality small proportion companies analyse model predictions test data representative synthetic data anonymised public data extent problem algorithmic bias exists financial services still relatively unclear given decisions around finance credit often highly opaque reasons commercial sensitivity competitiveness even apparent differences outcomes different demographic groups without extensive access models used companies assessments individuals access protected characteristic data difficult determine whether differences due biased algorithms underlying societal economic structural causes insights work financial services fed wider recommendation around collecting protected characteristics data set chapter consider problems inherent fairness unawareness approaches chapter bias algorithmic financial services case study bias insurance algorithms propublica found people minority neighbourhoods average paid higher car insurance premiums residents neighbourhoods despite similar accident costs journalists could confirm cause differences suggest biased algorithms may blame like organisation using algorithms make significant decisions insurers must mindful risks bias systems take steps mitigate unwarranted discrimination however may instances using proxy data may justified example car engine size may proxy sex also material factor determining damage costs giving insurers cause collect process information related another complication insurers often lack data identify proxies exist proxies theory located checking correlations different data points protected characteristic question colour car ethnicity yet insurers reluctant collect sensitive information fear customers believing data used directly discriminate financial conduct authority conducted pricing practices household insurance firms one key findings risk firms could discriminate consumers using rating factors pricing based either directly indirectly data relating derived protected characteristics fca since done work including market study initiating public debate fair pricing related possible harms insurance industry proxies theory located checking correlations different data points protected characteristic question example colour car ethnicity yet insurers reluctant collect sensitive information fear customers believing data used directly discriminate centre data ethics innovation propublica minority neighborhoods pay higher car insurance premiums white areas risk financial conduct authority pricing practices retail general insurance sector household insurance bias algorithmic financial services centre data ethics cdei barometer ico alan turing institute guidance part explaining decisions made explainability models used financial services key explainability refers ability understand summarise inner workings model including factors gone model set section explainability key understanding factors causing variation outcomes systems different groups assess whether regarded fair polling undertaken review possible safeguards could put place ensure algorithmic process fair possible easy understand explanation came second list six options human oversight context financial services explainability algorithm important regulators banks customers banks developing algorithms explainability key requirement order better oversight systems identify mitigate discriminatory outcomes example giving loans using explainable algorithm makes possible examine directly degree relevant characteristics acting proxy characteristics causing differences outcomes different groups means may valid reasons loans disproportionately given one group another properly understood explained customers explainability crucial understand role model played decision made regulators understanding decision reached vital knowing whether organisation met legal requirements treated people fairly process indeed expert panel convened barometer discussions viewed lack explainability regulators significantly greater risk lack transparency consumers algorithmic decisionmaking financial lack explainability machine learning models highlighted one top risks respondents bank england fca survey survey highlighted use cases credit scoring explainability priority banks opting logistic regression techniques elements ensure decisions could explained customers required however research shown organisations banking insurance continuing select interpretable models applications increasingly using opaque challenger models alongside purposes feature engineering selection comparison insight interviews banks reported using treebased models random forests claim generate accurate predictions however acknowledged size complexity models made difficult explain exactly work key variables drive predictions result logistic regression techniques elements continue popular type use case provide higher degree apparent explainability approaches breaking procedures neural networks order justify decision made particular customer transaction interview credit reference agency described example team calculated impact every input parameter final score used information return factors biggest impact format customerspecific still general enough work across entire population means low credit score could explained simple statement rent paid time nonetheless even approaches explain models system level understand credit denied always directly available individual level explanations customers may difficult assign one factor rather combination banks developing algorithms explainability key requirement order better oversight systems identify mitigate discriminatory outcomes review bias algorithmic financial services centre data ethics royal society encouragement arts manufactures commerce artificial intelligence real public engagement ico guide data protection explaining decisions made cases firms required provide information individual decisions includes gdpr articles particular fca rules lenders industry standards standards lending practice risks explainability may always come type model used considerations example commercial sensitivities concerns people may game exploit model know much works interestingly public attitudes research conducted suggested customers could consider circumstances commercial interests could supersede individuals rights example making financial decisions recognition providing detailed explanation could backfire helping fraudsters outwit system interests overruled firms interviewed reported mostly designing developing tools apart sometimes procuring underlying platforms infrastructure cloud computing mean intellectual property considerations impinge explainability standards nonetheless may commercial sensitivities concerns around gaming risks clearly documented outset justified necessary documentation clear benefits organisations individuals society explaining algorithmic financial services providing explanations individuals affected decision help organisations ensure fairness outcomes different groups across society moreover organisations makes business sense way building trust customers empowering understand process providing opportunity challenge ico alan turing institute explainability guidance may ico alan turing institute published organisations explain decisions made individuals affected guidance sets key concepts different types explanations along tailored support senior management teams policies procedures organisations put place ensure provide meaningful explanations affected individuals fca fed guidance ensure takes account opportunities challenges facing banks explaining decisions customers providing explanations individuals affected decision help organisations ensure fairness outcomes different groups across society review bias algorithmic financial services centre data ethics behavioural insights team perceptions fairness algorithms proxy information financial services acceptability use algorithms financial services higher sectors difficult gauge polling undertaken review asked aware algorithms used support decisions context four sectors looked report financial services option selected majority people around contrast people aware use local government interviews financial companies evident making efforts understand public acceptability mainly context customers example financial organisations interviewed conducted consumer polling focus groups understand public felt use payment data another interview learnt bank gauged public acceptability focus customer vulnerability conducting surveys interviews also considering impact new product customers risk management framework moreover product goes annual review takes account problems example customer complaints order better understand public attitudes conducted public engagement exercise behavioural insights team bit online platform predictiv measured participants perceptions fairness banks use algorithms loan decisions particular wanted understand people fairness perceptions banking practices varied depending type information algorithm used loan decision example use variable could serve proxy protected characteristic sex ethnicity found average people moved twice much money away banks use algorithms loan application decisions told algorithms draw proxy data protected characteristics social media data surprisingly historically risk discriminated society feel strongly unfair bank use proxy information protected characteristics example directionally women punish bank used information could act proxy sex strongly men however people thought fair use proxy variable produced accurate result brings question whether legitimate proxies example salary although could function proxies sex ethnicity may also accurately assist bank making decisions around loan eligibility experiment also found people less concerned use social media data data relates sex ethnicity however frequency individual uses social media impact concerned use informing loan decisions experiment highlighted challenges framing questions bank use algorithms unbiased nuanced way research needed use proxies forms data financial services give financial organisations confidence innovating way deemed acceptable public public acceptability use algorithms financial services higher sectors difficult bias algorithmic financial services centre data ethics bias fairness financial services currently seen unjustified barrier innovation additional guidance support would beneficial majority respondents fca bank england survey said consider prudential regulation fca regulations unjustified barrier deploying algorithms view supported organisations interviewed may fca responded constructively increased use algorithms proactively finding ways support ethical innovation however respondents survey noted challenges meeting regulatory requirements explain decisionmaking using advanced complex algorithms moreover firms also highlighted would benefit additional guidance regulators apply existing regulations whilst positive fca seen constructive regulator future regulation may need adapted adjusted account developments algorithms order protect consumers aippf identify may case whilst also identifying good practice cdei work cdei observer financial conduct authority bank england public private forum explore means support safe adoption machine learning artificial intelligence within financial services whilst positive fca seen constructive regulator future regulation may need adapted adjusted account developments algorithms order protect consumers bank england prudentially regulates supervises financial services firms prudential regulation authority pra bias algorithmic financial services policing chapter centre data ethics innovation summary overview findings adoption algorithmic early stage tools currently operation varied picture across different police forces levels usage approaches managing ethical risks police forces identified opportunities use data analytics scale better allocate resources significant risk without sufficient care systematically unfair outcomes could occur use algorithms support introduces new issues around balance security privacy fairness clear requirement strong democratic oversight meaningful engagement public needed uses police technology acceptable clearer national leadership needed around ethical use data analytics policing though strong momentum data ethics policing national level picture fragmented multiple governance regulatory actors one body fully empowered resourced take ownership clearer steer required home office recommendation government recommendation home office define clear roles responsibilities national policing bodies regards data analytics ensure access appropriate expertise empowered set guidance standards first step home office ensure work underway national police chiefs council policing stakeholders develop guidance ensure ethical oversight data analytics tools appropriately police suppliers police forces conduct integrated impact assessment investing new data analytics software full operational capability establish clear legal basis operational guidelines use tool details integrated impact assessment include set report commissioned rusi police forces classify output statistical algorithms form police intelligence alongside confidence rating indicating level uncertainty associated prediction police forces ensure appropriate rights access algorithmic software national regulators able audit underlying statistical models needed instance assess risk bias error rates intellectual property rights must restriction scrutiny future cdei work cdei applying testing draft ethics framework police use data analytics police partners projects developing underlying governance structures make framework bias algorithmic policing centre data ethics background lammy review institutional racism defined stephen lawrence inquiry collective failure organisation provide appropriate professional service people colour culture ethnic origin stephen lawrence inquiry ethnicity facts figures strategic review policing england wales sir michael barber head major review police service prime minister launches police recruitment drive police foundation policing public value available kit malthouse police funding written statement policing codes college policing national decision model code ethics key reference points policing notable government reviews issue bias policing important considering risks opportunities around use technology policing example lammy found bame individuals faced bias including overt discrimination parts justice system prior lammy review public inquiry fatal stabbing black teenager stephen lawrence branded metropolitan police force institutionally racist recently race disparity highlighted important disparities treatment outcomes bame communities along lower levels confidence police among younger black adults findings mind vital consider historic current disparities inequalities looking algorithms incorporated policing whilst current evidence police algorithms racially biased one certainly see risks algorithms entrenching amplifying widely documented human biases prejudices particular bame individuals criminal justice system police long significant pressure scrutiny predict prevent reduce crime martin hewitt qpm chair national police chiefs council npcc senior police leaders highlighted policing environment changed profoundly many ways policing mission expanded volume complexity taken place backdrop diminishing minister boris johnson announcement recruit new police officers one headline policy signals government commitment respond mounting public unease local visibility police officers decentralised nature policing england wales means force developing plan respond new pressures challenges police forces access digital material ever expected use data identify connections manage future risks indeed million ministerial funding january police technology programmes amongst infrastructure national priorities demonstrates government commitment police innovation response incentives innovate police forces looking data analytics tools derive insight inform resource allocation generate predictions drawing insights predictions data requires careful consideration independent oversight right expertise ensure done legally ethically line existing policing despite multiple legal frameworks codes setting clear duties police facing new challenges adhering law following codes development use data analytics drawing insights predictions data requires careful consideration independent oversight right expertise review bias algorithmic policing centre data ethics innovation babuta alexander oswald marion analytics algorithms policing england wales towards new policy framework royal united services institute referred gangs violence matrix amnesty international trapped matrix available ico ico finds metropolitan police service gangs matrix breached data protection laws mayor london mayor intervention results overhaul met gangs matrix predictive models subject ongoing empirical validation involves revisiting models quarterly basis ensure accurate adding study innovation avon somerset constabulary avon somerset constabulary successful building data science expertise office data analytics one tools qlik sense software product connects force internal databases local authority datasets applies predictive modelling produce individual intelligence profiles assist force triaging offenders according perceived level risk although avon somerset constabulary operate data ethics committee model like west midlands police governance oversight processes place moreover predictive models subject ongoing empirical validation involves revisiting models quarterly basis ensure accurate adding value theory tools help spot patterns activity potential crime lead effective prioritisation allocation scarce police resources range data driven tools developed deployed police forces including tools help police better integrate visualise data tools help guide resource allocation decisions inform decisions individuals someone likelihood reoffend however limited evidence base regarding claimed benefits scientific validity cost effectiveness police use example empirical evidence around effectiveness actuarial tools predict reoffending however experts disagree statistical theoretical validity individual tools needs done establish benefits technology order technology must tested controlled proportionate manner following national guidelines use tools policing also carries significant risk met police gangs example highly controversial intelligence prioritisation tool use since tool intends identify risk committing victim violence london amnesty international raised serious concerns gangs matrix particular featured disproportionate number black boys young men people kept database despite lack evidence reliance addition gangs matrix found information commissioner office breached data protection laws enforcement notice issued met since mayor london sadiq khan announced gangs matrix highlighting number people black african caribbean background added database dropped per cent per cent gangs matrix likely closely scrutinised civil society regulators policymakers evident without sufficient care use intelligence prioritisation tools policing lead outcomes biased particular groups systematically unfair regards many scenarios tools helpful still important balance struck automated decisionmaking application professional judgement discretion appropriate care taken internally consider issues fully critical public trust policing police forces transparent tools used review bias algorithmic policing centre data ethics conducted extensive stakeholder engagement last year understand key challenges concerns development use data analytics tools sector cdei published research paper facial recognition technology covers police use live facial recognition technology along uses see example richardson rashida schultz jason crawford kate dirty data bad predictions civil rights violations impact police data predictive policing systems justice institute kearns ian rick muir data driven policing public value police foundation policing machine liberty rusi sent freedom information requests police forces england wales interviewed people police forces technology providers academia civil society government regulation ran roundtables jointly cdei techuk babuta alexander oswald marion analytics algorithms policing england wales towards new policy framework rusi west midlands police crime commissioner ethics committee approach given breadth applications areas technology used law enforcement chose focus use data analytics policing derive insights inform operational make predictions include biometric identification automated facial digital forensics intrusive surveillance however opportunities risks potential approaches discuss remain relevant technology issues policing build strengthen existing research publications commissioned new independent research royal united services institute rusi aim research identify key ethical concerns particular issue bias propose future policy address issues incorporated findings rusi chapter relevant throughout report also issued call evidence use algorithmic tools efforts mitigate bias engagement public issues governance regulation gaps across four sectors addressed report including policing receiving diverse range responses conducted extensive stakeholder engagement last year understand key challenges concerns development use data analytics tools sector example spoken local police forces including avon somerset durham essex hampshire police scotland south wales spotlight working west midlands police west midlands police one leading forces england wales development data analytics data analytics lab lead force national data analytics solution pcc also set ethics review data science projects developed lab advise pcc chief constable whether proposed project sufficiently addressed legal ethical considerations met representatives west midlands police pcc office multiple times throughout project invited observe meeting ethics committee also interviewed contributed rusi report development policing framework interested seeing going forward extent forces follow west midlands pcc ethics committee model hope continue working closely west midlands future policing work review bias algorithmic policing centre data ethics established partnership cabinet office race disparity unit rdu government unit collates analyses publishes government data experiences people different ethnic backgrounds order drive policy change disparities found drawn expertise better understand algorithmic could disproportionately impact ethnic minorities partnership included jointly meeting police forces local authorities along rdu advisory group contributing roundtables rusi reviewing report recommendations met senior representatives policing bodies including national police chiefs council npcc majesty inspectorate constabulary fire rescue services hmicfrs police ict company college policing association police crime commissioners apcc regulators interest sector including information commissioner office also engaged teams across home office interest police draft framework support police develop data analytics ethically cdei developing draft framework support police innovating ethically data intended police project teams developing planning develop data analytics tools also help senior police identify problems best addressed using data analytics along suited technological solution framework structured around agile delivery cycle sets key questions asked stage tested framework small group stakeholders police forces academics civil society plan release widely following publication review feedback received date also highlighted public debate around policing missing complex issues current public commentary polarised without building common consensus acceptable police use police risk moving ahead without public buyin cdei exploring options facilitating public conversation going forward testing framework police forces future cdei work cdei applying testing draft ethics framework police use data analytics police partners projects developing underlying governance structures make framework partnership included jointly meeting police forces local authorities along rdu advisory group contributing roundtables rusi reviewing report recommendations review bias algorithmic policing centre data ethics findings cdei call evidence summary responses review bias algorithmic couchman hannah policing machine liberty robin moore compendium research analysis offender assessment system oasys national offender management service ministry justice analytical series guardian met uses software deployed see ethnic groups specialise areas crime develo pment use across police forces england wales picture varied responses received call wider research know challenges defining meant algorithmic tool consequently understanding extent scale adoption line difficult say certainty many police forces england wales currently developing trialling using algorithms due part different definitions also lack information sharing forces rusi research surfaced different terms used refer data analytics tools used police forces example several interviewees considered term policing problematic given many advanced analytics tools used entities different groups would accurate describe tools rather instance scoring offenders according perceived likelihood reoffending comparing selected characteristics within specified group necessarily imply individual expected commit crime rather suggests higher level risk management required level assigned individuals within cohort rusi sorted application data analytics tools developed police following categories predictive mapping use statistical forecasting applied crime data identify locations crime may likely happen near future recent data suggests police forces england wales currently using developing individual risk assessment use algorithms applied personal data assess risk future offending example offender assessment system oasys offender group reconviction scale ogrs routinely used prison probation service hmpps measure individuals likelihood reoffending develop individual risk management data scoring tools use advanced machine learning algorithms applied police data generate risk scores known offenders complex algorithms used forecast demand control centres triage crimes investigation according predicted solvability examples data scoring tools include harm assessment risk tool hart developed deployed durham police uses supervised machine learning classify individuals terms likelihood committing violent offence within next two years use qlik sense cots analytics platform avon somerset link data separate police databases generate new insights crime patterns integrated offender management model development currently deployed west midlands police makes predictions probability individual move committing low middling levels harm via criminal activity perpetrating harmful offending also reports individual forces buying similar technology example origins software reportedly currently used metropolitan police service previously used several forces including norfolk suffolk west midlands police forces software intends help identify whether different ethnic groups specialise particular types crime come strong critique equality race relations campaigners argue clear example police forces racial profiling particularly fraught time police black community england wales police forces currently taking variety different approaches development algorithmic systems ethical safeguards community engagement data science bias algorithmic policing centre data ethics see example richardson rashida schultz jason crawford kate dirty data bad predictions civil rights violations impact police data predictive policing systems justice institute home office police powers procedures england wales year ending march nesta age algorithm bias ensuring fairness requires looking entire process set earlier report think crucial take broad view whole process considering different ways bias enter system might impact fairness context policing means looking development algorithm also context deployed operationally design testing stage significant risk bias entering system due nature police data algorithms trained police data biased due either unrepresentative crime distributed serious cases reflecting unlawful policing practices certain communities certain crimes example police officer interviewed rusi research highlighted young black men likely stopped searched young white men purely human bias indeed backed home office data released last year stating identify black black british times likely stopped searched officer white another way police data provide misrepresentative picture individuals disadvantaged socio demographic backgrounds likely engage public services frequently means data held algorithms could risk calculating groups data held police posing greater risk empirical research needed assess level bias police data impact potential bias challenge considered stage use sensitive personal data develop data analytics tools whilst models may include variable race areas postcode function proxy variable race community deprivation thereby indirect undue influence outcome prediction biases data understood managed early could lead creation feedback loop whereby future policing crime predicted could also influence high low risk certain crimes areas deemed data analytics tool potentially perpetuate exacerbate biased criminal justice outcomes certain groups individuals deployment stage bias may occur way human receiving output algorithm responds one possibility decisionmaker automated output without applying professional judgement information opposite also possible human decisionmaker feels inherently uncomfortable taking insights algorithm point nervous use simply ignores cases human bias suggests different risk level balance important ensure due regard paid insights derived whilst ensuring professional applies expertise understanding wider context relevant factors argued example dame cressida dick keynote address launch event report data analytics policing police officers may better equipped many professionals apply suitable level scepticism outcome algorithm given weighing reliability evidence fundamental general professional practice without sufficient care multiple ways bias enter system outcomes systematically unfair lead bias discrimination individuals within particular groups means looking development algorithm also context deployed bias algorithmic policing centre data ethics babuta alexander oswald marion analytics algorithms policing england wales towards new policy framework rusi ibid npcc apcc national policing digital strategy digital data technology strategy need strong national leadership ethical use data analytics tools key finding rusi research widespread concern across law enforcement community regarding lack official national guidance use algorithms policing respondents suggesting gap addressed matter without national guidance initiatives developed different standards varying levels oversight scrutiny example police forces england wales established local ethics committees currently resourced look digital projects instead police crime commissioners like west midlands established data ethics committees provide independent ethical oversight police data analytics projects however given absence guidance unclear whether force setting data ethics committees upskilling existing ones whether regional centralised national structure set provide digital ethical oversight police forces review existing police ethics committees would useful order develop proposals ethical oversight data analytics projects similarly lack national coordination oversight means data initiatives developed local level lead pockets innovation experimentation however also risks meaning efforts duplicated knowledge lessons transferred across forces systems made interoperable described senior officer interviewed part rusi project patchwork quilt uncoordinated delivered different standards different settings different outcomes work underway national level led national police chiefs council order develop coordinated approach data analytics policing reflected national digital policing sets intention develop national data ethics governance model provide clear lines accountability data algorithm use top policing organisations continue supported ensure consistent approach across police forces moreover hmicfrs included national work area example establishing external reference group police use data analytics view incorporating use data analytics effectiveness future crime data integrity inspections rusi report sets detail policy framework data analytics policing involve cdei developing draft framework support police project teams addressing legal ethical considerations developing data analytics tools without clear consistent national guidance coordination oversight strongly believe potential benefits tools may fully realised risks materialise recommendations government recommendation home office define clear roles responsibilities national policing bodies regards data analytics ensure access appropriate expertise empowered set guidance standards first step home office ensure work underway national police chiefs council policing stakeholders develop guidance ensure ethical oversight data analytics tools appropriately bias algorithmic policing centre data ethics techuk police funding settlement tech investment needed police project teams address new challenges whilst crucial national policy framework developed without significant investment skills expertise police forces framework implemented effectively police forces expected accountable systems engage developers make ethical decisions including considerations significant investment needed announcement recruit new police officers provides opportunity bring diverse set skills however work needed ensure existing police officers equipped use data analytics tools also welcome announcement january police chief scientific advisor dedicated funding investment science technology first steps addressing skills gap based rusi research engagement police stakeholders know wide range skills required ranging limited legal data science evaluation particular research rusi highlighted insufficient expert legal consultation development phase data analytics projects leading problematic delay adhering legal requirements developing mechanism specialist expertise legal accessed forces would help ensure expertise incorporated outset developing tool moreover examples police force data protection officer involved discussions beginning project able highlight project may interact gdpr support completion data protection impact assessment similarly upskilling needed police ethics committees provide ethical oversight data analytics projects based rusi research engagement police stakeholders know wide range skills required ranging limited legal data science bias algorithmic policing centre data ethics deliberation police use technology urgently needed decisions police make daily basis neighbourhoods individuals prioritise monitoring affect data techniques used inform decisions great interest significance society large moreover due wider public sector funding cuts police increasingly required respond example evidence suggests police spending less time dealing theft burglary time investigating sexual crime responding mental health longstanding peelian principles define british approach policing central police force behave legitimacy eyes public values core peelian principles integrity transparency accountability continue relevant today particular light ethical considerations brought new technologies research rsa highlights people feel strongly use automated decision systems criminal justice system percent people oppose strongly oppose use domain sectors financial services moreover people least familiar use automated systems criminal justice system percent either familiar familiar use findings suggest risk police forces move quickly developing tools without engaging meaningfully public could significant public backlash loss trust police use data failure engage effectively public therefore ethical risk risk speed innovation police many existing ways engaging public relationships community groups police crime commissioners pcc west midlands pcc introduced community representative role ethics committee increase accountability use data analytics tools however civil society representative interviewed rusi highlighted ethics committees could act fig leaf wider discussions police public take steady increase public trust police tell truth since promising overarching trend signals opportunity police policymakers technologists regulators ensure data analytics tools policing designed used way builds legitimacy trustworthy eyes public percent people oppose strongly oppose use automated decision systems criminal justice police foundation policing public value quote rick muir director police foundation strategic view policing sir michael barber head major review police service royal society encouragement arts manufactures commerce artificial intelligence real public engagement ipsos mori veracity index survey respondents trust police tell truth increase since see bias algorithmic policing local government chapter centre data ethics innovation government summary overview findings local authorities increasingly using data inform across wide range services whilst tools still early phase deployment increasing demand sophisticated predictive technologies support efficient targeted services tools present genuine opportunities local government used support decisions however tools considered silver bullet funding challenges cases require significant additional investment fulfil potential possible increase demand services data infrastructure data quality significant barriers developing deploying tools investing necessary developing advanced systems national guidance needed priority support local authorities developing using tools ethically specific guidance addressing identify mitigate biases also need wider sharing best practice local government recommendation government develop national guidance support local authorities legally ethically procure develop algorithmic decisionmaking tools areas significant decisions made individuals consider compliance guidance monitored future cdei work cdei exploring best support local authorities responsibly ethically develop technologies including possible partnerships central local government national guidance needed priority support local authorities developing using tools ethically specific guidance addressing identify mitigate biases also need wider sharing best practice local bias algorithmic local government centre data ethics background local government association local government funding moving conversation open access government changing face local government digital transformation oxford internet institute university oxford data science local government eubanks virginia automating inequality tools profile police punish poor martin press outlined chapter policing sector considering use tools part full shown certain people likely overrepresented data held local authorities lead biases predictions authorities responsible making significant decisions individuals daily basis individuals making decisions required draw complex sources evidence well professional judgement also increasing pressure target resources services effectively following reduction billion local authority funding last competing pressures created environment local authorities looking digital transformation way improve efficiency service whilst research found machine learning approaches predictive technologies local government nascent stage growing interest way maximise service delivery target early intervention way saving resources line citizen needs bringing together multiple data sources representing existing data new forms technologies guide providing contextualised picture individual needs beyond decisions individuals tools help predict map future service demands ensure sufficient sustainable resourcing delivering important services however technologies also come significant risks evidence shown certain people likely overrepresented data held local authorities lead biases predictions related problem number people within subgroup small data used make generalisations result disproportionately high error rates amongst minority groups many applications predictive technologies false positives may limited impact individual however particularly sensitive areas deciding intervene case child may risk false negatives positives carry significant consequences biases may mean certain people likely experience negative effects risks acute using technologies support individual areas adult children services reason focused predominantly use data science local government used welfare social care healthcare transportation housing planning environment sustainability waste management education policing public safety source data science local government oxford internet institute bright bias algorithmic local government centre data ethics findings guardian one three councils using algorithms make welfare decisions dencik data scores governance investigating uses citizen scoring public services data justice lab cardiff university guardian one three councils using algorithms make welfare decisions work local government sector began desk based research facilitated call evidence landscape summary commissioned evidence gathering provided broad overview challenges opportunities presented predictive tools local government wanted ensure research informed accounts challenges implementing using technologies met spoke broad range people organisations included researchers based academic policy organisations tool providers local authorities industry bodies associations relevant government departments difficult map widespread algorithmic local government multiple attempts map usage algorithmic tools across local authorities many researchers found investigation guardian found minimum councils invested software contracts cover identifying benefit fraud identifying children risk allocating school however include additional use cases found report data justice lab research group based cardiff university data justice lab used freedom information requests learn tools used frequently however many challenges approach one fifth requests delayed never receiving part local authorities heard often significant challenge presented inconsistent terminology used describe algorithmic systems leading varied reporting across local authorities using similar technologies also sometimes difficult coordinate activities across whole authority service delivery areas operate relatively rising interest use predictive tools local government local authorities keen emphasize algorithms support rather replace decisionmakers particularly sensitive areas children social services interviews found local authorities concerned current narrative focused heavily automation rather focus towards using data make evidence based decisions risk concerns around public reaction media reporting topic disincentivize transparency however likely cause suspicion technologies local authorities appear opaque public may harm trust citizens way understand data used deliver public services believe introducing requirements promote transparency across public sector help standardise reporting support researchers build public trust see chapter discussion given rising interest use predictive tools local government local authorities keen emphasize algorithms support rather replace particularly sensitive areas children social services review bias algorithmic local government comparing local government policing many overlaps risks challenges technologies policing local government cases public sector organisations developing tools data may high quality certain populations likely represented could lead unintentional discrimination sectors often rely procuring software may necessary capacity capability question providers risks around bias discrimination scope greater sharing learning two sectors wider public sector around tackle challenges well considering adopting practices worked well places example local authorities may want look certain police forces set ethics committees way providing external oversight data projects similarly initiatives develop integrated impact assessments taking account data protection equality legislation may applicable contexts tool development approaches local authorities developed algorithmic tools others tools procured approaches local authorities developed tools integrated analytical hub used bristol city council bristol developed hub response government troubled families programme provided financial incentives local authorities could successfully identify support families hub brings together datasets covering wide range topics including school attendance crime statistics children care data domestic abuse records health problem data adult involvement alcohol drug datasets used develop predictive modelling targeted interventions offered families identified risk one benefits using approaches offer local authorities greater control data used also require fuller understanding organisation data quality infrastructure useful monitoring system however building tools often require significant investment internal expertise may feasible many local authorities also carry significant risks inhouse project ultimately work software increasing number providers offering predictive analytics data analysis software support software support detecting fraudulent benefit claims reportedly used around local providers offer predictive software brings together different data sources uses develop models identify target services use cases varied include identifying children risk adults requiring social care risk homelessness software helps earlier interventions potential bring costs however relies tools accurate precise far limited evaluation efficacy interventions centre data ethics innovationboth sectors local government policing often rely procuring third party software may necessary capacity capability question providers risks around bias discrimination house commons library troubled families programme england dencik data scores governance investigating uses citizen scoring public services data justice lab cardiff university guardian one three councils using algorithms make welfare decisions dencik data scores governance investigating uses citizen scoring public services data justice lab cardiff university bias algorithmic local government centre data ethics innovation providers offer specialist data science expertise likely available local authorities likely valuable experience previous work local authorities however also risks around costs procuring technologies transparency accountability also particularly important procuring tools commercial sensitivities may prevent providers wanting share information explain model developed local authorities responsibility understand decisions made regardless whether using developing approach seen way outsource complex decisions local authorities also consider manage risks around bias may outside scope provider service see section discussion public sector procurement transparency local authorities struggle data quality data sharing implementing tools multiple challenges local authorities face introducing new technologies due legacy systems local authorities often struggle maintaining data infrastructure developing standardised processes data sharing many conversations companies partnered local authorities found phase took lot longer expected due challenges led costly delays need reprioritise resources local authorities wary introducing tools particularly cases data infrastructure requires significant investment many local authorities investing basic data requirements likely reap higher rewards introducing advanced technologies stage also associated risk legacy systems poor data quality poor data quality creates significant challenges without good quality representative dataset algorithm face challenge poor quality training data results poor quality algorithm one challenges identified data scientists data pulled different sources data scientists always necessary access correct data algorithms good training data interrogating data quality data sources used develop new predictive tool top priority prior procuring new software spotlight cdei work data sharing one challenges frequently mentioned local authorities wanting explore opportunities presented technologies concerns around data sharing often systems require bringing together different datasets physical barriers poor infrastructure cultural barriers insufficient knowledge share data line data protection legislation often mean innovation slow even cases clear benefits example often hear children social services social workers always access data need assess whether child risk whilst data may held within local authority clear legal basis social workers access local authorities experience various challenges facilitating sharing data sharing effective also needs consideration share data whilst retaining trust individuals organisations recent report data explores challenges potential solutions detail dencik data scores governance investigating uses citizen scoring public services data justice lab cardiff university cdei addressing trust public sector data use bias algorithmic local government centre data ethics works children social care ethics review machine learning children social care departments department education oversee children social care ministry housing communities local government mhclg well placed support coordinate development national guidance needed govern use algorithms delivery public services currently little guidance local authorities wanting use algorithms assist found whilst many local authorities confident understanding data protection risks less clear legislation equality act human rights act applied risk without understanding applying frameworks tools may breach law works centre children social care recently commissioned review ethics machine learning approaches children social care conducted alan turing institute university oxford rees centre also found national guidance priority ensure ethical development deployment new review concludes cautious thoughtful inclusive approach using machine learning children social care needed facilitated series recommendations including nationally mandated standards research echoed found work stakeholders felt strongly national guidance needed protect vulnerable groups misuse data including reducing risk unintentional biases whilst research looked need guidance children social care similar challenges likely arise across range services within local government make important decisions individuals housing adult social care education public health therefore think guidance applicable across range areas recognising likely places supplementary detailed guidance necessary particularly regulatory frameworks differ taken together strong case national guidelines setting responsibly develop introduce algorithms local government government departments department education ministry housing communities local government mhclg department health social care best placed support coordinate development national guidance local government association also started project bringing local authorities together understand challenges opportunities aim bringing expertise together develop guidance national guidelines look build upon work recommendations government recommendation government develop national guidance support local authorities legally ethically procure develop algorithmic tools areas significant decisions made individuals consider compliance guidance bias algorithmic local government centre data ethics technologies save money may result significant challenges local authorities variety motivations introducing algorithmic tools many focused wanting improve decisionmaking however given significant reduction income last decade drive towards using technology improve efficiencies service delivery within local government research exploring uptake across local government oxford internet institute found deploying tools primary motivation unlikely yield results expected state case many projects often built around idea save money current climate intense financial difficulty understandable also believe fundamentally wrong way conceive data science government context many useful projects short term least save focus predictive tools often grounded idea early intervention way identify someone risk put assistive measures place early situation managed prior escalation thus reducing overall resources way thinking may result less demand overall however likely lead increased workload investment preventative services challenging ethical issue around required someone identified heard examples local authorities held adopting new tools would cost much follow intelligence provided due duty care placed local authorities also concern staff may blamed following leads case later develops therefore councils need carefully plan deploy resources response potential increase demands services wary viewing tools silver bullet solving resourcing greater support sharing lessons best practice joint working local authorities local authorities often experience similar challenges networks share lessons learned often hoc informal rely local authorities knowing authorities used similar tools local government association work started bringing knowledge experience together important first step also opportunities central government learn work undertaken within local government miss innovation taking place lessons learned challenges similar sectors local digital collaboration unit within ministry housing communities local government also set provide support training local authorities undertaking digital innovation projects local digital collaboration unit oversees local digital fund provides financial support digital innovation projects local government greater support fund particularly projects looking case studies identifying mitigating bias local government algorithms evaluating effectiveness algorithmic tools public engagement sharing best practice would add significant value research found local authorities thought fund helpful initiative however felt greater investment would improve access benefits cost effective long term oxford internet institute university oxford data science local government oxford internet institute found deploying tools primary motivation unlikely yield results bias algorithmic local government centre data ethics innovation iii addressing challenges enabling fair innovation introduction workforce diversity protected characteristic data monitoring outcomes detecting mitigating bias anticipatory governance regulatory environment introduction current landscape legal background role regulators regulatory tools transparency public sector identifying issue delivering public sector transparency public sector procurement next steps future challenges acknowledgements centre data ethics innovation part surveyed issue bias algorithmic part studied current state detail across four sectors move identify challenges identified addressed progress made far needs happen next three main areas consider enablers needed organisations building deploying algorithmic tools help fair way see chapter regulatory levers formal informal needed incentivise organisations create level playing field ethical innovation see chapter public sector major developer user technology show leadership transparency see chapter inherent links areas creating right incentives succeed right enablers place help organisations act fairly conversely little incentive organisations invest tools approaches fair insufficient clarity expected norms lots good work happening try make decisionmaking fair remains long way see status quo follows addressing challenges introduction impact bias algorithms could help address bias measurement bias data available ever help organisations understand impacts mitigating bias lots academic study open source tooling available support bias butbuilding algorithms replicate existing biased mechanisms embed even exacerbate existing inequalities collection protected characteristic data patchy significant perceived uncertainty ethics legality willingness individuals provide data see section uncertainties concerning legality ethics inferring protected characteristics decision processes whether using algorithms exhibit bias form fail certain tests fairness law offers limited guidance organisations adequate ways address relatively limited understanding use tools practice support fair ecosystem many tools align equality law uncertainty usage tools issues legality approaches law perceived accuracy though often may suggest incomplete notion accuracy see section review bias algorithmic addressing challenges centre data ethics support range consultancy services available help issues workforce diversity strong stated commitment government industry improving diversity leadership governance many organisations understand strategic drivers act fairly proactively complying data protection obligationsobligations anticipating ethical risks transparency transparency use impact algorithmic would help drive greater consistency regulation good regulation support ethical innovation butan immature ecosystem clear industry norms around services relevant professional skills important legal clarity see section still far little diversity tech sector see section recent focus data protection due arrival gdpr especially privacy security aspects risks fairness equality issues even though also required gdpr identifying historical current bias comfortable thing organisations risk public opinion penalise proactively identify address bias governance needs compliance current regulations needs consider possible wider implications introduction algorithms anticipate future ethical problems may emerge see section insufficient incentives organisations transparent risks going alone danger creating requirements create public perception risks organisations even would help reduce risks biased public sector identified issue could lead development use algorithmic see chapter regulators currently equipped deal challenges posed algorithms continued nervousness industry around implications gdpr ico worked hard address recent guidance help remains way build confidence interpret gdpr context see chapter governance key theme throughout part review organisations regulators ensure risks bias anticipated managed effectively trivial get right clear scope organisations better considering potential impacts algorithmic tools anticipating risks advance terms anticipatory governance anticipatory regulation sometimes used describe approach though arguably anticipatory governance regulation simply part good governance regulation chapter consider organisations approach chapter role regulators law chapter habit increased transparency public sector use tools could encourage review bias algorithmic addressing challenges enabling fair innovation chapter centre data ethics innovation fair innovation summary overview findings many organisations unsure address bias practice support needed help consider measure mitigate unfairness improving diversity across range roles involved technology development important part protecting certain forms bias government industry efforts improve must continue need show results data needed monitor outcomes identify bias data protected characteristics available often enough one cause incorrect belief data protection law prevents collection usage number lawful bases data protection legislation using protected special characteristic data monitoring addressing discrimination genuine challenges collecting data innovative thinking needed area example potential trusted intermediaries machine learning community developed multiple techniques measure mitigate algorithmic bias organisations encouraged deploy methods address bias discrimination however little guidance choose right methods embed development operational processes bias mitigation treated purely technical issue requires careful consideration wider policy operational legal context insufficient legal clarity concerning novel techniques area may compatible equality government recommendation government continue support invest programmes facilitate greater diversity within technology sector building current programmes developing new initiatives gaps recommendation government work relevant regulators provide clear guidance collection use protected characteristic data outcome monitoring processes encourage use guidance data address current historic bias key sectors recommendation government ons open secure research service broadly wider variety organisations use evaluation bias inequality across greater range activities recommendation government support creation development public private partnerships especially focused identification reduction biases issues specific groups ons government statistical service work partnerships regulators promote harmonised principles data collection use private sector via shared data standards development recommendations regulators recommendation sector regulators industry bodies help create oversight technical guidance responsible bias detection mitigation individual sectors adding detail existing guidance data protection new guidance equality act improving diversity across range roles involved development deployment algorithmic tools important part protecting bias algorithmic enabling fair innovation centre data ethics innovation centre data ethics innovation advice industry organisations building deploying algorithmic tools make increased diversity workforce priority applies data science roles also wider operational management oversight roles proactive gathering use data industry required identify challenge barriers increased diversity recruitment progression including senior leadership roles organisations operating within deploy bias detection mitigation tools developed must mindful relevant equality law along across much europe different organisations face historical issues attract significant societal concern otherwise believe bias risk need measure outcomes relevant protected characteristics detect biases algorithmic otherwise must address uncovered direct discrimination indirect discrimination outcome differences protected characteristics lack objective justification organisations ensure mitigation efforts produce new forms bias discrimination many bias mitigation techniques especially focused representation inclusion legitimately lawfully address algorithmic bias used responsibly however risk introducing positive discrimination illegal equality act organisations consider legal implications mitigation tools drawing industry guidance legal advice guidance organisation leaders boards responsible governance organisations deploying using algorithmic tools support significant decisions individuals ensure leaders place accountability understanding capabilities limits tools considering carefully whether individuals fairly treated process tool forms part making conscious decision appropriate levels human involvement process putting structures place gather data monitor outcomes fairness understanding legal obligations carried appropriate impact assessments especially applies public sector citizens often choice whether use service decisions made individuals often bias algorithmic enabling fair innovation centre data ethics introduction clear evidence wider public commentary research many organisations aware potential bias issues keen take steps address however picture variable across different sectors organisations many feel right enablers place take action organisations uncertain approach issues fairness including associated reputational legal commercial issues improve fairness decisionmaking needs easy possible organisations identify address bias number factors required help build algorithmic tools machine learning models fairness mind sufficient diversity workforce understand potential issues bias problems cause availability right data understand bias data models access right tools approaches help identify mitigate bias ecosystem expert individuals organisations able support governance structures anticipate risks build opportunities consider wider impact algorithmic tool affected confidence efforts behave ethically challenging bias lawfully eliminating discrimination attract support organisational leadership relevant regulatory bodies strategies achieved individual organisations wider ecosystem needs enable act way effective commercially viable always better acknowledge biases understand underlying causes address far possible correct approach ensuring fairness algorithmic tool depend strongly use case context notion considered fair much legal ethical philosophical idea mathematical one never holistic applicable across cases good practice team follow seeking ensure fairness algorithmic tool investigate issue chapter review bias algorithmic enabling fair innovation centre data ethics increasing recognition algorithms cause bias alone rather technology may encode amplify human biases one strongest themes responses call wider research engagement need diverse technology workforce better able interrogate biases may arise throughout process developing deploying operating algorithmic tool diverse teams biases likely identified less likely replicated systems lot make technology sector diverse report tech nation found tech workers perhaps worrying little changed last years compared sectors engineering seen significant increase proportion women becoming engineers gender gap similarly represented senior levels tech companies although representation people bame backgrounds proportionate population broken ethnicity see black people underrepresented margin priority improve representation organisations also undertake research understand ethnicity intersects characteristics well whether representation mirrored senior less data forms diversity spurred calls greater focus disability inclusion within tech similarly work needs done terms age socioeconomic background geographic spread across important note technology sector well areas example tech workforce much international many workforce relevant algorithmic course limited technology professionals diverse range skills necessary within teams organisations properly experience benefits diversity equality beyond training recruitment technology companies need support workers building inclusive workplaces key retaining well attracting talented staff different backgrounds workforce diversity cdei cdei bias review call evidence summary responses tech nation diversity inclusion tech companies ibid new statesman tech london tech week missing voice tech nation diversity inclusion tech companies although representation people bame backgrounds proportionate population broken ethnicity see black people underrepresented margin review bias algorithmic enabling fair innovation centre data ethics see tech talent charter million boost diversity tech roles innovation online training adults comprehensive list see lot activities aimed improving current landscape government providing financial support variety initiatives including tech talent founded group organisations wanting work together create meaningful change diversity tech currently charter around signatories ranging small big businesses intending grow end government also launched million digital skills innovation fund specifically helping underrepresented groups develop skills move digital jobs government office council conducting wide range work area including helping drive diversity tech workforce well recently securing million funding students underrepresented backgrounds study related recommendations government recommendation government continue support invest programmes facilitate greater diversity within technology sector building current programmes developing new initiatives gaps also huge number industry initiatives nonprofits aimed encouraging supporting underrepresented groups technology approaches people supporting efforts already helping raise profile tech diversity problem well supporting people want either move tech sector develop within government industry support work better advice industry organisations building deploying algorithmic decisionmaking tools make increased diversity workforce priority applies data science roles also wider operational management oversight roles proactive gathering use data industry required identify challenge barriers increased diversity recruitment progression including senior leadership increasing momentum around initiatives springing within government grassroots campaigns hope soon see measurable improvement data diversity tech huge number industry initiatives nonprofits aimed encouraging supporting underrepresented groups technology sector review bias algorithmic enabling fair innovation centre data ethics kilbertus gascon censor veale gummadi weller blind justice fairness encrypted sensitive attributes international conference machine learning icml see example uri notice html respondents comfortable sharing information age sex ethnicity sharing disability religious belief sex information new employers order test prevent unintentional bias issue key part understanding whether process achieving fair outcomes measurement organisations may need compare outcomes across different demographic groups assess whether match expectations organisations must data demographic characteristics groups making decisions recruitment especially public sector collection protected characteristic data defined equality act monitoring purposes become less common sectors removing collecting protected characteristic data ensure fair outcomes although removes possibility direct discrimination may make impossible evaluate whether indirect discrimination taking place highlights important tension avoid direct discrimination part process protected characteristic attributes considered algorithm order assess overall outcome hence assess risk indirect discrimination data protected characteristics calls wider data collection reflecting acceptance helps promote fairness equality areas bias could cdei supports calls think greater collection protected characteristic data would allow fairer algorithmic many circumstances section explore issues need overcome make happen often need monitor outcomes important even algorithm involved decision introduction algorithms makes pressing machine learning detects patterns find relationships data humans may see able fully understand although machine learning models optimise objectives given human data analysed reflects historical subconscious bias imposed blindness prevent models finding perhaps obscure relationships could lead similarly biased outcomes encoding future decisions repeatable way therefore right time investigate organisational biases take actions required address number reasons organisations currently collecting protected characteristic data including concerns perceptions collecting protected characteristic data permitted data protection law incorrect seems common perception see discussion may difficult justify collection data protection law store use data appropriate way separate main decisionmaking process service users customers want share data may concerned asked survey work suggests necessarily true although may elsewhere data could provide evidence base organisational outcomes biased whether new algorithmic process historically section consider needed overcome barriers organisations public private sector collect use data often responsible way organisations need collect use protected characteristic data services may require assessment may find inclusion harm good however many engage collection present protected characteristic data monitoring outcomes review bias algorithmic enabling fair innovation centre data ethics centre data ethics innovation protection concerns research suggests degree confusion data protection law affects collection retention use protected characteristic data data protection law sets additional conditions processing special category data includes many protected characteristics equality act discussed chapter well forms sensitive data currently protected characteristics biometric data figure overlap protected characteristics equality law special categories personal data data protection lawreview bias algorithmic enabling fair innovation centre data ethics organisations spoke believed data protection requirements prevent collection processing use special category data test algorithmic bias discrimination case data protection law sets specific conditions safeguards processing special category data explicitly includes use monitoring equality collection processing use special category data allowed substantial public interest among specific purposes set data protection law schedule data protection act sets specific public interest conditions meet requirement including equality opportunity treatment act allows processing special category data necessary purposes identifying keeping review existence absence equality opportunity treatment groups people specified relation category view enabling equality promoted maintained schedule notably provision also specifically mentions equality rather discrimination allows data address broader fairness equality considerations rather discrimination defined equality human rights law however provision data protection act clarifies allow using special category data individual decisions schedule causes substantial damage distress individual schedule addition collection data retention also needs thought organisations may want monitor outcomes historically disadvantaged groups time would require longer data retention periods needed individual use cases may lead tension monitoring equality data protection practice restrictions much less onerous sometimes described organisations recently published ico guidance data sets approaches assessing issues including guidance special category data section report sets details equality data protection law apply algorithmic data protection law sets specific conditions safeguards processing special category data explicitly includes use monitoring equality ico guidance data protection bias algorithmic enabling fair innovation centre data ethics innovation open data institute monitoring equality digital public services open data institute protected characteristics practice equality human rights commission public sector equality duty data protection equality human rights commission public sector equality duty need guidance diversity monitoring recruitment pockets public sector increasingly viewing collection data protected characteristics essential monitoring unintentional discrimination services open data institute recently explored public sector organisations consider collecting protected characteristic data help fulfil responsibilities public sector equality recognise accepted practice collecting publishing data uses digital services makes hard tell whether discriminate ehrc provides deal data protection issues collecting data support obligations public sector equality duty yet update significant changes data protection law gdpr data protection act consider implications algorithmic data collection needs addressed also consistent guidance public sector organisations wanting collect protected characteristic data specifically equality monitoring purposes become standard practice practice essential testing algorithmic discrimination protected groups organisations need assured following guidance making systems fair reducing legal risk also minimising unintended consequences personal data collection use thus helping maintain public trust picture complicated private sector organisations legal responsibility public sector equality equalities law requires organisations avoid discrimination little guidance practically identify algorithmic contexts without guidance psed private sector organisations manage different expectations customers employees investors public measure manage risks algorithmic also concerns balancing fairness privacy interviews financial institutions many focused principles data minimisation within data protection legislation cases felt collecting data may inappropriate even data touch upon tools models insurance example concerns around public trust whether providing information could affect individual insurance premium organisations think carefully introduce processes secure trust transparent possible data collected used stored people access control data building public trust difficult especially looking assess historic practices may hide potential liabilities organisations may fear collecting data identify expose patterns historic bad practice however data provides key means addressing issues bias discrimination therefore reducing risk long term although public services normally sit within single national jurisdiction private organisations may international different jurisdictions different requirements collection protected characteristic data may even prohibited french constitutional council example prohibits data collection processing regarding race religion international organisations may need help satisfy specific nationally devolved regulation exceptions general principle collection protected special characteristic data good thing cases action needed urgently required due context entirely obvious biases collecting protected characteristic data unnecessary others may seen disproportionately difficult gather relevant data identify bias others still may impossible provide privacy small groups small number service users customers particular characteristic overcoming navigating barriers concerns require combination effective guidance strong promotion new norms centralised authority even regulatory compulsion review bias algorithmic enabling fair innovation centre data ethics government recommendation government work relevant regulators provide clear guidance collection use protected characteristic data outcome monitoring processes encourage use guidance data address current historic bias key sectors alternative approaches guidance first step innovative thinking may needed new models collecting protecting inferring protected characteristic data models include safe public collecting protected characteristic data behalf organisations securely testing algorithms processes without ever providing data companies could responsibility relevant sector regulator government organisation office national statistics also models private sector company could collect store data securely offering individuals guarantees privacy purpose carrying testing behalf companies third party service organisations collect protected characteristic data explicitly sometimes infer data example extracting likely ethnicity individual name postcode used within actual process proxies present key bias risks using information relation individual presents substantial issues transparency accuracy appropriateness agency cases collecting protected characteristic data infeasible identifying proxies protected characteristics purely monitoring purposes may better option keeping processes blind however clear risks around potential type monitoring undermine trust organisations need think carefully proceed ethically legally responsibly inferred personal data data protection law still legally personal data thus subject relevant laws issues described right reasonable inference current academic development needed around concepts models exist would work practice legal clarity necessary first step followed economic viability technical capability security public trust however models success work ons secure research service described nhs data safe well ongoing research projects successful model could developed private sector companies would able audit algorithms bias without individuals required hand sensitive data multiple organisations believe research needed develop proposal role auditing consider future cdei work area kilbertus gascon kusner veale gummadi weller blind justice fairness encrypted sensitive attributes international conference machine learning icml ico need ensure lawfulness fairness transparency systems wachter sandra mittelstadt brent right reasonable inferences data protection law age big data columbia business law review nhs research scotland data safe alan turing institute enabling trust models differential privacy ongoing successful model could developed private sector companies would able audit algorithms bias without individuals required hand sensitive data multiple organisations review bias algorithmic enabling fair innovation centre data ethics office national statistics ons methodology working paper series number synthetic data pilot baseline data organisations determine collecting protected characteristics appropriate assessing bias often need collect information service users customers compare relevant wider often national demographic data hard tell decision negative effect group without sense considered normal lack relevant representative wider data make difficult public private organisations tell processes biased develop responsible algorithmic tools response relevant data already made available publicly including census survey data published ons devolved administrations also made significant volumes data widely accessible number government departments sectorspecific datasets portals add landscape others detailed population data accessed ons secure research service provides wide variety national scale information including survey administrative data resources usage service managed safes safe projects people settings data outputs framework restricted purposes research evaluation analysis often restricts access academic research groups may opportunities widen service support evaluation diversity outcomes regulators delivery organisations regulators help promoting key public datasets specific value sector along guidance material accessible industry wider availability aggregate demographic information business use would also allow better data gathering better synthetic data generation publicly available synthetically augmented plausible versions surveys beyond labour force would help users find develop use cases government announcements included million three years help ons share higherquality data across government link combine datasets new ways example inform policy evaluate interventions recommendations government recommendation government ons open secure research service broadly wider variety organisations use evaluation bias inequality across greater range activities short term organisations find publicly held data insufficient need engage partnership peers bodies hold additional representative demographic information create new resources private sphere approaches include industry specific data sharing initiatives open banking finance presumed open energy discussion better regulation executive trusted sectorspecific data intermediaries recommendations government recommendation government support creation development public private partnerships especially focused identification reduction biases issues specific groups office national statistics government statistical service work partnerships regulators promote harmonised principles data collection use private sector via shared data standards development review bias algorithmic enabling fair innovation centre data ethics centre data ethics innovation open banking open banking consumer perspective study open banking competition markets authority cma intervened ecosystem require nine largest banks grant direct transaction level data access licensed startups although compliance enforcement sits cma open banking represents regulatory partnership much data partnership fca providing financial oversight ico providing data protection open banking led regulated companies providing new services including financial management credit scoring result access credit debt advice financial advice likely widen turn expected allow better service provision underrepresented groups provides opportunity address unfairness systemic biases new forms digital exclusion bias may yet wider partnerships include projects within administrative data research programme bringing together government academia public bodies increasing number developmental sandboxes aimed industry government support see section new data ecosystems created around serviceuser data organisations like new global open finance centre excellence provide coordination research support empowering organisations share data trusted bodies enable industry wide implementation simple specific common data regimes relatively quick wins achievable sectors open data standards active development open banking open energy open banking led regulated companies providing new services including financial management credit scoring result access credit debt advice financial advice likely widen turn expected allow better service provision groups review bias algorithmic enabling fair innovation centre data ethics centre data ethics innovation house commons justice committee court tribunal reforms second report session lord chancellor lord chief justice senior president tribunals transforming justice system administrative data research data first harnessing potential linked administrative data justice system ongoing study monitoring bias digital transformation courts accessing protected characteristic data monitor outcomes necessary introducing algorithmic decisionmaking also making major changes significant processes majesty courts tribunal service hmcts currently undergoing digital transformation aimed largely making court system affordable fair including online dispute resolution automated fixed penalties minor offences guilty part transformation recognised need information people entering exiting judicial protected characteristic data would allow hmcts assess effectiveness different interventions level dependency uptake different parts judicial system within different groups senior justices would largely prefer see general reduction number people going criminal courts greater diversity use civil courts hard objectively measure outcomes whether courts acting fairly without bias without data order achieve goals hmcts focused access protected characteristic data predominantly data linkage inference wider administrative data worked government statistical service harmonisation team academic researchers rebuild data architecture support resulting information intended valuable ministry justice designing fair interventions functioning courts also eventually made available independent academic research via administrative data research office national statistics one example drive toward new forms data collection designed test assure fair processes services within public bodies also illustrative project navigating data protection act substantial public interest provision gdpr assess risks around legal exposure essential public bodies establish whether digital services involve personal data classed statistical research sit within legislative especially true dealing data necessarily accompanied consent majesty courts tribunal service hmcts currently undergoing digital transformation aimed largely making court system affordable fair including online dispute resolution automated fixed penalties minor offences guilty review bias algorithmic enabling fair innovation detecting mitigating bias centre data ethics innovation contract ref text html previous section argue preferable seek identify bias address rather hope avoid unawareness high level focus area academic literature increasing number practical algorithmic fairness tools appeared last three years approaches detecting bias include comparing training population datasets see representative analysing drivers differences outcomes likely cause bias example could shown certain recruiters organisation held measurable biases compared recruiters controlling characteristics would possible train algorithm less biased subset data excluding biased group analysing relevant model variables correlate different groups example qualifications factor model recommending recruitment analysis show extent results job offers made particular groups different approaches necessary different contexts organisations bias monitoring analysis necessary part whether algorithmic monitoring suggests biased process question address ensuring data collected section necessary sufficient important first step methods detailed need proportionate organisational need directly mitigate bias models number interventions disposal generally positive development ecosystem complex organisations see need clarity mitigation tools techniques appropriate legal circumstances crucially missing practical guidance create deploy monitor audit adjust fairer algorithms using effective tools techniques available important recognise growing literature toolsets algorithmic fairness often address part issue quantified wider interventions promote fairness equality remain key success part review faculty analyse assess compare various technical approaches bias mitigation section informed technical work outputs work made available case study bias detection medical school case mentioned section interesting example bias detection program developed match human admissions decisions accuracy despite bias school still higher proportion students admitted london medical schools human admissions officers biases would probably never demonstrated use program medical school equipped current understanding assess algorithm bias motivated perhaps would able use algorithm reduce bias rather propagate organisations bias monitoring analysis necessary part decisionmaking whether algorithmic bias algorithmic enabling fair innovation centre data ethics note machine learning literature fairness terms used throughout report take specific often narrower definitions discrimination sometimes used refer different outcomes different groups statistical ability distinguish bias favourable unfavourable treatment group statistical quantitative properties field study create mathematical system unbiased called algorithmic fairness report use discrimination bias common language sense defined chapter rather statistical meanings note concept fairness discussed section narrower described definitions fairness want model development include definition fairness must tell relevant model definition measure however single mathematical definition fairness apply result academic literature seen dozens competing notions fairness introduced merits drawbacks many different terminologies categorising notions none complete ultimately humans must choose notions fairness algorithm work taking wider notions considerations account recognising always aspects fairness outside statistical definition fairness definitions grouped notion fairness sought stage development involved first instance fall broad categories procedural outcome fairness discussed section within technical aspects machine learning procedural fairness approaches often concern information used system thus include fairness unawareness rarely effective strategy statistical concept fairness applied algorithms focused achieving unbiased outcomes rather concepts fairness explicit measurement equality across results different groups necessary approaches within outcome fairness make additional distinctions causal observational notions fairness well individual group notions individual notions compare outcomes individuals see treated differently however circumstances generally highly specific individuals making difficult compare without common features group notions aggregate individual outcomes common feature group compare aggregated outcomes group individual notions mutually exclusive idealised fair algorithm could achieve simultaneously observational approaches deal entirely measurable facts system whether outcomes decisions data mathematical definitions types model causal approaches consider effects different choices interventions typically requires deeper understanding system algorithm interacts technical review area faculty describe way categorise different bias mitigation strategies within notions fairness see table also identified four group observational notions highlighted currently practical approaches implement developers relatively easy compute providing meaningful measures simple differences groups necessarily mean appropriate choice fairness definition contexts majority existing bias mitigation tools available developers address conditional demographic parity equalised odds focused removing sensitive attributes data ultimately humans must choose notions fairness algorithm work taking wider notions considerations account recognising always aspects fairness outside statistical bias algorithmic enabling fair innovation centre data ethics example different definitions play practice seen criminal justice system per following case individual demographic parity outcomes different protected groups equally distributed statistically independent members one group likely achieve given outcome different group successes one group imply successes failures another decision level demographic parity might mean proportion men women applying loans successful kind fairness also applied assigning risk scores regardless success threshold applied conditional demographic parity legitimate risk factors might mean consider fair discriminate certain groups age car insurance difficulty sits deciding factors qualify legitimate may perpetuating historical biases equalised odds separation qualified unqualified candidates treated regardless protected attributes true positive rates protected groups false positive rates chance qualified individual overlooked unqualified individual approved across protected groups however different groups different rates education repayment risk qualifier equalised odds result different groups held different standards means equalised odds capable entrenching systematic bias rather addressing calibration outcomes protected group predicted equal reliability outcomes found consistently overpredicted group possibly due lack representative data needs made calibration also capable perpetuating demographic parity conditional demographic parity equalised odds calibration sub group fairness individual fairnesscasual unresolved discrimination proxy discrimination meritocratic fairness counterfactual fairnessthe table shows commonly used approaches examples sit within wider definitions described visual demonstrations notions shown web app accompanies cdei training biased model bias algorithmic enabling fair innovation centre data ethics innovation dieterich william mendoza christina brennan tim compas risk scales demonstrating accuracy equity predictive sam pierson emma feller avi goel sharad huq aziz algorithmic cost fairness larson jeff mattu surya kirchner lauren angwin julia analyzed compas recidivism kleinberg jon mullainathan sendhil raghavan manis inherent fair determination risk scores chouldechova alexandra fair prediction disparate impact study bias recidivism prediction instruments study compas given risk score compas criminal recidivism proportion defendants reoffend roughly independent protected attribute including ethnicity calibration otherwise risk score white person would mean something different black person propublica criticism model highlighted black defendants reoffend roughly twice likely given scores indicating risk recidivism white defendants however ensuring equal risk scores among defendants offend equalised odds would result losing calibration least degree fully satisfying measures proves attributes individuals protected otherwise apparently linked recidivism generally equality opportunity generalised form equalised odds calibration model satisfies calibration risk category proportion defendants reoffend regardless race way achieving recidivism rate higher one group individuals group predicted highrisk consequently means model make false positives group others meaning equalised odds satisfied similarly recidivism model satisfies demographic parity chance defendant ends particular risk category regardless race one group higher recidivism rate others means models must make false negatives group maintain demographic parity means equalised odds satisfied similar arguments apply notions worth noting none fairness metrics take account whether given group likely arrested another treated differently given prosecution service example serves illustrate mutual incompatibility many metrics distinct limitations context model satisfies calibration risk category proportion defendants reoffend regardless race review bias algorithmic enabling fair innovation centre data ethics calders toon kamiran faisal pechenizkiy mykola building classifiers independency constraints icdmw proceedings ieee international conference data mining workshops zemel rich swersky kevin pitassi toni dwork cynthia learning fair representations international conference machine learning larson jeff mattu surya kirchner lauren angwin julia analyzed compas recidivism sam pierson emma feller avi goel sharad huq aziz algorithmic cost fairness bias organisation understood different statistical definitions fairness relevant context relate institutional goals used detect potentially mitigate bias statistical approaches detection protocols interventions take place algorithm deployed protocols interventions generally concern training data aiming detect remove sources unfairness model built modified data used algorithmic approach changes exist fundamental level however nature given application inform data definitions fairness used organisation seeks equalise odds particular outcomes different groups equalised odds approach needs informed outcomes prior round model output interventions present machine learning literature incorporate model outcomes inputs use protected attributes methods require access protected attributes training data test finance clear companies place emphasis detecting mitigating bias stages carefully selecting variables involving human judgement loop methods applied model training analyse affect way model operates typically involves model architecture training objectives including potential fairness metrics modification often retraining model intensive process resulting high level specification particular problem allow models retain higher level performance sometimes new goals methods constrained optimisation used address demographic parity equalised odds rapidly evolving field often highly model dependent also biggest opportunity terms systemic fairness many approaches need formalised incorporated commonly used toolsets importantly accompanied legal certainty see next page approaches concern model outputs seeking detect correct unfairness decisions approach requires scores decisions original model corresponding protected attributes labels otherwise describe data used approaches usually without model modification retraining however effectively flag treat symptoms bias original causes also often disconnected model development relatively easy distort making risky deployed part broader oversight process different interventions different stages sometimes combined striving achieve baseline level fairness model via looking bias particularly sensitive important decisions postprocessing attractive approach care must taken however combinations interventions hinder bias mitigation methods stage intervention notion fairness shown appendix detailed references found faculty bias identification mitigation algorithms published separately protocols interventions generally concern training data aiming detect remove sources unfairness model built review bias algorithmic enabling fair innovation centre data ethics kleinberg jon mullainathan sendhil raghavan manis inherent fair determination risk scores challenges number challenges facing organisations attempting apply statistical notions fairness practical situations using statistical notions fairness appropriately statistical definitions fairness deliver specific results specific constraints struggle encompass wider characteristics lend mathematical formulation clear framework logistical legal selecting definitions decisions measure fairness impose need extensive contextual understanding domain knowledge beyond issues data science first instance organisations strive understand stakeholder expectations around fairness scale impact retention agency consider setting desired outcomes given practitioner forced extent choose mathematically trade different definitions techniques inform trade currently limited although exceptions seems gap literature regarding different notions fairness general maturity industry making decisions holistic way relying data science teams make isolation compatibility accuracy large part machine learning literature fairness concerned fairness accuracy ensuring introduction fairness metrics minimal impacts model accuracy pure statistical sense often genuine imposing constraint fairness may lower statistical accuracy rate often false thinking holistically applying fairness constraint recruitment algorithm sifting cvs might lower accuracy measured loss function large dataset necessarily mean company recruiting sifting worse candidates company sense accuracy free historic bias effects accuracy relevant even models attempt satisfy fairness metrics ways run counter wider notions fairness random allocation positions company would likely satisfy demographic parity would generally considered fair implementing specific fairness measure also fundamentally changes nature model trying achieve may make models less accurate compared prior versions apparent incompatibility lead models seen less desirable less effective making choices replicate past debiasing credit models might require accepting higher risk loans thus greater capital reserves mentioned choices exist isolation accuracy fairness issue notions accuracy based average outcomes swayed outcomes specific usually large demographic groups may miss conceal substantial biases unexpected less evident parts model output accuracy one individual always mean accuracy another organisations need consider round understand limitations purely statistical notions fairness accuracy clear decision making framework logistical legal selecting definitions decisions measure fairness impose need extensive contextual understanding domain knowledge beyond issues data science review bias algorithmic enabling fair innovation centre data ethics causality reasons unfairness causes unfairness part definitions must assessed organisational level techniques look outcomes understand come biases may exist except specific circumstances defining fairness based causal developed limited due difficulty validating underlying apparent causal factors definition factors introduce bias especially less well understood groups less data static measurement unintended consequences definitions fairness static sense measure snapshot population particular moment time however static view fairness neglects decisions real world taken sequence making intervention model predictions results way decisions implemented cause population change time failing account risks leading interventions actively cases supposedly fair intervention could lead greater scope unintended consequence strategic manipulation part individuals however cost manipulation typically higher disadvantaged group differing costs manipulation result disparities protected groups implementing process tackle unfairness organisations must deploy sufficient contextaware oversight development teams must ask inadvertently created potential new kinds bias checking back reference data especially useful longer time techniques look outcomes understand come biases may exist kusner matt joshua loftus russell chris silva ricardo counterfactual fairness advances neural information processing systems pdf kilbertus niki mateo parascandolo giambattista hardt moritz janzing dominik schölkopf bernhard avoiding discrimination causal reasoning advances neural information processing systems garg sahaj perot vincent limtiaco nicole taly ankur chi beutel alex counterfactual fairness text classification robustness conference ethics society chiappa silvia gillam thomas counterfactual fairness aaai conference artificial intelligence russell chris kusner matt loftus joshua ricardo silva worlds collide integrating different counterfactual assumptions fairness advances neural information processing systems edited guyon luxburg bengio wallach fergus vishwanathan garnett curran associates kusner matt joshua loftus russell chris silva ricardo counterfactual fairness advances neural information processing systems pdf liu lydia dean sarah rolf esther simchowitz max hardt moritz delayed impact fair machine learning international conference machine learning lily immorlica nicole wortman vaughan jennifer disparate effects strategic manipulation acm conference fairness accountability transparency bias algorithmic enabling fair innovation centre data ethics ico guidance data protection policy issues although bias mitigation techniques seem complex mathematical encoding fundamental policy choices concerning organisational aims around fairness equality legal risks organisations must bring wide range expertise making decisions better set common language understanding machine learning equality law communities would assist seeking detect bias processes address good thing however need care bias mitigation techniques listed applied interventions affect outcomes decisions individuals even intent improve fairness must done way compatible data protection equality law many algorithmic fairness tools currently use developed regulatory regime based different set principles includes different ideas fairness rely threshold levels notably rule enable affirmative action address imbalances tools developed may fit purpose legal jurisdictions advice industry organisations operating within deploy tools developed must mindful relevant equality law along across much europe different uncertainty presents challenge organisations seeking ensure use algorithms fair legally compliant guidance needed area example general need clarity interpretation discussed chapter understanding current position protection law bias mitigation interventions discussed involve processing personal data therefore must lawful basis data protection law broadly speaking considerations apply use data must collected processed stored lawful fair transparent manner specific explicit legitimate purposes terms use must adequately communicated people describes ico provided guidance ensure processing type complies data protection law along examples recently published guidance data processing data support development fair algorithms legitimate objective provided lawful equality act see next page broadly speaking right controls put place data protection law seem present barrier techniques nuances consider especially preprocessing interventions involving modification labels training data usual circumstances modifying personal data inaccurate would inappropriate however alterations made training data anonymised used outside model development contexts justified data protection legislation care taken required care might include ensuring model features related back individual information stored never used directly make decisions individual lawful basis processing data way support training model whether consent another basis even intent improve fairness must done way compatible data protection equality bias algorithmic enabling fair innovation centre data ethics ico guide general data protection regulation gdpr special category data care needed dealing special category data requires additional protections data protection special category data allowed used measuring bias explicitly excludes decisions individuals would include many mitigation techniques particularly processing instead automated processing special category data would need rely explicit consent subjects one small number explicit exceptions enough rest proportionate means legitimate ends provision case fairer models otherwise applies equality law position regarding equality act less clear mitigation approaches discussed section intended reduce bias including indirect discrimination however risk techniques used could cause new direct discrimination even positive discrimination promote equality disadvantaged group generally illegal equality act yet possible give definitive general guidance exactly techniques would would legal given situation organisations need think basis issues consider might include explicit use protected characteristic relevant proxies reweight models achieve fairness metric applications feature modification decision threshold modification carries risk organisations need think whether consequence using technique could disadvantage individual explicitly basis protected characteristic direct discrimination otherwise place individuals disadvantage lead indirect discrimination resampling data ensure representative set inputs likely acceptable even disparate impact across different groups potential discrimination would indirect likely justifiable proportionate means legitimate end though need caution legal risk attempting mitigate bias overplayed organisation aim legitimate decisions address taken carefully due regard requirements equality act law generally supportive involving broad team decisions documenting equality impact assessment good practice bias exists organisation identify nondiscriminatory approach mitigate seems ethical responsibility done level machine learning model wider action may required organisations developing deploying algorithmic ensure mitigation efforts lead direct discrimination outcome differences without objective justification despite complexity algorithmic fairness approaches essential facilitate widespread adoption algorithmic advice industry organisations face historical issues attract significant societal concern otherwise believe bias risk need measure outcomes relevant protected characteristics detect biases algorithmic otherwise must address uncovered direct discrimination indirect discrimination outcome differences protected characteristics lack objective justification organisations ensure mitigation efforts produce new forms bias discrimination many bias mitigation techniques especially focused representation inclusion legitimately lawfully address algorithmic bias used responsibly however risk introducing positive discrimination illegal equality act organisations consider legal implications mitigation tools drawing industry guidance legal need caution legal risk attempting mitigate bias bias algorithmic enabling fair innovation centre data ethics best approach depends strongly use case context interviews organisations finance sector reveal commonly used approach companies use mix external tools general appetite adapting tools internal uses among companies consulted none developed tools scratch recruitment found vendors machine learning tools established processes examining models bespoke tools elaborate processes three stages checks dummy data sampled data models prior deployment post deployment checks anonymised data customers used adjustments correction audits conducted academic institutions particularly focused identifying sources bias firms used mixture proprietary techniques software test models terms mitigation lot done within current legislative framework regulators need keep eye way law applied guidance needed guide ethical innovation whether law might need change future engagement public industry required many sectors identify notions fairness bias mitigation approaches acceptable desirable recommendations regulators recommendation sector regulators industry bodies help create oversight technical guidance responsible bias detection mitigation individual sectors adding contextspecific detail existing guidance data protection new guidance equality act think likely significant industry ecosystem need develop skills audit systems bias part highly specialised skill organisations able support part important consistency problem addressed part regulatory standards sectors may require independent audit systems elements ecosystem might licenced auditors qualification standards individuals necessary skills audit bias likely form part broader approach audit might also cover issues robustness explainability engagement public industry required many sectors identify notions fairness bias mitigation approaches acceptable bias algorithmic enabling fair innovation centre data ethics anticipatory governance within organisation especially large one good intentions individual teams often insufficient ensure organisation whole achieves desired outcome proportionate level governance usually required enable look like context approach unlike areas health safety security management yet agreed standard approach include however increasing range tools approaches available clear given pace change wide range potential impacts governance space must anticipatory anticipatory governance aims foresee potential issues new technology intervene occur minimising need advisory adaptive approaches responding new technologies deployment tools ways working organisations already exist help proactively iteratively test approaches emerging challenges still active development goal reduce amount individual regulatory corrective action replace collaborative solutions reduce costs develop best practice good standards policy practice practical terms assessment impacts risks consultation affected parties core within individual organisations however critical simply followed tick box procedures organisations need show genuine curiosity short medium long term impacts increasingly automated ensure considered views wide range impacted parties within organisation wider society assessments must consider detail algorithm implemented whether appropriate circumstances interacts human many published frameworks sets guidance offering approaches structuring governance including guidance gds alan turing institute targeted primarily public different approaches appropriate different organisations key questions covered include following guidance organisation leaders boards responsible governance organisations deploying using algorithmic tools support significant decisions individuals ensure leaders place accountability understanding capabilities limits tools considering carefully whether individuals fairly treated process tool forms part making conscious decision appropriate levels human involvement making process putting structures place gather data monitor outcomes fairness understanding legal obligations carried appropriate impact assessments especially applies public sector citizens often choice whether use service decisions made individuals often list far exhaustive organisations consider factors early part governance process better placed form robust strategy fair algorithmic deployment chapters discuss specific assessment processes data protection impact assessments equality impact assessments human rights impact assessments provide useful structures see example many highlighted open source list curated institute ethical machine learning leslie david understanding artificial intelligence ethics safety alan turing institute review bias algorithmic enabling fair innovation regulatory environment chapter centre data ethics innovation findings regulation help address algorithmic bias setting minimum standards providing clear guidance supports organisations meet obligations enforcement ensure minimum standards met presents genuinely new challenges regulation brings question whether existing legislation regulatory approaches address challenges sufficiently well currently little case law statutory guidance directly addressing discrimination algorithmic current regulatory landscape algorithmic consists equality human rights commission ehrc information commissioner office ico sector regulators industry bodies stage believe need new specialised regulator primary legislation address algorithmic bias however algorithmic bias means overlap discrimination law data protection law sector regulations becoming increasingly important particularly relevant use protected characteristics data measure mitigate algorithmic bias lawful use bias mitigation techniques identifying new forms bias beyond existing protected characteristics sectorspecific measures algorithmic fairness beyond discrimination existing regulators need adapt enforcement algorithmic provide guidance regulated bodies maintain demonstrate compliance algorithmic age regulators require new capabilities enable respond effectively challenges algorithmic larger regulators greater digital remit may able grow capabilities others need external government recommendation government issue guidance clarifies equality act responsibilities organisations using algorithmic include guidance collection protected characteristics data measure bias lawfulness technical bias mitigation techniques recommendation development guidance implementation government assess whether provides sufficient clarity organisations obligations leaves sufficient scope organisations take actions mitigate algorithmic bias government consider new regulations amendments equality act address recommendations regulators recommendation ehrc ensure capacity capability investigate algorithmic discrimination protected groups may include ehrc reprioritising resources area ehrc supporting regulators address algorithmic discrimination sector additional technical support ehrc recommendation regulators consider algorithmic discrimination supervision enforcement activities part responsibilities public sector equality duty recommendation regulators develop compliance enforcement tools address algorithmic bias impact assessments audit standards certification regulatory sandboxes recommendation regulators coordinate compliance enforcement efforts address algorithmic bias aligning standards tools possible could include jointly issued guidance collaboration regulatory sandboxes joint regulatory environment summaryreview bias algorithmic regulatory environment centre data ethics centre data ethics innovation industry industry bodies standards organisations develop ecosystem tools services enable organisations address algorithmic bias including standards auditing certification services algorithmic systems organisations developers create future cdei work cdei plans grow ability provide expert advice support regulators line existing terms reference include supporting regulators coordinate efforts address algorithmic bias share best practice cdei monitor development algorithmic extent new forms discrimination bias emerge include referring issues relevant regulators working government issues covered existing regulators need adapt enforcement algorithmic provide guidance regulated bodies maintain demonstrate compliance algorithmic age review bias algorithmic regulatory environment centre data ethics introduction report shown problem algorithmic bias ways organisations try address problem good reasons organisations address algorithmic bias ranging ethical responsibility pressure customers employees useful incentives companies try right thing extend beyond minimum standards creating competitive advantage firms earn public trust however regulatory environment help organisations address algorithmic bias three ways government set clear minimum standards legislation prohibits unacceptable behaviour government regulators industry bodies provide guidance assurance services help organisations correctly interpret law meet obligations finally regulators enforce minimum standards create meaningful disincentives organisations fail meet obligations alternatively regulatory environment unclear requirements weak enforcement creates risk organisations inadvertently break law alternatively risk prevents organisations adopting beneficial technologies situations barriers ethical innovation addressed clear supportive regulation technologies present range new challenges regulators rapid development new algorithmic systems means interact many aspects daily lives technologies power transform relationship people services across industries introducing ability segment populations using algorithms trained larger richer datasets however seen work risks approaches reinforcing old biases introducing new ones treating citizens differently due features beyond control ways may aware regulatory approach every sector takes place individuals need adapt respond new practices algorithmic brings given widespread shift necessary reflect whether existing regulatory legislative frameworks sufficient deal novel challenges well compliance enforcement may operate increasingly world example regulatory approaches rely individual complaints may sufficient time people always aware algorithm impacted life similarly pace change development technologies may mean certain approaches slow respond new ways algorithms already impacting people lives regulators need ambitious thinking considering ways algorithms already transforming sectors future may require government regulators already recognised need anticipatory regulation respond challenges regulation fourth industrial lays challenge need proactive flexible regulation enabling greater experimentation appropriate supervision supporting innovators actively seek compliance also details need regulators build dialogue across society industry engage global partnerships nesta regulation inclusive collaborative iterative experimental methods including sandboxes experimental testbeds use open data interaction regulators innovators cases active engagement public section look current landscape steps required regulation fourth industrial revolution department business energy industrial strategy renewing regulation anticipatory regulation age disruption nesta seen work risks approaches reinforcing old biases introducing new ones treating citizens differently due features beyond control ways may aware bias algorithmic regulatory environment centre data ethics current landscape regulatory environment made multiple regulators enforcement agencies inspectorates ombudsmen report call regulators simplicity range responsibilities powers accountabilities regulators typically granted powers primary legislation established although private regulators may set industry selfregulation regulators explicit remit address bias discrimination enabling legislation others may need consider bias discrimination decisionmaking regulating sectors practice however mixed picture responsibility prioritisation issue algorithms necessarily replace mechanisms wholesale instead fit existing processes therefore rather new algorithmic regulatory system existing regulatory environment needs evolve order address bias discrimination increasingly world key piece legislation governs discrimination equality act act provides legal framework protect rights individuals provides discrimination law protect individuals unfair treatment including algorithmic discrimination underlying rights also set human rights act establishes european convention human rights law decision made organisation basis recorded information case significant decisions data protection act general data protection regulation gdpr also relevant legislation controls personal information used organisations businesses government sets data protection principles includes ensuring personal information used lawfully fairly transparently data protection law takes higher level relevance case algorithmic decisions inherently specific clauses related automated processing profiling apply see next page details data protection law takes higher level relevance case algorithmic decisions inherently specific clauses related automated processing profiling apply review bias algorithmic regulatory environment centre data ethics lord sales justice supreme court algorithms artificial intelligence law sir henry brooke lecture baili support legislation two primary regulators equality human rights commission ehrc equality act human rights act information commissioner office ico data protection act gdpr however given range types decisions made use algorithmic tools clearly limit far regulators define oversee acceptable practice many sectors significant decisions made individuals specific regulatory framework oversight decisions made sector regulators clear role play algorithmic bias ultimately issue decisions made organisations inherently sectors algorithmic already significant relevant enforcement bodies already considering issues raised algorithmic tools carrying dedicated research increasing internal skills capability respond overall picture complex reflecting overlapping regulatory environment different types decisions called new algorithms regulator example lord sales supreme believe best response issue bias given many regulatory challenges raised inevitably typically algorithms form part overall decisionmaking process regulated sector level however coordinated support alignment regulators may required see address challenge across regulatory landscape called new algorithms regulator example lord sales supreme court believe best response issue bias given many regulatory challenges raised inevitably bias algorithmic regulatory environment centre data ethics legal background detailed discussion direct indirect discrimination see section equality human rights commission public sector equality duty see cdei providing expert input work equality act equality act act legally protects people discrimination sets nine protected characteristics unlawful discriminate basis age disability gender reassignment marriage civil partnership pregnancy maternity race religion belief sex sexual orientation act prohibits direct discrimination indirect discrimination victimisation harassment based also establishes requirement make reasonable adjustments people disabilities allows require positive action enable encourage participation disadvantaged groups act also establishes public sector equality requires public sector bodies address inequality activities act effect england wales scotland although northern ireland similar principles covered different legislation legal differences scope protected characteristics political opinions protected northern ireland thresholds indirect discrimination practical differences public sector equality duty however purpose report use language act section act requires public bodies actively consider outcomes given policy currently effect scotland commence wales next year increasingly large parts public sector contracted must show given due diligence issues ahead time part development oversight chain recent controversies exam results highlighted broad public concern disparities provisions apply area individuals treated differently regardless whether algorithm involved decision human rights act also protects discrimination human rights act establishes european convention human rights domestic law act explicitly prohibits discrimination article enjoyment rights freedoms set forth convention shall secured without discrimination ground sex race colour language religion political opinion national social origin association national minority property birth broader set characteristics notably preventing discrimination based language political opinion property however also provides narrower protection act applies specifically realising human rights act means government bodies discriminate based characteristics granting protecting rights right fair trial article freedom expression article freedom assembly article council europe recently established adhoc committee cahai consider potential legal framework support application based human rights democracy rule bias algorithmic regulatory environment centre data ethics gdpr defines data processing broadly article collection recording organisation structuring storage adaptation alteration retrieval consultation use disclosure transmission dissemination otherwise making available alignment combination restriction erasure destruction see example ico guide general data protection regulation gdpr principle lawfulness fairness transparency ico guide general data protection regulation gdpr rights related automated decision making including profiling data protection law data protection act alongside general data protection regulation gdpr regulates personal information organisations businesses government data protection act supplements tailors gdpr domestic law data protection law organisations processing personal data must follow data protection principles includes ensuring information used lawfully fairly transparently data protection law gives individuals data subjects gdpr language number rights relevant algorithmic example right find information organisations store including data used additional rights organisation using personal data fully automated processes profiling legal significant effects individuals introduction data protection act gdpr make organisations liable significant financial penalties serious breaches led strong focus data protection issues top level organisations significant supporting ecosystem guidance consultancy helping organisations comply wide range data protection provisions highly relevant generally automated widespread public commentary positive negative approaches training deploying tools compliant gdpr sets provisions relating algorithmic bias discrimination including principle data processing lawful fair article general principle personal data must processed lawfully fairly transparent manner lawfulness requirement means data processing must compliant laws including equality act fairness requirement means processing unduly detrimental unexpected misleading data subjects provisions around illegality discriminatory profiling recital gdpr advises organisations avoid form profiling results discriminatory effects natural persons basis racial ethnic origin political opinion religion beliefs trade union membership genetic health status sexual orientation processing results measures data subjects right subject solely automated process significant effects article states data subject shall right subject decision based solely automated processing including profiling produces legal effects concerning similarly significantly affects ico organisations proactive obligations bring details rights attention individuals article rights data subjects withdraw consent processing data time article right object data processing carried legal basis consent data protection legislation provides several strong levers ensure procedural fairness however inherent limitations thinking fair decisions purely lens data protection processing personal data processing significant contributor algorithmic decisions decision considerations less directly relevant data may apply data protection therefore seen entirety regulation applying algorithmic decisions efforts comply data protection law must distract organisations considering ethical legal obligations example defined equality comply data protection law must distract organisations considering ethical legal obligations example defined equality bias algorithmic regulatory environment centre data ethics financial conduct authority fair treatment customers ofcom statement making communications markets work customers framework assessing fairness broadband mobile home phone pay see section detailed discussion types unfair biasconsumer protection specific legislation beyond three acts additional laws establish fair unfair conduct specific area decisionmaking laws also apply principle conduct made supported algorithm although often untested case law consumer protection law consumer rights sets consumer rights around misleading sales practices unfair contract terms defective products services law sets standards commercial behaviour typically enforced ombudsmen regulated sectors particularly consumer facing set additional requirements fair treatment notably financial conduct authority fair treatment customers ofcom framework assessing fairness telecommunications algorithmic decisions would still remain subject rules though always clear algorithmic could meet practice requirement consumers provided clear information appropriately informed point sale straightforward apply algorithmic processes consumers confident dealing firms fair treatment customers central corporate culture less clear limitations current legislation previously discussed equality act defines list protected characteristics unlawful use basis less favourable treatment characteristics reflect evidence systematic discrimination point time evolve new forms discrimination emerge recognised society legal system also multiple situations algorithms could potentially lead unfair bias amount discrimination bias based cases may expect emergence new protected characteristics cover issues reflect society recognising new forms discrimination amplified algorithms rather use algorithms creating new type discrimination cases algorithms could lead bias based arbitrary characteristics would practical address issues discrimination law biases based characteristics differ algorithm may identified advance situations challenge current equality legislation imply entirely new framework required algorithmic examples data protection legislation would offer affected people levers understand challenge process decisions reached furthermore requirement fair data processing gdpr could mean kind bias noncompliant data protection law legally untested public sector bias based arbitrary characteristics could also challenged human rights act article prohibits discrimination based status although specific type arbitrary bias would also need tested courts therefore believe evidence justify entirely new legislative regulatory regime algorithmic bias furthermore specific regulatory regime algorithmic bias would risk inconsistent standards bias discrimination across algorithmic decisions believe would unworkable requirement consumers clear information appropriately informed afterthe point bias algorithmic regulatory environment matter urgency set report clearly risks algorithmic lead discrimination centre data ethics innovation current focus clarifying existing legislation applies algorithmic decisionmaking ensuring organisations know comply algorithmic context alongside effective enforcement laws algorithmic matter urgency set report clearly risks algorithmic lead discrimination unlawful application current legislation must clear enforced accordingly ensure bad practice reduced much possible case law equality act legislation sets principles minimum requirements behaviour principles need interpreted order applied practice interpretation occur individual decision makers regulators interpretation definitive tested courts growing body case law addresses algorithms data protection law examples litigation algorithmic algorithm supported decisions challenged equality act absence case law interpretations inherently somewhat speculative one examples use facial recognition technology south wales police recently challenged via judicial review data protection equality act study facial recognition technology one legal cases test regulatory environment algorithmic bias use live facial recognition technology police forces following concerns around violations privacy potential biases within system facial recognition technology frequently criticised performing differently people different skin tones meaning accuracy many systems often higher white men compared people south wales police trialled use live facial recognition public spaces several occasions since trials challenged judicial review found unlawful court appeal august one grounds successful appeal south wales police failed adequately consider whether trial could discriminatory impact specifically take reasonable steps establish whether facial recognition software contained biases related race sex court found meet obligations public sector equality note case evidence specific algorithm biased way south west police failed take reasonable steps consider judgement new report goes press seems likely could significant legal implications public sector use algorithmic suggesting public sector equality duty requires public sector organisations take reasonable steps consider potential bias deploying algorithmic systems detect algorithmic bias ongoing basis see example risks section cdei recent snapshot paper facial recognition technology bridges south wales court appeal case review bias algorithmic regulatory environment centre data ethics case ecli see summary aware litigation use challenged equality act means little understanding equality act requires relation technology whilst clear algorithm using protected characteristic input model making decisions basis would likely constitute discrimination less clear circumstances use variables correlate protected characteristics would considered indirectly discriminatory equality act states cases apparent bias may constitute indirect discrimination involves proportionate means achieving legitimate aim guidance case law help organisations understand interpret context however algorithmic perhaps less clear example ruling european court justice case made unlawful insurers charge different rates based sex car insurance providers routinely charged higher premiums men based higher expected claims profile insurers responded pricing insurance opaque algorithms based observable characteristics occupation car model size engine even telematics tracked individual driver behaviour change eliminated direct discrimination sex arguably shifted pricing towards objective measures insurance risk however auto insurance prices remain significantly higher men unclear legally untested algorithms cross legitimate pricing based risk indirect discrimination based proxies sex occupation lack case law meant organisations often left figure appropriate balance look international standards necessarily reflect equality framework uncertainty area risk fairness constraint innovation guidance appropriate good practice would help organisations navigate challenges well help understand parameters considered acceptable within law regulations guidance government regulators several ways provide clearer guidance interpret law types guidance regulations differ legal status audience statutory codes practice provided regulators clarify existing law applies particular context typically prepared regulator presented minister parliament codes guidelines legal nature targeted courts lawyers specialists professionals technical guidelines similar statutory codes prepared regulator without statutory backing courts required follow generally consider whether organisation followed evidence must draw existing statute case law focus apply existing law particular situations regulators also issue guidance information advice particular audiences employers service providers could extend beyond current statute case law must compatible existing law ehrc guidance harmonised statutory codes focused making existing legal rights obligations accessible different audiences employers affected individuals ico guidance often takes similar approach though ico guidance offers additional best practice recommendations organisations required follow find another way meet legal issues algorithmic bias raised report require clarification existing law practical guidance supports different stakeholders understand meet obligations review bias algorithmic regulatory environment centre data ethics issues algorithmic bias raised report require clarification existing law practical guidance supports different stakeholders understand meet obligations particular organisations need clarity lawfulness bias mitigation techniques understand address bias clarification existing law requires detailed knowledge employment law bias mitigation techniques work effort led government order provide official sanction government policy draw relevant expertise across broader public sector including ehrc cdei recommendations government recommendation government issue guidance clarifies equality act responsibilities organisations using algorithmic include guidance collection protected characteristics data measure bias lawfulness bias mitigation techniques possible work clarify existing legal obligations could still leave specific areas uncertainty whether organisations lawfully mitigate algorithmic bias avoiding direct positive discrimination highlight undesirable constraints possible believe situation would unacceptable could leave organisations ethical often legal obligation monitor algorithmic bias risks make unable deploy proportionate methods address bias find case clarity amendments equality law could required example help clarify lawful positive action means context mitigating algorithmic bias might cross line unlawful positive discrimination government clarify amend existing law issuing supplementary regulations statutory instruments regulations usually implemented minister presenting statutory instrument parliament areas regulator specifically authorised issue rules regulations also legally enforceable financial conduct authority fca handbook however equality act ehrc regulators power regulations would need issued minister current law unable provide enough clarity allow organisations address algorithmic bias government issue regulations help clarify law recommendations government recommendation development guidance implementation government assess whether provides sufficient clarity organisations obligations leaves sufficient scope organisations take actions mitigate algorithmic bias government consider new regulations amendments equality act address beyond clarifying existing obligations organisations need practical guidance helps meet obligations include obligations equality law also includes concepts fairness best practices advice beyond minimum standards described recommendation believe many specific issues methods likely private sector industry bodies also play leadership role facilitate best practice sharing guidance within industry possible work clarify existing legal obligations still leave uncertainties whether organisations lawfully mitigate algorithmic bias avoiding direct positive discrimination highlight undesirable constraints possible review bias algorithmic regulatory environment centre data ethics equality human rights commission powers equality human rights commission civil political rights great britain submission use algorithms make decisions develop deployed differently depending context sector algorithmic taking place increasingly across sectors industries novel ways algorithmic bias ehrc ico explicit responsibilities regulate also responsibilities within mandate sector regulator equality human rights commission equality human rights commission ehrc statutory body responsible enforcing equality act well responsibilities national human rights institution duties include reducing inequality eliminating discrimination promoting protecting human rights ehrc carries functions variety means including providing advice issuing guidance ensure compliance law also take investigations substantial breaches law suspected however resource intensive investigations limited high priority areas addition investigations ehrc uses approach strategic litigation pursue legal test cases areas law ehrc less likely involved individual cases rather directs people equality advisory support service given broad mandate ehrc leverages limited resources working collaboratively regulators promote compliance equality act example incorporating equality human rights standards compliance enforcement also produce joint guidance collaboration sector strategic plan ehrc highlights technology affects many equality human rights concerns currently strand work specifically addressing risks technologies instead implications new technologies justice system transport provision decisionmaking workplace captured within specific programmes march ehrc called suspension use automated facial recognition predictive algorithms policing england wales impact independently scrutinised laws improved however specific response report yet appear part wider strand ehrc continues monitor development implementation tools across policy areas identify opportunities strategic litigation clarify privacy equality implications also recently completed inquiry experiences people disabilities criminal justice system including challenges arising move towards digital justice undertaken research potential discrimination using recruitment due importance equality act governing bias discrimination ehrc key role play supporting application enforcement equality act algorithmic ehrc shown interest issues believe prioritise enforcement equality act relation algorithmic partly involve ehrc enforcement also room leverage reach sector regulators ensuring necessary capability carry investigations provide guidance specific contexts technologies present genuine shift discrimination operates century ehrc also need consider whether sufficient technical skills area carry investigations enforcement work might build expertise role regulatorsreview bias algorithmic regulatory environment centre data ethics innovation equinet european network equalities bodies meeting new challenges equality increased digitisation use artificial intelligence robin allen dee masters june ico ico turing consultation explaining decisions guidance ico ico investigation police use facial recognition technology public places ico need ensure lawfulness fairness transparency systems regulators recommendation ehrc ensure capacity capability investigate algorithmic discrimination protected groups may include ehrc reprioritising resources area ehrc supporting regulators address algorithmic discrimination sector additional technical support ehrc equalities bodies across europe facing similar challenges addressing new issues others previously identified need additional information commissioner office information commissioner office ico independent regulator information rights responsible implementation enforcement number pieces legislation including data protection act gdpr ico range powers carry work require organisations provide information issue assessment notices enable assess whether organisation complying data protection regulation finds breach data protection regulation issue enforcement notice telling organisation needs bring compliance including power instruct organisation stop processing impose significant financial penalties breaches annual total worldwide turnover ico broad remit focused challenge overseeing new legislation interpretation application gdpr still evolving case law legislation remains limited organisations public still adapting new regime ico played prominent role internationally thinking regulatory approaches relevant activities included leading regulators working group providing forum regulators relevant organisations including cdei share best practice collaborate effectively developing request government detailed guidance explainability partnership alan turing publishing guidance data protection aims help organisations consider legal obligations data protection develop tools guidance statutory code contains advice interpret relevant data protection law applies recommendations good practice organisational technical measures mitigate risks individuals may cause exacerbate activity part reflection increased scope responsibilities placed organisations within data protection act also reflects gradual growth importance technologies several decades efforts useful pushing forward activity space ico recently stated bias algorithms may fall data protection law via equality act dpa requires processing lawful compliance equality act also requirement data protection ico also makes clear guidance data protection also includes broader fairness requirements example fairness data protection context generally means handle personal data ways people would reasonably expect use ways unjustified adverse effects ico impose significant financial penalties breaches annual total worldwide bias algorithmic regulatory environment centre data ethics sector specialist regulators sectors studied review relevant bodies include financial conduct authority fca financial services ofsted children social care inspectorate constabulary fire rescue services policing recruitment fall remit specific sector regulator although area focus ehrc sector regulators areas studied detail review ofgem energy services services remit competition markets authority cma also relevant obligations within consumer protection legislation consumers treated public sector equality duty whilst equality act applies public private sector provisions public sector public sector equality duty psed duty sets legal mandate public authorities undertake activity promote equality public authority must exercise functions due regard need eliminate discrimination harassment victimisation conduct prohibited act advance equality opportunity persons share relevant protected characteristic persons share foster good relations persons share relevant protected characteristic persons share public authorities include sector regulators therefore deliver commitments set obligations equality act provide necessary mandate regulators work towards eliminating risks discrimination algorithmic within public authority must exercise functions due regard need eliminate discrimination harassment victimisation conduct prohibited act bias algorithmic regulatory environment centre data ethics innovation financial conduct authority guidance firms fair treatment vulnerable customers financial conduct authority regulatory sandbox mixed picture well enforcement bodies equipped respond bias algorithmic decisionmaking regulators fca explored specific research proactive understanding addressing concerns regulatory guidance draft guidance fair treatment vulnerable fca also deployed innovations regulatory sandbox temporarily reduces regulatory requirements selected products services exchange direct supervision guidance regulators example cma taking action build expertise activities area however many others well resourced relevant expertise treating issue priority particular challenges enforcement bodies sectors tools particularly novel case study financial conduct authority set chapter financial services sector one use algorithmic tools growing development deployment one key enforcement bodies sector fca responsibility consumer protection fca focused lot attention sector use technology big data identified key research priority spoken publicly use big data algorithmic approaches could raise ethical issues including concerns algorithmic bias committed work investigate issues financial markets present strategies reducing potential harm fca joint survey bank england use machine learning financial institutions demonstrates focus area following study established working group address issues fca sees role support safe beneficial ethical resilient deployment technologies across financial sector acknowledges firms best placed make decisions technologies use integrate business regulators seek ensure firms identify understand manage risks surrounding use new technologies apply existing regulatory framework way supports good outcomes consumers fca sees role support safe beneficial ethical resilient deployment technologies across financial bias algorithmic regulatory environment centre data ethics innovation algorithmic grows expect see similar responses sector bodies areas high stakes decisions made people lives might involve developing technical standards tools assessed fairness appropriate routes challenge redress individuals believe role support ehrc within regulatory remit work regulators well cdei advice coordination demonstrates need regulators sufficiently resourced deal equality issues related use technology sectors also raises question equality legislation applied regardless use algorithms concern also raised women equalities committee report enforcing equality act law role equality human rights stated public bodies enforcement bodies using powers secure compliance equality act areas responsible bodies far better placed equality human rights commission could ever combat kind routine systemic discrimination matters legal requirements clear employers service providers public authorities simply ignoring realistic expectation facing regulators fca ofgem cma also need ensure fair treatment vulnerable customers within remit issue discrimination regulators set guidelines unfair treatment monitor outcomes group regulatory activity conducted separately sector scope greater collaboration enforcement bodies share best practice develop guidance well sufficiently skilled resourced carry work cdei play key role providing advice regulators well coordinating activities recommendation regulators recommendation regulators consider algorithmic discrimination supervision enforcement activities part responsibilities public sector equality duty consumer facing regulators fca ofgem cma also need ensure fair treatment vulnerable customers within remit issue discrimination regulators set guidelines unfair treatment monitor outcomes group bias algorithmic regulatory environment centre data ethics regulatory tools ico guide general data protection regulation gdpr dpia retrieved march equality human rights commission equality impact assessments allen masters beyond enforcement guidance range tools help organisations meet regulatory requirements range proactive supervision models methods assure whether organisations compliant processes suitably skilled staff complementary tools considered regulators industry attempt address algorithmic bias regulatory sandboxes regulatory sandbox differentiated regulatory approach regulator provides direct supervision new products services controlled environment supervision range advice whether new practices compliant limited exemptions existing regulatory requirements number regulators currently offer support sector fca ofgem ico main focus initiatives help organisations understand operate effectively within regulatory frameworks help regulators understand innovative products services interact existing regulations however service useful organisations adopting new business models innovative approaches persistent problems may fit existing regulations examples include new applications blockchain technology fca sandbox energy trading ofgem sandbox use health social care data reduce violence london ico sandbox addressing algorithmic bias important area regulatory complexity closer regulatory supervision may helpful particularly new innovations adopted easily fit existing regulatory model regulators existing sandboxes consider applications algorithmic bias serious risk potentially additional engagement ehrc regulators sectors seeing accelerated deployment algorithmic could consider regulatory sandbox approach provide greater support supervision innovations may need new ways addressing algorithmic bias impact assessments organisations already required produce data protection impact assessments dpias processing personal data high risk individual rights freedoms assessments must consider risks rights freedoms natural persons generally including impact society whole consequence issues like discrimination may considered within remit data protection impact assessments however sector work suggests practice bias discrimination often considered within dpias public sector organisations also required due regard number equality considerations exercising functions focused addressing obligations organisations equality act equality impact assessments often carried public sector organisations prior implementing policy ascertaining potential impact equality though required law considered good practice way facilitating evidencing compliance public sector equality duty efforts extend role equality impact assessments broadly assess risks fairness raised particularly areas like bias algorithmic regulatory environment centre data ethics course good practice update impact assessments time indeed gdpr requires dpias revisited change risk profile see gdpr article always clear trigger point organisation invest time discussion issue see allen masters cloisters september legal education foundation matter automated data processing government decision making available ico guide general data protection regulator gdpr certification bias discrimination incorporated existing equality data protection impact assessments part internal governance quality assurance processes however research indicated variety challenges using impact assessments addressing algorithmic bias regulatory approach limited evidence regarding effectiveness impact assessments providing useful course correction development implementation new technologies impact assessment process usefully uncover resolve compliance issues throughout development use algorithms found impact assessments usually treated static document completed either beginning end development process therefore capture dynamic nature machine learning algorithms algorithmic bias issues likely occur therefore hard regulate impact assessment shows one point time seen one tool complemented others also efforts combine equality data protection concerns combined algorithmic impact integrated impact could effective way remove duplication support consistent way managing regulatory ethical risks raised technologies including fairness may also help highlight regulators organisations tensions different aspects current law guidance audit certification one frequently cited challenges governance algorithmic decisionmaking around organisations demonstrate compliance equality legislation individuals subject algorithmic systems appear opaque commentators often refer fears around risk hide variables making decisions concerns led calls ways assure algorithmic systems met particular standard fairness calls often framed terms auditing certification impact assessments could also used assess measures algorithmic appropriateness privacy safety algorithmic bias lack explainability also raises challenges burden proof discrimination cases equality act section reverses burden proof meaning outcomes data suggest algorithmic discrimination occurred courts assume occurred unless accused discriminating organisation prove otherwise enough organisation say believe discrimination occurred needs explicitly demonstrate therefore essential organisations know would constitute proportionate level proof systems unintentionally discriminating protected many contexts organisations required meet standards regulations including health safety cyber security financial standards systems evolved ecosystems services allow organisations prove customers regulators met standard ecosystems include auditing professional accreditation product certification parts assurance ecosystem starting emerge firms offering ethics consultancy calls auditing certification however efforts tend focused data protection accuracy rather fairness discrimination ico recently published guidance data protection sets set key considerations development system focused largely compliance data protection principles also touches areas data protection relate discrimination including discussion legal basis upon collect sensitive data testing bias however guidance directly address compliance equality law including lawfulness mitigation ico also announced process assessing gdpr certification could used show algorithmic gdpr compliant steps reflect real progress governance algorithms algorithmic bias discrimination would inevitably secondary concern data protection centred framework review bias algorithmic regulatory environment centre data ethics ico guide general data protection regulator gdpr guidance data protection ico guidance data protection ico published guidance data july guidance aimed two audiences compliance focus data protection officers dpos general counsel risk managers senior management ico auditors technology specialists including machine learning experts data scientists software developers engineers cybersecurity risk managers guidance provide ethical design principles use corresponds application data protection principles currently equivalent assurance ecosystem bias discrimination algorithmic see gap need filled time require increasing standardisation guidance steps prevent measure mitigate algorithmic bias national institute standards technology nist agency united states department commerce provides model external auditing algorithms could emerge nist developed facial recognition vendor tests requested access commonly used facial recognition algorithms test black box conditions subjecting set validated test images initially started efforts benchmarking false positive false negative rates algorithms allowing compared based accuracy test extended examine racial bias found many algorithms much higher error rates particularly false positives women minority ethnic groups also found algorithms much lower demographic bias often algorithms accurate general analysis allowed benchmarking standards based accuracy evolve performance comparisons algorithmic role nist seen trusted independent third party standards body algorithm developers however function necessarily need conducted government regulators given sufficient expertise commonly agreed standards testing certification standards could easily provided industry bodies trusted intermediaries well testing certification algorithmic systems need good practice standards organisations individuals developing systems relevant ecosystem training certification ecosystem private third sector services support organisations address algorithmic bias encouraged growth opportunity professional services strong growing area economy including providing audit related professional services number areas many companies already looking services provide help others build fair algorithms showing leadership area ensure fairness citizens also unlock opportunity growth showing leadership area ensure fairness citizens also unlock opportunity growth review bias algorithmic regulatory environment centre data ethics regulators recommendation regulators develop compliance enforcement tools address algorithmic bias impact assessments audit standards certification regulatory sandboxes advice industry industry bodies standards organisations develop ecosystem tools services enable organisations address algorithmic bias including standards auditing certification services algorithmic systems organisations developers create coordination alignment algorithmic bias likely grow importance report shows regulators need update regulatory guidance enforcement respond challenge given overlapping nature equality data protection regulations risk could lead fragmented complex environment regulators need coordinate efforts support regulated organisations guidance enforcement tools need forums practical collaboration supervision enforcement activities ideally regulators avoid duplicative compliance efforts aligning regulatory requirements jointly issue guidance regulators also pursue joint enforcement activities sector regulators pursue organisations sector support regulators like ico require additional dedicated work coordinate efforts regulators traditionally focused regulatory responsibility however increasing effort regulatory collaboration areas regulators network formally brought together economic sector regulators collaboration joint projects similar efforts collaborate explored sector regulators addressing algorithmic need coordinate efforts support regulated organisations guidance enforcement tools see also recommendation bias algorithmic regulatory environment centre data ethics innovation regulators recommendation regulators coordinate compliance enforcement efforts address algorithmic bias aligning standards tools possible could include jointly issued guidance collaboration regulatory sandboxes joint investigations future cdei work cdei plans grow ability provide expert advice support regulators line existing terms reference include supporting regulators coordinate efforts address algorithmic bias share best practice cdei monitor development algorithmic extent new forms discrimination bias emerge include referring issues relevant regulators working government issues covered existing laws regulations cdei plans grow ability provide expert advice support regulators line existing terms reference review bias algorithmic regulatory environment transparency public sector chapter centre data ethics innovation public sector summary validation techniques including detecting errors risks data overview findings making decisions individuals core responsibility many parts public sector increasing recognition opportunities offered use data algorithms use technology never reduce real perceived accountability public institutions citizens fact offers opportunities improve accountability transparency especially algorithms significant effects significant decisions individuals range transparency measures already exist around current public sector processes window opportunity ensure get transparency right algorithmic adoption starts increase supply chain delivers algorithmic decisionmaking tool often include one suppliers external public body ultimately responsible ultimate accountability fair always sits public body limited maturity consistency contractual mechanisms place responsibilities right place supply government recommendation government place mandatory transparency obligation public sector organisations using algorithms significant influence significant decisions affecting individuals government conduct project scope obligation precisely pilot approach implement require proactive publication information decision use algorithm made type algorithm used overall decisionmaking process steps taken ensure fair treatment individuals recommendation cabinet office crown commercial service update model contracts framework agreements public sector procurement incorporate set minimum standards around ethical use particular focus expected levels transparency explainability ongoing testing fairness advice industry industry follow existing public sector guidance transparency principally within understanding ethics safety guidance developed office alan turing institute government digital service sets governance framework responsible innovation projects public sector government place mandatory transparency requirement public sector organisations using algorithms significant influence significant decisions affecting bias algorithmic transparency public sector centre data ethics identifying issue guardian government review disability benefit claims bbc personal independence payments claims reviewed government update progress review national audit office reforming border immigration system brauneis robert goodman ellen algorithmic transparency smart city august yale law tech home office visas immigration operational guidance dwp guide hmrc internal guidance manuals see example department health social care personal information charter department work pensions personal information charter home office personal information charter foi release publication spending data local authorities transparency data hmrc headcount payroll data january public sector ensuring fairness public sector uses algorithms crucial public sector makes many highest impact decisions affecting individuals example related individual liberty entitlement essential public services also precedent failures large scale necessarily algorithmic processes causing impacts large number individuals example assessments disability immigration examples demonstrate significant impact decisions made scale public sector organisations wrong expect highest standards transparency accountability lines accountability different public private sectors governments bear special duties accountability expect public sector able justify evidence decisions moreover individual option using commercial service whose approach data agree option essential services provided state already specific transparency obligations measures relevant fair public sector example publication internal process documentation large scale processes within home department work freedom information offers citizens ability access wide range information internal workings public sector organisations subject access requests data protection act enable individuals request challenge information held also applicable private sector organisations publish personal information charters describing manage personal information line data protection publication equality impact assessments decisionmaking practices strictly required equality act often conducted part organisations demonstrating compliance public sector equality duty various existing public sector transparency policies enable understanding wider structures around example publication workforce parliamentary questions representation mps disclosure related legal challenges decisionmaking judicial review inquiries investigations statutory bodies commissioners behalf individuals bias algorithmic transparency public sector centre data ethics see procurement policy note updated may committee standards public life continuing importance ethical standards public service providers guidance managing public money oxford internet institute university oxford data science local government differing accounts example investigation guardian last year showed councils using algorithmic risk assessment tools particularly determine eligibility benefits calculate entitlements new statesman revealed experian secured british councils data justice lab research late showed local authorities quarter police authorities using algorithms prediction risk assessment assistance committee standards public life artificial intelligence public standards united nations human rights office high commissioner also opportunity government set example highest levels transparency government strong levers disposal affect behaviour either direct management control use algorithmic decisionmaking strategic oversight delivery bodies example policing nhs setting high ethical standards manages private sector service delivery also offers potential lever strong standards transparency public sector raise standards private sector example different context mandation cyber essentials certification new public sector contracts improved public sector cyber security also cyber security marketplace service providers supply public private sector quote public right expect services delivered responsibly ethically regardless delivered providing committee standards public life public bodies duty use public money way conducive efficiency given potential benefit use algorithms support done well optimising deployment scarce could argued public sector responsibility trial new technological approaches nonetheless must done way manages potential risks builds clear evidence impact upholds highest standards transparency problem currently difficult find algorithmic systems public sector using problem makes impossible get true sense scale algorithmic adoption public sector therefore understand potential harms risks opportunities regard public sector innovation recent report committee standards public life public standards noted adoption public sector remains limited examples development consistent cdei observed sectors looked review nonetheless varying accounts could lead perception intended opacity government citizens quote government increasingly automating use data new technology tools including evidence shows human rights poorest vulnerable especially risk contexts major issue development new technologies government lack special rapporteur extreme poverty human rights philip alstonreview bias algorithmic transparency public sector centre data ethics vishwanath kaufmann toward transparency new approaches application financial markets world bank research observer mortier haddadi henderson mcauley crowcroft interaction human face society weller transparency motivations challenges centre data ethics innovation approach governance technology neill onora reith question trust open university royal society science open enterprise value transparency case transparency made multiple contexts including government yet term transparency ambiguous mean different things different contexts considered universal example publishing details algorithm could lead gaming rules people understanding algorithm works disincentivise development relevant intellectual property another risk actors misaligned interests could abuse transparency way sharing selective pieces information serve communication objectives purposefully manipulating audience however able mitigate risks consider transparency within context decisions made public sector seen end alongside principles good including accountability also assume greater transparency public sector organisations inevitably lead greater trust public sector fact providing information intelligible public could fail inform public even foster concern baroness onora neill established principle intelligent accountability reith lecture since spoken need intelligent transparency summarised spotlight onora neill principle intelligent transparency according onora neill principle intelligent transparency information accessible interested people able find easily intelligible able understand useable address concerns assessable requested basis claims useful requirements bear mind considering type transparency desirable given simply providing information sake automatically build trust baroness onora neill established principle intelligent accountability reith lecture since spoken need intelligent transparency review bias algorithmic transparency public sector centre data ethics ted blog trust intelligently spiegelhalter david trust algorithms harvard data science review david spiegelhalter built onora neill work articulating need able interrogate trustworthiness claims made algorithm made algorithm quote trust requires intelligent judgement trustworthiness want others trust two things first trustworthy requires competence honesty reliability second provide intelligible evidence trustworthy enabling others judge intelligently place refuse onora neill trust intelligently sir david spiegelhalter built onora neill work articulating need able interrogate trustworthiness claims made algorithm made algorithm led produce following set questions expect able answer good tried new parts real world would something simpler transparent robust good could explain works general anyone interested could explain individual reached conclusion particular case know shaky ground acknowledge uncertainty people use appropriately right level scepticism actually help practice questions helpful starting point public sector organisations evaluating algorithm developing using considering sort information need know share order ensure meaningful publics eyes review bias algorithmic transparency public sector centre data ethics innovation delivering public sector transparency based discussion believe concrete action needed ensure consistent standard transparency across public sector related use algorithmic recommendations government recommendation government place mandatory transparency obligation public sector organisations using algorithms significant influence significant decisions affecting individuals government conduct project scope obligation precisely pilot approach implement require proactive publication information decision use algorithm made type algorithm used overall process steps taken ensure fair treatment individuals work needed precisely scope define meant transparency rooting thinking neill principle intelligent transparency spiegelhalter questions expect trustworthy algorithm provide solid basis ensure careful thinking algorithm information published scope use word significant clearly requires careful definition significant influence means output machine learning model likely meaningful affect overall decision made individual providing automation routine process informing meaningful way assessing risk categorising applications way influences outcome significant decision means decision direct impact life individual group individuals data protection act decision significant decision produces adverse legal effect concerning individual otherwise significantly affects although according data protection act applies specifically fully automated significant decisions would suggest similar interpretation includes decisions made human input potential examples algorithmic would scope shown figure figure decisions differentiated influence algorithms decision significance overall decision review bias algorithmic transparency public sector centre data ethics committee standards public life artificial intelligence public standards defining impactful significant decisions due consideration paid decisions relate potentially sensitive areas government policy may low levels trust public sector institutions could include social care criminal justice benefits allocation definition public sector context could sensibly aligned used equality act freedom information act exemptions general scoping statement clearly needed require careful consideration potential reasons exemption transparency risks compromising outcomes publication many details could undermine use algorithm enabling malicious outsiders game fraud detection use case intellectual property cases full details algorithm model proprietary organisation selling believe possible achieve balance achieve level transparency compatible intellectual property concerns suppliers public sector already achieved areas suppliers accept standard terms around public sector spending data etc detailed thinking around area needs worked part government detailed design transparency processes national security defence may occasional cases existence work area placed public domain general view risks areas managed careful actual information published keeping details sufficiently high level area likely require general exemption scoped principles freedom information legislation information published defining precise details published complex task require extensive consultation across government elsewhere section sets proposed draft scope need refined government considers response recommendation number views expressed previously example committee standards public life report defines openness use interchangeable transparency report fundamental information purpose technology used affects lives citizens must disclosed starting point would anticipate mandatory transparency publication include overall details process used description used within process including humans provide oversight decisions overall operation process overview developed covering example type machine learning technique used generate model description data trained assessment known limitations data steps taken address mitigate steps taken consider monitor fairness explanation rationale overall process designed way including impact assessments covering data protection equalities human rights carried line relevant legislation important emphasise limited detailed design algorithm also needs consider impact automation within overall process circumstances algorithm applicable indeed whether use algorithm appropriate bias algorithmic transparency public sector centre data ethics mitchell margaret model cards model reporting government canada algorithmic impact assessment propublica new york city moves create accountability algorithms treasury review quality assurance government government models leslie david understanding artificial intelligence ethics safety alan turing institute local government transparency code already common practice public sector however identifying right level information algorithm novel aspect examples elsewhere help guide example google model aim provide explanation model works experts alike model cards assist exploring limitations bias risk asking questions model perform consistently across diverse range people vary unintended ways characteristics like skin colour region change government canada algorithmic impact assessment questionnaire designed help organisations assess mitigate risks associated deploying automated decision system part wider efforts ensure responsible use new york city council passed algorithmic accountability law resulted setting task force monitor fairness validity algorithms used municipal agencies whilst ensuring transparent accountable united kingdom departmental returns prepared different parts government part macpherson review government modelling office turing institute government digital service understanding ethics safety set governance framework responsible innovation projects public sector within guidance document provide definition transparency within ethics including interpretability system justifiability processes outcome guidance starting point along ideas examples set report government considering precisely set information makes sense public sector cdei happy provide independent input work fit existing transparency measures listed variety existing public sector transparency measures related theme public commentary use algorithms potentially undermine transparency accountability government seek demonstrate case fact existing foi dpa obligations arguably already give individuals right request access information listed scope moreover initiatives like local government transparency sets minimum data local authorities publishing frequency published published good examples build regards proposing transparency effective transparency whilst obligations proactive disclosure foi dpa always effective transparency tool practice often reactive making publication information truly proactive process help government build expectations eventually published early stages projects structure releases consistent way hopefully helps external groups journalists academia civil society engage data published effective way time fewer genuine misunderstandings communication manage overhead responding large numbers similar reactive bias algorithmic transparency public sector centre data ethics house commons science technology committee algorithms fourth report session law society algorithms criminal justice system guidance gender pay gap reporting overview process transparency house lords science technology select committee law society recently recommended parts public sector maintain register algorithms development use quote government produce publish maintain list algorithms significant impacts used within central government along projects underway planned public service algorithms aid private sector involvement also house lords science technology select quote national register algorithmic systems created crucial initial scaffold openness learning law society algorithms criminal justice system cdei agrees significant advantages government citizens central coordination around transparency example would enable easier comparisons across different organisations promoting consistent style transparency moreover delivery innovation benefits allowing public sector organisations see peers however implementing transparency process coordinated way across entire public sector challenging task much greater extent either proposals quoted use local government social care settings discussed section would included either examples number comparators consider levels coordination centralised central government gds spend controls devolved individual organisations publication transparency data central publication across public private sector gender pay data portal gender pay gap suspect sensible middle ground case complexities coordinating register across entire public sector would high subtle differences published transparency data might well apply different sectors therefore conclude starting point set overall transparency obligation government decide best way coordinate considers implementation natural approach implementation pilot specific part public sector example could done services run directly central government departments subset making use existing coordination mechanisms managed government digital service likely collection registers might best approach public sector organisations scope sector register remaining responsible publishing equivalent transparency data bias algorithmic transparency public sector centre data ethics see reference spiegelhalter trust algorithms harvard data science review house lords select committee artificial intelligence ready willing able report session relationship transparency explainability uphold accountability public sector organisations able provide kind explanation algorithm operates reaches conclusion david spiegelhalter says trustworthy algorithm able show working want understand came conclusions crucially working needs intelligible nonexpert audience therefore focusing publishing algorithm source code technical details demonstration transparency red herring area explainability previous reports research focused black box indeed house lords select committee expressed unacceptable deploy system could substantial impact individuals life unless generate full satisfactory explanation decisions take extremely difficult black box case many key administrative decisions often based well structured data may need develop highly sophisticated black box algorithms inform decisions often simpler statistical techniques may perform well algorithm proposed limitations explainability black box organisation able satisfactorily answer spiegelhalter questions particular around whether something simpler would good whether explain works reaches mentioned chapter ico ati jointly developed guidance organisations explain decisions made guidance offers several types examples explanations different contexts decisions along advice practicalities explaining decisions internal teams individuals whilst guidance directed exclusively public sector contains valuable information public sector organisations using make decisions also potential public sector organisations publish case studies examples applying guidance explain decisions made ultimately algorithmic element decisionmaking process unexplainable untransparent undermines extent public sector organisation able publish intelligent intelligible information whole process uphold accountability public sector organisations able provide kind explanation algorithm operates reaches conclusion review bias algorithmic transparency public sector centre data ethics public sector procurement committee standards public life continuing importance ethical standards public service providers development delivery algorithmic tool often include one suppliers whether acting technology suppliers business process outsourcing providers even development delivery algorithmic tool purely internal always reliance externally developed tools libraries open source machine learning libraries python supply chain models ultimate accountability good always sits public body ministers still held account parliament public overall quality fairness decisions made along locally elected councillors police crime commissioners relevant committee standards public life noted public right expect services delivered responsibly ethically regardless delivered providing transparency mechanisms discussed section form part overall accountability therefore need practical different potential supply chain models supply chain models examples possible models outsourcing process follows public body houseit partner business process oursourcing policy accountability body public body public body operational operational training datapublic body public body public body partypublic body supplier public body partysupplier supplier subcontractor public body partymodel tool development underlying algorithms librariespublic body mostly open source potentially party proprietarypublic body mostly open source potentially party proprietarysupplier mostly open source potentially party proprietaryreview bias algorithmic transparency public sector centre data ethics guidelines procurement world economic forum government first pilot procurement guidelines world economic forum committee standards public life artificial intelligence public standards issues around defining managing supply chain sensible way common government procurement services dependent technology source ownership data machine learning model trained make interdependency customer supplier complex context many others model trained data provided customer straightforward flow requirements fairness supplier contract ability meet requirements dependent part customer data public sector issue wider marketplace ecosystem around contracting fully developed natural desire top tree push responsibilities ethics legal compliance systems supply chain common practice number areas tupe regulations create obligations organisations involved transfer services suppliers related employees providing services commonly understood standard clauses included contracts make clear financial liabilities associated sit similar notion commonly understood contractual wording exist case pros cons position positive side ensures organisations responsibility overall process attempt pass onto suppliers without properly considering picture conversely means may limited commercial incentive suppliers supply chain really focus products services support ethical legally compliant issue office working partnership world economic forum developed detailed draft effective procurement public sector includes useful consideration ethics issues handled procurement helpful step forward encouraging government taking leading role getting right globally recent committee public life report public standards noted firms feel public sector often capability make products services explainable rarely asked procuring technology public guidance aims help address clearly implement effectively across public sector guidance drafted focused projects primarily focused buying solutions relevant situation increasingly becomes generic technology present whole variety use cases much public sector procurement implicitly within wider contracts unlikely necessarily desirable procurement teams across areas focus specifically procurement amongst range guidance best practice similar issues occur common underlying requirements around data protection cyber security open book accounting part approach taken include standard terms model contracts framework agreements used across public sector capture minimum set core principles never achieve much careful thought contract right outcome specific context help establish minimum common standard ecosystem around contracting fully bias algorithmic transparency public sector centre data ethics crown commercial service artificial intelligence similar approach taken ethics procurement activity specific focus procurement teams need designing specific requirements applicable use case drawing office world economic forum guidelines use algorithmic specifically expected could form part possible supplier solutions output based requirement common baseline requirement needed give contracting authority ability manage risk life given range different possible use cases difficult place highly specific requirements model contract focus enabling contracting authority appropriate level oversight development deployment algorithmic decisionmaking tool oversee whether fairness considerations taken account along rights reject request changes central government extent wider public sector centrally managed set procurement policies model contracts framework agreements underpin majority procurement processes mainly managed cabinet office government commercial function policy model contracts crown commercial service framework agreements work already underway bodies incorporate findings office procurement guidelines procurement activities new framework however scope cover procurement activity could potentially result purchasing service recommendations government recommendation cabinet office crown commercial service update model contracts framework agreements public sector procurement incorporate set minimum standards around ethical use particular focus expected levels transparency explainability ongoing testing fairness developing details terms government need consult marketplace ensure eventual terms commercially palatable intention recommendation find balance gives commercial mechanisms public bodies manage concerns bias algorithmic indeed ethical concerns around impose burden market disproportionate risk common terms within public sector procurement developing standard terms government may want draw support office central government extent wider public sector centrally managed set procurement policies model contracts framework agreements underpin majority procurement processes review bias algorithmic transparency public sector chapter next steps future challenges centre data ethics review considered complex rapidly evolving field recognising breadth challenge focused heavily surveying maturity landscape identifying gaps setting concrete next steps plenty across industry regulators government manage risks maximise benefits algorithmic next steps fall within cdei remit keen help industry regulators government taking forward practical delivery work address issues identified future challenges may arise government industry bodies regulators need give help organisations building deploying algorithmic tools interpret equality act context drawing understanding built review cdei happy support several aspects work space example supporting development guidance application equality act algorithmic supporting government developing guidance collection use protected characteristics meet responsibilities equality act identifying potential future need change law intent reduce barriers innovation drawing draft technical standards work produced course review inputs help industry bodies sector regulators government departments defining norms bias detection mitigation supporting government digital service seek scope pilot approach transparency growing ability provide expert advice support regulators line terms reference including supporting regulators coordinate efforts address algorithmic bias share best practice example invited take observer role financial conduct authority bank england public private forum explore means support safe adoption machine learning artificial intelligence within financial services intent support work draw lessons relatively mature sector share others government clear responsibilities sit tracking progress across sectors area driving pace steps future challenges summaryreview bias algorithmic next steps future challenges centre data ethics noted need ecosystem skilled professionals expert supporting services help organisations getting fairness right provide assurance development needs happen organically believe action may needed catalyse cdei plans bring together diverse range organisations interest area identifying would needed foster develop strong accountability ecosystem opportunity manage ethical risks also support innovation area potential companies offer audit services worldwide course review number public sector organisations expressed interest working apply general lessons learnt specific projects example supporting police force local authority develop practical governance structures support responsible trustworthy data innovation looking across work listed future challenges undoubtedly arise see key need national leadership coordination ensure continued focus pace addressing challenges across sectors government clear wants coordination sit number possible locations example central government directly regulator cdei government clear responsibilities sit tracking progress across sectors area driving pace change cdei agrees future priorities government hope able support area review necessity partial look wide field indeed prominent concerns around algorithmic bias emerged recent months unfortunately outside core scope including facial recognition impact bias within platforms target content considered cdei review online targeting monitoring function continue monitor development algorithmic extent new forms discrimination bias emerge include referring issues relevant regulators working government issues covered existing review suggests many steps needed address risk bias overlap tackling ethical challenges example structures good governance appropriate data sharing explainability models anticipate return issues bias fairness equality much future work though likely one ethical issue wider projects interested knowing projects listed cdei future work please get touch via bias cdei plans bring together diverse range organisations interest area identifying would needed foster develop strong accountability ecosystem review bias algorithmic next steps future challenges acknowledgements chapter centre data ethics grateful input engagement wide range individuals organisations throughout review including following note inclusion list imply organisations individuals reviewed endorsed final contents review represent considered views cdei external review group final report anna thomas institute future work nick radcliffe university edinburgh global finance centre excellence reuben binns university oxford robin allen cloisters law hub government better regulation executive beis department digital culture media sport department education government digital service government equalities office home office ministry housing communities local government office race disparity unit regulators biometrics forensics ethics group competition markets authority equality human rights commission financial conduct authority information commissioner office wider public sector bristol city council local government association maidstone local authority national police chief council office national statistics research innovation works children social care west midlands police police crime commissioner think tanks unions professional bodies etc acas ada lovelace institute british association social workers cipd fawcett society institute future work open data institute odi prospectroyal united services institute particular marion oswald alexander babuta social finance dee masters cloisters law hub industry accenture association british insurers behavioural insights team ernst young faculty data science institute faculty actuaries mastodon monzo predictivehire recruitment employment confederation slaughter may finance xantura interviews finance recruitment organisations anygood applied arctic shores bank england barclays equifax etiq experian headstart hirevue hsbc institute chartered accountants england wales mastercard mevitae nationwide oleeo relx unilever visa workday academia alan turing institute data ethics group david leslie alan turing institute jonathan bright oxford internet institute michael veale uclacknowledgementsreview bias algorithmic acknowledgements centre data ethics appendix bias mitigation methods stage intervention notion fairness detailed references techniques found faculty bias identification mitigation algorithms published demographic parity conditional demographic parity equalised oddsdata reweighting resampling calders kamiran pechenizkiy faisal kamiran calders label modification calders kamiran pechenizkiy faisal kamiran calders luong ruggieri turini feature modification feldman optimal clustering constrained optimisation zemel calmon louizos constrained optimisation agarwal zafar valera rodriguez regularisation kamishima naive bayes balance models group calders verwer naive bayes training via modified labels calders verwer splits adaptation kamiran calders pechenizkiy adversarial debiasing zhang adel constrained optimisation adversarial debiasing zhang adel passing cond variable adversarial constrained optimisation predictive equality agarwal zafar valera gomez rodriguez woodworth adversarial debiasing zhang adel decision threshold modification roc curve constrained optimisation hardt price srebro woodworth naive bayes modification model probabilities calders verwer leaves relabelling kamiran calders pechenizkiy label modification lohia kamiran karim zhang bias algorithmic appendices centre data ethics calibration individual fairness counterfactual fairness subgroup fairnessoptimal clustering constrained optimisation zemel prediction via causal graph kusner unconstrained optimisation constrained optimisation dwork biega gummadi weikum game kearns kearns information withholding pleiss achieves simultaneously relaxation equalised odds label modification lohia bias algorithmic appendices horse guards avenue london cdei
