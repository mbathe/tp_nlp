report meredith whittaker institute new york university google open research kate crawford institute new york university microsoft research roel dobbe institute new york university genevieve fried institute new york university elizabeth kaziunas institute new york university varoon mathur institute new york university sarah myers west institute new york university rashida richardson institute new york university jason schultz institute new york university school law oscar schwartz institute new york university research assistance alex campolo gretchen krueger institute new york university december contents executive intensifying problem amplifying widespread faulty science dangerous history affect facial recognition ampliﬁes civil rights risks automated decision systems experimenting society bears burden emerging solutions bias busting formulas fairness limits technological fixes broader industry applications toolkits system ethics needed fairness infrastructural accounting hidden labor deeper race gender power strategic litigation policy research organizing emergent work licensed creative commons international license institute institute new york university interdisciplinary research institute dedicated understanding social implications technologies ﬁrst university research center focused speciﬁcally social signiﬁcance founded led kate crawford meredith whittaker one institutes world works broad coalition stakeholders including academic researchers industry civil society policy makers affected communities identify address issues raised rapid introduction across core social domains produces interdisciplinary research help ensure systems accountable communities contexts meant serve applied ways promote justice equity institute current research agenda focuses four core areas bias inclusion rights liberties labor automation safety critical infrastructure recent publications include litigating algorithms major report assessing recent court cases focused government use algorithms system map longform essay produced partnership share lab investigates human labor data planetary resources required operate amazon echo impact assessment aia report helps affected communities stakeholders assess use algorithmic public agencies algorithmic accountability policy toolkit geared toward advocates interested understanding government use algorithmic systems also host expert workshops public events wide range topics workshop immigration data automation trump era brennan center justice center privacy technology georgetown law focused trump administration use data harvesting predictive analytics machine learning target immigrant communities data genesis working group convenes experts across industry academia examine mechanics dataset provenance maintenance roundtable machine learning inequality bias berlin robert bosch academy gathered researchers policymakers across europe address issues bias discrimination fairness machine learning related technologies annual public symposium convenes leaders academia industry government civil society examine biggest challenges face moves everyday lives symposium addressed intersection ethics organizing accountability examining landmark events past year people registered event free open public recordings program available website information available recommendations need regulate expanding powers agencies oversee audit monitor technologies domain implementation systems expanding rapidly without adequate governance oversight accountability regimes domains like health education criminal justice welfare histories regulatory frameworks hazards however national safety body general standards certiﬁcation model struggle meet sectoral expertise requirements needed nuanced regulation need approach prioritize technology focuses application within given domain useful examples approaches include united states federal aviation administration national highway traﬃc safety administration recognition affect recognition need stringent regulation protect public interest regulation include national laws require strong oversight clear limitations public transparency communities right reject application technologies public private contexts mere public notice use suﬃcient high threshold consent given dangers oppressive continual mass surveillance affect recognition deserves particular attention affect recognition subclass facial recognition claims detect things personality inner feelings mental health worker engagement based images video faces claims backed robust scientiﬁc evidence applied unethical irresponsible ways often recall pseudosciences phrenology physiognomy linking affect recognition hiring access insurance education policing creates deeply concerning risks individual societal level industry urgently needs new approaches governance report demonstrates internal governance structures technology companies failing ensure accountability systems government regulation important component leading companies industry also need internal accountability structures beyond ethics guidelines include employee representation board directors external ethics advisory boards implementation independent monitoring transparency efforts third party experts able audit publish key systems companies need ensure infrastructures understood nose tail including ultimate application use companies waive trade secrecy legal claims stand way accountability public sector vendors developers create automated decision systems use government agree waive trade secrecy legal claim inhibits full auditing understanding software corporate secrecy laws barrier due process contribute black box effect rendering systems opaque unaccountable making hard assess bias contest decisions remedy errors anyone procuring technologies use public sector demand vendors waive claims entering agreements companies provide protections conscientious objectors employee organizing ethical whistleblowers organizing resistance technology workers emerged force accountability ethical decision making technology companies need protect workers ability organize whistleblow make ethical choices projects work include clear policies accommodating protecting conscientious objectors ensuring workers right know working ability abstain work without retaliation retribution workers raising ethical concerns must also protected whistleblowing public interest protection agencies apply laws products services hype around growing leading widening gaps marketing promises actual product performance gaps come increasing risks individuals commercial customers often grave consequences much like products services potential seriously impact exploit populations vendors held high standards promise especially scientiﬁc evidence back promises inadequate consequences unknown companies must beyond pipeline model commit addressing practices exclusion discrimination workplaces technology companies ﬁeld whole focused pipeline model looking train hire diverse employees important overlooks happens people hired workplaces exclude harass systemically undervalue people basis gender race sexuality disability companies need examine deeper issues workplaces relationship exclusionary cultures products build produce tools perpetuate bias discrimination change focus needs accompanied practical action including commitment end pay opportunity inequity along transparency measures hiring retention accountability transparency require detailed account full stack supply meaningful accountability need better understand track component parts system full supply chain relies means accounting origins use training data test data models application program interfaces apis infrastructural components product life cycle call accounting full stack supply chain systems necessary condition responsible form auditing full stack supply chain also includes understanding true environmental labor costs systems incorporates energy use use labor developing world content moderation training data creation reliance clickworkers develop maintain systems funding support needed litigation labor organizing community participation accountability issues people risk harm systems often least able contest outcomes need increased support robust mechanisms legal redress civic participation includes supporting public advocates represent cut social services due algorithmic decision making civil society organizations labor organizers support groups risk job loss exploitation infrastructures enable public participation programs expand beyond computer science engineering disciplines began interdisciplinary ﬁeld decades narrowed become technical discipline increasing application systems social domains needs expand disciplinary orientation means centering forms expertise social humanistic disciplines efforts genuinely wish address social implications stay solely within computer science engineering departments faculty students trained research social world expanding disciplinary orientation research ensure deeper attention social contexts focus potential hazards systems applied human populations executive summary core cascading scandals around questions accountability responsible systems harm understand harms remedy points intervention additional research regulation needed ensure interventions effective currently answers questions frameworks presently governing capable ensuring accountability pervasiveness complexity scale systems grow lack meaningful accountability oversight including basic safeguards responsibility liability due process increasingly urgent concern building reports report contends central problem addresses following key issues growing accountability gap favors create deploy technologies expense affected use maximize amplify surveillance especially conjunction facial affect recognition increasing potential centralized control oppression government use automated decision systems directly impact individuals communities without established accountability structures unmonitored forms experimentation human populations limits technological solutions problems fairness bias discrimination within topic identify emerging challenges new research provide recommendations regarding development deployment regulation offer practical pathways informed research policymakers public technologists better understand mitigate risks given institute location regional expertise concentrated report focus primarily context also several world largest companies based accountability gap growing technology scandals shown gap develop proﬁt likely suffer consequences negative growing larger smaller several reasons including lack government regulation highly concentrated sector insuﬃcient governance structures within technology companies power asymmetries companies people serve stark cultural divide engineering cohort responsible technical research vastly diverse populations systems deployed gaps producing growing concern bias discrimination due process liability overall responsibility harm report emphasizes urgent need stronger research regulation amplifying widespread surveillance role widespread surveillance expanded immensely china many countries worldwide seen growing use sensor networks social media tracking facial recognition affect recognition expansions threaten individual privacy accelerate automation surveillance thus reach pervasiveness presents new dangers magniﬁes many longstanding concerns use affect recognition based debunked pseudoscience also rise affect recognition attempts read inner emotions close analysis face connected spurious claims people mood mental health level engagement guilt innocence technology already used discriminatory unethical purposes often without people knowledge facial recognition technology poses dangers reinforcing skewed potentially discriminatory practices criminal justice education employment presents risks human rights civil liberties multiple countries governments rapidly expanding use automated decision systems without adequate protections civil rights around world government agencies procuring deploying automated decision systems ads banners eﬃciency yet many systems untested poorly designed tasks resulting illegal often unconstitutional violations individual rights worse make errors bad decisions ability question contest remedy often diﬃcult impossible agencies attempting provide mechanisms transparency due process basic rights trade secrecy similar laws threaten prevent auditing adequate testing systems drawing proactive agency efforts recent strategic litigation outline pathways ads accountability rampant testing systems wild human populations silicon valley known move fast break things mentality whereby companies pushed experiment new technologies quickly without much regard impact failures including bears risk past year seen growing number experiments deploying systems wild without proper protocols notice consent accountability experiments continue due part lack consequences failure harms occur often unclear responsibility lies researching assigning appropriate responsibility liability remains urgent priority limits technological ﬁxes problems fairness bias discrimination much new work done designing mathematical models considered fair machines calculate outcomes aimed avoiding discrimination yet without framework accounts social political contexts histories mathematical formulas fairness almost inevitably miss key factors serve paper deeper problems ways ultimately increase harm ignore justice broadening perspectives expanding research fairness bias beyond merely mathematical critical ensuring capable addressing core issues moving focus parity justice move ethical principles year saw emergence numerous ethical principles guidelines creation deployment technologies many response growing concerns social implications studies show types ethical commitments little measurable effect software development practices directly tied structures accountability workplace practices codes guidelines rarely backed enforcement oversight consequences deviation ethical codes help close accountability gap truly built processes development backed enforceable mechanisms responsibility accountable public interest following report develops themes detail reﬂecting latest academic research outlines seven strategies moving forward fairness research beyond focus mathematical parity statistical fairness toward issues justice tracking full stack infrastructure needed create including accounting material supply chains many forms labor required create maintain systems deeper interdisciplinarity race gender power new policy interventions strategic litigation coalitions researchers civil society organizers within technology sector approaches designed positively recast ﬁeld address growing power imbalance currently favors develop proﬁt systems expense populations likely harmed introduction social challenges past year seen accelerated integration powerful artiﬁcial intelligence systems core social institutions backdrop rising inequality political populism industry scandals major movements inside outside technology companies pushing greater accountability justice report focuses themes examines gaps ethics meaningful accountability role organizing regulation short dramatic year normal year cambridge analytica seeking manipulate national elections using social media data algorithmic targeting would biggest story one many scandals facebook series disasters including massive data breach september multiple class action lawsuits discrimination accusations inciting ethnic cleansing myanmar potential violations fair housing act hosting masses fake russian accounts throughout year company executives frequently summoned testify mark zuckerberg facing senate april european parliament may zuckerberg mentioned technologies times congressional testimony company problems particularly complex areas censorship fairness content moderation facebook one crisis news broke march google building systems department defense drone surveillance program project maven news kicked unprecedented wave technology worker organizing dissent across industry june trump administration introduced family separation policy forcibly removed immigrant children parents employees amazon salesforce microsoft asked companies end contracts immigration customs enforcement ice less month later revealed ice modiﬁed risk assessment algorithm could produce one result system recommended detain immigrants custody throughout year systems continued tested live populations domains serious consequences march autonomous cars killed drivers pedestrians may voice recognition system designed detect immigration fraud ended cancelling thousands visas deporting people error documents leaked july showed ibm watson producing unsafe incorrect cancer treatment recommendations investigation september revealed ibm also working new york city police department nypd build ethnicity detection feature search faces based race using police camera footage thousands people streets new york taken without knowledge permission sampling extraordinary series incidents response included growing wave criticism demands greater accountability technology industry systems build turn companies made public calls regulate technologies like facial recognition others published ethics principles increased efforts produce technical ﬁxes issues bias discrimination systems many ethical technical approaches deﬁne problem space narrowly neither contending historical social context providing mechanisms public accountability oversight due process makes nearly impossible public validate current problems fact addressed numerous scholars noted one signiﬁcant barrier accountability culture industrial legal secrecy dominates development many technologies black boxes industrial cultures create many fundamental building blocks required understand systems ensure certain forms accountability training data data models code dictating algorithmic functions implementation guidelines software business decisions directed design development rarely accessible review hidden corporate secrecy laws current accountability gap also caused incentives driving rapid pace technical research push innovate publish ﬁrst present novel addition technical domain created accelerated cadence ﬁeld technical disciplines broadly comes cost considering empirical questions context use substantively engaging ethical concerns similarly technology companies driven pressures launch iterate assume complex social political questions handled policy legal departments leaving developers sales departments free responsibility considering potential downsides move fast break things culture provides little incentive ensuring meaningful public accountability engaging communities likely experience harm particularly problematic accelerated application systems sensitive social political domains presents risks marginalized communities challenge create better governance greater accountability poses particular problems systems woven fabric government public institutions lack transparency notice meaningful engagement accountability oversight creates serious structural barriers due process redress unjust discriminatory decisions year report assess many pressing issues facing tools deployed institutions govern everyday life focus biggest industry players number companies able create scale small power reach global evaluate current range responses industry governments researchers activists civil society large suggest series substantive approaches make ten speciﬁc recommendations finally share latest research policy strategies contribute greater accountability well richer understanding systems wider social intensifying problem space identifying pressing social implications year look closely role widespread surveillance multiple countries around world implications rights liberties particular consider increasing use facial recognition subclass facial recognition known affect recognition assess growing calls regulation next share ﬁndings government use automated decision systems questions raises fairness transparency due process systems protected trade secrecy laws prevent auditing close examination finally look practices deploying experimental systems wild testing human populations analyze gain greatest risk experiencing harm amplifying widespread surveillance year seen amplify surveillance techniques analyze video audio images social media content across entire populations identify target individuals groups researchers advocates long warned dangers mass data collection surveillance raises stakes three areas automation scale analysis predictive capacity speciﬁcally systems allow automation surveillance capabilities far beyond limits human review analytics thus serve centralize capabilities hands small number actors systems also exponentially scale analysis tracking across large quantities data attempting make connections inferences would diﬃcult impossible introduction finally provide new predictive capabilities make determinations individual character risk proﬁles raising possibility granular population controls china offered several examples alarming surveillance year know largely government openly acknowledges however important note many infrastructures already exist elsewhere often produced promoted private companies whose marketing emphasizes beneﬁcial use cases use tools law enforcement government rarely open public scrutiny review much know infrastructures capabilities could easily turned surveillant ends without public disclosure oversight depending market incentives political china military automated surveillance technology deployed monitor large portions population often targeting marginalized groups reports include installation facial recognition tools hong border using ﬂocks robotic drones ﬁve provinces across country widely reported social credit monitoring system illustrates surveillance systems mobilized means social control oppressive use systems reportedly occuring xinjiang autonomous region described economist police state like surveillance uighur ethnic minority area pervasive ranging physical checkpoints programs uighur households required adopt han chinese oﬃcials family widespread use surveillance cameras spyware sniffers biometric data collection sometimes stealth machine learning tools integrate streams data generate extensive lists suspects detention camps built government discipline group estimates number people detained camps range hundreds thousands nearly one million infrastructures unique china venezuela announced adoption new smart card known carnet patria integrating government databases linked social programs could enable government monitor citizens personal ﬁnances medical history voting activity united states seen similar efforts pentagon funded research social media surveillance help predict population behaviors immigration customs enforcement ice agency using investigative case management system developed palantir powered amazon web services deportation operations system integrates public data information purchased private data brokers create proﬁles immigrants order aid agency proﬁling tracking deporting individuals examples show systems increase integration surveillance technologies models social control amplify power data magnifying stakes misuse raising urgent important questions basic rights liberties protected faulty science dangerous history affect recognition also seeing new risks emerging unregulated facial recognition systems systems facilitate detection recognition individual faces images video used combination tools conduct sophisticated forms surveillance automated offering ability observe interpret speech distance among host surveillance tracking techniques facial recognition raises particular civil liberties concerns facial features personal form biometric identiﬁcation extremely diﬃcult change hard subvert opt operations unlike tracking tools facial recognition seeks use much simply recognizing faces identiﬁed face linked forms personal records identiﬁable data credit score social graph criminal record affect recognition subset facial recognition aims interpret faces automatically detect inner emotional states even hidden intentions approach promises type emotional weather forecasting analyzing hundreds thousands images faces detecting mapping expressions true reactivates long tradition physiognomy pseudoscience claims facial features reveal innate aspects character personality dating ancient times scientiﬁc interest physiognomy grew enormously nineteenth century became central method scientiﬁc forms racism discrimination although physiognomy fell favor following association nazi race science researchers worried reemergence physiognomic ideas affect recognition applications idea systems might able tell student customer criminal suspect really feeling type person intrinsically proving attractive corporations governments even though scientiﬁc justiﬁcations claims highly questionable history discriminatory purposes case affect detection reveals machine learning systems easily used intensify forms classiﬁcation discrimination even basic foundations theories remain controversial among psychologists scientist closely associated affect detection psychologist paul ekman asserted emotions grouped small set basic categories like anger disgust fear happiness sadness surprise studying faces according ekman produces objective reading authentic interior direct window soul underlying belief idea emotions ﬁxed universal identical across individuals clearly visible observable biological mechanisms regardless cultural context ekman work deeply criticized psychologists anthropologists researchers found theories hold sustained scrutiny psychologist lisa feldman barrett colleagues argued understanding emotions terms rigid categories simplistic physiological causes longer tenable nonetheless researchers taken work fact used basis automating emotion detection contextual social cultural factors emotional signiﬁers expressed play larger role emotional expression believed ekman peers light new scientiﬁc understanding emotion simplistic mapping facial expression onto basic emotional categories likely reproduce errors outdated scientiﬁc paradigm also raises troubling ethical questions locating arbiter someone real character emotions outside individual potential abuse power justiﬁed based faulty claims psychiatrist jamie metzl documents recent cautionary example pattern diagnosing black people schizophrenia supported civil rights movement affect detection combined facial recognition potential magnify political abuses psychological proﬁling realm education universities considered using affect analysis software students university thomas minnesota looked using system based microsoft facial recognition affect detection tools observe students classroom using webcam system predicts students emotional state overview student sentiment viewable teacher shift teaching way ensures student engagement judged system raises serious questions multiple levels system simplistic emotional model simply grasp complex states would student contest determination made system different students seen happy others angry teacher redirect lesson privacy implications system particularly given case pilot program evidence students informed use outside classroom also seeing personal assistants like alexa siri seeking pick emotional undertones human speech companies even going far patent methods marketing based detecting emotions well mental physical health emotion measurement company affectiva promises promote safer driving monitoring driver occupant emotions cognitive states reactions driving experience face yet little evidence systems actually work across different individuals contexts cultures safeguards put place mitigate concerns privacy bias discrimination operation furthermore seen large literature bias fairness classiﬁcations nature direct impacts human lives also serve data train inﬂuence systems raises stakes use affect recognition emphasizing critically examined use severely restricted facial recognition ampliﬁes civil rights concerns concerns intensifying facial recognition increases racial discrimination biases criminal justice system earlier year american civil liberties union aclu disclosed orlando police department washington county sheriff department using amazon rekognition system boasts perform face recognition across tens millions faces detect faces challenging crowded washington county amazon speciﬁcally worked sheriff department create mobile app could scan faces compare database least mugshots amazon representative recently revealed talk considering applications orlando network surveillance cameras could used conjunction facial recognition technology ﬁnd person interest wherever might city addition privacy mass surveillance concerns commonly raised use facial recognition law enforcement also intersected concerns racial biases researchers aclu university california berkeley tested amazon rekognition tool comparing photos sitting members united states congress database containing photos people arrested results showed signiﬁcant levels inaccuracy amazon rekognition incorrectly identiﬁed members congress people arrest database moreover false positives disproportionately occurred among members congress error rate nearly compared white members results echo string ﬁndings demonstrated facial recognition technology average better detecting people people better detecting men women response aclu amazon acknowledged rekognition results signiﬁcantly skewed using facial database appropriately given deep historical racial biases criminal justice system law enforcement databases unlikely appropriately despite serious ﬂaws ongoing pressure civil rights groups protests amazon employees potential misuse technologies amazon web services ceo andrew jassy recently told employees feel really great really strongly value amazon rekognition providing customers sizes types industries law enforcement law amazon alone implementing facial recognition technologies unaccountable ways investigative journalists recently disclosed ibm new york city police department nypd partnered develop system included ethnicity search custom feature trained thousands hours nypd surveillance footage use facial recognition software private sector expanded well major retailers venues already begun using technologies detect shoplifters monitor crowds even scan unhappy customers using facial recognition systems instrumented affect detection capabilities concerns ampliﬁed lack laws regulations currently federal legislation seeks provide standards restrictions requirements guidance regarding development use facial recognition technology fact existing federal legislation looks promote use facial recognition surveillance immigration enforcement employment veriﬁcation domestic systems laws piecemeal none speciﬁcally address facial recognition among biometric information privacy act illinois law sets forth stringent rules regarding collection biometrics law mention facial recognition given technology widely available many requirements obtaining consent reasonably interpreted apply recently several municipalities local transit system adopted ordinances seek create greater transparency oversight data collection use requirements regarding acquisition surveillance technologies would include facial recognition based expansive deﬁnition ordinances opposition use facial recognition tools government agencies growing earlier year joined aclu research advocacy organizations calling amazon stop selling facial recognition software government agencies aclu uncovered documents showing law enforcement use amazon rekognition api members congress also pushing amazon provide information gone calling outright ban scholars woodrow hartzog evan selinger argue facial recognition technology tool oppression perfectly suited governments display unprecedented authoritarian control machine necessitating extreme caution diligence applied contemporary digital ecosystem critiquing stanford gaydar study claimed deep neural network accurate humans predicting sexuality facial images frank pasquale wrote scientiﬁc research programs best pursued might one kade crockford director technology liberty program aclu massachusetts also wrote favor ban stating artiﬁcial intelligence technologies like face recognition systems fundamentally change balance power people government technologies dangerous balance power must microsoft president brad smith called government regulation facial recognition rick smith ceo law enforcement technology company axon recently stated accuracy thresholds facial recognition tools need making operational events year strongly underscored urgent need stricter regulation facial affect recognition technologies regulations severely restrict use public private sector ensure communities affected technologies ﬁnal arbiters whether used especially important situations basic rights liberties risk requiring stringent oversight audits transparency linkages permitted private government databases point given evidence hand policymakers funding furthering deployment systems public spaces risks automated decision systems government past year seen substantial increase adoption automated decision systems ads across government domains including criminal justice child welfare education immigration often adopted theory improve government eﬃciency ads seek aid replace various processes policy determinations however underlying models often proprietary systems frequently untested deployment many community advocates raised signiﬁcant concerns lack due process accountability community engagement auditing case tammy dobbs moved arkansas signed state disability program help cerebral palsy program state sent qualiﬁed nurse assess tammy determine number caregiver hours would need tammy spent waking hours wheelchair stiffness hands initial assessment allocated hours home care per week fast forward state assessor arrived new ads laptop using proprietary algorithm system calculated number hours tammy would allotted without explanation opportunity comment discussion reassessment program allotted tammy hours per week massive sudden drop tammy chance prepare severely reduced quality life tammy situation exceptional according legal aid arkansas attorney kevin liban hundreds individuals disabilities also received dramatic reductions hours without meaningful opportunity understand contest allocations legal aid subsequently sued state arkansas eventually winning ruling new algorithmic allocation program erroneous unconstitutional yet much damage lives affected done arkansas disability cases provide concrete example substantial risks occur governments use ads decisions immediate impacts vulnerable populations individual assessors may also suffer bias ﬂawed logic impact decisions nowhere near magnitude scale single ﬂawed ads across entire population increased introduction systems comes time according world income inequality database united states highest income inequality rate western countries moreover federal reserve data shows wealth inequalities continue grow racial wealth disparities tripled last years current policies set exacerbate problems alone seen executive order cutting funding social programs serve country poorest citizens alongside proposed federal budget signiﬁcantly reduce affordable housing implementation onerous work requirements medicaid proposal cut food assistance beneﬁts seniors people disabilities context policies agencies immense pressure cut costs many looking ads means automating hard decisions real effects need many ads systems often implemented goal less context austerity policies frequently designed conﬁgured primarily achieve goals ultimate effectiveness evaluated based ability trim costs often expense populations tools ostensibly intended serve researcher virginia eubanks argues seems like effort lower program barriers remove human bias often opposite effect blocking hundreds thousands people receiving services problems arise frequently diﬃcult remedy ads designed implemented ways easily allow affected individuals contest mitigate adverse incorrect decisions additionally human discretion ability intervene override system determination often substantially limited removed case managers social workers others trained understand context nuance particular person situation workers become mere intermediaries communicating inﬂexible decisions made automated systems without ability alter unlike civil servants historically responsible decisions many ads come private vendors frequently implemented without thorough testing review auditing ensure ﬁtness given domain systems typically built explicit form oversight accountability makes discovery problematic automated outcomes diﬃcult especially since errors evidence discrimination frequently manifest collective harms recognizable pattern across many individual cases detecting problems requires oversight monitoring also requires access data often neither available advocates public monitored government agencies example houston federation teachers sued houston independent school district procuring ads use student test data make teacher employment decisions including teachers promoted terminated revealed one district single employee could explain even replicate determinations made system even though district access underlying data teachers sought contest determinations told black box system simply believed could questioned even teachers brought lawsuit claiming constitutional civil rights labor law violations ads vendor fought providing access system worked result judge ruled use ads public employee cases could run afoul constitutional due process protections especially trade secrecy blocked employees ability understand decisions made case subsequently settled district agreeing abandon ads similarly los angeles county adopted ads assess imminent danger harm children predict likelihood family child welfare system within months county perform review system assess eﬃcacy using predictive analytics child safety welfare death child system failed identify county leadership directed review raised serious questions regarding system validity review speciﬁcally noted system failed provide comprehensive picture given family instead focus broad strokes without giving weight important virginia eubanks found similar problems investigation ads developed private vendor use allegheny county system produced biased outcomes signiﬁcantly oversampled poor children working class communities especially communities color effect subjecting poor parents children frequent investigation even face acknowledged issues bias potential error domains systems rapidly adopted ministry social development new zealand supported use predictive ads system identify children risk maltreatment despite recognizing system raised signiﬁcant ethical defended grounds beneﬁts plausibly outweighed potential harms included reconﬁguring child welfare statistical issue cases highlight need greater transparency oversight accountability adoption development implementation ads also need examination limitations systems overall economic policy factors accompany push apply systems virginia eubanks investigated allegheny county use ads child welfare looked number case studies show ads often adopted avoid obfuscate broader structural systemic problems society problems often beyond capacity agencies address meaningfully automated systems also proposed strategy combat problems within government systems years criminal justice advocates researchers pushed elimination cash bail shown disproportionately harm individuals based race socioeconomic status time failing enhance public safety response new jersey california recently passed legislation aimed addressing concern however instead simply ending cash bail replaced pretrial assessment system designed algorithmically generate risk scores claim predict whether person free detained jail awaiting trial shift policies cash bail automated systems risk assessment scoring still relatively new proceeding even without substantial research examining potential amplify discrimination within criminal justice system yet early indicators raise concern new jersey law went effect state experienced decline pretrial population advocates expressed worry racial disparities risk assessment system persist similarly california legislation passed earlier year many criminal justice advocates pushed end cash bail supported earlier version bill opposed ﬁnal version due risk assessment requirement education policy also feeling impact automated decision systems university college london professor among argued replace standardized testing suggesting ucl knowledge lab aiassess trusted assessment children knowledge understanding serve replace augment traditional testing however much like forms growing body research shows automated essay scoring systems may encode bias certain linguistic ethnic groups ways replicate patterns marginalization unfair decisions based automated scores assigned students historically systemically disadvantaged groups likely profound consequences children lives exacerbate existing disparities access employment opportunities resources implications educational ads beyond testing areas school assignments even transportation city boston spotlight year two failed efforts address school equity via automated systems first school district adopted school assignment algorithm intended provide students access higher quality schools closer home city goal increase racial geographic integration school district report assessing impact system determined opposite shortened student commutes ultimately reduced school integration researchers noted part impossible system meet intended goal given history context within used geographic distribution quality schools boston already inequitable racial disparities played role placement schools created complications could overcome algorithm following boston school district tried use algorithmic system improve inequity time designing reconﬁgure school start times aiming begin high school later middle school earlier done effort improve student health performance based recognition students circadian rhythms different ages optimize use school buses produce cost savings also aimed increase racial equity since students color primarily attended schools inconvenient start times compounded long bus rides city developed ads optimized goals however never implemented signiﬁcant public backlash ultimately resulted resignation superintendent case design process failed adequately recognize needs families include deﬁning reviewing system goals proposed system parents children high school middle school would need reconﬁgure schedules vastly different start end times putting strain without ﬂexibility national association advancement colored people naacp lawyers committee civil rights economic justice opposed plan school district failure appreciate parents color parents often rely jobs lack work schedule ﬂexibility may able afford additional child care failed efforts demonstrate two important issues policymakers must consider evaluating use systems first unaddressed structural systemic problems persist likely undermine potential beneﬁts systems addressed prior system design implementation second robust meaningful community engagement essential system put place included process establishing system goals purpose algorithmic impact assessment aia framework community engagement integral part ads accountability process part design stage well implementation affected communities opportunity assess potentially reject use systems acceptable call fundamental ﬂaws system put place validity legitimacy system vastly improved engagement serves communities government agencies parents color parents boston meaningfully engaged assessing goals school start time algorithmic intervention concerns might accounted design system saving city time resources providing model oversight accountability government use algorithmic systems impossible systems making recommendations black vendors insist trade secrecy keep systems opaque makes path redress appeal extremely diﬃcult vendors waive trade secrecy legal claims would inhibit ability understand audit test systems bias error issues important people government study effects systems understand automated recommendations made able trust validity even critical whose lives negatively impacted systems able contest appeal adverse decisions governments cautious automated decision systems may promise cost savings eﬃciencies governments third party vendors ultimately held responsible failings without adequate transparency accountability oversight systems risk introducing reinforcing unfair arbitrary practices critical government determinations policies experimenting society bears burden last ten years funding focus technical research development accelerated efforts ensuring systems safe received resources attention currently established methods measuring validating monitoring effects systems wild systems tasked signiﬁcant decision making effectively tested live populations often little oversight clear regulatory framework example march uber navigating phoenix suburbs failed see woman hitting killing last march tesla conﬁrmed second driver killed accident car autopilot technology engaged neither company suffered serious consequences case uber person minding autonomous vehicle ultimately blamed even though uber explicitly disabled vehicle system automatically applying brakes dangerous situations despite fatal errors alphabet waymo recently announced plans early rider program phoenix residents sign waymo test subjects driven automatically process many claim occasional autonomous vehicle fatality needs put context existing ecosystem many deaths happen without however regulations liability regimes govern humans machines differently risks generated interactions cleanly fall discrete regulatory accountability category strong incentives regulatory jurisdictional arbitrage exist many domains example fact phoenix serves site waymo uber testing accident early year arizona perhaps swayed promise technology jobs capital made oﬃcial state allowed practice since fully autonomous vehicles without anyone behind wheel permitted public roads policy put place without regulatory scaffolding would required contend complex issues raised terms liability accountability words phoenix new times arizona agreed step aside see technology develops something goes wrong well plan regulatory accountability gap clearly visible uber death case apparently caused combination corporate expedience disabling automatic braking system backup driver distraction autonomous vehicles arguably present straightforward dangers human safety domains also raise serious concerns example ibm watson oncology already tested hospitals across globe assisting patient diagnostics clinical care increasingly effectiveness promises ibm marketing questioned investigative reporters gained access internal documents paint troubling picture ibm system including recommending unsafe incorrect cancer system still trial phase raised serious concerns incentives driving rush integrate technology lack clinical validation research attesting ibm marketing claims effectiveness events slowed deployment healthcare recently food drug administration fda issued controversial decision clear new apple watch features electrocardiogram ekg ability notify user irregular heart rhythm safe consumers concerns fda may moving quickly attempt keep pace innovation joined concerns around data privacy security similarly deepmind health decision move streams application tool designed support nurses health practitioners umbrella google caused worry deepmind promise share data patients would broken children young adults frequently subjects experiments earlier year revealed pearson major vendor inserted interventions one commercial learning software programs test students would respond without consent knowledge students parents teachers company tracked whether students received messages learning software attempted completed problems students psychological testing unknowing populations especially young people education system raises signiﬁcant ethical privacy concerns also highlights growing inﬂuence private companies purportedly public domains lack transparency due process accompany current practices deployment integration see examples real harms come biased inaccurate systems evidence industry willingness conduct early releases experimental tools human populations amazon recently responded criticized monetizing people wedding baby registries deceptive advertising tactics constantly repeated pattern market dominance proﬁts valued safety transparency assurance without meaningful accountability frameworks well strong regulatory structures kind unchecked experimentation expand size scale potential hazards emerging solutions bias busting formulas fairness limits technological fixes past year seen growing consensus systems perpetuate amplify bias computational methods inherently neutral objective recognition comes wake string examples including evidence bias algorithmic pretrial risk assessments hiring algorithms aided work fairness accountability transparency machine learning community community center emerging body academic research bias fairness producing insights nature issues along methods aimed remediating bias approaches operationalized industrial settings search algorithmic fairness many deﬁnitions fairness along strategies achieve proposed past years primarily technical community work informed development new algorithms statistical techniques aim diagnose mitigate bias success techniques generally measured one another computational deﬁnition fairness based mathematical set results however problems techniques ultimately aim remedy deep social historical roots cleanly captured discrete mathematical representations others brief survey prominent approaches understanding deﬁning issues involving algorithmic bias fairness harms describe effects systems unfairly withhold services resources opportunities harms captured much attention dedicated building technical interventions ensure fair systems part theoretically possible quantify harms remediation however seen less attention paid ﬁxing systems amplify reproduce representational harms harm caused systems reproduce amplify harmful stereotypes often ways mirror assumptions used justify discrimination inequality keynote conference neural information processing neurips cofounder kate crawford described way historical patterns discrimination classiﬁcation often construct harmful representations people based perceived differences reﬂected assumptions data inform systems often resulting allocative harms perspective requires one move beyond locating biases algorithm dataset consider role harmful representations human identity way harmful representations shaped shape social cultural understandings fairness strategies attempt diagnose mitigate bias considering dataset either data used training model input data processed model applying methods data aimed detecting whether encodes bias individuals groups based characteristics race gender socioeconomic standing characteristics typically referred protected sensitive attributes majority observational fairness approaches categorized form either classiﬁcation parity calibration proposed sam sharad goel observational fairness strategies increasingly emerged efforts community contend limitations technical fairness work provide entry points disciplines strategies declare machine learning model fair depend protected attributes data set instance strategy considers pretrial risk assessment two defendants differ based race gender identical terms personal information fair assigned risk strategy often requires omitting protected attributes proxies data set used train model proxies attributes correlated protected attributes zip code correlated race parity declares model fair predictive performance equal across groupings deﬁned protected attributes example classiﬁcation parity would ensure percentage people algorithm turns loan actually creditworthy false negative rate black white populations practice strategy often results decreasing accuracy certain populations order match others strategies look less data outcome system produced decision prediction approaches work ensure outcomes depend protected attributes example case pretrial risk assessment applying calibration strategy would aim make sure among pool defendants similar risk score proportion actually reoffend release across different protected attributes race several scholars identiﬁed limitations approaches observational fairness respect argue important cases protected race included data used train inform system order ensure equitable decisions example goel discuss importance including gender pretrial risk assessment women reoffend less often men many jurisdictions risk assessments tend overstate recidivism risk women lead unnecessarily harsh judicial result jurisdictions use risk assessment tools cases counter widespread view deleting suﬃcient information data sets eventually debias system since correlations variables dataset almost always exist removing variables result little information thus poor predictive performance without ability measure potential harms post hoc secondly argued different mathematical fairness criteria mutually exclusive hence generally possible except highly constrained cases simultaneously satisfy calibration form classiﬁcation parity impossibility results show fairness strategy makes implicit assumptions fair also highlight inherent mathematical facing aiming mitigate various forms bias based one another fairness deﬁnition ultimately ﬁndings serve complicate broader policy debate focused solving bias issues mathematical fairness tools make clear solving complex policy issues related bias discrimination indiscriminately applying one fairness metrics unlikely successful mean metrics useful observational criteria may help understanding around whether datasets systems meet various notions fairness bias subsequently help inform richer discussion goals one hopes achieve deploying systems complex social contexts proliferation observational fairness methods also raises concerns potential provide false sense assurance researchers often nuanced sense limitations tools others might implement may ignore limits looking quick ﬁxes idea treated methods systems free bias safe use sensitive domains provide dangerous sense false relies heavily mathematical deﬁnitions fairness without looking deeper social historical context legal scholar frank pasquale observes algorithms alone meaningfully hold algorithms increased attention problems fairness bias positive development expressed concern mathematization shira mitchell argued statistical thinkers political sphere aware hazards supplanting politics expert discourse general every statistical intervention conversation tends raise technical bar entry reduced conversation technical speaking statistics power merely providing power new tools marginalization unquantiﬁed political concerns concerns new upcoming work hutchinson mitchell surveys ﬁfty years attempts construct quantitative fairness deﬁnitions across multiple disciplines work recalls period researchers focused deﬁning fairness educational assessments ways echo current fairness debate efforts stalled unable agree broad technical solutions issues involved precedents emphasize fairness accountability transparency machine learning community discovering without tight connection real world impact added value new fairness metrics algorithms machine learning community could minimal order arrive meaningful research fairness algorithmic bias must continue pair expertise perspectives communities outside technical disciplines within broader approaches dobbe drawn deﬁnition bias proposed early design vsd literature propose broader view fairness vsd theorized nineties batya friedman helen nissenbaum asserts bias computer systems system bias reﬂected data informs systems embedded assumptions made construction computer system bias manifests operation systems due feedback loops dissonance system dynamic social cultural contexts vsd approach one way bring broader lens issues emphasizing interests perspectives direct indirect stakeholders throughout design process another approach social systems analysis ﬁrst described kate crawford ryan calo nature method combines quantitative qualitative research methods forensically analyzing technical system also studying technology deployed social settings proposes engage social impacts every design deployment regulation technology across life cycle also seen increased focus examining provenance construction data used train inform systems data shapes systems view world understanding created meant represent essential understanding limits systems informs initial remedy problem group researchers led timnit gebru proposed datasheets datasets standardized form documentation meant accompany datasets used train inform systems paper looks standardizing provenance models approaches allow practitioners overseeing assessing applicability within given context better understand whether data shapes given model appropriate representative potentially possessing legal ethical issues advances fairness formulas strong signs ﬁeld accepted concerns real however limits narrow mathematical models continue undermine approaches broader perspectives included approaches fairness bias must take account allocative representational harms debate deﬁnitions fairness bias must recognize give voice individuals communities affected formulation fairness excludes impacted populations institutional context system deployed limited industry applications toolkits system tweaks year also seen several technology companies operationalize fairness deﬁnitions metrics tools last year four biggest companies released bias mitigation tools ibm released fairness tool kit includes nine different algorithms many fairness metrics developed researchers fairness accountability transparency machine learning community toolkit intended integrated software development pipeline early stages data training process use speciﬁc mathematical models deploy bias mitigation strategies google people research group pair released tool dashboard allowing researchers visualize effects different bias mitigation strategies metrics well tool called facets supports around fairness metric use microsoft released python package meant help implement binary classiﬁer subject developer intended fairness constraint facebook announced creation testing tool called fairness flow internal tool facebook engineers incorporates many algorithms help identify bias machine learning models even accenture consulting ﬁrm developed internal software tools help clients understand essentially eliminate bias industry standards bodies also taken fairness efforts response industry public sector requests accountability assurances institute electrical electronics engineers ieee recently announced ethics certiﬁcation program autonomous intelligent systems hopes creating marks attest broader public system transparent accountable fair effort new ieee published certiﬁcation underlying methods hard see given complexity issues settling one certiﬁcation standard across contexts systems would ultimately ensuring systems used safe ethical ways similar concerns arisen contexts privacy certiﬁcation programs rapid industrial adoption academic fairness methods rush certiﬁcation see eagerness solve eliminate problems bias fairness using familiar approaches skills avoid need signiﬁcant structural change fail interrogate complex social historical factors play combining academically credible technical fairness ﬁxes certiﬁcation check boxes runs risk instrumenting fairness ways lets industry say ﬁxed problems may divert attention examining ongoing harms also relieves companies responsibility explore complex costly forms review remediation rather relying quick ﬁxes tools certiﬁcations issues bias fairness require deeper consideration robust accountability frameworks including strong disclaimers automated fairness relied truly eliminate bias systems ethics enough recommendation report advised ethical codes meant steer ﬁeld accompanied strong oversight accountability seen rush adopt codes many instances offered means address growing controversy surrounding design implementation systems seen strong oversight accountability backstop ethical commitments revealed google working pentagon project systems drone debate role weapons systems grew intensity project maven generated signiﬁcant protest among google employees successfully petitioned company leadership end involvement program current contract expired way response google ceo sundar pichai released public set seven guiding principles designed ensure company work socially responsible ethical principles include commitment socially beneﬁcial avoid creating reinforcing unfair also include section titled applications pursue includes weapons technologies whose principal purpose implementation cause directly facilitate injury people direct response company decision renew contract department defense clear public would oversee implementation principles ethics board named google alone companies including microsoft facebook police body camera maker axon also assembled ethics boards advisors teams addition technical membership organizations moved update several ethical codes ieee reworked code ethics reﬂect challenges autonomous systems researchers association computing machinery acm called restructuring peer review processes requiring authors technical papers consider potential adverse uses work common practice universities including harvard nyu stanford mit offered new courses ethics ethical development practices aimed identifying issues considering ramiﬁcations technological innovation implemented scale university montreal launched process formulate declaration responsible development includes expert summits open public deliberations input citizens developments encouraging noteworthy heart development declared taking ethics seriously ethical initiatives help develop shared language discuss debate social political concerns provide developers company employees stakeholders set value statements objectives actions later judged also educational often work raising awareness particular risks within given institution externally amongst broader concerned public however developing socially equitable systems require ethical language however may see two classes problems current approach ethics ﬁrst enforcement accountability ethical approaches industry implicitly ask public simply take corporations word say guide conduct ethical ways public may able compare post hoc decision made company guiding principles allow insight decision making power reverse guide decision analysis google principles lucy suchman pioneering scholar human computer interaction argues without requisite bodies deliberation appeal redress vague ethical principles like evil right thing trust form corporate also potential displace forestall comprehensive binding forms governmental regulation ben wagner vienna university economics business argues unable unwilling properly provide regulatory solutions ethics seen easy soft option help structure give meaning existing words ethical codes may deﬂect criticism acknowledging problems exist without ceding power regulate transform way technology developed applied fact former facebook operations manager claims trust facebook regulate taken account evaluating ethical codes industry second problem relates deeper assumptions worldviews designers ethical codes technology industry response proliferation corporate ethics initiatives greene undertook systematic critical review vision statements ethical one ﬁndings statements tend adopt technologically deterministic worldview one ethical agency decision making delegated experts narrow circle adjudicate ethical concerns around behalf rest statements often assert promises great beneﬁts risks universal humanity without acknowledgement speciﬁc risks marginalized populations rather asking fundamental ethical political questions whether systems built documents implicitly frame technological progress inevitable calling better building empirical study use codes beginning preliminary results promising one recent study found explicitly instructing engineers consider acm code ethics decision making observed effect compared control however researchers ﬁnd media historical accounts ethical controversies engineering like volkswagen dieselgate may prompt reﬂective practice perhaps revealing evidence limitations emerging ethical codes corporations act formulate among list applications google promises pursue part principles technologies whose purpose contravenes widely accepted principles international law human tested earlier year investigative journalists revealed google quietly developing censored version search engine relies extensively capabilities chinese market dragonﬂy organizations condemned project violation human rights law violation google principles google employees also organized effort writing project cancelled continued development explained light clear commitment company principles although google ceo defended obvious need accountability oversight industry far move toward ethics meeting need likely part due incentives working implementations drastic momentary drop facebook twitter share price occurred announced efforts combat misinformation increase spending security privacy efforts excuse pursue ethically driven agenda suggest wary relying companies implement ethical practices voluntarily since many incentives governing large publicly traded technology corporations penalize ethical action mechanisms serve meaningful forms accountability requires external oversight transparency put place ensure exists external system checks balances addition cultivation ethical norms values within engineering profession technology needed next released report fairness formulas debiasing toolkits ethical guidelines rare fact commonplace today shows far ﬁeld come yet much needs done outline seven strategies future progress issues fairness justice debate bias fairness approach issues power hierarchy looking position produce proﬁt systems whose values embedded systems sets objective functions contexts intended work within echoing association computing machinery acm researcher call acknowledgement negative implications requirement peer review much attention must paid ways used tool exploitation control must also cautious reframe political questions technical concerns framed technical ﬁxes debiasing solutions rarely allow questions appropriateness eﬃcacy system altogether interrogation institutional context ﬁxed system ultimately applied example debiased predictive algorithm accurately forecasts crime occur used law enforcement harass oppress communities color still essentially unfair system end deﬁnitions fairness must expand encompass structural historical political contexts algorithmic systems deployed furthermore fairness term easily important questions fair context always asked example making facial recognition system perform equally people light dark skin may type technical progress terms parity technology disproportionately used people color communities really fair deﬁnitions fairness face hard limit remain purely contained within technical domain short parity infrastructural thinking order better understand track complexities systems need look beyond technology hype account broader context shaping shaped social material forces edwards argue dealing infrastructures need look whole array organizational forms practices institutions accompany make possible inﬂect development new requires experimental methodological approaches theory building expanding beyond narrow analyses individual systems isolation consider local global scale also requires considering ways technologies entangled social relations material dependencies political purposes anatomy system essay map cofounder kate crawford professor vladan joler took single amazon echo analyzed forms environmental labor resources required develop produce maintain ﬁnally dispose sleek seemingly simple object ask alexa play favorite song drawn massive interlinked chain extractive processes involves lithium mining bolivia clickworkers creating training datasets southeast asia container ships international logistics vast data extraction analysis alexa voice service avs across distributed data centers process ends ﬁnal resting place consumer gadgets rubbish heaps ghana pakistan china anatomy system project points approaches employ contending global implications nature value extraction exploitation developing world developed world helps illuminate darker corners rarely considered analysis systems particular infrastructural analysis shows black boxes within black boxes algorithmic level also levels trade secrecy laws labor practices untraceable global supply chains rare earth minerals used build consumer devices obscure material impacts systems intensive human work maintaining repairing practices like content moderation data training nick seaver puts see human loop need look bigger tracing across sociotechnical layers understand calling full stack supply chain human nonhuman components make global scale systems many sociotechnical data infrastructures needed make function include training data test data apis data centers ﬁber networks undersea cables energy use labor involved content moderation training set creation constant reliance clickwork develop maintain systems see global environmental labor implications tools everyday convenience meaningfully advocate fairness accountability transparency systems without understanding full stack supply chain accounting hidden labor systems another emerging research area expect see greater impact focuses underpaid unrecognized workers help build maintain test systems hidden human labor takes many supply chain work digital crowdsourced clickwork traditional service industry jobs hidden labor exists stages pipeline producing transporting raw minerals required create core infrastructure systems providing invisible human work often backstops claims magic systems deployed products services communications scholar lilly irani refers hidden labor research draws attention experiences clickworkers microworkers perform repetitive digital tasks underlie systems like labeling training data reviewing ﬂagged content workers hidden labor essential making systems work usually poorly compensated study united nations international labor organization ilo surveyed microworkers countries routinely offered labor popular microtask platforms like mechanical turk crowdﬂower microworker clickworker report found substantial number people earned local minimum wage despite respondents advanced degrees specializing science technology similarly content moderation work screening problematic content posted social media platforms news feeds also paid poorly spite essential emotionally diﬃcult labor lost technical research community begun call attention crucial marginalized role labor consider responsibility intervene silberman others discuss researchers conducting studies increasingly dependent upon cheap crowdsourced labor note years term crowdsourcing went appearing less scientiﬁc articles online microworkers unregulated current labor laws researchers asked reconsider counts ethical conduct research community silberman argue treating crowdworkers coworkers paying minimum wage determined client location need additional institutional review board irb oversight practice examining hidden human labor draws lineage feminist research concept invisible work instance originated studies unpaid women care work investigations organizational settings relied upon emotional labor particularly traditionally feminized ﬁelds like nursing ﬂight attendants researchers found common activities taken female workers soothing anxious patients managing unruly customers formally recognized compensated work spite essential feminist legacy invisible work useful contextualizing new forms labor understanding characterization work essential often written narrative rarely counted compensated article automation charade astra taylor proposes term fauxtomation call attention gap marketing rhetoric seamless product service messy lived reality automation frequently relies unsung human labor automation taylor cautions ideological function well technological making case critiques popular narratives around future labor posit workers replaced machines sees claims functioning disempower workers leverage workers demand better wages beneﬁts face impending automation saw narrative deployed former mcdonald ceo rensi cited growing fight movement impetus company introduction automated kiosks replace cashiers workers fought better pay would ultimately worse reasoned demand living wages would force company automate eliminate examining claim two years see entirely true automation workers still needed mcdonald added kiosks chicago ﬂagship store location reopened employees kiosks introduced integration automation workplace aimed automating worker tasks managing monitoring assessing workers alex rosenblatt ethnography uber drivers details precarity uncertainty produced depending whims centralized platform one livelihood algorithmic logic governs applications arbitrarily bar drivers work result unreliable wages unexpected costs nudge people working longer hours resulting unsafe driving conditions platforms isolate workers making concerted activity labor organizing diﬃcult also function create signiﬁcant information asymmetries companies aiming extract value workers workers even seen increasing dissent workers prominent examples protest include delivery riders striking alongside fast food industry employees rideshare drivers calling job protections silicon valley contractors working security food janitorial services within major technology companies also organized seeking living wage protections among thousands workers labor alongside technology worker peers classiﬁed independent contractors designation often paid low wages provided beneﬁts protections also rarely counted oﬃcial employee numbers even though make large portion technology industry workforces perform essential work example year contract workers outnumber google direct employees ﬁrst time company history increasing wave dissent makes visible social tensions heart practice hiding marginalizing important forms labor physical emotional ﬁnancial costs treating workers like bits code devaluing work highlighted recent news articles describing conditions amazon warehouse workers contracted prime delivery drivers amazon warehouse workers recently went strike europe protesting harsh conditions according one striking worker start company healthy leave broken human many workers requiring surgeries related workplace conditions recognizing labor required make work help better understand implications development use research areas also helps reexamine focus technical talent narratives describing creation recognize technical skills account portion much larger effort enables question numerous labor policies focus pushing workers acquire coding data science skills way ensure counted compensated also help identify likely beneﬁt along production deployment pipeline likely harmed deeper interdisciplinarity researchers developers engaged building technologies signiﬁcant implications diverse populations broad ﬁelds like law sociology medicine yet much development happens far removed experience expertise groups led call expand disciplinary makeup engaged design development critique beyond purely technical expertise since seen movement direction recently mit announced plans establish new college computing aims advance pioneering work ethical use societal impact fostering integrated training educating bilinguals future mit president rafael reif described initiatives critical becomes deeply embedded areas like healthcare criminal justice hiring housing educational systems experts domains essential ensure works envisioned integrating disciplinary perspectives important merely languages acquired computer scientists engineers seeking expand work new disciplines leading work instead social science humanities centered contributors ﬁeld foundational knowledge future direction enabling leverage new modes analysis methodological intervention race gender power year groundswell political action emerged around issues discrimination harassment inequity technology industry especially ﬁeld rising concern weaves together number related issues biases systems failed diversity inclusion efforts within industry academia grassroots efforts confront sexual harassment abuse power workplaces classrooms resonating broader metoo movement saw issues relating diversity inclusion artiﬁcial intelligence rise public agenda conference neural information processing systems members artiﬁcial intelligence machine learning communities began voicing concerns long standing problems harassment discrimination conference settings leading protestnips movement aimed highlighting examples toxicity community need address among things provoked change conference acronym longstanding subject sensitivity gendered historical connotations conference previously referred nips goes neurips also saw renewed focus initiatives devoted creating platforms inclusion ﬁeld black women machine learning latinx queer alongside appointment diversity inclusion chairs series changes design neurips intended foster equity inclusion among participants industry saw growing technology worker movement intersected issues google walkout particular took agenda acknowledged race class sexuality intertwined forms discrimination walkout explicitly aimed center needs company temporary contract workers vendors lack job security beneﬁts privileged technology workers efforts led signiﬁcant structural end forced arbitration sexual harassment claims across number largest companies industry arenas corporate boards ignored otherwise refused address shareholder proposals targeting discriminatory workspaces year google dismissed proposal would tie executive compensation progress made diversity inclusion apple refused mandate would require diversify board senior management across efforts advocates diversity ﬁnding intersections move address gender harassment abuse within technology community forms inequity abuses power still uphill battle increased attention problems bias systems yet see much research within fairness bias debate focused state equity diversity ﬁeld indeed reliable ﬁgures representation diﬃcult come although limited data exist recent estimate produced wired element found researchers contributed three leading machine learning conferences women gender gap replicated large technology ﬁrms like facebook google whose websites show research staff women reliable data state racial diversity ﬁeld retention rates people color collectively limited evidence suggests ﬁeld even less diverse computer science whole historic low point women make computer science majors united states decline high point trends even dramatic compared stem ﬁelds gender diversity shown marked improvement yet new problems wired survey signiﬁcantly different study ﬁeld published ieee expert found published authors journal prior four years women female grad students mit computer science artiﬁcial intelligence labs thoroughly documented experiences toxic working environments report barriers equality academia women computer science time address connection discrimination harassment community bias technical products produced community scholars science technology studies long observed values beliefs create technologies shape technologies create expanding ﬁeld frame reference recognize connection ensure better equipped address problems raised rapid proliferation sensitive social domains one researcher put bias datasets conferences recent example illustrates connections discriminatory practices within culture produces system mirrored ampliﬁed system amazon recently developed experimental system help rank job candidates trained system data reﬂecting company historical hiring preferences hoping eﬃciently identify qualiﬁed candidates system work expected based company historical hiring showed distinct bias women candidates downgrading resumes candidates attended two colleges even penalizing resumes contained word uncovering bias company attempted system adjusting algorithm treat terms fairly work project eventually scrapped discrimination embedded deeply within system system built reﬂect amazon past hiring practices uprooted using debiasing approach commonly adopted within ﬁeld scholars like saﬁya noble mar hicks observed clear connecting longstanding patterns discrimination harassment ways artiﬁcial intelligence technologies amplify contribute marginalization social inequity patterns cultural discrimination often embedded systems complex meaningful ways need better understand effects felt different communities space long overlooked research sorely needed publishing dedicated report issues research project dedicated examining challenges strategic litigation policy interventions year saw increase court challenges use automated systems particularly government agencies use decisions affect individual rights recent report called litigating algorithms documented ﬁve recent case studies litigation involving use automated systems medicaid disability beneﬁts cases public teacher employment evaluations juvenile criminal risk assessment criminal dna analysis ﬁndings brought light several emerging trends first cases provided concrete evidence governments routinely adopting automated decision systems ads measures produce cost savings streamline work yet failing assess systems might disproportionately harm populations meant serve particularly vulnerable little recourse even knowledge systems deeply affecting lives many cases single government employee could explain automated decision correct errors audit results determination series vendor contractor agreements almost avenues understanding contesting impact systems shielded legal protections trade secret law second government agencies invested real efforts ensure fairness due process protections remained place switching decisions ones typical audit appeals accountability mechanisms totally absent automated system design fortunately successful strategic litigation lawyers american civil liberties union aclu idaho legal aid arkansas houston federation teachers legal aid society new york various public defenders able secure victories clients challenge unlawful uses based part constitutional administrative due process litigation claims playbook litigate algorithms still written report uncovered several useful strategies support solutions protections first arguments based procedural due process presented serious challenges trade secrecy claims private vendors vast majority judges ruling right assert constitutional civil rights protections outweighs risk intellectual property misappropriation second failure notify affected individuals communities matters agencies neglected engage community groups concerning use systems often judged failed appropriately provide opportunity public notice comment meaning implementation systems potentially unconstitutional third interdisciplinary collaboration important trying determine systems fail especially submitting evidence judges cases lawyers worked closely technical social science experts judges able learn scientiﬁc ﬂaws systems well social ramiﬁcations harms looking forward anticipate future strategic litigation cases produce many lessons interventions generate greater understanding remedial accountability systems even situations government agencies attempted disclaim ownership understanding control combined tools algorithmic impact assessment framework alongside robust regulatory oversight regimes begin identify measure necessary intervene efforts use automated systems ways produce harm however order continue build recent progress lawyers community activists represent individuals suits need greater funding support well networks domain experts draw help advise strategy audit systems research organizing emergent coalition rapid deployment related systems everyday life concern already signs slowing recognizing set strategies emerged drawing traditions activism organizing demand structural changes greater accountability social activism technologists nothing new early computer professionals social responsibility formed oppose use computers warfare recently never technology pledge rallied thousands workers various technology sectors sign promise build databases conduct data collection could used target religious minorities facilitate mass deportations organizing activism draws long tradition scale new technology sector technology workers joining forces civil society organizations researchers opposition employers technical business decisions google employees kicked publicly visible organizing opposing project maven pentagon effort apply google machine vision capabilities department defense drone surveillance researchers human rights organizations joined cause june google announced would abandon project amazon salesforce microsoft employees petitioned leadership end contracts immigrations customs enforcement ice supported immigration advocacy organizations amazon employees also joined aclu petitioning company stop selling facial recognition law enforcement responding aclu work exposing existing contracts following maven google employees rose project dragonﬂy version google search engine enabling censorship surveillance planned chinese market response media reports disclosed secretive effort employees requested ethical oversight accountability joined amnesty international call cancel project signing name publicly open letter coincided amnesty international protests biggest moment occurred early november google workers walked around globe action called walkout real change walkout characterized google company abuse power systemic racism unaccountable organizers called leadership meet ﬁve demands including ending pay opportunity inequity eliminating forced arbitration cases sexual harassment discrimination adding employee representative board directors week walkout google met small portion demands agreeing end forced arbitration cases sexual harassment notably ignoring discrimination move quickly replicated throughout industry facebook square ebay airbnb following suit joining forces researchers civil society groups new wave labor organizing mirrors calls greater diversity openness within research domain movements incorporating diverse perspectives across class sector discipline working ensure capable understanding true costs company practices including impact systems build google workers participated walkout expanded coalition across class sector emphasizing contract workers demands situating within growing movement tech across country including teachers workers others using strength numbers make real recent surge activism largely driven whistleblowers within technology companies disclosed information secretive projects journalists disclosures helped educate public traditionally excluded access helped external researchers advocates provide informed analysis establishing shared ground truth whistleblowing helped build broad coalitions characterize movements critical role ethical whistleblowing last year also highlighted social importance lack protections make disclosures broad coalition technology worker organizers researchers civil society playing increasing role push accountability technology sector many engineering employees considerable bargaining power uniquely positioned demand change employers applying power push greater accountability presents hopeful model labor organizing public interest especially given current lack government regulation external oversight meaningful levers capable reviewing steering technology company decision making conclusion year saw systems rapidly introduced social domains leaving increasing numbers people risk techniques still offer considerable promise rapid deployment systems without appropriate assessment accountability oversight create serious hazards urgently need regulate systems particular attention paid facial affect recognition inform policies rigorous research regulation effective legal technological barriers prevent auditing understanding intervening systems removed back recommended ﬁrst report computer fraud abuse act cfaa digital millennium copyright act dmca used restrict research accountability auditing year companies waive trade secrecy legal claims would prevent algorithmic accountability public sector governments public institutions must able understand explain decisions made particularly people access healthcare housing employment line question longer whether harms biases systems debate settled evidence mounted beyond doubt last year next task addressing harms particularly urgent given scale systems deployed way function centralize power insight hands increasingly uneven distribution costs beneﬁts accompanies centralization need deeper analyses full stack supply chain behind systems track development deployment across product life cycle take account true environmental labor costs furthermore long overdue technology companies directly address cultures exclusion discrimination workplace lack diversity ongoing tactics harassment exclusion unequal pay deeply harmful employees companies also impacts products release producing tools perpetuate bias discrimination current structure within development deployment occurs works meaningfully addressing pressing issues position proﬁt incentivized accelerate development application systems without taking time build diverse teams create safety guardrails test disparate impacts exposed harm systems commonly lack ﬁnancial means access accountability mechanisms would allow redress legal appeals arguing greater funding public litigation labor organizing community participation algorithmic systems shift balance power across many institutions workplaces imperative balance power shifts back public favor require signiﬁcant structural change goes well beyond focus technical systems including willingness alter standard operational assumptions govern modern industry players current focus discrete technical ﬁxes systems expand draw disciplines histories strategies capable providing deeper understanding various social contexts shape development use systems universities turn focus study social implications computer science engineering longer unquestioned center collaborate equally social humanistic disciplines well civil society organizations affected communities fortunately beginning see new coalitions form researchers activists lawyers concerned technology workers civil society organizations support oversight accountability ongoing monitoring systems important connections grow protections needed including commitment technology companies provide protections conscientious objectors want work military policing contracts along protections employees involved labor organizing ethical whistleblowers last year revealed many hardest challenges accountability justice systems moved deeper social world yet extraordinary moments potential well signiﬁcant public debates hopeful forms protest may ultimately illuminate pathways consequential positive change endnotes pioneers stuart russell peter norvig point history artiﬁcial intelligence produced clear deﬁnition seen variously emphasizing four possible goals systems think like humans systems act like humans systems think rationally systems act report use term refer broad assemblage technologies early algorithmic systems deep neural networks rely array data computational infrastructures technologies span speech recognition language translation image recognition predictions traditionally relied human capacities across four goals russell norvig identify new recent developments ability collect store large quantities data combined advances computational power led signiﬁcant breakthroughs ﬁeld last ten years along strong push commercialize technologies apply across core social domains see stuart russell peter norvig artiﬁcial intelligence modern approach englewood cliffs prentice hall cadwalladr emma revealed million facebook proﬁles harvested cambridge analytica major data breach guardian march rosen security update facebook newsroom september eidelson facebook tools used screen older job seekers lawsuit claims bloomberg may editorial board think facebook problem look asia bloomberg october liptak government alleges facebook enabled housing discrimination verge august weise russian fake accounts showed posts million facebook users usa today october shaban craig timberg elizabeth dwoskin facebook google twitter testiﬁed capitol hill said washington post october casey newton mark zuckerberg appearance european parliament yields empty spectacle verge may harwell solve facebook vexing problems mark zuckerberg says ask washington post april conger dell cameron google helping pentagon build drones gizmodo march paulas new kind labor movement silicon valley atlantic september shaban amazon employees demand company cut ties ice washington post june jacob kastrenakes salesforce employees ask ceo contract border protection agency verge june colin lecher employee letter denouncing microsoft ice contract signatures verge june sonnad border agents hacked risk assessment system recommend detention time quartz june wakabayashi uber car kills pedestrian arizona robots roam new york times july sonnad flawed algorithm led deport thousands students quartz may ross ike swetlitz ibm watson recommended unsafe incorrect cancer treatments stat july joseph kenneth lipp ibm used nypd surveillance footage develop technology lets police search skin color intercept september see timeline events see kate crawford meredith whittaker year review medium october jon evans techlash techcrunch june microsoft calls facial recognition technology rules given potential abuse guardian july ram innovating criminal justice northwestern university law review february rebecca wexler life liberty trade secrets stanford law review may danielle keats citron frank pasquale scored society due process automated predictions washington law review frank pasquale black box society secret algorithms control money information cambridge harvard university press sculley jasper snoek alex wiltschko ali rahimi winner curse pace progress empirical rigor international conference learning representations iclr vancouver crawford test facebook atlantic july molly jackman lauri kanerva evolving irb building robust review industry research washington lee law review online june zoltan boka facebook research ethics board needs stay far away facebook wired june sandvig sessions challenge cfaa prohibition uncovering racial discrimination online september american civil liberties union simone browne dark matters surveillance blackness durham duke university press alvaro bedoya fbi surveillance martin luther king tells modern spy era slate january james ball julian borger glenn greenwald revealed spy agencies defeat internet privacy security guardian september shoshana zuboff big surveillance capitalism prospects information civilization journal information technology march shen facial recognition tech comes hong border south china morning post july chen china robotic spy birds take surveillance new heights south china morning post june morris china block travel bad social credit fortune march vanderklippe chinese blacklist early glimpse sweeping new control globe mail january china turned xinjiang police state like economist may feng louise lucas inside china surveillance state financial times july berwick new venezuelan created china zte tracks citizen behavior reuters november ahmed pentagon wants predict protests using social media surveillance motherboard october hao amazon invisible backbone behind ice immigration crackdown mit technology review october behind ice empower llc mijente national immigration project immigrant defense project october shillingford visual speech recognition arxiv preprint july machine vision algorithm learns recognize hidden facial expressions mit technology review november gray face german physiognomic thought lavater auschwitz detroit wayne state university press sharrona pearl faces physiognomy britain cambridge harvard university press aguera arcas margaret mitchell alexander todorov physiognomy new clothes medium may leys fear become scientiﬁc object kind object representations leys offered number critiques ekman research program recently ruth leys ascent affect genealogy critique chicago university chicago press fridlund human facial expression evolutionary view san diego academic press feldman barrett emotions natural kinds perspectives psychological science march erika siegel molly sands wim van den noortgate paul condon yale chang jennifer karen quigley lisa feldman barrett emotion fingerprints emotion populations investigation autonomic features emotion psychological bulletin example despite criticism government accountability oﬃce transportation security administration invested one billion dollars spot program aimed identifying potential terrorists based behavioral indicators see aviation security tsa limit future funding behavior detection activities washington government accountability oﬃce november metzl protest psychosis schizophrenia became black disease boston beacon press lieberman sentiment analysis allows instructors shape course content around students emotions inside higher education february knight emotional intelligence might virtual assistant secret weapon mit technology review june affectiva automotive affectiva accessed november weiner aclu amazon software matched members congress mugshots orlando sentinel july amazon rekognition announces face recognition text image recognition improved face detection amazon web services november adzima using amazon rekognition identify persons interest law enforcement amazon web services june das image video rekognition based aws amazon web services summit seoul snow amazon face recognition falsely matched members congress mugshots american civil liberties union july buolamwini timnit gebru gender shades intersectional accuracy disparities commercial gender classiﬁcation conference fairness accountability transparency new york wood thoughts machine learning accuracy aws news blog july fussell amazon accidentally makes case giving face recognition tech police gizmodo july menegus amazon breaks silence aiding law enforcement following employee backlash gizmodo august lipp ibm used nypd surveillance footage develop technology lets police search skin bitar jay stanley stores shop secretly using face recognition american civil liberties union march brandon walmart scan unhappy shoppers using facial recognition cue apocalypse venturebeat august legal workforce act cong securing america future act cong strong visa integrity secures america act cong security immigration reform act cong information privacy act ilcs statutes current end regular session general assembly hofer bart board approves surveillance ordinance lake merritt development ktvu september letter nationwide coalition amazon ceo jeff bezos regarding rekognition american civil liberties union june congress still wants answers amazon facial recognition tech cnet november hartzog evan selinger facial recognition perfect tool oppression medium august wang michael kosinski deep neural networks accurate humans detecting sexual orientation facial images journal personality social psychology pasquale machine learning facially invalid communications acm august crockford massachusetts ban face recognition technology wbur august see also hartzog selinger facial recognition perfect tool smith facial recognition technology need public regulation corporate responsibility microsoft issues july sidney fussell axon ceo says face recognition accurate enough body cams yet gizmodo august litigating algorithms challenging government use algorithmic decision systems new york institute september dillon reisman jason schultz kate crawford meredith whittaker algorithmic impact assessments practical framework public agency accountability new york institute april micah altman alexandra wood effy vayena framework algorithmic fairness ieee security privacy may healthcare algorithm started cutting care one knew gradín world income inequality database united nations university october lei nine charts wealth inequality america urban institute lisa dettling joanne hsu lindsay jacobs kevin moore jeffrey thompson recent trends race ethnicity evidence survey consumer finances washington board governors federal reserve system september restuccia sarah ferris helena bottemiller evich behind trump plan target federal safety net politico december booker white house budget calls deep cuts hud february kodjak federal judge blocks medicaid work requirements kentucky june rachel garﬁeld implications work requirements medicaid data say henry kaiser family foundation june dean president budget would shift substantial costs states cut food assistance millions center budget policy priorities may pegg niamh mcintyre child abuse algorithms science fiction reality guardian september scharfenberg computers solve problem may like answer boston globe september joe flood fires computer formula big ideas best intentions burned new york determined future cities new york riverhead books eubanks created poverty algorithms make away guardian may morales federal court rules idaho department health welfare medicaid class action aclu idaho march litigating nash examination using structured decision making predictive analytics assessing safety risk child welfare los angeles county los angeles oﬃce child protection may eubanks automating inequality tools proﬁle police punish poor new york martin press vulnerable children predictive modelling wellington new zealand ministry social development accessed november automating inequality talbot case cash bail new yorker august levin imprisoned algorithms dark side california ending cash bail guardian september maddie hanna happened new jersey stopped relying cash bail philadelphia inquirer february dea civil rights coalition calls end core element bail reform spotlight july white california ended cash bail many reformers unhappy politico magazine august rose luckin towards artiﬁcial assessment systems nature human behaviour march bridgeman catherine trapani yigal attali comparison human machine scoring essays differences gender ethnicity country applied measurement education january madnani building better tools support fairness automated scoring proceedings first acl workshop ethics natural language processing valencia association computational linguistics brien evaluation equity boston public schools assignment policy boston boston area research initiative july computers solve problem may like reisman jason schultz kate crawford meredith whittaker algorithmic impact assessments practical framework public agency accountability new york institute april litigating algorithmic accountability policy toolkit new york institute october algorithmic impact uber car kills pedestrian stewart tesla autopilot involved another deadly crash wired march uber driver streamed voice deadly arizona crash report says los angeles times june early rider program waymo accessed november marshall wan save lots lives put imperfect cars road asap wired november moore autonomous vehicles continue drive without rules phoenix new times november uber driver streamed voice deadly arizona crash report swetlitz ibm watson recommended unsafe incorrect cancer chen apple watch fda clearance actually means verge september vermette apple watch approval marks shift device development approvals med device online october rajiv leventhal patient records breached protenus finds healthcare informatics magazine november hern google betrays patient trust deepmind health move guardian november herold pearson tested messages learning software mixed results education week april winkler laura stevens new parents complain amazon ads deceptive wall street journal november angwin jeff larson laura kirchner machine bias software used across country predict future criminals biased propublica may jeffrey dastin amazon scraps secret recruiting tool showed bias women reuters october mitchell mirror mirror reﬂections quantitative fairness arvind narayanan tutorial fairness deﬁnitions politics accessed november barocas kate crawford aaron shapiro hanna wallach problem bias allocative versus representational harms machine learning measure model mix computer instrument annual sigcis conference philadelphia crawford trouble bias conference neural information processing systems long beach sharad goel measure mismeasure fairness critical review fair machine learning arxiv preprint july barocas moritz hardt fairness machine learning conference neural information processing systems long beach narayanan fairness deﬁnitions politics jan redlining banned years ago still hurting minorities today washington post march dwork nicole immorlica adam kalai mark leiserson decoupled classiﬁers eﬃcient machine learning conference fairness accountability transparency january kleinberg inherent algorithmic fairness abstracts acm international conference measurement modeling computer systems sigmetrics new york acm alexandra chouldechova fair prediction disparate impact study bias recidivism prediction instruments arxiv preprint stat october pasquale odd numbers real life august benthall critical reﬂections fat historical idealist perspective dataactive april mirror mirror reﬂections quantitative hutchinson margaret mitchell years test fairness lessons machine learning arxiv preprint november dobbe sarah dean thomas gilbert nitin kohli broader view bias automated reﬂecting epistemology dynamics arxiv preprint math stat july friedman helen nissenbaum discerning bias computer systems interact chi conference companion human factors computing systems new york acm friedman helen nissenbaum bias computer systems acm transactions information systems july crawford ryan calo blind spot research nature october meredith whittaker data genesis primordial soup eyeo festival minneapolis gebru datasheets datasets arxiv preprint march mitchell model cards model reporting arxiv preprint october hutson jessie taft solon barocas karen levy debiasing desire addressing bias discrimination intimate platforms proceedings acm interaction cscw november singh michael hind fairness attacking bias angles ibm developer september wexler tool probing machine learning models google blog september james wexler facets open source visualization tool machine learning training data google blog june agarwal reductions approach fair classiﬁcation arxiv preprints march gershgorn facebook says tool detect bias artiﬁcial intelligence quartz may kahn accenture unveils tool help companies insure fair bloomberg june ethics certiﬁcation program autonomous intelligent systems ecpais ieee standards association accessed november truste settles ftc charges deceived consumers privacy seal program federal trade commission november shane cade metz daisuke wakabayashi pentagon contract became identity crisis google new york times november pichai google principles keyword june boyle microsoft turning sales ethics top researcher eric horvitz says geekwire april jordan novet facebook forms ethics team prevent bias software cnbc may axon policing technology ethics board axon accessed november ethically aligned design version vision prioritizing human autonomous intelligent systems new york ieee global initiative ethics autonomous intelligent systems brent hecht lauren wilcox jeffrey bigham johannes schöning ehsan hoque jason ernst yonatan bisk luigi russis lana yarosh bushra anjum danish contractor cathy time something mitigating negative impacts computing change peer review process acm future computing blog march tugend colleges grapple teaching technology ethics new york times november déclaration montréal pour développement responsable intelligence artiﬁcielle accessed november microsoft professional program artiﬁcial intelligence microsoft accessed november suchman corporate accountability robot futures june wagner ethics escape regulation proﬁling cogitas ergo sum mireille hildebrandt amsterdam amsterdam university press forthcoming parakilas trust facebook regulate new york times january greene anna laura hoffman luke stark better nicer clearer fairer critical assessment movement ethical artiﬁcial intelligence machine learning hawaii international conference system sciences maui forthcoming mcnamara justin smith emerson acm code ethics change ethical decision making software development proceedings acm joint meeting european software engineering conference symposium foundations software engineering new york acm google gallagher google plans launch censored search engine china leaked documents reveal intercept august deibert rebecca mackinnon xiao qiang lokman tsui open letter google reported plans launch censored search engine china amnesty international august kate conger daisuke wakabayashi google employees protest secret work censored search engine china new york times september bergen google ceo tells staff china plans exploratory backlash bloomberg august phillips facebook stock plunge shatters faith tech companies invincibility new york times october rupert neate twitter stock plunges wake user decline guardian july general description justice fairness see john rawls justice fairness restatement erin kelly cambridge harvard university press hecht time something green fair risk assessments precarious approach criminal justice reform workshop fairness accountability transparency machine learning stockholm green putting ustice fat berkman klein center february crawford engineer politics royal society london july edwards geoffrey bowker steven jackson robin williams introduction agenda infrastructure studies journal association information systems may leigh star ethnography infrastructure american behavioral scientist november paul edwards thomas misa philip brey infrastructure modernity force time social organization history sociotechnical systems modernity technology cambridge mit press shannon mattern big data ice rocks soils sediments places november plantin carl lagoze paul edwards christian sandvig infrastructure studies meet platform studies age google facebook new media society crawford vladan joler anatomy system amazon echo anatomical map human labor data planetary resources institute share lab september jackson rethinking repair media technologies essays communication materiality society tarleton gillespie pablo boczkowski kirsten foot eds cambridge mit press seaver anthropology algorithms cultural anthropology yuan cheap labor drives china ambitions new york times november mary gray siddharth suri humans working behind curtain harvard business review january irani hidden faces automation xrds december berg digital labour platforms future work towards decent work online world geneva international labour organization september roberts commercial content moderation digital laborers dirty work intersectional internet race sex class culture online saﬁya umoja noble brendesha tynes new york peter lang silberman responsible research crowds pay crowdworkers least minimum wage communications acm february kaplan daniels invisible work social problems arlie russell hochschild managed heart commercialization human feeling berkeley university california press taylor automation charade logic magazine october kasperkevic ceo suggests replacing employees robots amid protests guardian may rensi thanks fight minimum wage mcdonald unveils kiosks nationwide forbes november pavlova mcdonald kiosks mean staff chicago flagship fewer bloomberg august rosenblat uberland algorithms rewriting rules work berkeley university california press chapman uber eats deliveroo riders going strike week independent october carolyn said uber lyft drivers fear getting booted work san francisco chronicle october conger google facebook security guards fighting earn living wage gizmodo july bergen josh eidelson inside google shadow workforce bloomberg july ong amazon patents wristbands track warehouse employees hands real time verge february hayley peterson missing wages grueling shifts bottles urine disturbing accounts amazon delivery drivers may reveal true human cost free shipping business insider september keck amazon workers across europe protest black friday citing grueling work conditions gizmodo november rafael reif letter mit community regarding mit stephen schwarzman college computing mit news october steve lohr plans college artiﬁcial intelligence backed billion new york times october manovich think without categories digital culture society institute gender race power outlining new research agenda medium november lum statistics problem medium december brown nips conference changes name following protests gross acronym gizmodo november corinna cortes board changing acronym neurips november nips name change neurips october whittaker one one organizers google walkout see also caroline donovan ryan mac google engineers organizing walkout protest company protection alleged sexual harasser buzzfeed alba caroline donovan square airbnb ebay said would end forced arbitration sexual harassment claims buzzfeed florentine alphabet dismisses action diversity inclusion cio june sara ashley brien apple board calls diversity proposal unduly burdensome necessary cnnmoney january simonite future women wired august snow diversity crisis cofounder black poisoning algorithms lives mit technology review february johnson sheila widnall frazier benya sexual harassment women climate culture consequences academic sciences engineering medicine washington national academies press clark hayes computer science incredible shrinking woman gender codes women leaving computing thomas misa hoboken wiley strok women ieee expert intelligent systems applications august barriers equality academia women computer science cambridge laboratory computer science artiﬁcial intelligence laboratory february winner artifacts politics daedalus merity bias datasets conferences community december amazon scraps secret recruiting tool showed bias noble algorithms oppression search engines reinforce racism new york nyu press apprich wendy hui kyong chun florian cramer hito steyerl pattern discrimination minneapolis university minnesota press forthcoming litigating algorithmic impact cpsr history computer professionals social responsibility june never pledge accessed november shane daisuke wakabayashi business war google employees protest work pentagon new york times november suchman lilly irani peter asaro google march business war must stopped guardian may mary wareham letter sergey brin sundar pichai march daisuke wakabayashi scott shane google renew pentagon contract upset employees new york times november amazon employees demand company cut ties ice behind ice caroline donovan employees another major tech company petitioning government contracts buzzfeed news june sheera frenkel microsoft employees question company contract ice new york times july peter kotecki burning man protesters raise awareness palantir amazon ice ties business insider august sandoval amazon employees sign letter asking jeff bezos stop selling software police business insider june kade crockford people tell amazon stop selling facial recognition tech police american civil liberties union june matt cagle nicole ozer amazon teams government deploy dangerous new facial recognition technology aclu free future may gallagher google china prototype links searches phone numbers intercept september menegus letter google workers sent leadership protest censored search engine china gizmodo august google employees dragonﬂy google employees google must drop medium november google must capitulate china censorship demands amnesty international november meredith whittaker one eight core organizers google walkout walkout real change google employees contractors participate global walkout real change medium november waters google ends forced arbitration sexual harassment claims financial times november gibson tech signals end forced arbitration sexual misconduct claims cbs moneywatch november walkout real change googlewalkout update collective action works need keep medium november noam scheiber google workers reject silicon valley individualism walkout new york times november example google highly secret dragonﬂy project censor search results china link chinese residents phone numbers search logs see google employees google must drop medium november metz tech giants paying huge salaries scarce talent new york times october recommendations report new york institute joler anatomy system one many examples dastin amazon scraps secret recruiting tool showed bias women healthcare algorithm started cutting care one knew katyal private accountability age algorithm ucla law review forthcoming
