algorithms artificial intelligence discrimination report fra bias algorithms artificial intelligence discrimination vienna european union agency fundamental rights reproduction authorised provided source acknowledged use reproduction photos material european union agency fundamental rights copyright permission must sought directly copyright holders neither european union agency fundamental rights person acting behalf agency responsible use might made following information luxembourg publications office european union print sbn pdf sbn photo credits cover stock page stock page stock page stock page stock page stockpage stock page stock page stock page stock page ink stock page miha stock page stock page stock everywhere affects everyone social media feeds social benefits receive thanks technology receive information interested administrative decisions made efficiently vaccines developed faster decisions become increasingly automated vital technology works risks vast think infamous case hundreds innocent families falsely accused committed fraud forced return social benefits many families immigration background demonstration use biased algorithm ultimately lead underlines technology needs regulated harness astonishing potential look hood understand algorithms really work become aware risks engaging report set lift curtain show algorithms work practice developed tested algorithmic models areas offensive speech detection predictive policing quickly found automated hate speech detection unreliable harmless phrases jewish muslim may get flagged offensive yet offensive content may easily slip also looked automated predictive policing found algorithms generate mistaken information essentially sending police wrong parts city major concern often police forces using technology tests highlight easily algorithms biased develop bias time lead discrimination mean need abolish stop investing artificial intelligence means humans still important role play need understand well algorithms work biases stay closely involved monitoring need always test applications context use contexts algorithms help great deal others may fit purpose high time dispel myth human rights block going forward human rights mean trustworthy technology trustworthy technology attractive technology long run also successful get right look forward future cures diseases beyond dreams right look delivery public services degree efficiency quality simply case today move right direction astonishing could achieve let work together getting right michael flaherty director contents foreword key findings fra opinions considering development bias algorithms time feedback loops uncovering biases language prediction tools striving language diversity increase access resources needed oversight algorithms endnotes artificial intelligence bias problem report tificial intelligence bias fundamental rights policy context bias algorithms endnotes feedback loops algorithms influence algorithms law bias predictive policing systems predictive policing european union challenges linked use predictive policing sim ulating feedback loops policing addres sing feedback loops concluding points endnotes ethnic gender bias offensive speech detection fund amental rights affected biased speech detection using artificial intelligence offensive speech detection speech detection algorithms tested bias resul ethnic gender bias offensive speech detection resul gender bias offensive speech detection addres sing bias speech detection concluding points endnotes looking forward sharpening fundamental rights focus artificial intelligence mitigate bias discrimination endnotes references annex glossary annex additional technical details simulation feedback loops annex iii technical details offensive speech detection analysis endnotes figures tables figure simplis tic illustration feedback loop figure schema tic description simulated predictive policing process figure esults simulation allocation patrols time simple probabilities simple machine learning algorithm figure sour bias simplified policing algorithms figure edicted crimes per day differing historical crime rates per neighbourhood figure edicted crimes per day historical crime rates reflecting true crime rates figure bias mitiga tion techniques reducing feedback loop formation figure verage false positive false negative rates across groups identity terms models english language figure verage false positive false negative rates across groups identity terms models german language figure verage false positive false negative rates across groups identity terms models italian language figure verage predictions violent speech selected origins figure ords contributions model prediction offensiveness selected sentences english model figure ords contributions model prediction offensiveness selected sentences italian model figure lik elihood comments using african american english dialect errors offensiveness predictions table edicted probabilities text offensive across selected groups table alse positive rates based gender used identity terms models table crime used inform parameter assumptions table char acteristics training data table example sen tences bias test dataset table erformance metrics models test datasets key findings fra opinions artificial intelligence algorithms affect people everywhere deciding content people see social media feeds determining receive state benefits technologies typically based algorithms make predictions support even fully automate among main goals using way increasing efficiency operating systems large scale central question fundamental rights concern happens algorithms become biased certain groups people women immigrants research agency fundamental rights fra highlights use affect many fundamental rights algorithms force good also violate right privacy lead discriminatory real impact people lives cautionary tale possible negative effects biased algorithms practice comes netherlands came light dutch tax authorities used algorithms mistakenly labelled around parents committed fraud childcare benefit applications many parents immigration background required pay back large sums led great financial psychological difficulties families concerned data protection authority concluded processing data system use despite recent sharper focus problem bias algorithms area still lacks tangible evidence base employs technical assessments algorithms practice outcomes exactly lead fundamental rights violations yet fully known assessments urgently needed fill gap report seeks contribute focus use cases algorithmic modelling use case term used software engineering among fields report loosely defines specific application technology certain goal used specified actor shows bias occur algorithms bias relates potential discrimination highlights complexities bias detection assessment potential discrimination illustrating bias occurs several stages different report demonstrates quick fix addressing bias algorithms clear needs system assessing mitigating bias using algorithms practice looking two use cases report questions need addressed include following ëwhat police repeatedly sent certain neighbourhoods based algorithms despite faulty crime predictions ëwhat legitimate content posted online certain groups gets deleted often others report explores questions based original research combining empirical evidence development testing algorithms fundamental rights discussions considering current policy developments based simulation experiments analysis conducted uncover examples bias algorithms two particular use cases discussed report ëpredictive policing testing simplified version fully automated predictive policing algorithm relation crime occurrence neighbourhoods focusing feedback loops defined following section ëoffensive speech detection development algorithms detect offensive speech tested ethnic gender bias european commission proposal april act aia reflects increased policy legislative focus proposal contains provisions relevant protection fundamental rights provisions include requirements risk management article including respect fundamental rights conformity assessment systems article notably respect focus report proposed aia also includes legal basis processing sensitive data detect monitor correct bias could lead discrimination article time writing report negotiations around proposed aia ongoing fundamental rights protection plays important role negotiations discussions around aia report aims inform policymakers human rights practitioners general public risk bias using thereby feeding ongoing policy developments acknowledged bias understood different ways analysis investigates bias context nondiscrimination one key concerns regarding fundamental findings pinpoint ways detect counteract forms bias may lead discrimination ultimate goal using algorithms way respects fundamental rights considering development bias algorithms time feedback loops first use case discussed report simulation feedback loop area predictive policing feedback loop occurs predictions made system influence data used update system means algorithms influence algorithms recommendations predictions influence reality ground reality becomes basis data collection update algorithms therefore output system becomes future input system bias algorithms therefore potentially reinforced time exacerbated feedback loops lead extreme results overestimate realities runaway feedback loops particularly problematic applied applications field law enforcement example area predictive policing first use case simulates predictive policing algorithm demonstrates several factors contribute formation feedback loops including low varying crime reporting rates different rates crime detection improper use machine learning three factors contribute formation feedback loops summarised algorithms bias deploying considering impact time provide guidance collect data sensitive attributes first data quality impact feedback loops fra analysis shows predictive policing low varying rates reporting victims witnesses lead creation biased feedback loops example predictions crime rates based low reporting rates fail reflect reality crime occurrence true crime rate lead false predictions wrong policy decisions fra repeatedly provided evidence showing low levels reporting police authorities relation people experiences discrimination crime based gender ethnicity age religious background among factors reporting rates influenced victims personal characteristics background shown fra report crime safety victims fra report results second european union minorities discrimination survey latter includes crime reporting rates among different ethnic minority immigrant groups evidence challenges accuracy official data sources used crime predictions second detection different types crime varies also influence data example crimes easier detect record example crime car theft people incentive report order make insurance claim crimes easy detect example fraud financial crimes certain population groups may often associated crimes easier detect may lead biased predictions time predictions overly focused types crime readily recorded police addition police may behave differently neighbourhoods assumed higher crime rates increased sense vigilance among police neighbourhoods may lead increase observed crimes also lead biased crime records third addition data quality machine learning algorithms tend put much weight training data simulations first use case looked report show runaway feedback loop defined occurs quickly machine learning models controlled overreacting small signals differences data consequence use techniques avoiding exaggerated predictions mirror training data strongly overfitting necessary algorithm opinion users predictive algorithms need assess quality training data sources influence bias may lead discrimination bias potential discrimination may developed amplified time data based outputs algorithmic systems become basis updated algorithms consequently algorithms used make support decisions people predictive policing need assessed regularly deployment special attention needs paid use machine learning algorithms automated legislator make sure regular assessments providers users mandatory part risk assessment management requirements algorithms development bias algorithms time feedback loops risks reinforcing creating dis criminatory practices affect groups protected characteristics ethnic origin disproportionate assess potentially disproportionate overpolicing certain groups assessments outputs algorithmic predictions needed respect composition target opinion better understand bias lead discrimination data protected characteristics may need collected users systems enable assessment potential discrimination data collection needs justified based strict necessity include safeguards relation protection use data article proposed aia provide clarity lawful processing sensitive data strictly necessary detect monitor potentially mitigate prevent bias discrimination clear legal basis contribute better detection monitoring prevention mitigation efforts using algorithms accompanied appropriate safeguards including aspects anonymisation pseudonymisation appropriate limitations respect collection storage accessibility retention additional implementing guidance collection sensitive data article considered notably respect use proxies outlining protected grounds ethnic origin sexual orientation need covered uncovering biases language prediction tools second use case discussed report simulation online hate speech detection systems offensive hate speech detection algorithms currently used practice based advanced machine learning methodologies natural language processing nlp given sheer magnitude online content major online platforms considerably increased efforts automatically detect predict potential online hatred developed tools however tools produce biased results several reasons notably level hatred associated different identity terms words indicating group identities varies considerably across data models form basis tools example sentences using term jew models lead much greater increase predicted level offensiveness term christian leads differences respect predictions offensive speech different groups differences also lead wrong predictions classifications several algorithms offensive speech detection specifically developed report based different methodologies different languages english german italian subsequently tested bias outcomes show terms lead considerably often predictions text offensive example english use terms alluding muslim gay jew often lead predictions generally text phrases offensive algorithms developed report terms muslim foreigner roma often lead predictions text offensive despite algorithms terms muslims africans jews foreigners roma nigerians trigger overly strong predictions relation offensiveness bias clearly points language differences predictions offensiveness different groups ethnic origin means people use phrases treated differently biased flagging blocking practices example lead differences access communication services based ethnicity example jewish person may use term jew often online content post may readily flagged offensive removed one reasons results terms strongly linked online hatred captured training data text datasets including examples hatred used creating algorithms predictions frequently overreact terms flagging text usually ethnic gender biases speech detection prediction models strong need assessed deploying algorithms fra opinion legislator ensure assessments discrimination mandatory deploying systems hate speech detection systems assessment potential discrimination necessary highlighting potential content assessment needed testing bias algorithms implementation law digital services act dsa proposed aia safeguard discrimination example provisions requiring providers users algorithms provide documentation carry assessments relation discrimination requirement increased transparency assessments algorithms first step towards safeguarding discrimination companies public bodies using speech detection required share information necessary assess bias relevant oversight bodies extent possible publicly oversight bodies relevant protecting fundamental rights equality bodies data protection authorities pay close attention potential discrimination prediction models exercising mandates actually offensive offensive result terms often considered offensive training data data used build algorithms phenomenon reflects strong presence hatred groups training data furthermore data analysis finds correlation offensiveness predictions algorithm likelihood text uses african american english refers version english frequently used black people united states correlation may lead higher likelihood african american english text predicted offensive even algorithms also exhibit bias relation gender categories certain terms gender categories terms investigated data languages use gendered nouns analysis shows available language models algorithms based large amount text lead gender bias bias lead differential predictions example considering feminine version term offensive masculine counterpart vice versa example feminine version muslim italian musulmana rated models negatively masculine counterpart musulmano also reflects intersectional hatred rating based gender combination ethnic origin religion highlights challenge using nlp gendered languages results show easily bias relation protected characteristics creep algorithms lead discrimination biases may already exist models adapted specific purpose used fields text classification machine translation based models learned universal language representations based large amounts text adapted specific purposes done report importantly results second use case demonstrate algorithms readily used automated content moderation hate speech take words linked protected characteristics isolation context indications presence offensive speech obligation respect principle nondiscrimination enshrined article treaty european union teu article treaty functioning european union tfeu requiring union combat discrimination number grounds articles charter fundamental rights equality law based list grounds prohibited grounds discrimination listed charter relevant comes use algorithms specific detailed provisions several directives also enshrine principle varying scopes application proposed equal treatment would provide even protection several grounds discrimination fills specific opinion legislation crucial safeguarding high level equality present analysis shows speech algorithms include strong bias people based many different characteristics ethnic origin gender religion sexual orientation consequence legislator member states strive ensure consistent high levels protection discrimination grounds including minimum sex racial ethnic origin religion belief disability age sexual orientation gender identity gender expression different areas life discrimination tackled using various existing laws safeguard fundamental rights addition legislation existing data protection laws also used address regarding use algorithms requirements use cases included proposed aia increase transparency allow assessment discrimination algorithms information respect use cases used enforce existing data protection laws finally equality bodies step efforts address discrimination complaints cases linked use algorithms order effectively employ specialised staff cooperate data protection authorities relevant oversight bodies striving language diversity nlp technologies availability related tools english improved considerably recent years nlp tools languages lagging far behind report uncovered clear imbalance tools knowledge available nlp technologies english available languages languages german italian results revealed quantity quality available data nlp tools insufficient resulting reduced performance algorithms detecting offensive speech also exhibiting strong levels need promote language diversity tools available natural language processing fra opinion member states consider measures foster language diversity nlp tools way mitigating bias algorithms improving accuracy data first step include promoting funding nlp research range languages english order promote use properly tested documented maintained language tools official languages member states also consider building repository data bias testing nlp repository conform standards data protection contain data languages enable testing biases continually updated maintained increase access resources needed oversight algorithms differences bias algorithms found project differ across languages also across tools methodologies used models based algorithms developed general language detection prediction tasks based large bodies text models needed speech detection algorithms many cases research found bias differed depending tools used means bias already embedded predeveloped speech models often developed large companies access vast amounts data computing power assessing documenting bias models challenging absence full documentation available tests identifying bias tools addition datasets often difficult obtain partly nlp researchers overly cautious avoid sharing data often lack knowledge data protection need increase knowledge awareness resources bias testing algorithms fra opinion increase application trustworthy compliant fundamental rights national funding fundamental rights assessments existing software algorithms needed support studies available generalpurpose algorithms would help deployers users tools easily conduct fundamental rights impact assessments use certain systems member states improve access data data infrastructures identifying combating risk bias algorithmic systems includes ensuring access data infrastructures researchers could achieved investment cloud computing storage infrastructures designed accordance standards data protection software safety energy efficiency researchers granted access infrastructure foster public scrutiny respect article dsa allows researchers better access data online platforms article used extent possible without bureaucratic obstacles allow easy widespread access data needed sole purpose research online platforms conduct improve availability evidence bias european commission european data protection board european data protection supervisor look need address issues correctly implementing data protection law relation data sharing respect sensitive data purpose researching monitoring discrimination without clearer guidance misinterpretation data protection law may unnecessarily stand way independent oversight risk bias algorithms endnotes dut parliamentary committee proposal regulation european parliament council laying harmonized rules artificial intelligence artificial intelligence act amending certain union legislative acts com final brussels april eur opean parliament amendment fra fra oposal council directive implementing principle equal treatment persons irrespective religion belief disability age sexual orientation proposed equal treatment directive com final algorithms used automate many tasks affect lives often scale matched work undertaken humans algorithms predict label analyse recommend opened new horizons support across many domains hold promise huge benefits economy society large alongside benefits however algorithms also pose risks fundamental rights therefore essential responsible development use algorithms examine possible impact people fundamental rights balancing act innovation fundamental rights also heart policy efforts regulate related technologies fra pointed previous reports big data approach guarantees high level protection possible wrongdoing related new one potential wrongdoing subject report involves risk bias algorithms tendency algorithms produce outputs lead disadvantage certain groups women ethnic minorities people disability imaginary problem one effects netherlands example tax authorities used algorithms mistakenly labelled around parents committed fraud childcare benefit applications disproportionate number parents immigration background required pay back large sums led great financial psychological difficulties families concerned data protection authority concluded processing data system use scandal become cautionary tale impact people decisions made supported biased algorithms become biased general answer simple enough bias resulting discrimination pervasive society rooted psychological social cultural dynamics hence reflected data texts used developing models use algorithms support processes often portrayed rational neutral machines technology neutral developed used humans bias present human may transferred living neighbourhood strong police presence stopped searched streets perhaps frequently reasons behind police presence based correct representative facts role police new crime prevention tool play bias play role imagine certain religious community active social media user several posts mentioning religion taken platform potentially offensive something people different religious background also experience extent could biased algorithms affect life artificial intelligence bias problem question exactly bias creeps predictions algorithms topic report requires complex answer two situations mentioned box stopped police social media posts one religion deleted central case studies report algorithms used predictive policing offensive speech detection tested clearly illustrate bias may occur leads discrimination term algorithm widely used context big data machine learning computer science algorithm sequence commands computer transform input output sequence commands sort random list people according age one example algorithm example provide computer random list input execute algorithm commands computer produces list sorted age output algorithms often used make predictions example predictions regarding profile people likely buy certain product weather forecast spam detection specific task algorithm fed data creates model used practice task following common machine learning terminology algorithm raw state model get algorithm trained data term artificial intelligence difficult define refer single tangible thing current technological developments processes general discussed umbrella refers increased automation tasks use machine learning automated decisionmaking core current discussions machine learning applications lies use algorithms fra getting future right report includes broader discussion related terms efforts clarify definition include work european commission high level expert group artificial intelligence oecd algorithms models artificial intelligence fra getting future right artificial intelligence fundamental rights luxembourg publications office oecd oecd framework classification systems oecd digital economy papers paris oecd publishing report time writing striving become first regional actor regulate using dedicated legal instrument set play leading role setting global gold standard comes addressing risks generated specific uses time assisting member states transitions digital societies including promoting development uptake fra stressed getting future right concrete examples allow thorough examination whether extent using technology interferes fundamental rights whether interference justified line principles necessity proportionality concrete examples need evidence based availability evidence data fundamental rights implications using algorithms increased recent years still lack evidence relation bias occurs leads discrimination recent study european parliamentary research service emphasised created humans susceptible bias bias frequently occurs machine learning applications trained data reflect certain demographic groups reflect societal study highlights additional challenge applications often black boxes making impossible consumer judge whether data used train fair representative turn makes biases hard detect determining bias comes therefore challenging exercise addition many different kinds notions biases necessarily negative harmful see section fra previous reports highlighted algorithms good data used develop practice biased data often easily slip algorithmic processes rendering flawed potentially discriminatory also sources bias algorithms may trained data necessarily biased unrepresentative case data used algorithms used make generalisations groups example face detection algorithm trained using predominantly male faces predictions might problematic applied female faces report looks hood algorithms context two particular use cases see next section demonstrate sources bias report also firmly places discussion bias context nondiscrimination discrimination negative result bias needs prevented regarding fra adds unique findings original applied research underpin body evidence bias suggesting concrete steps counter bias questions bias detected counteracted applications centre discussions around regulating findings help point policymakers potential safeguards mitigation strategies avoid unwanted bias discrimination technologies report intended read policymakers working providers users results report also relevant human rights practitioners academics dealing topic new technologies fundamental rights finally report informs general public risks bias using fra first report subject published highlighted fact algorithms good data fed data outdated incorrect incomplete poorly selected results questionable endless amounts data quickly produced internet lack quality control regarding data produced used serious concern fra focus paper published reemphasised risk systems based incomplete biased data lead inaccurate outcomes infringe people fundamental rights including also explored importance using data ensure algorithms including need developers providers algorithms transparent data used systems order help prevent possible fundamental rights violations one highly relevant area fundamental rights considerations context law enforcement relation facial recognition technology dealt fra report report provides overview technology law enforcement authorities plan use technology main fundamental rights concerns relation use report fra highlighted concrete examples bias algorithms addition highlighting need studies potential discrimination resulting use systems professionals fra interviewed report underscored results complex machine learning algorithms often difficult understand explain leads conclusion research better understand explain results explainable could also help better detect discrimination using fra done area date fra bigdata discrimination datasupported decision making luxembourg publications office fra data quality artificial intelligence mitigating bias error protect fundamental rights luxembourg publications office fra facial recognition technology fundamental rights considerations context law enforcement luxembourg publications office fra getting future right artificial intelligence fundamental rights luxembourg publications office see section information main challenges associated discrimination use report cover fra gathered evidence report engaging two practical examples use algorithms order get better understanding extent potential bias assess legal implications particularly respect fundamental rights overall design research developed fra conducted consortium data scientists lawyers led rania wazir based findings fra carried analysis drafted report two use cases covered following examples first example elaborated chapter analyses algorithms exacerbate bias time often referred formation runaway feedback loops based simulation study applied example predictive policing contexts feedback loop occurs decisions based predictions made system influence data used retrain update system runaway feedback loops perpetuate biases data also actually increase example police forces advised monitor one area based predictions influenced biased crime records police detect crime area second example elaborated chapter analyses ethnic gender biases offensive speech detection algorithms algorithms developed based offensive speech datasets using different approaches including models models developed datasets models tested invented phrases see predictions change different terms related ethnic groups gender example sentence hate used populated various terms groups african european see terms trigger offensive predictions chapters present main findings emerged analysis two use cases remainder chapter presents brief outline policy context legal framework relating biased algorithms tificial intelligence bias fundamental rights policy context institutions become increasingly engaged area bias fundamental rights considerations respect policy positions recently legislative proposals european parliament resolution fundamental rights implications big data called strong enforcement fundamental rights relation new european council called european approach emphasised strategic guidelines need ensure europe digitally sovereign policy shaped way embodies societal values european commission published communication established high level expert group artificial intelligence committed putting forward legislation coordinated european approach human ethical implications february european commission published white paper setting policy options meeting twin objectives promoting uptake addressing risks associated certain uses new technology april commission published proposal negotiations aia proposal ongoing european parliament council time writing report aia proposal provisions regulating different risk categories applications forms part european commission digital strategy key step endeavour make law fit digital age aia proposal part tranche proposals must understood tandem including dsa provisions recommender systems risk assessment relation fundamental rights expressly mentioning prohibition discrimination large online platforms digital markets act provisions hardware operating systems software distribution draft machinery regulation revising machinery relation health safety machinery draft data governance act concerning frameworks revision product liability relation bias algorithms biases algorithmic systems may lead discrimination also potential amplify discrimination potential scale application feedback however important distinguish bias platform involving eight intergovernmental organisations complementary mandates cooperate help policymakers public navigate international governance landscape access necessary tools inform policy development initiative launched september intergovernmental organisations currently involved council europe european commission fra development bank organisation economic development oecd united nations united nations educational scientific cultural organization world bank group highlights international efforts council europe efforts create international instrument regulate unesco recommendation ethics information see processes international level term bias different meaning depending context used particular discipline comes example law computer science therefore important clarify meaning context report bias refer following treatment based protected characteristics discrimination crimes refers inclination person group based protected characteristics ethnic origin gender religion colour sexual orientation discrimination defines situation individual disadvantaged way basis one multiple protected grounds crimes committed bias motivation particularly severe example result biases people based assumed characteristics definitions often used legal contexts social sciences bias understood sense necessary proper functioning statistical machine learning algorithm example machine learning model differentiate oranges pears bias towards labelling round orange objects oranges use bias mainly found computer science machine learning bias refers systematic difference estimated parameter true value statistical bias exists data adequately measuring intended measure example gross domestic product per capita necessarily good measure standard living country account inequality income distribution addition data resulting statistical estimates may representative target population example sample general population contains men women said biased towards men bias mainly understood way statistics origin context deep learning bias also name estimated parameter fixed number indicating average baseline estimate linear weight functions neural networks called bias often referred constant term intercept classical regression analysis purely technical term relevant present discussions although used neural networks bias analysed context discrimination legal normative concept report discrimination mainly linked prejudices picked enshrined data may also result statistical bias get lost translation fra handbook european nondiscrimination law luxembourg publications office chapter first forms bias relate protected characteristics example algorithmic model differentiates people based whether pet directly target protected characteristic pet unlikely act proxy protected characteristic second even algorithm contains bias related protected characteristic result may still discriminatory decision taken based algorithmic system lead less favourable treatment justifiable purpose employed example algorithm chooses song predetermined playlist hear next may gender bias questionable whether presenting songs different order constitutes less favourable treatment algorithms may also contain bias related protected characteristics justifiable relation genuine occupational requirements age limits certain jobs requiring physical therefore determine legal implications algorithm must always assessed within particular context purpose use bias algorithms may lead direct discrimination reliance protected characteristic leads less favourable treatment normally occur coded parameters training data input data include features directly indicate protected characteristic predictive policing algorithm includes information ethnicity residents particular neighbourhood content moderation algorithm contains information ethnic origin author particular post information easily spotted directly included allows assessment differential treatment based characteristics often however algorithmic bias leads indirect discrimination inclusion proxies proxy seemingly neutral piece information nevertheless strongly related protected characteristic example shoe size proxy gender names proxy ethnicity discrimination resulting use proxies difficult prevent potentially limitless number proxies correlation protected characteristic evident various extents example selection certain neighbourhoods enhanced policing activities may correspond neighbourhoods composed mainly certain minorities geographical area may reason discrimination composition population neighbourhoods may someone brings claim discrimination user social network whose posts tend removed claims discrimination based ethnicity burden proof usually rests upon person establish presumption discrimination statistical evidence example based discrimination testing useful purpose generally accepted rule kind statistical bias triggers presumption discrimination take different forms discrimination takes place person receives less favourable treatment another comparable situation based protected ground discrimination takes place apparently neutral provision criterion practice puts people particular protected characteristic disadvantage compared others several grounds discrimination involved multiple discrimination grounds operate separately intersectional discrimination grounds interact inseparable occur association person treated less favourably based another person protected characteristic person protected characteristic information see fra handbook european law luxembourg publications discrimination discrimination scholars proposed certain formalised schemes assess disadvantages across demographic groups called conditional demographic disparity starting point assess potential discrimination also well established appropriate fairness metrics arguably always depend specific clear rule puts users particular ethnicity disadvantage trigger presumption threshold counts sufficiently significant discrimination defined abstract must assessed basis taking account number presumption discrimination established burden shifts alleged defendant must provide evidence less favourable treatment based protected presumption discrimination rebutted defendant must prove either victim similar situation comparator difference treatment based objective factor unconnected protected ground could done providing insight real source code applying least appropriate fairness demonstrating algorithm works help post hoc explanation tools showing dependency protected characteristic finally scholars suggest algorithmic systems may discriminate along lines existing human prejudice discriminatory behaviour also based new grounds profiling identities based combination behavioural demographic far categorisations serve proxies existing protected characteristics gender race new combinations already covered existing european law however algorithmic systems treat individuals less favourably collection characteristics bear link existing protected characteristics new groups disadvantaged individuals may emerge result scholars argue prevention discrimination linked legally protected grounds may longer sufficient groups could example comprise people play online games dog owners sad teenagers fact algorithmic systems often lack transparency complicates detection discrimination previously noted example proving direct form discrimination fully disclosed relatively simple machine learning algorithm statistical model logistic regression may relatively straightforward algorithms may directly indicate extent predictions changed based protected characteristics gender however detecting proving discrimination becomes difficult algorithm fully disclosed complex still evidence discrimination could sought running tests systems using experimental methods output algorithms differs information protected characteristics changes risk discrimination consequence variety methods need developed deployed investigate potential bias address potential discrimination moreover absence opportunities test algorithms using experimental methods information help indicate discrimination use explainable technologies showing information data contributes predictions report bigdata discrimination decision making fra highlighted processing personal data related protected characteristics may needed detect potentially mitigate discriminatory outcomes using algorithms data collection however needs comply data protection law includes specific requirements data linked protected characteristics general processing special categories personal data revealing among things racial ethnic origin religious beliefs sexual orientation prohibited except grounds justification listed article general data protection regulation gdpr two grounds broadest scope application detailed points data subject explicit consent necessity reasons substantial public interest basis national law latter must proportionate aim pursued respect essence right data protection provide suitable specific measures safeguard fundamental rights interests data subject detecting algorithmic bias discrimination specifically mentioned justification processing sensitive personal data data racial origin sexual orientation gdpr may currently unclear extent processing lawful view data protection legislation proposed aia may change article proposal explicitly mentions bias monitoring detection correction separate justification processing sensitive categories personal data part quality standards systems data controllers process sensitive data based article gdpr line article aia proposal implement appropriate safeguards fundamental rights freedoms natural persons whose sensitive personal data processed proposed law still negotiated time writing report european commission guidance note collection use equality data based racial ethnic origin highlights relevance collecting data protected characteristics purpose counteracting discrimination inequality respect racial ethnic protection measuring discrimination artificial intelligence endnotes fra fra dut parliamentary committee eur opean commission fra toa european parliamentary research service fra fra fra eur opean parliament resolution march fundamental rights implications big data privacy data protection nondiscrimination security ini eur opean council eur opean commission der leyen eur opean commission eur opean commission dir ective european parliament council may machinery amending directive recast machinery directive ouncil directive july approximation laws regulations administrative provisions member states concerning liability defective products product liability directive see richar dson fra wachter see example opinion advocate general léger may para case cjeu inge nolte landesversicherungsanstalt hannover december see jeu ingrid fww gmbh july cjeu maria kowalska freie und hansestadt hamburg june cjeu helga nimz freie und hansestadt hamburg february cjeu weerd née roks others bestuur van bedrijfsvereniging voor gezondheid geestelijke maatschappelijke belangen others february fra wachter see example musto kennedy german ethics commission point wachter mittelstadt wachter fra chapter provides analysis feedback loops respect potential bias based computer simulation test simplified version predictive policing algorithm relation crime occurrence neighbourhoods however aim analysis examine predictive policing rather analyse feedback loops occur circumstances feedback loop occurs predictions made system influence data used update system algorithms influence algorithms recommendations predictions influence reality ground example algorithm predictions crime occurrence changes behaviour police officers turn influences detection crime detected crimes fed back system feedback loops common many machine learning systems feedback figure provides simplistic illustration feedback loop figure simplis tic illustration feedback loop aireal worldinﬂuences feeds source fra machine learning models include algorithms learned accomplish given task trained data identify pattern used make predictions new unseen data example models determine candidate based imagine living neighbourhood strong police presence standard living perceived safety neighbourhood increase considerably police presence neighbourhood remains high police regularly stop search people streets reasons behind police presence based correct representative facts role police new crime prevention tool play bias play role could feedback loop affect life feedback loops algorithms influence algorithms given credit past offensive speech detection algorithm detect offensive speech based tagged offensive past prediction models used human resources contexts screen potential applicants position update based actual future performance approved applicants applicants rejected machine learning models batch models online learning models continue learn deployment model produces prediction decisions made observed results added training data next round training classic feedback loop situation predictions made system influence data fed back future predictions policing context predictive policing systems help determine neighbourhoods patrolled based available existing crime data data include observed reported incidents feedback loops described create prophecy example system detects crime district decides send patrols crimes recorded district corresponding data fed back system reinforcing system belief crime district feedback system described runaway causes winner takes situation ends recommending one solution overestimates results predictive policing systems would mean system likely send police neighbourhoods based data fed system regardless true crime rate would lead overpolicing neighbourhoods underpolicing others simulation study section show based simplified example several parameters may influence formation feedback loops include following reporting rates include crimes reported police witnesses victims distribution refers percentage police patrols distributed across neighbourhoods crime measures likely police detect crime particular neighbourhood crime distribution assumed distribution crime taking place including reported detected unrecorded crime number known reality included simulation study using range different values explore impact feedback loop formation addition influence different algorithms analysed mitigation measures limit influence feedback loops simulation based theoretical assumptions tested artificially mirroring many iterations time results simulation exemplify situations may play time analysis applications likely complex however order make realistic assumptions possible parameters taken data data general population crime experiences rates reporting police collected crime victimisation surveys section provides overview fundamental rights applicable law relevant use case predictive policing example used throughout chapter section provides overview current actual use predictive policing section outlines known challenges linked use predictive policing section presents results simulations carried analysis section concludes law bias predictive policing systems fundamental rights affected overpolicing underpolicing overpolicing understood purpose report disproportionate police particular area relation true crime rate underpolicing means disproportionate police particular area relation true crime rate adverse fundamental rights debate concerning discriminatory adverse effects predictive policing fundamental rights far mostly focused overpolicing concerns individuals present overpoliced areas might negatively affected namely respect fundamental rights overpolicing involve concrete action taken particular individuals including police stops searches identity checks intrusion homes actions affect right physical integrity article charter right respect private family life article charter right data protection article charter individuals arrested right liberty security article charter affected overpolicing may also chilling effect way individuals express views gather publicly accessible spaces affecting freedom expression information article charter freedom assembly association article charter overpolicing may affect fundamental rights including potential link racial profiling similar discriminatory police activities underpolicing also detrimental fundamental rights lack police presence particular area may put people living higher risk becoming victims crime posing risk variety fundamental rights ranging rights life physical integrity articles charter right property article charter example individuals fear becoming victims crime also negatively affect enjoyment range fundamental rights overpolicing may amount discrimination negative effects associated see lead less favourable treatment individuals based protected characteristics instance use predictive policing algorithms may result overpolicing areas mainly inhabited certain ethnic minorities whereby area becomes proxy ethnic origin european court human rights ecthr consistently held state duty investigate potential causal link police officers alleged racist attitudes mistreatment suffered individuals critique true biased algorithms initial assumption police crime district district based directly protected characteristic dominance particular ethnic group district would amount direct however predictive policing model pursues seemingly objective goal prevention crime seemingly objective means higher incidence rate crime certain neighbourhood ends targeting groups associated protected characteristic others indirect discrimination type discrimination difficult prove subject justification legitimate aim measure proportionate police resources limited overpolicing certain areas compared others regardless true crime rate ultimately also affects rights individuals outside overpoliced areas underpolicing failing intervene example women report intimate partner violence also significant fundamental rights consequences ecthr emphasised states failing adequately respond certain forms violence considered breach convention example ecthr found turkey breach article european convention human rights basis gender conjunction right life prohibition articles case police failed adequately address domestic violence case clear threats known perpetrator domestic extensive ecthr jurisprudence clarifying state obliged carry effective investigations crimes general population experienced police stop past year experienced one past five years according fra fundamental rights survey based around interviews across north macedonia united kingdom police stops often concern men young people people ethnic minorities especially muslims people heterosexual example people consider part ethnic minority stopped police months survey opposed people consider part ethnic minority immigrants ethnic minorities trust police depends experience stopped police whether perceive stops ethnic profiling perception subjected ethnic profiling stopped police previous five years common among immigrants descendants immigrants south asia greece roma netherlands portugal according fra second european union minorities discrimination survey roma travellers survey comparison minorities surveyed lower perception experienced discriminatory police stop example russian minority baltic states never felt stopped police ethnic background poland slovenia respectively recent immigrants countries outside felt stopped ethnic origin source fra stops european union addition interaction police public overpoliced underpoliced areas also different based assumptions learned responses concerning areas people inhabit may reflected quality police encounter public example respect police stop whereby stopped areas overpoliced receive less respectful treatment stopped areas underpoliced something documented several studies instance beyond scope current use case model respect available data see example results fra surveys showing certain groups people roma north african backgrounds often experience disrespectful behaviour secondary law relation predictive policing law enforcement deals processing personal data purposes prevention investigation detection prosecution criminal offences execution criminal penalties article law enforcement directive addressed automated individual prohibited unless authorised member state law provides appropriate safeguards least right obtain human intervention moreover authorities wish use predictive policing algorithms carry data protection impact however law enforcement directive may apply predictive policing contrast predictive policing predictions context predictive policing usually include personal data rather use aggregated statistics addition may fully automated based solely automated processing without meaningful human involvement decision process predictions support decisions send simulations used analysis fully automated real world usually form meaningful human involvement deciding allocate police patrols article law enforcement directive would probably apply realworld use algorithms aia proposal see section contains specific requirements feedback loops pursuant article aia addresses systems continue learn placed market put service design systems ensure biased outputs due feedback loops duly addressed appropriate mitigation measures provision applies systems categorised high risk applies law enforcement use cases area predictive policing currently apply predictive policing systems negotiations proposed regulation ongoing time writing report analysed stage predictive policing european union various law enforcement agencies across used currently using least testing algorithmic systems predictive policing systems predict crime might take predictive policing involves application analytical techniques particularly quantitative techniques identify likely targets police intervention prevent crime solve past crimes making statistical beyond using crime statistics data systems use training input include geographical data landmarks important infrastructure social cultural economic information algorithms developing using algorithmic profiling bias may introduced step process avoid subsequent potential violations fundamental rights information technology experts officers interpreting data clear understanding fundamental rights fra guide explains profiling legal framework regulates conducting profiling lawfully necessary comply fundamental rights crucial effective policing border management information see fra preventing unlawful profiling today future guide luxembourg publications handbook preventing unlawful profiling today future guide used range logistic complex machine learning methods however various reasons trade secrets data privacy issues confidentiality law enforcement activities security purposes little generally known algorithms used predictive policing data used train models ensuing police actions taken result predictions summary predictive policing models proprietary making difficult research understand precise functioning one exception tool used predict areas crime based historical crime data including information type crime location time alongside internationally relevant predictive policing systems predpol hunchlab systems currently deployed europe example one widespread systems austria germany switzerland precobs german software developed institut für musterbasierte prognosetechnik precobs system assessing likelihood certain areas experience burglaries given time span algorithm based theory phenomena identifies burglaries likely followed crimes vicinity uses geographical data combined police statistics burglary locations time occurrence items stolen modus operandi deduce patterns corresponding professional serial burglars predict likely burglaries theory research use systems notably relation criminological studies crime hotspots two predictive policing systems deployed netherlands syri cas syri system designed help government identify individuals risk engaging fraud areas social security tax labour law predpol precobs cas examples predictive policing syri provides example predictive policing system syri received lot media attention owing district court hague ruling ecli violates right privacy contained article european convention human rights result discontinued dutch government second system cas deployed national scale netherlands making netherlands first country world adopt predictive policing nationwide still use rather focusing individuals cas programmed identify future crime hotspots areas divided squares metres categorised potential risk crime occurring area colour coding squares results heat maps used police investigate areas distribute police patrols predictive policing systems europe attacher predictive policing frühwarnsystem für die polizei magazin öffentliche sicherheit vol egbert krasmann predictive policing eine ethnographische studie neuer technologien zur vorhersage von straftaten und ihre folgen für die polizeiliche praxis projektabschlussbericht hamburg hamburg university april gers tner predictive policing context residential burglary empirical illustration basis pilot project german european journal security research vol townsley homel chaseling infectious burglaries test near repeat hypothesis british journal criminology vol egbert krasmann predictive policing eine ethnographische studie neuer technologien zur vorhersage von straftaten und ihre folgen für die polizeiliche praxis projektabschlussbericht hamburg hamburg university april sherman gartin buerger hot spots predatory crime routine activities criminology place criminology vol strikwerda predictive policing risks associated risk assessment police journal theory practice principles august ibid challenges linked use predictive policing literature predictive policing currently raises three main problems predictive policing systems transparency potential runaway feedback lack transparency accountability covers two aspects first owing intellectual property law often also complexity algorithms used law enforcement officers may information training required fully comprehend algorithm makes difficult detect errors biases model predictions addition lack transparency restricts possibility independent research results lack objective assessment mechanisms action effectiveness predictive policing systems second issue biased data fra previously pointed recurrent concern reliance historical crime data may biased incomplete could lead predictive policing systems reproduce entrench existing discriminatory main objective predictive policing identify areas high risk crime individuals high risk committing crimes police take action prevent crime systems based premise past crime events contain patterns predicting future crime events therefore require historical crime data function however decades criminological research shown limitations approach police databases complete census criminal offences constitute representative random sample mitigate limitations official crime statistics accurately assessing real extent nature crime number countries carry victimisation surveys randomly sample population ask experiences crime ranging property crime violent crime recently online fraud related internet crime importantly surveys ask people whether report experiences crime police allows estimate much crime undercounted official crime statistics dark figures example national crime victimization survey carried united states since british crime survey undertaken united kingdom since swedish crime survey carried sweden since fra surveys adopt classic crime victimisation survey model asking experiences crime victimisation reporting police ranging targeted surveys specific groups population ethnic minorities immigrants roma jewish respondents specific subject areas violence women alongside fra fundamental rights survey general population third challenge associated predictive policing systems subject present analysis potential runaway feedback loops police crime statistics consist two kinds sources may associated bias errors crime incidents observed detected police referred detected crime crime incidents reported police victims witnesses third parties crime noted crimes equally observable example violence offences committed openly public places versus less visible crimes business fraud tax evasion causing certain types crime overrepresented crime statistics furthermore police practices biased prejudiced meaning gets perceived crime stopped controlled hence likely detected largely depends demographics turn crime incidents reported police recorded therefore may feature official police crime statistics time victimisation surveys also provide evidence crime reporting varies type crime status victims combined effect police crime data crime observability overall detection depend crime type demographic characteristics perpetrator crime statistics dreißigacker befragung zur sicherheit und kernbefunde der dunkelfeldstudie des landeskriminalamtes hannover kriminologisches forschungsinstitut niedersachsen ensign friedler neville scheidegger venkatasubramanian runaway feedback loops predictive policing proceedings conference fairness accountability transparency pmlr fra getting future right artificial intelligence fundamental rights luxembourg publications office richardson schultz crawford dirty data bad predictions civil rights violations impact police data predictive policing systems justice nyu law review vol fra rights matter police stops fundamental rights survey luxembourg publications office may lum isaac predict serve significance magazine vol murrià sobrino gonzález años encuesta victimización del área metropolitana barcelona vigencia uso las encuestas seguridad las metrópolis barcelona institut estudis regionals metropolitans barcelona sim ulating feedback loops policing two main simulations conducted order demonstrate feedback loops may occur predictive policing first simulation built simple machine learning classification model explores parameters influence process feedback loop formation generally second simulation employs complex model offering realistic setting feedback loops form predictive policing systems synthetic datasets generated simulations meaning data artificially produced randomly created computer necessary statistical properties understood simulation distribution crime fictitious city datasets correspond real crime events however parameters based data mentioned depicted figure simulation included several parameters investigated influence formation feedback loops include crime reporting rates police distribution observability crime likelihood detecting certain crime assumed true crime distribution parameters take account recorded crimes reported detected true crime distribution additionally includes assumed nonrecorded crime parameters remain constant throughout simulation initially determined value changes time identifying formation feedback loops basic assumptions simulation study limited respect number parameters investigated analysed figure schema tic description simulated predictive policing process crime takes place true crime rate police records predictive algorithm allocate police according predictions crime observed policecrime reported police source fra two simulations conducted vary according policing strategy allocation police areas based predictions including allocation police according levels crime effective policing allocation majority police fixed number affected areas hotspot policing prediction algorithms used including simple probabilistic models complex machine learning models number areas also referred neighbourhoods cells differs two used first simulation whereas multiple areas used second simulation assumptions include following average number true crime events kept constant within simulation assumed one third true number crime events observed recorded police visit cell increases likelihood crime event detected factor five assumption used hold parameter constant order ease interpretation model results reality likelihood crime detected may vary across neighbourhoods based different police behaviour within neighbourhoods fra surveys show differences across ethnic groups experiences police treatment see section simulation necessarily limited smaller number assumptions order keep various factors constant allow testing variations among parameters selected details assumptions parameters outlined annex report mitigation strategies employed tested final step mitigation strategies suggested based sources bias identified research tested simulation experiments mitigation strategies include behavioural changes actors victims police technical methods remove data imbalances step includes assuming reporting rates changing varying one hand technical solutions avoid predictions closely follow training data technical measures avoid overfitting explanations mitigation strategies results testing provided section simulation exploring different sources bias first results based simple probabilistic model two neighbourhoods means police patrols distributed according crime distribution based historical police data simulation police always sent proportion crimes observed statistical machine learning tools used case runaway feedback loops formed following conditions satisfied true crime distribution uniform means crime happen every neighbourhood likelihood reporting rates neighbourhoods words proportion victims reporting crime behaviour across neighbourhoods police act exactly way sent different neighbourhoods true value initial patrol distribution reproduces previously published theoretical however soon true crime distribution deviates uniform runaway feedback loops formed assuming crime reporting rates small around zero mitigation measures adopted machine learning algorithm used may feedback loop occur simple probabilistic model simple machine learning model called naive bayes used example shown figure true crime rate neighbourhoods initial allocation police one neighbourhood observed probabilistic model distribution patrols represented orange blue lines change time lines remained horizontal despite true distribution uniform initial historical bias maintained throughout simulation contrast naive bayes predictions parameters gradually contributed generation runaway feedback loop assigned weeks police resources district even situation true crime rate uniform reporting rates police zero neighbourhoods naive bayes model forms runaway feedback loop first several instances introduction machine learning model increases unpredictability predictive policing system even true crime rate corresponding allocation patrols uniform constant naive bayes model ends runaway feedback loop however model ends assigning police patrols entirely matter chance running simulation times model assigned patrols district five occasions assigned patrols district five occasions amount time took runaway feedback loop form also varied greatly took weeks cases even weeks one case means use machine learning algorithms exacerbate formation feedback loop figure resul simulation allocation patrols time simple probabilities simple machine learning algorithm probabilistic model runaway feedback looprunaway feedback loop completely ormed weeksnaive bayes model patrols patrolsß week week timedistrict district source fra thus see soon predictive model included system runaway feedback loops formed combinations parameters machine learning models amplify small differences may enshrined historical data simulations end assigning police patrols neighbourhood highest crime rate input data determination runaway feedback loop assign police patrols random chance differences true crime rates small less moreover behaviour reproduced also logistic regression importantly behaviour observed additional measures taken whereas contexts humans would intervene interpret question apply results initial simulations simply show machine learning algorithms tend react random patterns may exacerbate addition various sets values relevant parameters explored influence individually several combinations investigated execution several simulations highlights different possible sources bias system surprisingly level crime reporting influences feedback loops crime reporting rates greater zero equal neighbourhoods formation runaway feedback loops reduced however neighbourhood low crime reporting rate coincides neighbourhood true crime rate higher mitigates formation feedback loops equilibrating reported crime however place victims report crime coincides true crime rates higher mitigation feedback loops diminished opposite case becomes relevant difference crime reporting rates neighbourhoods large doubled true crime distribution close uniform case neighbourhood lower true crime ends largest portion police patrols phenomenon enhances relevance crime reporting observed feedback loops usually form around neighbourhood highest true crime however observed true crime rates distorted excessive differences crime reporting behaviour across neighbourhoods finally also consider parameter crime observability indicates likelihood crime observed police present neighbourhood crime reporting zero crime observability may relevant true crime rate runaway feedback loop ends sending police patrols area lower overall crime rate crime observable effect sometimes mitigated crime reporting rates certain circumstances also reinforced result change much initial crime distribution varied three different sources bias analysed first experiment summarised visualised figure first source bias observed crime reported differential crime reporting rates second source bias may stem machine learning models may overreact random noise chance data amplify small differences third source bias related differential crime observability sources bias included simulation example may assumed police apply different policing methods different rapport inhabitants area depending whether regard area high low levels crime simulation cover potential sources bias effect increasing bias feedback loops holds true figure sour bias simplified policing algorithms crime takes place bias introduced differential crime reporting rates internal bias themachine learningmodel ampliﬁes smalldifferences policepatrols according therecorded crime ratescreates sampling bias crime observedwhere patrols police records predictive algorithm allocate police according predictions crime observed crime reported source fra based findings following conclusions drawn initial distribution patrols seems least relevant factor formation feedback loops compared relevant parameters internal bias machine learning models accelerates formation feedback loops even situations real differences could amplified captures random variation creates fictitious differences progressively amplifies crime reporting rates equal across neighbourhoods play important role mitigating formation feedback loops extent mitigation depends true crime rates however erroneous allocation police patrols may occur crime reporting rates differ significantly across neighbourhoods true distribution crime close uniform allocation police patrols may also occur crime observability differs across neighbourhoods overall based fictitious simulation cases found crime reporting rates crime observability true crime distribution relevant formation feedback loops addition identify three types sources potential misallocation police patrols first due runaway feedback loops occurs police excessively allocated neighbourhood highest crime rate second occurs neighbourhood lower crime rate higher concentration observable crime feedback loops form sending police patrols neighbourhood less crime observable third occurs crime reporting rates vary greatly neighbourhoods assignment police neighbourhoods less crime reporting clearly leads erroneous allocation patrols recent study analysing victimisation datasets bogotá colombia draws similar conclusions bias introduced differential crime reporting simulation earthquake policing model hotspot policing earthquake policing model see annex description different first experiment spatial distribution different neighbourhoods method allocating police patrols different based hotspot policing simulation starts observing feedback loops formed neighbourhoods true crime rates crime observability across neighbourhoods historical crime rates differ five neighbourhoods higher rates neighbourhoods figure shows feedback loop formed around five top cells iterations simulation one year one iteration represents one day estimated rates five cells highest historical crime rates increase whereas cells reduce despite true crime rate remaining constant uniform second year distribution maintained except random fluctuations estimated values figure predicted crimes per day differing historical crime rates per neighbourhood day day notes cells correspond areas fictitious city colours indicate predicted number crimes cell necessarily whole numbers integers predictions based averages source fra historical crime rates also relevant true crime rates extent higher neighbourhoods depends magnitude difference rates result surprising fact also reflects unlikely situation historical crime rates extent linked actual crime rates however still important acknowledge historical data drive algorithm predictions future crime observations next example unbiased data assumed means historical data equal true crime rates see patterns strongly reinforced overestimated one year process shown figure simulation also observed another simulation cells overestimation cells highest historical crime rates could observed one year iterations figure predicted crimes per day historical crime rates reflecting true crime rates day day notes cells correspond areas fictitious city colours indicate predicted number crimes cell necessarily whole numbers integers predictions based averages source fra without mitigation techniques iterations one year simulation tested conditions feedback loops formed means estimated rates cells highest historical crime rates fixed number top cells significantly increased reducing estimated values cells furthermore even historical data unbiased algorithm form feedback loop concentrated top cells distort estimation true crime rate important pay attention results simulation since historical rates completely dominate final result large difference initial crime rates top cells crime cells compared true crime distribution close uniform leads preservation initial historical crime rates overestimation crime top five cells bias mitigation strategies formations feedback loops described based theoretical models run repeatedly computer computer simulation creating artificial data although extent initially based real data described annex reality policing much complex police consider many aspects sending patrols certain neighbourhoods police work people present streets purely simple simulation reflect reality complexities simulations exemplify sources bias may lead overpolicing underpolicing certain neighbourhoods may put people disadvantage certain neighbourhoods composed people particularly disadvantaged groups notably ethnically segregated neighbourhoods following aspects mitigate feedback loop formation one sources bias low biased crime reporting rates increasing reporting rates reduce feedback loops may lead overpolicing solutions prevent values becoming extreme needed machine learning known potentially focus much patterns training data called overfitting prevent algorithms predicting values extreme technical solutions called regularisation employed means extreme predictions avoided adding mathematical restriction algorithm choice value restriction needs close scrutiny prevent feedback loops time allow predictions useful solutions suggested literature downsampling introduced ensign based assigning certain probabilities recording crime events counteract overly strong predictions mitigation techniques placed prediction cycle figure technique limitations combination mitigation techniques offers promising approach mitigating bias figure bias mitigation techniques reducing feedback loop formation crime takes place higher values prevent runaway feedback loopformation posteriori regularisationfor allocation policepatrols prevents extremevalues samplingbias police records predictive algorithm allocate police according predictions crime observed crime reported source fra addres sing feedback loops concluding points simulation experiments report simple models used automated based limited set criteria available data however predictive policing system would embedded law enforcement agencies humans interacting making decisions stages promising approach feedback loop mitigation would benefit investigation entire predictive policing life cycle installation review control procedures taking account model potential police bias rather simulation model looked would need take account sources bias respect different actors impact system following considered model reporting rates affect policing victims witnesses crime general population relevant mitigating overpolicing underpolicing feedback loops thus mitigated increasing awareness society trying improve crime reporting rates general particularly neighbourhoods reporting rates low linked vulnerable groups society increasing trust police one important way enhancing reporting rates whereby onus public report crime also police encourage crime reporting make accessible building trust fra fundamental rights survey showed lack trust police one reason people reporting burglary either general lack trust expect police would anything moreover experienced police stops based assumed ethnic profiling trust police police play important role comes different rates detecting crime among considerations related observability various crimes street crime versus fraud fundamental rightscompliant policing address police behaviour potentially different depending neighbourhoods police patrolling awareness raising regarding policing behaviour leads differential crime observability supports fairer policing respect police may see prejudices confirmed algorithms generally may put much trust predictions produced algorithms overreliance hamper necessary human review algorithmic awareness limitations possible failures predictive policing algorithms among police officers one crucial aspect avoid unfair policing practices result police overreliance algorithmic predictions developers play important role comes addressing internal bias machine learning algorithms police developers working together critically examine assignment police patrols according observed crime rates application technical mitigation techniques based simulation testing prior deployment systems required avoid inefficient potentially discriminatory policing practices communities also involved talking developers police authorities better understand potential bias results show feedback loops easily occur fully automated settings strategies single focus debiasing historical crime data seem effective simple technical solutions bias mitigation regularisation methods showed success simulations also indicate main danger runaway feedback loops systemic causes particular differential crime observability crime reporting rates left unaddressed issues potential perpetuate bias discrimination deeper investigation sources bias needed predictive policing systems safe deploy chapter looked simulations simplified predictive policing algorithms however kinds machine learning models used predictive situations employment decisions credit rating fraud prediction name findings section wider implications use impact fundamental rights various contexts example credit risk model learn future outcomes credit repayment applicants granted credit hence considered low risk applicants probably receive loan additional information whether decision correct models also run risk runaway feedback loops many lessons learned simulation also apply include necessity maintaining clean source fresh data meaning data influenced model predictions case crime reporting predictive policing importance decisions based model predictions made importance effective control review throughout model life cycle report shows results simulation predictive policing see predictions evolve many iterations several challenges linked researching feedback loops first challenge relates availability data crimes including information location time type crime datasets readily available member states readily available united kingdom united states general granularity available crime data public domain limited also reduces ability research simulate real crime events space time limited information available implementation commercially developed algorithms used law enforcement without information potential biases feedback loop formation mitigation strategies difficult study accurately experiments described show certain techniques methods mitigate development feedback loops training guidelines police officers could potentially mitigate effects existing feedback loops however transparency lacking trade secrets fact algorithms owned private companies thus proprietary currently researchers relevant actors test exact functionality proprietary algorithms extent effects due either lack information algorithm lack available data simulation experiments use case used simple models used automated however predictive policing system would embedded law enforcement agency humans interacting making decisions stages turn human intervention also analysed relation data based predictive policing models used interpreted respect potential encountered researching feedback loops endnotes ensign see fra see example ecthr boacă others romania january see fra see thr osman united kingdom merits october paras article convention may also imply certain circumstances positive obligation authorities take preventive operational measures protect individual whose life risk criminal acts another ecthr đorđević croatia merits july paras failure protect disabled person harassment judged violation prohibition see example context domestic violence ecthr opuz june thr romania fra fra dir ective european parliament council april protection natural persons regard processing personal data competent authorities purposes prevention investigation detection prosecution criminal offences execution criminal penalties free movement data repealing council framework decision law enforcement directive see ecital law enforcement directive safeguards see art law enforcement directive article working party see article orking party erris gerstner strikwerda mastrobuoni hardyns rummens erry hun mohler mohler mohler ger stner bennett moses chan winston richar dson lum isaac ensign fra mosher eißigacker ensign akpinar ensign see fra fra fra law society chapter analyses extent bias occurs respect offensive speech detection focusing terms related ethnicity gender prediction algorithms built purpose tested bias using experimental methods algorithms based publicly available data created predict likelihood certain text phrases classified offensive algorithms fed invented text phrases muslim female great phrases varied using different words relate potential grounds discrimination controlled changing terms provide direct evidence bias predictions analysis points towards extent bias respect offensive speech detection classification relation certain groups muslims jews foreigners also shows bias varies considerably across different algorithms different languages already embedded available language tools analysis provides insights complexity bias algorithms discusses bias may linked discrimination certain groups results intended help inform policy debates address bias speech detection algorithms results also serve inform developers users look bias investigate whether system may contribute discriminatory outputs analysis assess well offensive speech detection algorithms used practice work whether used rather examines algorithms classification certain phrases offensive may lead bias despite strong indications deficiencies algorithms concluded research whether algorithms fit purpose exact usage depends context however results telling speech detection algorithms rely heavily certain words algorithms used without assessment bias view basic premises actual development usage chapter starts discussion fundamental rights affected use biased speech detection algorithms followed discussion around actual usage challenges using algorithms content moderation efforts chapter outlines methodology analysis presenting sentence muslim offensive offensive buddhist objectively modern algorithms sometimes assess text offensive muslim buddhist results classic example bias algorithms use one specific term triggers much higher predictions offensive ethnic gender bias offensive speech detection fund amental rights affected biased speech detection online hatred become everyday reality many population groups across whether expressed form hate speech harassment incitement violence hatred many professionals working victims hate crime indicate hate speech internet growing fra survey perceptions antisemitism highest single incidence rate reported antisemitic harassment related furthermore fra survey violence women showed women widespread women reporting experienced online hatred pervasive challenging moderate owing scale complexity online communication increased considerably recent years companies running online platforms striving moderate content shared services context content moderation example based offensive speech recognition content interfere range fundamental rights online content means online platforms fail refuse take action offensive content particular offensive content also illegal online hatred may interfere different rights depending particular content right respect private family life article charter right life article charter right physical mental integrity article charter right freedom thought conscience religion article charter right article charter rights child article charter studies shown online hatred lead depression suicide exposure concern online hatred may also lead people engage less frequently express less freely thereby negative impact freedom expression information article charter emergence legislation automated content moderation systems recently drawn policymakers attention side coin means unjustified blocking content suspension termination user fundamental right primarily put risk right freedom expression information article charter example key provisions french avia law aims combat online hate speech struck french constitutional council deemed necessary proportionate relation freedom expression fundamental rights also affected content depending particular context include right freedom thought conscience religion article charter right freedom assembly association article charter freedom conduct business article charter also involve right article charter case biased takedown similar content based protected characteristics use algorithms amplify fundamental rights risks may also impact right effective remedy article charter difficult explain algorithms used make respect ensuring fundamental rights compliance relation offensive online speech detection legitimate concerns content underline primary need achieve proportionate accurate response practice democratic societies point addressed section information society services provided online platforms essential services available public open person prepared subscribe terms conditions needed open account access services falls within scope racial equality gender goods services meaning direct indirect discrimination grounds ethnicity gender focus offensive speech detection model chapter prohibited access understood broadly example covering deletion posts especially suspension even termination accounts directly affects access service however must taken account gender goods services directive apply media member states still choose address areas national law going beyond directive minimum requirements furthermore certain limitations terms grounds discrimination covered directives present neither racial equality directive gender goods services directive directly addresses discrimination based sexual orientation gender identity religion prohibition discrimination based sexual orientation religion currently exists employment would subject proposed equal treatment directive enacted yet beyond gender identity mentioned recital victims rights context criminal according court justice european union gender identity partly covered principle equal treatment men legal protection discrimination based religion currently also limited nevertheless one may argue many comments referring people identify lesbian gay bisexual transgender intersex lgbti jewish muslim fall either racial equality directive gender goods services directive discrimination based sexual orientation gender identity religion predominantly affects specific race gender decisions made algorithms based processing personal data may qualify automated individual article gdpr generally prohibits automated decisions legal effects data subjects similarly significantly affect automated individual decision within meaning gdpr means made exclusion meaningful human however fully automated decisions permissible necessary conclusion performance contract based national law lays suitable measures safeguard data subject rights freedoms legitimate interests based explicit consent data subject automated decision based controller must provide appropriate safeguards order protect privacy data subject safeguards include right contractual relationship exists recipients providers online platforms contracts contain conditions basis provider online platform entitled remove limit access user content suspend user platform whether one directives applies contractual terms practices inconsistent spirit fundamental rights primary law may considered unfair national consumer law implementing unfair contract terms directive uctd general clauses national law public policy uctd addresses unfair terms consumer contracts individually negotiated article uctd including contracts online platforms article uctd uctd includes annex proposed blacklisted contract terms considered unfair general clause covering sort contract terms national courts need consider inconsistency fundamental rights line doctrine indirect effects particularly applying general clause article uctd considerations may lead particular contract terms binding article uctd german federal supreme court bundesgerichtshof addressed unfair practices related terms conditions online platform court emphasised providers operating online platforms set objectively verifiable community standards beyond legal requirements sanction violations removing individual posts even blocking access network however must include provision terms conditions inform user immediately following removal post advance intended blocking give user opportunity make counterstatement followed renewed decision absence provision terms use invalid pursuant section german civil code bürgerliches gesetzbuch provision implementing article uctd dsa require providers intermediary services including online platforms include information restrictions impose terms conditions must include information policies procedures measures tools used purpose content moderation including algorithmic human review well rules procedure internal complaint handling system article dsa online platforms also obligation include information terms conditions main parameters used recommender systems well options recipients service modify influence main parameters article dsa least one option recommender system shall based profiling large online platforms article dsa applying enforcing restrictions providers intermediary services must due regard fundamental rights enshrined charter freedom expression freedom pluralism media fundamental rights freedoms article dsa rights relation contractual terms conditions council directive eec april unfair terms consumer contracts unfair contract terms directive bundesgerichtshof iii iii july paras data subjects obtain human intervention express point view contest decision particularly sensitive personal data within meaning article gdpr even stricter prohibition automated decisions applies exceptions context explicit consent data subject processing based substantial public extent article conjunction articles gdpr also includes right receive explanation whether includes limited information right subject using artificial intelligence offensive speech detection testifying congress april mark zuckerberg facebook chief executive officer stated yet ready used hate speech detection hate speech reported facebook users zuckerberg indicated likely ready support hate speech detection facebook use detecting hate speech increased considerably since facebook detected hate proposed aia requires providers systems subject training validation testing data appropriate data governance management practices ensure relevant representative free errors complete article aia context content moderation algorithms help combat bias may result discriminatory practices legal ground also added process special categories personal data sensitive data extent strictly necessary ensure bias monitoring detection correction article aia furthermore aia also require systems shall designed developed way including appropriate interface tools effectively overseen natural persons aim preventing minimising risks health safety fundamental rights article aia data quality artificial intelligence act speech posted platform first quarter share increased first quarter percentage rate likely driven tools help flag content humans decide action take post deletion remaining detected hate speech reports made facebook users services rate first quarter concerns million pieces content actioned relation hate speech actioned refers action taken facebook including potential deletion addition google working speech detection algorithms content moderation model perspective available application programming interface used filter toxic speech example comments reportedly helped new york times increase number articles website allow users post comments content moderation made efficient filtering tool support content moderation time developers tool issued warnings limitations stating tool makes errors unable detect patterns toxicity seen yet capable used automated content moderation hate speech particularly relation illegal hate speech many academics warned limitations using algorithms online content moderation dispelled narratives could easily solve hate speech example algorithms take context account way humans considering sender recipient message information available algorithms privacy reasons furthermore lack representative highquality datasets develop algorithms taking account differences speech patterns changing patterns speech remains difficult use algorithms may increase opacity content moderation increase challenges linked fairness without proper safeguards tools lead censorship biased enforcement laws platforms terms potential increase discrimination one challenges using algorithms support speech detection content moderation purposes certain message hateful readily judged person addressed way judged may differ people hence universal assessment offensiveness certain pieces text people often disagree level offensiveness certain phrases significant differences assessing content offensive based demographics assessing example man may consider offensive may well perceived offensive woman way round challenges quality usefulness data fixed labels offensiveness therefore final assessment hatefulness online content made humans however practicality given volume online data content seemingly insurmountable sheer volume online content large platforms deal necessitates support content moderation activities algorithms example second half twitter deleted million pieces content included million pieces content hateful conduct another million abuse consequence clear algorithms useful tool identify potentially offensive online content stage development automatic removal suspected hate speech problematic several levels explained different areas example spam filters work relatively note email spam filters work without automated algorithmic decisions spam facebook deals identified however patterns offensive toxic otherwise hateful speech complex making difficult detect beyond content moderation offensive speech detection algorithms based language models widely used many domains include automated translation speech recognition question responses sentiment analysis makes societal risk harm algorithms used inadequate intended purpose real serious threat respect fundamental rights compliance following section highlights one fundamental rights challenges based algorithms developed purpose report applied research following analysis show bias certain groups embedded offensive speech detection algorithm respect categorisation speech detection algorithms tested bias explore abovementioned challenges potential biases speech detection algorithms fully fledged offensive speech detection algorithms developed tested bias selected groups respect categorisation content machine learning models algorithms research developed three languages english german italian using publicly available datasets contained text labelled offensive offensive languages three different types algorithms developed resulting three models per language nine different tests relatively simple trained training dataset standard methodology simply based words occur without considering order words approach logistic regression advanced uses tools work known semantic relationships existing words word embeddings neural networks even advanced varies relationship words depending neighbouring words sentence predictions language model available fully trained machine learning model adapted specific purposes combination new training data see annex descriptions terms means model especially model include existing generalpurpose language prediction tasks developed using training datasets used research data used training testing three models developed using publicly available labelled training datasets english dataset includes posts collected twitter group researchers researchers subsequently used crowdsourcing collecting input various people internet annotate posts offensive offensive datasets used german language collected twitter manually annotated researchers offensive offensive contain posts respectively dataset collected amnesty international italy relation treatment women lgbt people contains posts facebook twitter annotated volunteers amnesty international italy datasets come different sources developed annotated different purposes also influences biases groups ability used settings following common machine learning practice test predictions data split three sets training dataset validation dataset test dataset stemming training datasets details datasets found annex iii bias analysis additional dataset created dataset referred bias test dataset based invented text phrases sentence templates words based developed existing bias test dataset includes neutral positive sentences love offensive sentences hate placeholder populated different identity terms muslim buddhist german italian sentences also used gendered nouns example feminine masculine versions muslim used nine different sentence templates used varied using different verbs hate love adjectives great strong disgusting dumb overall led dataset sentences english language half sentences rated research team offensive half italianlanguage bias test dataset twice large english one terms gendered masculine feminine versions example sentences language fed three models predict offensiveness based algorithm rules categorisation developed based training data led predictions sentences either offensive predictions investigated bias analysing differences predictions template sentences using different identity terms examples sentences details provided annex iii offensive speech judge text predictions based data considered offensive basis judgements made mainly researchers comes level uncertainty people may disagree whether certain text offensive differences fact another source bias offensive speech detection algorithms one challenge questions quality usefulness data fixed labels offensiveness perception may vary research suggests challenges overcome several people involved labelling creates another practical challenge terms human resources costs developing training usually target message people addressed message looked help judge level offensiveness office united nations high commissioner human rights published guidance threshold test considering limitation freedom expression assessing incitement hatred including various aspects position speaker social political context however offensiveness strongly depends context speech sender message yet assessment usually included offensive speech detection algorithms analysis also missing contextual data assesses text based combinations words points limited ability models actually predict offensiveness based text alone present analysis making point correct assessments rather shows systematic differences algorithms respect certain terms everything else held constant bias assessed unwanted bias offensive speech detection models tested looking false positive rate fpr false negative rate fnr reflecting fact companies using algorithms flag overwhelming majority content deemed offensive hate speech proportion increasing research team replicated process respect algorithm categorisation categorisation rated correct according human assessment research team fpr defined percentage comments rated research team classified offensive model fnr percentage comments rated offensive research team classified model analysis looks equality fprs fnrs across different groups interest applying equalised odds metric fprs fnrs relevant false positives lead unwarranted censorship false negatives lead failure detect offensive speech targets abuse continuing subjected offensive comments reality choices made design algorithm prefer higher fpr lower fnr way round depending acceptability error either side bias defined model performing better protected characteristics others based work dixon bias certain target groups analysed analysis looks question whether algorithms often incorrectly label text including references certain groups offensive speech detection models proprietary thus available bias testing therefore offensive speech detection models built scratch conduct experiments report required use sufficiently large labelled datasets availability datasets determined choice languages could investigated performance models terms accurate predictions based different datasets reported annex iii resul ethnic gender bias offensive speech detection results provided section show terms linked selected protected characteristics people contribute classifying text offensive may also lead higher likelihood potential errors certain terms terms used invented text phrases thereby ensuring differences arose reflect bias linked terms alone results biases described section need understood light methodology applied described previous section also means patterns found analysis necessarily transferable algorithms developed offensive speech detection methodologies contexts apparent results analysis also differ across different models developed however results still indicate certain patterns relevant also likely occur models applications furthermore results biases necessarily lead discrimination certain errors resulting biased predictions may necessarily lead less favourable treatment may countered human review speech detection predictions otherwise mitigated ords make difference bias selected groups average predictions considerable differences respect predictions offensiveness speech different ethnic groups nationalities test dataset differences also lead varying predictions sentences offensive based certain threshold described fpr indicates percentage comments rated inoffensive research team classified offensive model model decisions based certain threshold example predicts likelihood text offensive text classified offensive means comments may flagged reviewers offensive even automatically deleted way model overreacts differences across identity terms used part experiment text held constant provide clear evidence overreactions certain terms used higher fpr usually comes lower fnr percentage offensive comments classified nonoffensive fnr indicates percentage offensive comments missed hence fpr indicates potential unwarranted censorship fnr indicates share comments rated humans offensive missed algorithm figure shows average fpr fnr groups identity terms across invented used test models dataset models upper panel shows fpr shows terms linked muslims often lead predictions offensive speech sentences rated research team average comments categorised research team comments predicted offensive model similarly comments considered research team including terms linked identities gay jew often misclassified model constituting average comments respectively upper panel figure shows overreaction terms happens three models indicates comes partly training data terms strongly represented comments considered offensive training data makes models consider terms alone strong indications text offensive bias strongest model based training data different models based algorithms potentially including additional different bias hence bias models mitigated extent use external information however importantly fpr term jew actually increases use word embeddings model meaning bias term jew already enshrined resources model also overreacts terms linked refugees nigerians white person black person models show higher fprs terms remaining groups models show low error rates comments due models predicting many comments rated offensive research team comments usually predicted groups terms terms used offensive comments miss half comments predicting offensive shown lower panel figure rather result weakness models identify comments difficult detect invented phrases bias linked main terms muslim gay jew relation overreaction comments turned around offensive comments rates misclassifying offensive speech including terms much lower however models still typically miss comments considered offensive research team figure verage false positive false negative rates across groups identity terms models english language false positive rate false positive rate muslimgayjew refugeenigerian white personblack personqueerturksomaligermanchristiansyrianbuddhistitalian irish personeuropean straight personmigrantafricangenderroma foreignereritrean muslimgayjew refugeenigerian white personblack personqueerturksomaligermanchristiansyrianbuddhistitalian irish personeuropean straight personmigrantafricangenderroma group identity termspegatnecremodel average model note iden tity terms grouped together bias test set contained variations terms plural singular versions source fra figure shows results models share comments considered research team classified offensive model average higher models term refugee shows extreme results highest fpr average however model tends classify virtually comments including term refugee offensive contrast model almost opposite classifying one five comments offensive every second offensive comment terms higher fprs muslim foreigner roma least classifying one three comments wrongly offensive side terms buddhist queer eritrean lead predictions offensive comments hence offensive speech using terms often missed figure verage false positive false negative rates across groups identity terms models german language false positive rate false positive rate muslimgay jew refugee nigerian white personblack personqueerturksomaligerman christiansyrianbuddhistitalian irish personeuropean straight personmigrant africangenderroma foreignereritrean muslimgay jew refugee nigerian white personblack personqueerturksomaligerman christiansyrianbuddhistitalian irish personeuropean straight personmigrant africangenderroma group identity termspegatnecremodel average model note iden tity terms grouped together bias test set contained variations terms plural singular gendered versions source fra figure shows error rates models several terms trigger high fpr particularly linked muslims africans jews foreigners roma nigerians average comments rated research team predicted offensive model considerably higher fpr terms european queer buddhist german model one without external information models particular misses offensive comments certain terms shows external models already include information offensiveness certain terms information either lead bias reduce figure verage false positive false negative rates across groups identity terms models italian language positive rate false positive rate muslimgay jew refugee nigerian white person black personqueerturksomaligerman christiansyrianbuddhistitalian irish personeuropean straight personmigrant african genderroma foreignereritrean muslimgay jew refugee nigerian white person black personqueerturksomaligerman christiansyrianbuddhistitalian irish personeuropean straight personmigrant african genderroma foreignereritrean group identity termspegatnecremodel average model note iden tity terms grouped together bias test set contained variations terms plural singular gendered versions source fra generally model results seen mostly reflecting bias training data means term muslim disproportionately often included posts rated offensive respect original training data bias may also result way training data collected potentially specifically looking hate speech muslims result absence contextual data term becomes strong indicator offensive speech finding reiterates importance transparently describing training datasets using algorithms certain tasks seen distribution colours figures biases changed models implies elements besides training data influenced model outputs model bias came mainly word embeddings model came mainly language model identity groups picked often subject high fprs across models languages include jews muslims various african nationalities immigrants refugees addition terms linked sexual orientation often subject false positives false negative results even varied high fnrs often linked identities perceived disadvantaged marginalised european buddhist german italian addition term eritrean high fnr showing hatred somewhat underestimated test dataset within language variations fprs fnrs various identity terms indicate bias models due bias training data also derived features model otherwise models would similar levels bias reflected fprs fnrs particular word embeddings language models also contain bias bias necessarily negative reflects features structures included previous training datasets may relevant settings however bias lead false potentially discriminatory predictions depending concrete application predictions results highlight importance testing algorithms bias different scenarios example may acceptable virtually text containing term muslim flagged offensive situation human reviewers check posts detail taking action time ensures fewer offensive comments posts using term missed approach risks missing offensive comments use term whereby writers use different also proxy words offensive towards muslims offensive language used groups well captured training data language models focusing specific terms figures provide insights biases general level terms used varied plural singular gay gays feminine masculine forms mostly comments gendered languages used cases turns singular plural form carries weight predictions example english language models term gays higher fpr singular form gay europeans higher fnr european german italian feminine masculine forms treated differently example german feminine form muslim lower fpr masculine form model reversed model section look closely differences due gender nouns german italian differences predictions across selected test sentences differences predictions across selected identity terms sentences stem bias terms introduce predictions hence simply using certain terms makes algorithms less likely predict offensive comments clearly exemplified table predicted probabilities one selected sentence shown sentence simply statement ich bin sono populated various identity terms table provides predictions selected religious identities buddhists christians jews muslims results vary across terms languages models genders germanlanguage models show similarities across gender religious identities models average prediction indicates certainty model text offensive models vary hence general comments predicted examples unsurprising sentences offensive slight tendency sentences using term muslim higher average prediction offensive percentage points also slightly higher tendency masculine term christian german predicted offensive feminine term models predictions differ hugely identity terms christian predicted low probability offensive may also linked potential double meaning english could also refer name christian buddhist gets higher score still generally predicted offensive ranging sentence jewish low predictions model model however model predicts likelihood sentence offensive model based existing language models apparently learned term jewish alone signifies offensive comment tendency even stronger comes term muslim average prediction offensive models provide certainty sentence muslim offensive models also vary considerably predicting sentence according identity terms used terms higher predictions models languages hence differences due training data used average predictions sono buddhista remain christians jews two identity terms model predicts low probability offensiveness model predicts high probability offensiveness hence training data mainly responsible higher predictions lowered external models containing information showing text actually offensive compared models languages terms linked muslims musulmana musulmano highest likelihood predicted offensive models italian language show considerable differences relation gender terms feminine terms christian muslim get higher predictions offensive masculine counterparts way round jew masculine version predicted offensive differences notable may lead biased results predictions offensive comments actually indicate potential real differences way hatred offensive comments expressed online provides evidence gendered intersectional discrimination hatred groups negative outcomes muslim christian women muslim christian men negative outcomes jewish men jewish women table predicted probabilities text offensive across selected groups text language genderpredicted probability offensive average minimum maximum ich bin buddhist ich bin buddhistin ich bin christ ich bin christin ich bin jude ich bin jüdin ich bin muslim ich bin muslimin buddhist christian jewish muslim sono buddhista sono buddhista sono cristiana sono cristiano sono ebrea sono ebreo sono musulmana sono musulmano notes italian term buddhista masculine feminine feminine masculine neutral another example biases predictions concerns regional origin exemplified using two invented sentences indicate violent speech one sentence expresses anger violent statement saying hate someone thinks killed sentence expresses violent statement think killed populated identity terms figure shows predictions sentences offensive based models developed project across three languages terms african european upper panel figure shows results average predictions three models german language average predictions given sentences used isolation also additional random text allow variation predictions additional random text identity terms hence impact differences shown panel shows model lower likelihood predicting sentences containing term european offensive compared african two models relatively similar two identity terms interestingly first sentence indicating someone hates fact someone violent thoughts receives higher predictions likely additional word hate good example models reacting specific terms rather context meaning sentence model particularly likely predict first sentence offensive second one similar result found models first sentence gets considerably higher predictions predictions fairly equal terms hence bias detected second violent sentence model shows strong difference lower prediction european african models italian language different models sentences strongly biased much higher predictions sentences including term african including term european models differentiate much two sentences model extreme figure verage predictions violent speech selected origins hate someone thinks killed think killedde model model model model model model probability offensive african european source fra almost sure sentence offensive includes term african almost certain sentence inoffensive includes term european shows biases historically disadvantaged groups europe people african descent face high levels hatred also shows inability detect potential hate speech groups results also show algorithms easily react single words relation predictions problematic relevance certain terms offensive speech detection investigated machine learning libraries provide explanations relevance certain features model leading certain predictions fact algorithms overreact certain terms understood users online platforms use knowledge avoid offensive comments singled avoid blocked example users posting hate speech use word juice instead jews avoid investigate behaviour trained models explainability library used generate explanations offensiveness predictions model lime assigns value word indicating much word contributed model prediction comment offensive model output selected phrases shows influences high fprs explained figure shows two examples model phrases used love irish muslims lesbian first case two identity terms used see influence phrases assessed highly likely offensive likelihood offensive respectively lime explanations show word muslims first example word lesbian second example almost exclusively responsible phrase predicted offensive first example word love actually contributes phrase less likely offensive come close counteracting negative contribution words word irish contrast muslims major impact makes phrase slightly less likely predicted offensive fact word love reduces likelihood content rated offensive discussed researchers used evade hate speech detection example phrase kill europeans rated likely offensive phrase kill europeans love predicted likely offensive simply adding word love may mean text predicted depending threshold offensiveness figure ords contributions model prediction offensiveness selected sentences english model love irish muslims lesbian prediction probabilities offensive offensive offensive offensive offensive prediction probabilities offensive offensive irish lesbian highlighted words love irish muslims text highlighted words lesbian amate tutte gli ebr love jews amate tutte gli ebr amore ove jews ove prediction probabilities offensive offensive offensive offensive offensive prediction probabilities offensive offensive ebrei amate gli ebrei amate tutti gliamore highlighted words amate tutti gli ebr amate tutti gli ebr amoretext highlighted wordssource fra figure shows two examples model italian language phrases amate tutti gli ebrei love jews amate tutti gli ebrei amore love jews love considered term ebrei jew strongly influences negative rating addition second love love jews love minimal impact offensiveness prediction text figure ords contributions model prediction offensiveness selected sentences italian model love irish muslims lesbian prediction probabilities offensive offensive offensive offensive offensive prediction probabilities offensive offensive irish lesbian highlighted words love irish muslims text highlighted words lesbian amate tutte gli ebr love jews amate tutte gli ebr amore ove jews ove prediction probabilities offensive offensive offensive offensive offensive prediction probabilities offensive offensive ebrei amate gli ebrei amate tutti gliamore highlighted words amate tutti gli ebr amate tutti gli ebr amoretext highlighted words source fra round analysis biases offensive speech detection based present datasets algorithms overall statistical analysis importance words relation higher scores carried across models test sentences regression analysis carried analysing offensiveness prediction phrases including words predictors including metadata model label models german italian languages also included gender used sentence predictor models english language important words leading higher predictions offensiveness general negative words include disgust someone thinks hate stupid followed words gay jew muslim side following words reduce likelihood offensiveness predictions send back primitive think love results analysis indicate words included test dataset relevant offensiveness predictions results reflect level hatred associated terms training dataset show high level hatred gay people jews muslims using identity terms online conversations strongly indicates offensive speech also leads algorithms falsely predicting offensive language models german language terms widerlich disgusting asylanten male asylum seekers colloquial klemptner male plumber hässlich ugly blöde stupid afrikaner male african also frau woman strongly contributed higher offensiveness ratings words reducing likelihood offensiveness predictions primitive primitive nutzlos useless liebe love denke think following words increase offensiveness predictions models italian language musulmani muslims musulmana female muslim musulmane male muslim stranieri foreigners schifoso lousy africani africans africana female african nigeriana female nigerian words relevant reducing offensiveness predictions europee male european itedeschi germans odio hate tedesche german queere queer gendered words used test datasets german italian language overall analysis shows masculine versions identity terms average across models phrases tend minimally reduce likelihood offensiveness predictions means slightly less often associated offensiveness topic gender predictions discussed section general results show targets groups affected bias varied languages highlighting shortcoming test dataset approach detecting bias identity terms investigated method initial templates constructed high level language cultural insight certainly highlight existence bias model well suited detecting possible forms unwanted bias wrong choice template word german including asylant asylum seeker flüchtling refugee identity terms could miss possible source bias model furthermore language models trained huge bodies text may include correlations fall outside preconceived notions grounds prejudice word embeddings already used many nlp applications supplanted language models embedded offensive speech detection models applications range content moderation systems social media online police surveillance even limited context consequences bias severe negative stereotyping censorship online harassment nlp technologies even wider application example screen employment candidates cover letters curriculum vitae create chatbots potential limitations discriminatory outcomes nlp applications need well understood widely deployed scaled bias may picked relation content text also relation way people speak based previous research research also included additional analysis tested offensive speech detection models potential bias relation dialect used means content posts may misclassified identity terms used also way author post uses language check possible bias relation dialect used posts dialect predictions training data compared offensiveness predictions likelihood comments using african american english established using slang library analysis shows correlation data labelled offensive probability comment written african american english dialect applies comments labelled offensive three models except comments model increased likelihood posts associated african american english labelled offensive even goes beyond fact training data may biased way rate predicting comments rated offensive research team offensive algorithm higher among posts likely associated african american english dialect figure shows result analysis figure likelihood comments using african american english dialect errors offensiveness predictions model odel lowestm edium highest lowest medium highest lowest medium predicted likelihood containing african american dialectpercentage posts predicted offensive source fra bias people based way speak resul gender bias offensive speech detection results preceding sections indicate gender differences predictions offensive speech german italian nuanced languages relation gender english gender variations identity terms also investigated languages variation predictions offensiveness explained extent looking metadata invented test sentences including type sentence model label gender used text analysis shows gendered sentence word included phrases used test algorithms masculine terms lead slightly lower predictions offensiveness means feminine words names likely offensive according training dataset used however specific overall analysis small difference average offensiveness rating comment dataset one percentage point lower phrases using masculine version terms using feminine version models use masculine identity terms leads lower prediction offensiveness average result indicates slight tendency hatred women training data however necessarily mean errors classifications differences error rates gender across models table shows fpr speech predicted offensive three models gender identity terms differences model fprs genders correlation observed three models fpr highest posts highest likelihood containing african american dialect confirms similar results existing research case model logistic regression case model neural network also applies offensive speech detection models based language models model determination whether correlation actually constitutes bias towards african american authors biased labelling would require use datasets including information ethnic origin authors analysis limited dialect predictions consistent particularly short strings text furthermore analysis could conducted english data tools available differentiate dialects available english however given results training offensive speech detection algorithms consistently shown perform worse languages english deeper investigation models fail various european dialects would highly relevant despite research limitations results nevertheless highlight fact way people speak picked offensiveness predictions easily lead biased predictions based potentially protected characteristics blodge green connor demographic dialectal variation social media case study english proceedings emnlp see also related github repository fact possible reliably predict certain dialect however bias found predictions still indicates association way people speak offensiveness predictions vidson bhattacharya weber racial bias hate speech abusive language detection datasets proceedings third workshop abusive language online sap card gabriel choi smith risk racial bias hate speech detection proceedings annual meeting association computational linguistics model considerably often erroneously predicts masculine terms offensive tendency reversed model patterns german italian results point different gender biases enshrined word embeddings available language models table alse positive rates based gender used identity terms models language model feminine masculine analysis looks impact selected gendered identity terms test dataset differences across predictions offensiveness results indicate gendered nature online hatred influence predictive models findings also indicate intersection relation hatred people based gender ethnic origin table section shows feminine term christian italian cristiana rated negatively masculine counterpart cristiano addition feminine version muslim italian musulmana gets negative rating masculine counterpart musulmano hand masculine term jew ebreo rated negatively feminine counterpart ebrea models may indicate gendered hatred training datasets could reflect actual differences hatred expressed also different ratings labelling process different interactions picked models important note gender differences intersections one area gendered bias women particular face considerable hatred online often expressed direct attacks women participating online conversations female politicians frequently subjected gendered hate campaigns attacks result hatred online hatred directed women also often accompanied threats sexual violence women aged reported experienced forms sexual example bias based gender noun german italian adds another dimension discussion online hatred indicates gender bias considered assessments algorithms used speech detection tasks hatred discrimination often intersectional example people may primarily discriminate people based gender combination ethnic origin incidents may picked included training data may also lead biased predictions owing potentially gendered nature hatred addressing bias speech detection concluding points results section telling speech detection algorithms rely heavily certain words indicators offensiveness terms muslim gay jew lead considerably higher predictions offensiveness terms models developed report differences vary across languages speech detection models bias partly exists algorithms built way able take contextual information account advanced methodologies using word correlations data sources mitigate issue extent however advanced methodologies rely existing generalpurpose tools suffer bias well may necessarily mitigate bias rather could increase introduce certain biases much ongoing research try mitigate bias available language example using different strategies reduce societal biases original model however development neutral training data respect certain characteristics gender ethnic origin raises question extent predictions actually neutral prevalence offensive online speech groups fact higher groups neutral training data could also lead problematic content missed researchers suggest mitigation strategies may negative impacts marginalising voices vulnerable results show algorithms used without assessment bias view actual use quick fix comprehensive assessment fundamental rights impact allows safe use tasks assisted speech detection algorithm users algorithm need ask extent people protected characteristics may put disadvantage example flagging many pieces text offensive compared groups assessments must take account training data outcomes predictions differences across potentially affected groups described assessments may well lead conclusion speech detection algorithms fit purpose certain tasks automated detection hate speech content moderation decisions need remain hands humans analysis also shows availability research nlp tools languages english lagging far behind availability english report uncovered clear imbalance tools knowledge available nlp technologies english available languages performance models german italian languages report considerably poorer ones focus english nlp development analysis also brings challenge developing tools using approaches work languages languages may context sensitive comes use words may also use gendered terms analysis german italian terms gender nouns shows gender bias also exists considerable progress made area nlp recent years much work needed safely use tools without risking increasing discrimination historically disadvantaged groups experiments report several challenges facing nlp research general encountered challenges listed language divide nlp tools available versions available many languages english language best served tools available languages german even italian computational resources existing nlp tools require considerable computational resources memory capacities often reach independent researchers hampers equal access research resources makes difficult reproduce existing research poor documentation many existing nlp tools poorly documented unreliable algorithms built top foundations liable unexplained errors failures data availability data easily available languages english order effective offensive speech detection algorithms must trained sufficiently large labelled datasets research datasets found difficult obtain languages english offensive speech standard definition offensive speech even much narrower term hate speech defined article council framework decision november yet given proper operational definition labelling publicly available datasets often available data may labelled crowdsourced reviewers insufficient training led availability datasets inconsistent labelling schemes making impossible combine datasets difficult compare results across models trained different datasets protection concerns researchers often uncertain regarding applicability data protection laws gdpr compliance major concern conducting research datasets potentially contain personal data comments made people online even public context considered personal data usually relatively easy trace author comment causing data fall purview gdpr gdpr however includes exceptions conduct reproducibility scientific research public interest articles gdpr first came force requisite legal expertise scarce nlp community researchers inclined towards overly restrictive interpretation data protection law often refraining collecting data furthermore even researchers collected labelled datasets often hesitant share data data protection reasons partly explains scarcity publicly available labelled datasets languages service change frequently given wealth data available online platforms collecting data social media platforms twitter facebook important source nlp research platforms regulate terms users make content available terms services set possibility data sharing available programming interfaces platforms terms service developers terms service subject frequent changes fact social media data particularly used offensive speech research obtained twitter currently least restrictive available programming interface access social media platforms created research bias towards twitter leaving unanswered question representative twitter data social media content general see twitter streetlight data scarcity distorts research limitations encountered researching bias speech detection endnotes fra fra fra äber horten katzer ouncil framework decision november combating certain forms expressions racism xenophobia means criminal law see also german law improve law enforcement social networks netzwerkdurchsuchungsgesetz october french avia law loi avia french draft law fight hate content internet adopted text may austrian law combat hate internet january huso vec onseil constitutionnel decision june fra ouncil directive june implementing principle equal treatment persons irrespective racial ethnic origin racial equality directive ouncil directive december implementing principle equal treatment men women access supply goods services gender goods services directive see art gender goods services directive see art employment equality directive prohibiting discrimination basis inter alia sexual orientation religion belief field employment occupation related areas dir ective european parliament council october establishing minimum standards rights support protection victims crime replacing council framework decision victims rights directive dir ective european parliament council october establishing minimum standards rights support protection victims crime replacing council framework decision jeu cornwall county council april fra see artini article orking party see artini see article orking party invoking recital gdpr different view see wachter ger shgorn osen erspective undated see conversation initiative web page mentioned llansó llansó gorwa duarter finck llansó gorw duart finck binns witter osen widinger ounta truß charitidis amnes international italy undated dix achdeva see ohchr erma rubin dix fra eimann masri ribeir gröndahl sant fra bolukbasi manzini zhao caliskan krause dathathri fra asked people travelling selected border crossing points views automated border controls majority indicated hope leads less discrimination human border guards carrying know automated tools far neutral necessarily less discriminatory report shows bias part development algorithms feedback loops occur increase bias discrimination people feedback loops biases predictions exacerbated time predictions algorithms become basis future training datasets example policing addition results show algorithms based nlp considerably biased relation certain ethnic groups also degree bias based gender relation masculine feminine versions terms german italian biases vary considerably across different models different impacts depending application algorithms report reveals checking systems thoroughly bias potential discrimination necessary everyone enjoy fair consistent decisions free bias discrimination time simply possible realistic mitigate biases discrimination datasets instances data heavily biased certain groups may difficult unbias data predictions level bias predictions needs thoroughly assessed relation harm may particular groups especially areas little research experiences applying algorithms thorough analysis bias impact applications relation potential discrimination precede deployment automation tools cases bias acceptable intended purpose algorithm may appropriate decide algorithm used abandoned conversely also important recognise bias speech detection algorithms may also lead positive effects example may result increased flagging hatred certain groups could useful purpose avoiding higher levels hate speech may also counteracted human review speech detection decisions post takedown account deletion made analysis feedback loops predictive policing highlights important aspect using algorithms influence behaviour people time may positive algorithms well developed tested looking forward sharpening fundamental rights focus artificial intelligence mitigate bias discrimination still many ways create biases feedback loops low data quality poorly developed machine learning algorithms lead predictions put certain groups people disadvantage particular highly automated settings prone feedback loops high levels automation considered areas impact people without meaningful human intervention oversight stages simulation developed research relatively simple future assessments feedback loops aim create scenarios provide better information people based protected characteristics put disadvantage may discriminatory overall report clearly corroborates need comprehensive thorough assessments algorithms terms bias algorithms used impact people time writing international organisations working frameworks make assessments related technologies mandatory including assessments bias professionals interviewed fra report artificial intelligence fundamental underscored results complex machine learning algorithms often difficult understand explain current report highlights challenges reveals uncomfortable reality silver bullet addressing bias feedback loops part prediction algorithms need monitored natural language models embedded bias making challenging potentially impossible obtain neutral outputs rich diversity languages diversity matched available tools developing using nlp future development algorithms needs accompanied bias measurements allow better understanding impact predictions way better consistent less discriminatory decisions become reality endnotes fra fra references abid farooqi zou large language models associate muslims violence nature machine intelligence vol abid farooqi zou persistent bias large language models aies proceedings conference ethics society akpinar chouldechova effect differential victim crime reporting predictive policing systems facct proceedings acm conference fairness accountability transparency amnesty international italy undated barometro dell odio project article working party opinion key issues law enforcement directive brussels european commission justice consumers article working party guidelines automated individual decisionmaking profiling purposes regulation brussels european commission justice benbouzid predict manage predictive policing united states big data society vol bennett moses chan algorithmic prediction policing assumptions evaluation accountability policing society vol veale van kleek shadbolt like trainer like bot inheritance bias algorithmic content moderation birkel church oberwittler der deutesche viktimierungssurvey wiesbaden bundeskriminalamt bojanowski grave joulin mikolov enriching word vectors subword information transactions association computational linguistics vol chang zou saligramma kalai man computer programmer woman homemaker debiasing word embeddings nips proceedings international conference neural information processing systems bommasani hudson adeli altman arora von arx bernstein bohg bosselut brunskill brynjolfsson buch card chen creel davis doumbouya durmus ermon castellon chatterji demszky etchemendy finn gale gillespie goel donahue ethayarajh goodman grossman guha hong jurafsky kalluri khattab kumar hashimoto hsu huang karamcheti henderson icard keeling hewitt jain khani mirchandani malik mitchell munyikwa koh ladhak krass krishna kuditipudi lee lee leskovec levent manning nair narayan newman nie niebles nilforoshan narayanan nyarko ogut orr papadimitriou park piech portelance potts raghunathan reich rong roohani ruiz ryan sagawa santhanam shih srinivasan ren sadigh tamkin taori thomas wang wang xie yasunaga zaharia zhang zhang zhang zhang zheng zhou liang opportunities risks foundation models workshop foundation models center research foundation models stanford university united states august brown mann ryder subbiah kaplan dhariwal neelakantan shyam sastry askell agarwal krueger henighan child ramesh ziegler winter hesse chen sigler litwin gray chess clark berner mccandlish radford sutskever amodei language models learners advances neural information processing systems vol conference neural information processing systems neurips vancouver canada december caliskan bryson narayanan semantics derived automatically language corpora contain biases science vol charitidis doropoulos vologiannidis papastergiou karakeva towards countering hate speech journalists social media online social networks media vol chaubard fang genthial mundra socher natural language processing deep learning lecture notes part course instructor manning winter court justice european union helga nimz freie und hansestadt hamburg february inge nolte landesversicherungsanstalt hannover december cjeu ingrid fww gmbh july cjeu maria kowalska freie und hansestadt hamburg june cjeu weerd née roks others bestuur van bedrijfsvereniging voor gezondheid geestelijke maatschappelijke belangen others february cjeu cornwall county council april dathathri madotto lan hung frank molino yosinski liu plug play language models simple approach controlled text generation eighth international conference learning representations iclr may davidson bhattacharya weber racial bias hate speech abusive language detection datasets proceedings third workshop abusive language online devlin chang lee toutanova bert deep bidirectional transformers language understanding proceedings conference north american chapter association computational linguistics human language technologies vol sorensen thain vasserman measuring mitigating unintended bias text classification proceedings conference ethics society aies dreißigacker befragung zur sicherheit und kriminalit kernbefunde der dunkelfeldstudie des landeskriminalamtes hanover kriminologisches forschungsinstitut niedersachsen duarte llansó loup mixed messages limits automated social media content analysis washington center democracy technology dutch parliamentary committee unknown injustice report parliamentary hearing committee childcare benefits scandal december ecthr european court human rights boacă others romania january đorđević croatia merits july ecthr romania april ecthr opuz june ecthr osman united kingdom merits october egbert krasmann predictive policing eine ethnographische studie neuer technologien zur vorhersage von straftaten und ihre folgen für die polizeiliche praxis project completion report hamburg hamburg university ensign friedler neville scheidegger venkatasubramanian runaway feedback loops predictive policing proceedings machine learning research vol european commission communication commission european parliament european council council european economic social committee committee regions artificial intelligence europe com final april european commission white paper artificial intelligence european approach excellence trust com final brussels february european commission proposal regulation european parliament council laying harmonized rules artificial intelligence artificial intelligence act amending certain union legislative acts com final brussels april commission shaping europe digital future european approach artificial intelligence european council new strategic agenda brussels european council european parliament draft report proposal regulation european parliament council harmonised rules artificial intelligence artificial intelligence act amending certain union legislative acts cod brussels april feltes guillen seguridad sentimiento seguridad bochum estudio cifra negra una gran ciudad alemana encuesta del metropolitana barcelona barcelona barcelona institute regional metropolitan studies ferris min automating injustice use artificial intelligence automated systems criminal justice europe london fair artificial intelligence online hate speech issue paper brussels centre regulation europe founta djouvas chatzakou leontiadis blackburn stringhini vakali sirivianos kourtellis large scale crowdsourcing characterization twitter abusive behavior proceedings twelfth international aaai conference web social media vol fra european union agency fundamental rights violence women survey main results report luxembourg publications office european union publications office fra fundamental rights agency survey results fra survey framework pilot smart borders travellers views experiences smart borders smart borders pilot project technical report annexes luxembourg publications office ensuring justice hate crime victims professional perspectives luxembourg publications second european union minorities discrimination survey main results luxembourg publications bigdata discrimination decision making luxembourg publications office fra big data algorithms discrimination luxembourg publications handbook european discrimination law luxembourg publications office fra preventing unlawful profiling today future guide luxembourg publications experiences perceptions antisemitism second survey discrimination hate crime jews luxembourg publications black luxembourg publications office fra data quality artificial intelligence mitigating bias error protect fundamental rights luxembourg publications fundamental rights report luxembourg publications office fra getting future right artificial intelligence fundamental rights luxembourg publications office fra crime safety victims rights luxembourg publications office fra rights matter police stops fundamental rights survey luxembourg publications office fra encouraging hate crime reporting role law enforcement authorities luxembourg publications office gehman gururangan sap choi smith real toxicity prompts evaluating neural toxic degeneration language models findings association computational linguistics emnlp german data ethics commission opinion data ethics commission berlin german data ethics commission gershgorn mark zuckerberg gave timeline take detecting internet hate speech quartz gerstner predictive policing context residential burglary empirical illustration basis pilot project germany european journal security research vol goldberg neural network methods natural language processing canada morgan claypool gonen goldberg lipstick pig debiasing methods cover systematic gender biases word embeddings remove proceedings conference north american chapter association computational linguistics human language technologies vol gorwa binns katzenbach algorithmic content moderation technical political challenges automation platform governance big data society vol gräber horten und bullycide medienkonsum cybermobbing und suizidalität von kindern und jugendlichen forensische psychiatrie psychologie kriminologie vol greenwald mcghee schwartz measuring individual differences implicit cognition implicit association test journal personality social psychology vol gröndahl pajola juuti conti asokan need love evading detection aisec proceedings acm workshop artificial intelligence security hardyns rummens predictive policing new tool law enforcement recent developments challenges european journal criminal policy research vol hovy spruit social impact natural language processing proceedings annual meeting association computational linguistics vol howard ruder universal language model tuning text classification proceedings annual meeting association computational linguistics vol hunt saunders hollywood evaluation shreveport predictive policing experiment santa monica rand corporation husovec responsible legislature speech risks rules delegated digital enforcement london london school economics johnson repeat burglary victimisation tale two theories journal experimental criminology vol katzer cybermobbing cologne springer spektrum kennedy jin davani dehghani ren contextualizing hate speech classifiers post explanation proceedings annual meeting association computational linguistics krause gotmare mccann keskar joty socher rajani gedi generative discriminator guided sequence generation findings association computational linguistics emnlp punta cana association computational linguistics lattacher predictive policing frühwarnsystem für die polizei magazin öffentliche sicherheit vol lewis liu goyal ghazvininejad mohamed levy stoyanov zettlemoyer bart denoising pretraining natural language generation translation comprehension proceedings annual meeting association computational linguistics liu ott goyal joshi chen levy lewis zettlemoyer stoyanov roberta robustly optimized bert pretraining approach llansó van hoboken harambam artificial intelligence content moderation freedom expression third session transatlantic working group content moderation online freedom expression rockefeller foundation bellagio center como italy november lum isaac predict serve significance magazine vol yao chong black tsvetkov black criminal caucasian police detecting removing multiclass bias word embeddings proceedings conference north american chapter association computational linguistics human language technologies vol marcus davis found new foundation gradient september article gdpr paal pauly eds bdsg munich crime terribly revealing information technology police productivity review economic studies vol mittelstadt individual group privacy big data analytics philosophy technology vol short brantingham schoenberg tita point process modeling crime journal american statistical association vol short malinowski johnson tita bertozzi brantingham randomized controlled field trials predictive policing journal american statistical association vol mosher miethe hart mismeasure crime thousand oaks sage gemmis lops semeraro generating post hoc natural language justifications recommender systems user modeling interaction vol nadeem bethke reddy stereoset measuring stereotypical bias pretrained language models proceedings annual meeting association computational linguistics international joint conference natural language processing vol oecd organisation economic development oecd framework classification systems oecd digital economy papers paris oecd office united nations high commissioner human rights incitement hatred papakyriakopoulos hegelich serrano marco bias word embeddings proceedings conference fairness accountability transparency park shin fung reducing gender bias abusive language detection proceedings conference empirical methods natural language processing emnlp pennington socher manning glove global vectors word representation proceedings conference empirical methods natural language processing emnlp perry mcinnis price smith hollywood predictive policing role crime forecasting law enforcement operations santa monica rand corporation perspective undated case studies peters neumann iyyer gardner clark lee zettlemoyer deep contextualized word representations proceedings conference north american chapter association computational linguistics human language technologies vol berlin kriminalitätsatlas narasimhan salimans sutskever improving language understanding generative technical report openai radford child luan amodei sutskever language models unsupervised multitask learners openai blog february shazeer roberts lee narang matena zhou liu exploring limits transfer learning unified transformer journal machine learning research vol singh guestrin trust explaining predictions classifier kdd proceedings acm sigkdd international conference knowledge discovery data mining richardson schultz crawford dirty data bad predictions civil rights violations impact police data predictive policing systems justice nyu law review vol rosen community standards enforcement report first quarter meta may sachdeva barreto von vacano kennedy assessing annotator identity sensitivity via item response theory case study hate speech corpus facct acm conference fairness accountability transparency seoul republic korea june sanh smaller faster cheaper lighter introducing distilbert distilled version bert august stoa scientific foresight unit european parliamentary research service ethics artificial intelligence issues initiatives brussels march sherman gartin buerger hot spots predatory crime routine activities criminology place criminology vol speer conceptnet numberbatch better word vectors april strikwerda predictive policing risks associated risk assessment police journal theory practice principles vol struß siegel ruppenhofer wiegand klenner overview germeval task shared task identification offensive language proceedings conference natural language processing konvens law society algorithms criminal justice system london law society tita ridgeway impact gang formation local patterns crime journal research crime delinquency vol townsley homel chaseling infectious burglaries test near repeat hypothesis british journal criminology vol twitter rules enforcement van sant fredheim abuse power coordinated online harassment finnish government ministers nato strategic communications centre excellence february vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin attention need nips proceedings international conference neural information processing systems rubin fairness definitions explained proceedings international workshop software fairness fairware von der leyen union strives agenda europe wachter affinity profiling discrimination association online behavioural advertising berkeley technology law journal vol mittelstadt floridi right explanation automated exist general data protection regulation international data privacy law vol wachter mittelstadt russell fairness automated bridging gap law computer law security review wachter mittelstadt russell bias preservation machine learning legality fairness metrics law west virginia law review vol weimann masri tiktok spiral antisemitism journalism media vol widinger mellor rauh griffin uesato huang cheng glaese balle kasirzedeh kenton brown hawkins stepleton biles birhane haas rimell hendricks isaac legassick irving gabriel ethical social risks harm language models deep mind wilson kelling broken windows atlantic march winston palantir secretly using new orleans test predictive policing technology verge february pathak wallace gururangan sap klein detoxifying language models risks marginalizing minority voices proceedings conference north american chapter association computational linguistics human language technologies zhou wang chang learning genderneutral word embeddings proceedings conference empirical methods natural language processing emnlp annex glossary term description crime observability measure likely police would observe crime present deep learning subset machine learning involving use artificial neural networks see also neural network one hidden layer downsampling assigning certain probabilities recording crime events counteract overly strong predictions earthquake policing model predictive policing model relies criminological research suggesting crime spread local environments process based assumption crime occurs certain place higher likelihood another crime take place vicinity equalised odds metric fairness metric requires false positive rates false negative rates model equal across demographics interest false negative type classification error made classification models perform binary labelling offensive speech detection model predicts text either offensive positive negative model falsely identifies comment fact offensive called false negative false positive see false negative model identifies comment offensive despite rated original test data called false positive feedback loop decisions based predictions made system influence data used retrain update system hotspot policing strategy considers area city grid cells allocates majority police patrols fixed number cells hotspots highest predicted risk crime language model system trained compute probability occurrence number words particular sequence systems trained way large body texts found retain important semantic features heart recent advances nlp logistic regression function often used binary classification schemes classification schemes choose two possibilities basically transforms question probability happening linear function using log odds function takes probability turns log model machine learning model product training machine learning algorithm training data simple example linear regression algorithm used predicting value variable based values variable original function look like algorithm trained determined values function model used predict based new input data naive bayes naive bayes classifiers family simple probabilistic classifiers based conditional probability probability occurrence something based occurrence something else likelihood text offensive given text contains term hate natural language processing field designing methods algorithms take unstructured natural language data input produce output goal nlp create algorithms process natural language order perform task ranging easy spellchecking keyword searching complex machine translation question answering sentiment analysis tasks neural network machine learning neural network series interconnected computational units organised layers accept multiple inputs produce one output deep neural networks consist several layers observed crime level crime observed police offensive speech speech may minimum cause distress person speech may incite hatred threat violence may worst threaten right life physical integrity people overfitting process whereby machine learning known potentially focus strongly patterns training data even patterns random irrelevant situations term description parameter statistics parameter statistical value mean variance used describe statistical population computer science parameter variable whose value needs set calling function subroutine program predictive policing application analytical techniques particularly quantitative techniques identify likely targets police intervention prevent crime solve past crimes making statistical predictions language models models enshrine rules usage words language based large corpora texts wikipedia rules language derived previous texts applied updated new data specific task probabilistic model police patrols distributed according crime distribution historical data example patrols sent region historical records indicate crime occurs recorded crime observed crime plus reported crime regularisation technical solution employed avoid predictions becoming strong involves adding additional parameter mathematical formulae algorithm reported crime level crime reported police victims witnesses runaway feedback loop feedback causes winner takes situation example repeatedly augmenting number police patrols sent neighbourhood regardless true crime rate sampling bias occurs members statistical population systematically likely selected sample others sentiment analysis computational study opinions sentiments emotions expressed text transfer learning fully trained machine learning models word sentence predictions used adapted new tasks true crime rate simulation studies data artificially created simulate reality one parameters included simulation assumed true crime rate may different detected reported crime rates allows measure much detected reported crimes lead biased predictions compared assumed true crime rate unbiased data historical data equal true crime rates word embeddings vector space models language represent word numbers words similar meanings also similar numerical values annex additional technical details simulation feedback loops predictive policing process several actors possible sources biases consider example certain types crime reported less case crime incidents differential observability police general complexity human behaviour cultural biases permeate historical datasets making difficult keep track factors influencing process since goal investigate formation feedback loops complexities avoided simplified approach taken focusing following parameters crime reporting behaviour represented single parameter factors influence detection crime include police patrol distribution observability crime finally fourth parameter needed reflect crime events happening given city figure describes components simulated policing process connected one another directed arrows following list provides detailed descriptions parameters crime reporting rates concern reports submitted witnesses victims police differ across neighbourhoods held constant simulations value based data german spanish victimisation set barcelona police distribution represents distribution police patrols districts related direct observations police patrols assumed number observations directly related presence police patrols per neighbourhood initial parameter value based recorded crime rates cities vienna berlin madrid barcelona set developers beginning simulation initial value based car theft data parameter changed evolution simulation based respective predictions relevant parameter identifying formation runaway feedback loops fact system bias measured looking difference true crime distribution value divergence common measure difference two distributions used measure difference observability crime measure likely police detect crime particular neighbourhood observability depend example type crime committed crimes likely detected car theft others less likely tax fraud experiment allow parameter depend crime type district order detect impact crime observability feedback loop formation context crime observability taken mean following crime type observability district crime type occurs district police happen patrolling chance observe record crime experiment order simplify simulations crime type fixed observability assumed across districts crime observation depends exclusively distribution patrols true crime distribution compared analysis real process main advantage simulation control parameters often unknown case true distribution crime across neighbourhoods difficult parameter infer statistics police records well documented proportion crime events never enter police records dark figure neither observed addition several factors influence statistics low rates victims reporting limited police resources grey corresponds crimes reported police recorded crime statistics nonetheless simulations research explore large set true crime rate distributions study influence true crime rate distribution formation feedback loops parameters implementation simulation setting standard values required assumed average crime events certain type per month example approximate number highway robbery events berlin illegal use drugs madrid recorded april sum mugging events two neighbourhoods barcelona june additional parameters include duration simulation number epochs equivalent years length historical data duration initial data approximately days algorithm update frequency duration epoch approximately seven days equivalent one week addition experiment two settings first kept historical data thus accumulating ever larger dataset simulation progressed time second always kept one year worth data thus forgetting everything older one year second setting also used experiment table shows sources crime data used specify values parameters table crime data used inform parameter assumptions city number districts period number types crime source vienna federal criminal office austria berlin subdistricts kriminalitätsatlas madrid madrid open data portal police statistics barcelona policia generalitat catalunya policing strategies assumed policing strategy part simulation decision based output predictive policing algorithm modelled part process allocating police according predictions shown figure two policing strategies considered police patrols proportionally predictions made algorithm example algorithm predicts crime events district police patrols assigned district strategy used mainly simulation described sometimes referred effective policing policing considers city grid cells allocates majority police patrols hotspots fixed number cells highest probability crime example hypothetical city cells strategy selects five cells highest probability crime dispenses majority police patrols five cells strategy mainly used simulation described prediction algorithms various algorithms tested algorithms models include simple probabilistic model allocates police patrols according recorded crime rate including reported detected crime neighbourhood based machine learning model used simulation machine learning algorithms including naive bayes logistic regression used simulation complex model referred earthquake policing model described detail number neighbourhoods simulation explores case two neighbourhoods using two neighbourhoods makes visualisations easier understand effects various parameters clearly observable however simulation also conducted two neighbourhoods runaway feedback loops also formed situations simulation experiments conducted multiple neighbourhoods simulation two neighbourhoods simple statistical machine learning models effective policing first simulation involves two neighbourhoods initially general probabilistic model used model takes simple statistical approach assumes police patrols distributed exactly according historical crime records example police records indicate crime happens neighbourhood neighbourhood patrols sent neighbourhoods respectively effective policing used basic model explore interplay parameters broadly understand predictive policing process main analysis carried using simple machine learning algorithm called naive bayes trained historical crime records predicts percentage crime across two neighbourhoods following day results probabilistic model compared predictions naive bayes model enables understanding effect using different algorithms observes much feedback loop formation process affected using machine learning algorithms compared first simple allocation according proportion recorded crime district addition corroborate validity results performance naive bayes model compared logistic regression model another commonly used relatively simple machine learning algorithm important note formation feedback loops context artefact system design system affects data used update system independent presence absence machine learning model prediction even simple probabilistic setting runaway feedback loops formed shown previous simulations show however inclusion machine learning models accelerates formation runaway feedback loops least settings mitigation measures implemented simulation multiple neighbourhoods complex machine learning models hotspot policing simulation uses hotspot policing strategy inspects performance several neighbourhoods including example berlin crime records used algorithm used special case point process first introduced predictive policing team researchers included cofounder predpol george mohler often referred predpol model literature however clear extent current commercial application corresponds original theoretical model described literature therefore model referred earthquake policing model rationale behind use earthquake policing model predictive policing relies criminological research suggesting crime spread local environments process particularly certain types crimes vandalism burglary gang however use earthquake policing model area predictive policing controversial since partly due underlying assumptions process incompatibilities nature earthquake policing model crime emerges cities also controversial tendency model form feedback analogy published earthquake policing hotspot policing assumed simulation means sending majority police cells highest predicted risk crime crime hotspots use ratio meaning five times many police assumed sent crime hotspot cells lower predicted risk crime earthquake policing model used predict cells higher risk crime frame simulation four main experiments study formation feedback loops run first experiment uses grid cells equal true crime rates across cells initial historical rates distributed uniformly exception five cells twice rates cells emulate regions higher crime rates simulation run iterations days means updates new police assignment visit five cells highest crime rates made second experiment considers distribution true crime rates set realistic situation one still simple interpret use crime statistics car thefts berlin furthermore order extract ratio top subdistricts respect cells first estimate average rates within top remaining groups calculate division ratio obtained data simulations grid cells means setting eight cells approximately top rates higher rest way ratio simulations third experiment also considers car theft events grid cells uniform true crime rates across cells total number areas dataset thus randomly select rates data used experiment observe simulation iterations simulated days computer simulation fourth experiment similar third extracts true crime rates dataset equivalent assuming data unbiased probably best setting observing whether feedback loops form fast process simulation run iterations days fourth experiment also run full dataset based subdistricts berlin data distributed grid cells historical true crime rates corresponding berlin car theft statistics simulation run iterations days police assignment distributed across cells highest predicted risk crime annex iii technical details offensive speech detection analysis offensive speech detection algorithms nlp field designing methods algorithms take input produce output unstructured natural language goal nlp create algorithms process natural language perform particular task ranging easy spellchecking keyword searching complex machine translation question answering sentiment analysis early statistical models nlp based simple supervised learning techniques text decomposed basic units words without care word order choice means neutral indicative bias languages morphologically richer english words often different meanings depending context grammar words may ideal basic unit meaning approach using words input without considering order called approach bags words used input features predictors task hand word embeddings word embedding method mapping words onto numerical vector space put simply means transforms words several numbers still preserving certain important semantic relationships means words similar meanings also closer together similar numerical representation word embeddings established learned saved reused many nlp tasks use word embeddings overcomes limitation traditional approach classifiers learned deal words already encountered training set another strong appeal word embeddings method unsupervised requires lot text data labelling closeness words drawn existing text thus word embeddings encompass ever larger vocabularies furthermore embeddings preserve form semantic relationship refers essence probability given word belonging particular context learned vast corpora used training word embedding allows subsequent models take advantage semantic relationships already encoded word representations general huge bodies text datasets required train word embeddings taken internet wikipedia selected social media platforms twitter reddit among popular sources however soon transpired associations learned text datasets desirable first research paper reach public consciousness raise alarm entitled man computer programmer woman homemaker debiasing word embeddings demonstrated word embeddings preserved associations proposed mathematical procedure debiasing word vector space using similar techniques article black criminal caucasian police detecting removing multiclass bias word embeddings tackled issue detecting mitigating racial bias word embeddings caliskan followed different strategy developed word embedding association test based implicit association test help test able detect wide variety human biases word embeddings trained large public corpora finally using word embedding association test conduct experiments gonen point debiasing techniques could merely hiding bias rather removing much research bias word embeddings based english one study bias german word finds debiasing methods developed english appropriate gendered languages suggests debiasing method german developed unfortunately debiased german word vectors provided research word embeddings debiased word vectors languages english could found language models early generation word embeddings described context independent word exactly one embedding regardless context subsequent generation word embeddings produced embeddings words context vector representation word depended also neighbouring words shortly thereafter starting pretrained language models developed distinguishing feature language models fully trained machine learning models unlike word embeddings used machine learning produce word vectors could used downstream tasks model used downstream task initial per task example known transfer learning process based two novelties first unsupervised task used models usually involves word predictions also prediction second significant new underlying neural network architecture transformers used language models competing terms sheer size vocabularies used training terms number parameters included feature promise initial expense developing training models used foundation algorithms adopted accomplish downstream tasks limited amounts labelled training data fact potential mind researchers convened stanford university august announce new paradigm field machine learning foundation development met scepticism noted marcus large pretrained statistical models almost anything least enough proof concept precious little reliably furthermore evidence mounting language models subject issues bias word embeddings research indicates gender stereotypes embedded pretrained language models methods properly measure bias investigated since tools developed word embeddings necessarily adapt well language models attempts develop strategies remove bias language research indicates debiasing efforts might negative consequences marginalised negative effect could debiasing could lead diminished model performance language used minority groups potential bias particularly troubling context language models building blocks countless nlp models wide range tasks without better understanding bias propagates downstream tasks effective measures mitigating serious risk perpetuating exacerbating biases discrimination picked pretrained language models algorithms used developing models models based three different machine learning algorithms developed using publicly available labelled datasets described simple approach using logistic regression davidson deep neural network word embeddings approach algorithm two initial choices made particular word embeddings choice neural network architecture word embeddings initial investigation showed embeddings comparable size quality exist english german italian embeddings used available three languages embeddings unavoidably larger however embeddings trained within framework ensure higher chance consistent quality fasttext also makes claim better suited morphologically complex languages training words combining subword information algorithm training algorithm convolutional neural network gated recurrent unit architectures tested allow comparison baseline architectures taken park conversationai github train offensive speech detection model using models used hugging face transformers library library contains implementations many language models distilbert implementation used development runs end development used multilingual versions three languages models performed best algorithm trained separately three datasets giving total nine models used testing bias algorithm training involves several choices need made indicated several alternatives investigated order find best results example model choice needed made convolutional neural network gated recurrent unit neural network architectures details needed decided experimentally therefore common practice split data following three sets dataset used train algorithm obtain model dataset used test trained model performance order choose best one dataset best model identified retrained combined dataset training validation data performance tested test data reported giving model performance bias analysis additional dataset used testing bias dataset generated templates test bias various identities includes invented text phrases used obtain predictions offensiveness dataset referred bias test dataset described detail datasets training data used build offensive speech detection algorithms table provides overview data used offensive speech detection model training conducted research based actual social media comments models trained data one dataset owing limited dataset size quality two datasets combined order train models commencing model training datasets reduced one field containing comment one field containing offensiveness label variables used model development process amnesty international italy kindly granted permission use dataset table characteris tics training data characteristicslanguage dataset name founta total number number samples offensive used samples offensive offensive offensive offensive platform twitter facebook twitter twitter twitter collection period context data collection development automated offensive language detection algorithmstreatment women lgbt people european parliamentary electionsidentification offensive languagedata collection hate speech journalism associated institution aristotle university thessaloniki cyprus university technology telefónica research university alabama birmingham university college london amnesty international italysaarland university darmstadt university applied sciences für deutsche sprachedatascouting international hellenic university characteristicslanguage category labels normal spam offensive abusive negative offensive negative problematic hate speech included offensive offence non hate speech offensive hate speech labelling strategy labels obtained crowdsourcingoffensiveness scored trained amnesty international italy volunteersmanually annotated one researchersthe data follow automated sampling procedure manual labelled bias test dataset special bias test dataset generated based templates developed dixon sentences contain offensive phrases based adjectives negative adjectives disgusting dumb useless primitive etc positive adjectives great smart impressive strong etc verbs negative verbs hate kill destroy abuse etc positive verbs like love respect support etc used sentences various identity terms developed used german irish muslim migrant test biases across different protected characteristics indicated collective noun columns table missing words filled lists order generate approximately equal number offensive negative sentences identity term templates word lists translated three languages german italian sentences also used gendered terms overall led dataset sentences half sentences rated research team offensive half rated bias test datasets twice large one terms gendered example sentences language predicted offensiveness three models led predictions investigated bias according identity terms used table provides examples sentences developed testing bias models sentences largely kept certain selected terms changed table example sentences bias test dataset example languageverb negative article masculine collective noun sentence english abuse whites abuse whites german missbrauche alle weißen missbrauche alle weißen italian abusare bianchi abusare tutti bianchi example languagearticle masculine collective noun sentence english think males killed think males killed german ich denke dass allemänner umgebracht werden sollen ich denke dass alle männer umgebracht werden sollen italian penso che tutti maschi dovrebbero essere uccisipenso che tutti maschi dovrebbero essere uccisi example languagearticle masculine adjective negative masculine collective noun sentence english smart buddhist smart buddhist german bist ein kluger buddhist bist ein kluger buddhist collective adjectiveadjective negative masculine italian sei buddista intelligente sei buddista intelligente performance algorithms section provides overview overall performance algorithms terms accurately detecting offensive comments three models trained three languages english german italian performance evaluated test datasets used capture well offensive comments predicted metric model performance classification models identify two classes positive negative often used trying measure well model detects positive class especially positive class makes smaller proportion test data results summarised table alongside several commonly used performance metrics models different datasets three models significantly outperformed corresponding models german italian true test set performance bias test set performance languages models showed big drop performance results test set results bias test set test set described used training models sampled data training data thus statistical linguistic properties bias test set hand different statistical properties led severe drop performance english even simple model performed well test set extra computing power resources used train neural networks solution model language model model resulted marginal improvements raises question whether marginal gain performance worth extra cost complexity developing models simple algorithms logistic regression used model fully determined training data initial parameters known means given data initial parameters logistic regression model always built case complex models often involve several random starting points methods ensuring results reproduced data parameters system used unfortunately neural networks complex guaranteed check reproducibility experiment results optimal architecture obtained models retrained times versions model used evaluate datasets model versions stable reproduced original model model results varied extent english italian meaning amount reproducibility however model versions varied significantly suffered great drop performance reasons failure reproducibility could established frame research see box challenges limitations encountered researching bias speech detection chapter table performance metrics models test datasets language model datasetperformance metric precision recall accuracy auc test set test set test set bias test set bias test set bias test set test set test set test set bias test set bias test set bias test set test set test set test set bias test set bias test set bias test set notes ecision proportion offensive text predicted correctly within instances predicted offensive recall proportion offensive text predicted correctly within instances text originally rated offensive combines precision recall calculating harmonic mean accuracy percentage predictions match labels across observations irrespective whether labelled offensive auc area curve another accuracy metric takes account possible thresholds making decision whether text offensive considering true positive rate true negative rate endnotes fra lum aac birk murrià average reporting rate around aken polizei berlin eißigacker fra eltes guillen ensign mohler lum aac ensign johnson wilson elling ita ridgeway benbouzid lum aac mohler gold berg chaubar spruit bojanowski bolukbasi anzini caliskan eenwald bolukbasi zhao gonen gold berg apakyriakopoulos speer radford eters ward ruder vlin radford liu raffel lewis vaswani anh bommas ani arcus davis gehman abid see also abid nadeem krause founta truß charitidis amnes international italy undated vidson ennington janowski ibid ark sanh devlin official hugging face implementations exist english german user upload italian liu amnes international italy undated founta amnes international italy undated truß charitidis dix getting touch person european union hundreds europe direct information centres find address centre nearest phone email europe direct service answers questions european union contact service freephone certain operators may charge calls following standard number email via finding information onlineinformation european union official languages available europa website publications download order free priced publications multiple copies free publications may obtained contacting europe direct local information centre see law related documents access legal information including law since official language versions lex open data open data portal provides access datasets data downloaded reused free commercial purposes promoting protecting fundamental rights across fra european union agency fundamental rights schwarzenbergplatz vienna austria tel fax intelligence everywhere affects everyone deciding content people see social media feeds determining receive state benefits technologies typically based algorithms make predictions support even fully automate report looks use artificial intelligence predictive policing offensive speech detection demonstrates bias algorithms appears amplify time affect people lives potentially leading discrimination corroborates need comprehensive thorough assessments algorithms terms bias algorithms used impact people fund ament rightseu charter discrimina tion informa tion society
