  1     June 2020   Governance of Artificial  Intelligence in Finance   Discussion document     AUTHORS   Laurent Dupont, Olivier Fliche, Su Yang   Fintech -Innovation Hub, ACPR            
  2   Table of Contents   1. Executive summary  3  2. Introduction  5  3. AI design and development principles  7  4. Evaluation of AI algorithms  19  5. Governance of AI algorithms  20  6. Public consultation  36  Appendices  42  7. Technical scope  43  8. Detailed description of the exploratory works  44  9. Explainability vs. interpretability  62  10. Technical aspects of explainability  64  11. Review of explanatory methods in AI  68  12. Review of potential attacks against an ML model  75  Bibliography  76  Thanks  82                            Front page: the CSIRAC, one of the first five computers put in service, supervised by its co -designer Trevor  Pearcey (archival photograph - November 5, 1952).  
  3   1. Executive s ummary   This discussion document follows upon work led by the ACPR on Artificial Intelligence (AI) since 2018.  In March 2019, after an initial report and a first public consultation, the ACPR launch ed, along with a  few actors in the financial se ctor, exploratory works aiming to shed light on the issues of explainability  and governance of AI – mainly understood as Machine Learning (ML). Composed of meetings and  technical workshops, they covered three topics: anti -money laundering and combating the  financing  of terrorism (AML -CFT), internal models (specifically credit scoring), and customer  protection. Two  focal areas emerged from those works, namely evaluation and governance of AI algorithms.   Evaluation   Four interdependent criteria for evaluating A I algorithms and tools in finance were identified:   1. Appropriate data management  is a fundamental issue  for every algorithm , as both  performance and regulatory compliance are conditional upon it . Ethical considerations, such  as fairness  of processing and the  absence of discriminatory bias, have to be taken into account  in this regard.    2. Performance  of an ML algorithm can be addressed using a variety of metrics . The range of  metrics available is  sufficient for assessing the accuracy of virtually any ML algorithm used in  finance , according to both technical and functional criteria. It is however sometimes necessary  to balance the selected criteria against the desired degree of explainability.   3. Stability describes how robust and resilient an ML algorithm’s behaviour  turns out to be over  its lifecycle. Due care must be taken to guarantee its generalizability to production data and to  continuously monitor risks of model drift once deployed in production.   4. Explainability , a close cousin of algorithmic transparen cy and inte rpretability, has to be put in   context in order to define its actual purpose. The “explanation” of a specific result or of the  algorithm’s behaviour  may prove necessary for end users (whether customers or internal  users); in other cases, it will  serve those tasked with the compliance or governance of th e  algorithm . The provided explanation thus aims to either inform the customer, ensure the  consistency of workflows wherein humans make decisions, or facilitate validation and  monitoring of ML model s. We therefore introduce four levels of explanation (observation,  justification, approximation, and replication ) in order to clarify the expectations in terms of  explainability of AI in finance , depending on  the targeted audience and the associated busine ss  risk.  Governance   Incorporating AI into business processes in finance  inevitably impacts their governance. We  recommend to particularly focus, as early as the algorithm’s design phase, on the following aspects.   Integration into business processes . Does the AI component fulfil  a critical function, by dint of its  operational role or of the associated compliance risk? Does the engineering process follow a well defined methodology throughout the ML lifecycle (from algorithmic design to monitoring in  production) , in the sense of reproducibility, quality assurance, architectural design, auditability, and  automation ?   Human/algorithm interactions.  Those can require a particular kind of explainability, intended either  for internal operators who need to conf irm or reject an algorithm’s output, or for customers who are  entitled  to understand the decisions impacting them or the commercial offers made to them. Besides,  processes involving AI often leave room for human intervention, which is beneficial or even ne cessary,  but also bears new risks . Such new risks include the introduction of biases into the explanation of an 
  4   algorithm’s output, or a stronger feeling of engaging one’s responsibility when contradicting the  algorithm than when confirming its decisions.   Security and outsourcing.  ML models are exposed to new kinds of attacks. Furthermore strategies  such as development outsourcing, skills outsourcing, and external hosting should  undergo careful risk  assessment . More generally, third -party risks  should be ev aluated.   Initial validation process . This process  must often be re -examined when designing an AI algorithm  intended for augmenting or altering an existing process. For instance, the governance framework  applicable to a business line may in some cases be maintained, while in other cases it will have to be  updated before putting the AI component into production.   Continuous validation process . The governance of an ML algorithm also presents challenges after its  deployment in production. For example, its cont inuous monitoring requires technical expertise and  ML-specific tools in order to ensure the aforementioned pri nciples are followed over time ( appropriate  data management, predictive accuracy, stability, and availability of valid explanations ).  Audit.  As fo r the audit (both internal and external) of AI -based systems in finance, exploratory works  led by the ACPR s uggest adopting a dual approach:    - The first facet is analytical. It combines analysis of the source code and of the data with  methods (if possible b ased on standards) for documenting  AI algorithms, predictive models  and datasets.   - The second facet is empirical. It leverages methods providing explanations for an individual  decision or for the overall algorithm’s behaviour , and also relies on two techniques for testing  an algorithm as a black box: challenger models  (to comp are against the model under test) and  benchmarking datasets , both curated by the auditor.   Such a multi -faceted approach is suitable both for internal auditors and for a superviso ry authority,  however the latter faces specific challenges due to the scope of its mission. In order to effectively audit  AI systems, it will need to build both theoretical and hands -on expertise in data science, while  developing a toolkit for the specific  purpose of AI supervision.   Public consultation   The analysis presented in this document is subject to public consultation. The objective is to submit to  financial actors and other concerned parties (researchers, service and solution providers, control  auth orities, etc.) guidelines sketched herein for feedback, and more broadly to gather any useful  comment, including on supervisory authorities’ best practices.    
  5   2. Introduction   2.1. Methodology   Following initial work and a December 2018 public consultation on the role of Artificial Intelligence (AI)  in finance, in 2019 the ACPR’s Fintech -Innovation Hub undertook exploratory works with a small  number of voluntary financial institutions in order to  shed light on the issues of explainability and  governance of AI in the sector. These exploratory works resulted in the avenues  for reflection   presented in this document. The technological spectrum considered here is detailed in appendix  7.  Financial actors are, as evidenced by the APCR’s first public consultation, particularly eager for  regulatory guidance pertaining to AI1. Indeed, this technology generates opportuni ties as well as risks  – operational and otherwise. One of a supervisory authority’s tasks is to provide such guidance, along  with practical implementation guidelines, with the aim of balancing freedom of innovation with  regulatory compliance and responsibl e risk management.   2.2. Exploratory works   The main goal of these exploratory works was to produce lines of thought on three topics, each related  to the ACPR’s main missions and detailed in what follows.   In each topic, the Fintech -Innovation Hub conducted a deep -dive analysis with voluntary actors in a  two-fold way:    In each case, meetings to present the AI algorithms in question along with the main  explainability and governance challenges.    In the case of the “primary”  workshops, a more technical phase involving d ata scientists on  both sides, exchanging on relevant methods and tools, including review sessions of the source  code and ML models developed by the actor.   These workshops are briefly described hereafter. Appendices provide more detailed, anonymized  descrip tions.   2.2.1.  Topic  1: Anti -money laundering and combating the financing of terrorism (AML CFT)   This topic’s key issue was whether AI can improve financial transaction monitoring, either by  complementing or by replacing traditional threshold mechanisms and other business rules. To tackle  this challenge, workshop participants introduced ML algorithms able to generate alerts (in addition to  traditional, rule -based systems already in place): those alerts are directly sent  for review to “level 2”  teams2, which streamlines and secures the alert review workflow. The resulting operational gain is  proven  while  the governance of two key AML -CFT proces ses studied in these workshops ( namely  declarations of suspicion and freezing of assets ) needs to be re-examin ed in li ght of the human  intervention requir ed by those processes, and of the continuous monitoring required for ML  algorithms.                                                               1 See also Cambridge Judge Business School, 2020.   2 Level 2 is in charge of compliance: see section 5.1.2  for explanations on the typical organization of internal  controls.  
  6   2.2.2.  Topic 2: Internal models in banking and insurance   This topic’s key issue was to determine how –and under which conditions – AI can be us ed in the design  of internal models.   Rather than studying internal models as a whole, the workshops have focused on credit granting  models; both are related insofar as scores produced by those models can also be used to build risk  classes, from which RWAs (Risk -Weighted Assets) are computed.   The workshops involved two actors: a banking group which designs and implements its credit scoring  models internally, and a consulting firm which provides a development platform for hybrid (ML - and  rule-based) models, tested in this case on the computation of the probability of default. Both  application scenarios demonstrated how in troducing ML impacts governance : the initial validation  process becomes more technically -oriented, monitoring tools become a requirement for  internal  review, and explanatory methods have to be integrated into continuous oversight as well as into audit  processes.   2.2.3.  Topic 3: Customer protection   This topic’s key issue was to ensure that AI algorithms, when used in the context of non -life insurance  product sales, always take into account the client’s best interests.   The ML model studied on this topic aimed at producing prefilled quotes for home insurance products.  That use case involved two main compliance requirements: fulfilling the duty to advise so as to  properly inform the customer, and offering a non -life insurance product which is consistent with the  requirements and needs expressed by the customer.    
  7   3. AI design and development principles   We suggest four evaluation principles for AI algorithms and models: appropriate data management,  performance, stability, and explainability.   The objectives represented by these principles need to be mutually balanced: simultaneously  maximizing all of them is usually impossible. They can be viewed as the cardina l points of a compass  by which to guide the design and development of an AI algorithm:     3.1. Appropriate data management principle   Evaluating the compliance of an algorithm and of its implementation requires covering a large  spectrum of requirements. At the ve ry core of those compliance requirements lies the proper  management of data during each stage of the design and implementation process of an AI algorithm,  as described in this section.   Input data   Defined as data fed to an algorithm during its conception, i nput data comprises reference data, training  data, and test (or evaluation) data. Proper management of input data is sometimes governed by sector specific regulation, for example data completeness and data quality requirements in the banking sector  are pre scribed by prudential norm BCBS 2393.  Data governance (Dai, 2016) is an essential function within any financial organization with a number  of data -driven business processes. Setting up a proper data governance for an AI algorithm will not  work if data sources fed to it are inappropriately managed, for example if they are fragmentary,  anecdotal, insufficiently durable, can be tampered with, or if the organization does not control their  lifecycle.                                                               3 Basel Committee on Banking Supervision's standard number 239 is an international standard, whose subject  title is  "Principl es for effective risk data aggregation and risk reporting" .  
  8   Pre-processing   Evaluation of an ML -based system also needs  to take into account operations performed on input data  prior to the machine learning phase itself. Pre-processing  may have an impact on the resulting model’s  performance (for example by over - or under -sampling training data) as well as on its ethical  acceptability (for example by excluding protected variables from training data).   Post-processing   Finally, evaluation should also include operations performed on the output (i.e. predictions or  decisions) of the model produced by the ML algorithm. Such post -processing may have a significant  impact as well, such as in the case of methods aiming to remove or reduce discriminatory biases4 from  already trained models – for example by cancelling out the dependency of predictions made by a  probabilistic model on sen sitive variables (Kamishima, 2012)5.  3.1.1.  Regulatory compliance   Regulatory compliance often includes requirements of two kinds:   - Compliance with regulation pertaining to data protection and individual privacy, starting with  GDPR in Europe.   - Taking into account re gulatory requirements specific to a domain or use case. For example in  insurance, it is prohibited to steer the sales process based on the customer’s capacity to pay:  the offer needs to be at least consistent with the demands and needs expressed by the  customer – not driven by the maximization of sales revenue from insurance products.   Compliance with the first category of requirements can be assessed by well -proven methods:  undesired biases can be detected, prevented or suppressed (by operating at any of th e  aforementioned stages: input data, pre - or post -processing), dependency on sensitive variables  (whether explicitly or implicitly present in the data) can be suppressed, etc.   The second category of compliance requirements, those which are sector -specific , often goes beyond  the scope of data management: this is the case of the obligation of means and of the performance  obligation in AML -CFT, which call for suitable explanatory methods.   Another example will illustrate the stakes of sector -specific regulatio n in further detail: that of an ML  system put in production in an insurance organization, which aims to target high -priority prospective  customers for a marketing campaign regarding a multi -risk insurance contract. The IDD (Insurance  Distribution Directive ), a 2016 European directive, introduced principles close to the equity principle  described in the next section: insurance product distributors should “always act honestly, fairly and  professionally in accordance with the best interests of their customers. ” Therefore, ML is only allowed  for customer targeting on the condition that the criteria used are based on the needs fulfilled by the                                                              4 The polysemy of the term “bias”  should be noted. It sometimes refers to a statistical, objective characteristic  of a predictor or estimator, other times to an unfair or unequal treatment whose p olarity and importance are  subjective and of an ethical or social nature. The presence of a statistical bias may lead to a fairness bias, but this  is neither a sufficient nor a necessary condition.   5 The issue of discriminatory biases is not specific to AI  either. The risk exists in any statistical model, e.g. it is  documented in the literature on “redlining” in banking economy. However that risk is amplified by the use of an  ML algorithm, in addition some detection and mitigation techniques for that risk a re also ML -specific.  
  9   product – and not on the customer’s capacity to subscribe6. The challenge is thus to correctly  appreciate the prospective  customers’ insurance needs, which are much more difficult to evaluate for  an algorithm than for a human. When ML is used, this requires using larger datasets (in breadth and in  depth), which in turn generates or increases data -induced risks such as implic it correlations with the  capacity to subscribe (which are difficult to detect) and more generally undesired biases (themselves  often latent, see next section). In short, using ML to implement a customer targeting system for  marketing should be conditioned upon mastering those risks and deploying tools to detect and  mitigate them.   3.1.2.  Ethics and fairness   Besides constraints stemming from sector -specific and cross -cutting regulations, ethical issues lie at  the core of the ever -increasing usage of AI in business p rocesses which impact individuals and groups  of people. Those issues include social and ethical concerns in the broadest sense, and particularly  questions of fairness raised by any automated or computer -aided decision process.   Ethics guidelines published b y the European Commission (European Commission High -Level Expert  Group on AI, 2019) illustrate both the importance of ethical issues and the blurred boundaries they  share with the other principles described in this section:   1. Human agency and oversight   2. Technical Robustness and safety   3. Privacy and data governance   4. Transparency   5. Diversity, non -discrimination and fairness   6. Societal and environmental well -being   7. Accountability.   These guidelines underline the broad spectrum of challenges related to ethics and fair ness in AI.  Specifically, the analysis of biases – especially those of a discriminatory nature – is an active research  domain, which schematically comprises the following stages:   - Carefully defining what constitutes a problematic bias – whether a classifica tion bias or  prediction bias, or an undesired statistical bias already present in input data – and the metrics  enabling to characterize and quantify such biases, including via explanatory methods  (Kamishima, 2012)  ;  - Determining to what extent biases presen t in the data are reflected, if not reinforced, by AI  algorithms   - Lastly, mitigating those biases whenever possible, either at the data level or at the algorithm  level.   Exploratory works conducted by the ACPR, along with a broader analysis of the financial sector,  showed that bias detection and mitigation were at an early stage in the industry: as of now, the  emphasis is put on internal validation of AI systems and on the ir regulatory compliance, without  pushing the analysis of algorithmic fairness further than was the case with traditional methods – in  particular, the risk of reinforcing pre-existing  biases tends to be neglected. This blind spot, however,  only reflects th e relative lack of maturity of AI in the industry, which has been introduced primarily                                                              6 Indeed, the conception of an insurance product should start by defining a target market, based on  characteristics of the group of customers whose needs are fulfilled by the product.  
  10   into the less -critical business processes (and those which bear little ethics and fairness risks): it can  thus be anticipated that the progressive industrialization of a dditional AI use cases in the sector will  benefit the currently very active research on those topics.     3.2. Performance principle   Performance of an ML algorithm can typically be assessed  using two types  of metrics:    Predictive performance metrics. For instance AUC (Area Under the Curve) – or alternatively F1  score – can be applied to algorithms which predict credit default risk of a physical or mora l  person. Such metrics can be categorized as KRI or Key Risk Indicators .   Business performance metrics, which can be categorized as KPI or Key Performance Indicators .  Two points of attention for such metrics are their consistency with the algorithm’s object ives  and their compatibility with its compliance requirements7.  Importantly, algorithmic performance of an AI algorithm is not a standalone objective: it needs in  particular to be weighed against the explainability principle. Subsequent sections will show that the  adequate explanation level depends, for a given scenario, on multiple factors and on the explanation’s  recipients. The choice of an explanation level in turn induces constraints on technological choices,  notably on the “simplicity” of the selected  algorithm.   An overview of the fundamental trade -off driving such choices is given in appendix  10.1.1 .    3.3. Stability  principle   The stability principle consists of ensuring that an ML algorithm’s performance and its main operational  characteristics are consistent over time. Expectations in terms of stability are all the more important                                                              7 It is for instance highly likely that maximizing sales revenue from insurance products is an inappropriate metric  for an ML algorithm used as part of the sales process: it might indeed introduce into the algorithmic process the  kind of conflicts of interest which regulation precisely aims at preventing.  APPROPRIATE DATA MANA GEMENT   All data processing should be as th oroughly documented as the other design stages of an AI  algorithm  (source code, performa nce of the resulting model, etc.) This documentation enables risk  assessment in the areas of regulatory compliance and ethics, and the implementation of tools for  detecting and mitigating undesired biases,  if need be.   ALGORITHMIC PERFORMAN CE  Performance metrics of an ML algorithm have to be carefully selected , so as to evaluate the  technical efficacy of the algorithm or alternatively its business  objectives . The inherent trade -off  between the algorithm’s simplicity and its efficacy has to be taken into account . 
  11   in the case of ML, as the dimension of data (in order to make up predictor variable s) tends to be much  larger than in traditional predictive or decisional algorithms8.  Three major sources of instability are herein identified. As of now, AI algorithms in production in the  financial sector sel dom take into account these instability sources, neither individually nor for their  overall effect. This may be due to the relative lack of maturity of AI engineering and operational  processes, and thus subject to change in the future. However ML instabili ty risks should not be  neglected since they generate significant operational and compliance risks. Hence a few mitigation  methods are suggested in the following for each of them.   3.3.1.  Temporal drift   Firstly, stability of an ML model implies absence of drift ov er time. This is essential since the distribution  of input data might deviate sufficiently to degrade the model’s performance as well as other  characteristics (such as compliance -related aspects or the absence of bias) – especially if it is not  periodicall y re-trained.   This temporal drift can be detected using somewhat classical monitoring and alert generation  mechanisms, which should however be built upon appropriate drift indicators and a well -tried  infrastructure. A key point in this regard is that tempo ral drift of a model is often linked to the evolution  of the training database, hence the very first stage when designing a drift monitoring tool – before  even taking into account data processing – consists of detecting structural changes in those input da ta.  3.3.2.  Generalization   Stability of an ML model can also be understood as robustness, in the sense of generalization power  when confronted with new data9. A lack of generalization power may have gone undetected during  the model validation, for example because test and validation datasets – though dissociated from  training data as they may be (“out -of-time” testing and “out -of-distribution” testing) – almost  inevitably differ from the real -world data fed to the model in production.   Lack of generalization power m ay be detected and (at least partially) remedied during the model  design and parameterization stages. However the resulting model should be subjected to continuous  monitoring because, just like temporal drift is ultimately inevitable, the model’s performan ce can  never be guaranteed to generalize sufficiently well to previously unseen data.   3.3.3.  Re-training   Lastly, re -training an ML model, whether periodically or on a quasi -continuous basis, does not solve all  instability issues, since it results at the very leas t in non -reproducible decisions on a given input                                                              8 It is even one of Big Data’s main characteristics, and a situation where techniques such as neural networks  particularly shine. Generally speaking, the predictive power of a classification model can be shown to increase  with the number of variables up to  a certain point, after which it degrades – a phenomenon called Hughes’ peak  (Koutroumbas, 2008) and associated to the “curse of dimensionality”. Dimensionality reduction is actually a very  common concern in ML (Shaw, 2009).   9 Generalization power and pred ictive bias are the two key criteria to balance when designing and tuning a  predictive model. Generalization is inversely proportional to the model’s variance, hence this arbitrage is referred  to as bias -variance trade -off: low bias is usually associated t o high performance on training and test data,  whereas low variance implies that the model generalizes well to new data.  
  12   between subsequent versions of the model. The main consequence of this instability source over the  course of the model’s lifecycle (thus punctuated by re -training phases) is a lack of determinism in the  over all system. This can become a problem when a particular decision must be reproduced (for  example to comply with GDPR’s right to access and opposition), possibly accompanied by an  explanation (which can be produced by one of the explanatory methods describe d hereafter).   This instability source, when it cannot be mitigated via a low -enough re -training frequency, can at least   be compensated by properly archiving all subsequent versions of an ML model used in production.     3.4. Explainability principle   Of the four principles exposed here, explainability is the one most distinctive of AI systems compared  to traditional business processes.   3.4.1.  Terminology   Notions of algorithmic explainability, transp arency, interpretability, and auditability are closely  related:   - Transparency is but a means (albeit the most radical) to make decisions intelligible: it implies  access to an ML algorithm’s source code and to the resulting models. In the extreme case of  com plete opacity, the algorithm is said to operate as a black box.   - Auditability means the practical feasibility of an analytical and empirical evaluation of the  algorithm, and aims more broadly at collecting explanations on its predictions, as well as  evaluating it according to the aforementioned criteria (data management , performance, and  stability).   - The distinction between explainability and interpretability has been the subject of numerous  debates, which are summarized in appendix 9. The term “explainability” is often related to a  technical, objective understanding of an algorithm’s behaviour  (and would thus be more  suitable for auditing), whereas interpretability seems more closely associated with a less  technical discourse (and would thus primarily target consumers and other individuals  impacted by the algorithm).   3.4.2.  Objectives   Explanations pertaining to an AI algorithm generally address the following questions:    What are the causes of a given decision or prediction?    What inherent uncertainty does the model carry?    Are the errors made by the algorithm similar to those due to human judgment?    Beyond the model’s prediction, what other pieces of information are useful (for example to  assist a human operator in making the final call)?  STABILITY   Potential instability sources which may affect AI algorithms deployed in the organization  over time  should be identified. For each such source, associated risks (operational, compliance risks, or  otherwise) should be assessed, and proportionate detection a nd mitigation methods implemented.  
  13   Thus the objectives of an explanation vary greatly, especially depending on the stakeholders  considered:   - Providing insights to domain experts and compliance teams.   - Facilitating the model’s review by the engineering and validation teams.   - Securing confidence fr om the individuals impacted by the model’s predictions or decisions.   An overview of the fundamental trade -off driving the technical choice of an algorithm based on the  types of explanations required is given in appendix  10.1.2 .  3.4.3.  Properties   An ideal explanation should have the following properties:   - Accurate : it should describe as precisely as possible the case studied (for a local explanation)  or the algorithm’s behavio ur (for a global explanation).   - Comprehensive : it should cover the entirety of motives and characteristics of the considered  decision(s) or prediction(s).   - Comprehensible : it should not require excessive effort in order to be correctly understood by  its intended recipients.   - Concise : it should be succinct enough to be grasped in a reasonable amount of time, in  accordance with the time and productivity constraints of the encompassing process.   - Actionable : it should enable one or more actions by a human opera tor, such as overriding a  prediction or decision.   - Robust : it should remain valid and useful even when input data are ever -changing and noisy.   - Reusable : it should be customizable according its intended recipients.   In practice, not all of these qualities are  simultaneously achievable. Besides, as previously mentioned,  they have to be balanced against other principles – notably performance. Thus these properties should  rather serve as comparison criteria between explanations provided by various methods, so as to select  the one most appropriate to a specific use case.   3.4.4.  Explanation levels   For simplicity’s sake, we adopt hereafter the term “explainability” rather than “interpretability” to  describe the broader concept (cf. section  9 on the terminology ). Algorithmic explainability aims to  demonstrate:   - On the one hand, how  the algorithm operates (which roughly matches the common meaning  of algorithmic transparency) .  - On the other hand, why the algorithm makes such and such decision (in other words an  interpretation of those decisions).   A key challenge of the “why” question is the auditability of an ML algorithm. As for the “how” of  explainability, associated challenges include:    - For human operators who interact with the AI system: to understand its behaviour   - For individuals affected by the system’s predictions or decisions (such as customers in a sales  context): to understand the underlying motives   - For those who designe d the system or are tasked with  check ing its compliance: to assess its  social and ethical acceptability, in order (among other things) to prove the absence of  discriminatory bias in its decision -making process.  
  14   The concept of an explanation level introduced here attempts to summarize in a single metric the  depth of an explanation10. This metric exists on a continuum, along which we define a four -level scale  of qualitatively distinct levels, which are described in the following.                                                                     10 This concept is thus by definition an over -simplification of the quality of an explanation. It aims at facilitating  the choice of a ta rget explainability level, without eliminating the need for a multi -dimensional analysis of the  explanations provided.  Level -1 explanatio n: observation   Such an explanation answers technically -speaking the question “How  does the algorithm work?”   and functionally -speaking the question “What is the algorithm’s purpose?”  This level can be  achieved:   - Empirically, by observing the algorithm’s outp ut (individually or as a whole) as a function  of input data and of the environment   - Analytically, via an information sheet for the algorithm (cf. appendix 11.1), the model, and  the data used, without requiring the analysis of the code and data themselves.   Level -3 explanation: approximation   Such an explanation provides an – often inductive – answer to the question “How does the  algorithm work?”  This level of explanation can be achieved, in  addition to level -1 and 2  explanations:   - By using explanatory methods which operate on the model being analysed (cf. appendix  11.3 ).  - Via a structural analysis of the algorithm, the resulting model and the data used. This  analysis will be all the more fruitful if the algorithm is designed by composition of multiple  ML buil ding blocks (hyper -parameter tuning or auto -tuning, ensemble methods, boosting,  etc.).  Level -2 explanation: justification   Such an explanation answers the question: “Why does the algorithm produce such a result?”  (in  general or in a specific situatio n). This level can be achieved:   - Either by presenting in a simplified form some explanatory elements from higher levels (3  and 4), possibly accompanied with counterfactual explanations ( cf. app endix 11.3 ).  - Or by having the ML model itself it has been trained to produce ( cf. appendix  11.2 ). 
  15      It should be noted that each level characterizes an explanation (or a type of explanation), rather than  an ML algorithm or model. Strictly speaking, it is about the level of intelligibility of the explanations  provided by the system , not about the intrinsic explainability of the system. Thus, a highly explainable  model such as a decision tree might lend itself to a level -4 explanation (by thoroughly detailing all its  branches) , but also to a level -1 explanation (by simply stating that it is a decision -tree predictor  operating on a given set of input variables) . The latter would suffice  in a case where the fine -grained  behaviour  of the model does not need to be – or must  not be – disclosed.   Under a m ore technical perspective, appendix  10.2  examines in further detail the technical feasibility  of higher -level (3 or 4) explanations: it presents an important hurdle to overcome (software  dependencies) along with a path to reach level 4 (replication).   The next sections describe two factors – among a number of them – driving the explanation level  required from an AI algorithm, especially in the financial sector: on the one hand the intended  recipients of the explanation, on the other hand the risk (both its nature and its severity) associated to  the considered process. Thus, the same algorithm might require a higher explanation level when its  inner behaviour  also needs t o be captured and/or when the explanation is provided in a particularly  sensitive context.   3.4.5.  Recipients of the explanation   The first key influence factor of the expected explanation level is the type of recipient targeted. This is  because the relevant form under which an explanation should be proposed in order to be effective  depends both on their technical and business proficiency and on their intrinsic motives for demanding  an explanation.   Hence different explanation levels could be applied to the same alg orithm depending on whether the  explanations serve an end user (who tries to check that they have not been treated unfairly by the  system, and for whom an explanation has to be intuitively intelligible) or an auditor (who needs to  understand the system’s t echnical architecture in detail and who is subjected to rigorous regulatory  requirements).   We hereafter describe three kinds of recipients for an explanation, and suggest each time what an  appropriate form of explanation looks like.  Level -4 explanation: replication   Such an explanation provides a demonstrable answer to the question  “How to prove that the  algorithm works correctly?”   This level of explanation can be achieved, in addition to level -1 to 3 methods, by detailed analysis  of the algorithm, model and data. In practice, this is feasible only by doing a line -by-line review of  the source code, a comprehensive analysis of all datasets used, an d an examination of the model  and its parameters.  
  16   Customer or consumer: s imple explanations   An example of explanation intended for a customer occurs in the context of insurance product sales:  the duty to inform makes it mandatory to explain to prospective customers why they were offered a  given insurance product and not another  one, furthermore those motives need to be cantered  around  the consistency of the contract (in the case of non -life insurance) or its adequacy (in the case of life  insurance).   The nature and terms of this explanation must therefore be intelligible and sati sfactory with regard to  the consumer (who cannot be required to master the intricacies of the sales process, nor the  implementation of the underlying algorithm).   Continuous monitoring: functional explanations   Internal review teams, particularly those taske d with continuous monitoring, need to verify the  model’s efficacy with respect to business objectives.   The focus in this case is put on the performance of the process involving AI, rather than on its internal  mechanics, thus the explanation given should be  of a functional nature.   Auditor: technical explanations   Thirdly, an auditor must ensure that the algorithm’s implementation is consistent with respect to its  specifications, including in terms of regulatory compliance and of technical requirements.   This e ntails, for example, verifying how an ML model is produced, but also check ing the absence of  discriminatory bias in that model . Therefore the explanation given must be technically accurate and as  representative as possible of the audited model.   3.4.6.  Associated risks   The second factor of influence on the required explanation level is the risk associated to the (total or  partial) replacement of a human -controlled process by an AI component.   The nature and severity of that risk are highly variable, as shown by the following examples:    AML -CFT: a process such as freezing of assets, which is subjected to a performance obligation,  bears an increased level of risk when AI is introduced, not only by dint of its critical role, but  also because its evaluation then depends o n comparing human and algorithmic efficiency.  More precisely still, the risk will be particularly elevated in a continuous monitoring or audit  situation (which has to assess that relative efficiency) and more moderate for a daily user of  the algorithm who keeps performing the same controls as when using a traditional transaction  monitoring system.    Internal models : introducing ML into e.g. the computation of solvency margins of a banking  institution has a direct impact on the assessment of its solvability ri sk, therefore the team who  designs the institution’s internal model will expect a satisfactory level of explanation for the  results of those computations.    Insurance : the insurance contract sales process has its own regulation, which imposes among  other thi ngs a duty to inform and the personalized presentation of reasons and justifications  to the customer, if need be. Conversely, ex-ante  customer segmentation in the insurance  sector relies mainly on accuracy objectives, without the same requirement in terms of  explainability.  
  17     3.4.1.  Examples of explanation levels by use  case   We attempt here to illustrate those somewhat abstract definitions of explanation levels and of their  driving factors through a few concrete use cases – all of which have been deployed by financial entities ,  and some of which were analysed  during the exploratory workshops conducted by the ACPR.   For each use case, the following table suggests an explanation level as a function of the  aforementioned criteria (targeted recipients and associated risk). Those suggestions are based on our  initial  market analysis, whose observations the present document aims to validate or correct (see  consultation  in section 6). EXPLAINABILITY   For each use case of AI, the impacted business processes  should be determined and the roles filled  by the AI component should be detailed. The types of recipients targeted by an explanation can  then be described, along with the nature of the associated risks. That entire context dictates the  level and form of an appropriate explanation for the AI algorithm, which must be agreed upon by  all stakeholders in the algorithm’s governance.  
  18     DomainBusines  processAI functionalityExplanation  recipientsContext Associated risk CustomerCompensation  processOperational risk (customer  satisfaction)1 Internal controlDaily oversight of  the process- Operational risk - Compliance risk (contract  honoring) - Financial risk2 AuditorEvaluation of the  algorithm- Operational risk - Compliance risk (contract  honoring) - Financial risk3 CustomerOnline quote  requestCompliance risk (customer  misinformation, failure to  perform duty to inform,  discriminatory biases...)2 Internal control or  internal auditorCompliance  checkingCompliance risk (customer  misinformation, failure to  perform duty to inform,  discriminatory biases...)3 Validation teamModel and model  update policy  validation- Solvency model risk  - Compliance risk4 Administrative,  management, and  supervisory  bodiesModel approval- Solvency model risk  - Compliance risk2 Level-2 agent Alert analysisNone (if the analyst's  methodology is not modified  by the algorithm)1 Internal controlContinuous  monitoring- Operational risk (false  negatives / false positives) - Compliance risk  (performance obligation)2 AuditorPeriodic  inspection- Operational risk (false  negatives / false positives) - Compliance risk  (performance obligation)3Internal  modelsModel designComputation of  solvency margins AML-CFTFreezing of  assetsAlertingUse case Explainability criteria Appropriate  explanation  level Insurance  contractsContract  managementCompensation  proposal Sales  proposalQuote prefilling
  19   4. Evaluation of AI algorithms   The following diagram represents the lifecycle of an AI algorithm and of the resulting model, from the  design and training phases to its use in production – and possible iterations to the learning stage, for  instance upon patching the algorithm. It attempts to put in pe rspective those implementation stages  with the appropriate validation steps, whether continuous or periodic, internal or external.   It also aims to show how each stage in the lifecycle benefits from a suitable evaluation process, based  on the four principle s previously mentioned, namely data management, performance, stability, and  explainability. Finally, it illustrates the multifaceted approach to evaluation detailed in  section 5.5.1 ,  which combines analytical and empirical evaluation.           EVALUATION OF AI  The lifecycle of AI algorithms introduced into each business process  should be detailed so as to list,  at each stage, which design and development principles (data management, performance, stability,  explainability) apply in particular, and which evaluation method is appropriate for that stage.  
  20   5. Governance of AI algorithms    Introducing ML algorithms into the financial sector often aims, be it via descriptive or pre dictive  methods, to automate or improve (e.g. by customizing it) a decision -making process which used to be  performed by humans. Therefore, the governance of those algorithms requires careful consideration  of the validation of each of these decision -making  processes. In particular, their regulatory compliance  objectives as well as their performance objectives are only achievable through a certain level of  explainability and traceability.   The following governance concerns need to be taken into account as ear ly as the design phase of an  algorithm11: integration of AI into traditional business processes; impact of this integration on internal  controls, specifically on the role assigned to humans in the new processes; relevance of outsourcing  (partially or fully)  the design or maintenance phases; and lastly, the internal and external audit  functions.   5.1. Governance principles in the financial sector   General governance principles applicable to any financial institution include the description of the  “control culture” policy implemented in the organization, the presentation of ethical and professional  norms it promotes, along with the steps taken to guarantee proper implementation of those norms  and the process in case of failure to do so. In addition to those principle s, other procedures should be  documented, such as how to detect and prevent conflicts of interest.   In this context, the most relevant elements of governance when introducing AI into business processes   appear to be the operational procedures within those processes , the extension of segregation of duties  to the management of AI algorithms, and the management of risks associated to AI. These elements  are briefly described in this section.   5.1.1.  Operational procedures   Operational procedures should be adjusted to the different activities performed, communicated, and  periodically updated, for example via a clear written documentation. Their main goals are to describe  how the various levels of responsibility are assigned, the resources devoted to internal control  mechani sms, the risk measurement, mitigation and monitoring systems implemented, and the  organization of compliance monitoring. Those procedures also list rules relative to IT security and to  business continuity planning.   5.1.2.  Segregation of duties   There are no organi zational standards relative to internal controls and risk management, only methods  which have been tried and tested when implementing such functions (COSO, Cobit, Risk IT, etc.).                                                              11 This document does not address another governance issue , which should nevertheless precede  any decision  to adopt a technical tool – independently of its usage of AI and of its business application –, namely the  cost/benefit analysis. In other words, o nly governance questions specific to the usage of AI in the financial sector  are considered here in. 
  21   Nevertheless, internal control mechanisms conventionally comprise multiple le vels of control, so as to  follow the “four -eyes principle”. Classically those levels are12:  - A level -1 control, within the business units which conduct their activities or perform their  duties in a controlled manner.   - A level -2 control, exercised by the unit managers or directors, or in more complex  organizations by teams responsible for internal controls (also referred to as internal oversight).   - A level -3 control, exercised by the internal audit directorate, which aims to guarantee the  proper implementation o f control mechanisms by periodically reviewing their operational  accuracy.   A clear segregation of duties must exist between business units which commit operations, and those  which record and monitor operations on an ongoing basis.   5.1.3.  Risk recognition and assessment   Organizations should perform a risk mapping, which must be periodically updated and evaluated, so  as to develop a coherent and comprehensive view of risks. They should also define and regularly  promote a solid, consistent risk culture dealing wi th risk awareness and with risk -taking behaviour .  Lastly, they should implement systems and procedures to guarantee a cross -cutting, prospective risk  analysis.   5.2. Integration in business processes   One of the main challenges for the governance of AI algorithms is their integration in existing  processes . Key factors to take into account are the role played by the algorithms within a business  process, the engineering methodology used, and who the end u sers are.   5.2.1.  Role of AI   The roles played by AI components in business processes  are highly variable.   The primary AML -CFT workshop ( section 8.1) illustrates how the fun ction of an ML model can be  operational, even business -critical: in that case, its role is to route certain alerts triggered by financial  transactions with a particularly high estimated risk directly toward level 2 (Compliance), thus inducing  an operationa l risk in case the Compliance team becomes overloaded. The critical function of the AI  component is also elevated in this case by the constraint of real -time operation: suspicious  transactions should be detected and reported with as small a lag as feasible .  Conversely, the incorporation of ML into the prospective customer selection process for the purposes  of commercial canvassing or cross -selling is not truly disruptive, and does not incur any significant  change in the business process .  5.2.2.  AI engineering meth odology   The definition of an appropriate engineering process for ML varies greatly depending on the business  process and on how the models are used. Two examples shall illustrate this variety of situations:                                                                12 The 2013 “CRD -IV” European directive ( Capital Requirements Directive) defines the basis for such an  organization within financial institutions.  
  22   - When ML is used by marketing teams (a common cas e, although not covered by the  exploratory works described herein), significant room for manoeuvre  is granted to model  building, which is often an iterative process since one -off model deployment is used e.g. to  feed a marketing campaign.   - Conversely, ML us e cases studied in the ACPR’s workshops require a more systematic  engineering process, closer to the best practices adopted by the software industry: build  automation, reproducibility of releases, QA (quality assurance) process, monitoring of the  models de ployed in production (including their stability over time). The engineering process  should thus meet more stringent requirements in this latter case.   Thus the AI engineering process can vary from a one -off build -and-deploy mode, through an iterative  build process, all the way to a continuous process which can also be fully automated, typically using  CI/CD13.  As for the delivery mode of the AI system itself, it is also variable between a manual delivery process  where only final artefacts (i.e. the ML models) are retained to be put in production, and at the other  extreme delivering the entire datasets and intermediate results from the algorithm’s execution and  model -building stages. A middle ground between those two approaches is the “managed services”  approach  offered by the consulting firm which participated in the workshop on probability of default  (section 8.4), which is composed of two elements: on the one hand a model  engineering workbench  which follows a systematic (albeit not fully automated) model -building approach but is controlled by  the solution provider, on the other hand an information sharing platform which enables the customer  who uses the ML model to perform  a complete review of the engineering process, and provides an  audit track independent from the execution of that process.     5.2.3.  AI end users   The impact of the introduction of AI in a business process primarily depends on who its end users are  – as opposed to personnel responsible for its internal control whose role will be examined in the next  section. Those end users may be internal such as marketing teams and business unit managers, or  external such as clients and prospects.                                                               13 CI/CD (Continuous Integration / Continuous Deployment) refers to general software engineering principles  based on automating the entire design and d evelopment process, which enables more frequent product releases  than traditional methods allow, without trading off their quality. This methodology is closely related to agile  methods as well as the DevOps approach, which associates the roles of software development and IT operations.  AI ENGINEERING METHODO LOGY   The AI engineering process should be designed to cover the entire algorithm lifecycle. Depending  on the  use case, a systematic approach may be necessary, in accordance with principles of model building automation, build reproducibility, quality assurance, and monitoring of the engineering  workflow.    In any case, full traceability of the AI design and engine ering process should be guaranteed.  
  23   In particular, maintaining the quality expected from the process requires examining whether a  particular form of explana tion should be provided to end users so as to clarify and motivate the  decisions and predictions impacting them.   Types of end users   In the case of integrating an ML component into an AML -CFT workflow (see details on these works in  the appendix), end users are level -1 and level -2 teams:   - Verifications performed by the Compliance team need to be adequate to this new approach  (which requires mastering the underlying technology).   - Model validation needs to be performed much more frequently than e.g. in the case of capital  requirements models, since drifts may occur in real time here (for example a false positive rate  which deviates from the norm), hence monitoring of the model must itself be feasible in (quasi)  real time.   In the case of the workshop on customer protection (section 8.5), prefilled quotes for a home insurance  contract are delivered to the customers themselves, which requires explaining the reasons for offering   a specific product. These reasons must be in line with the customers’ requirements and needs.   Human -machine interactions   It is essential that end users of an algorithm, insofar as they are internal users tasked with ensuring the  accuracy and quality of a business process , remain independent from the machine. This is because  human experts are able to spot manifest errors made by the algorithm, which also offers the benefit  of contributing to its performance and stability (i.e. two out the four design princi ples presented in this  document).   AI also provides additional leverage to check the absence of systematic biases or temporal drift in the  decisions made – or the advice given – by an automated process in finance: in such a situation,  introducing ML into pr ocesses enables to decrease the operational risk.   Human intervention in a decision -making process delegated to software is not inconsequential, as it  introduces a new kind of risk: the downside of enabling a human operator to validate the decisions is  that they may become liable, especially in cases where they contradict the algorithmic result rather  than confirm it. Besides, humans sometimes modify their own behaviour  when interacting with a  machine: they may tend to systematically follow the algorithmic r esults, including the erroneous ones,  rather than engaging their liability by rebutting them.   This issue of independence from the algorithm and responsibility towards its decisions are of course  related to the explainability principle, since a human operat or needs to understand the mainspring of  a given decision in order to, if need be, counter it with a more appropriate one.   Lastly, human intervention might introduce bias – desired or not – into the explanations associated to  an algorithm’s output, whenev er the explanations provided or amended by the operator become  disconnected from the actual underlying factors which led to that output: the explanations become  distorted or overridden, furthermore transparency is no longer guaranteed for the algorithm, wh ich  may hide some of its deficiencies. A basic recommendation appears relevant in this regard, namely not  allowing human intervention to define or formulate the algorithm’s explanations.  
  24     5.3. Internal control system   The other major impact of introducing AI algorithms pertains to the continuous va lidation of those  algorithms, and specifically internal control procedures.   5.3.1.  Organization of internal controls   Monitoring algorithmic performance and detecting its potential drift over time requires a different  design of the human validation process . For example, the AML -CFT workshop (section 8.1) illustrated  how the partial replacement of level -1 operators by an ML algorithm may decrease the capacity to  evalua te the process efficiency in the future, at least in terms of false negatives (alerts not raised by  the system, and thus corresponding to transactions which will not be analysed  by the human eye): this  is why some of those operators have been assigned to m anual labelling  in parallel with the algorithm’s  execution, thus continuously yielding new training data.   As for the organizational structure of internal controls, an ML algorithm often aims to replace (partially  or fully) the tasks performed by the level -1 team (reviewed by their hierarchy) and/or level -2 team (in  charge of compliance checks), but probably not level -3 (in charge of internal controls) – although  nothing precludes this automation stage to be achieved in a more distant future. The users of th e  algo rithm’s output are thus – as should be – not part of the team tasked with monitoring its behaviour ,  nor are they its designers.     5.3.2.  Initial functional validation   In the case of the workshop on credit scoring models (section 8.3), a pre -deployment model validation  process was defined, with the involvement of technical teams (in charge of building and validating AI END USERS   The scope and conditions of human intervention in AI -driven business processes should be well defined. In particular, AI integration into those processes should be planned according to end users’  needs. If end users include both internal and external indiv iduals, the respective forms of  algorithmic explanations appropriate to each of those should be articulated.   Algorithmic results may  also have to be submitted to a human validation  process. This validation  should be governed by rules documented as part of the internal control procedures, both because  human responsibility becomes engaged and because the algorithm may modify human behavior  and judgment .   ORGANIZATION OF INTER NAL CONTROLS AND AI  Internal control procedures of AI algorithms should, to the extent possible, i nvolve both technical  specialists and domain experts. Indeed, monitoring those algorithms requires initial technical  validation of the components involved, their continuous monitoring, and adequate management  of compliance risks generated or reinforced by the ML.  
  25   models, both locally and globally within the banking grou p in question) and of the compliance and risk  management department.   In particular, any model deployment (be it a model creation or the patch for an issue affecting an  already -deployed model) requires validation by the group -level Risk Committee, among oth er things  to approve the chosen risk management strategy. The use of ML within these models is thus  considered and evaluated by stakeholders across the organization: by technical experts, domain  specialists, and within high -level committees. This approach appears beneficial and applicable to other  use cases, possibly with variations according to how critical the impacted business function is.     5.3.3.  Ongoing functional validation   Exploratory works on the AML -CFT topic, wherein an ML algorithm detects anomalies to be analysed   by teams at levels 1 and 2, have shed light on how such an algorithm requires more sophisticated  methods for ongoing review than traditional methods do. This incl udes continuously monitoring the  proper calibration of the algorithm: volume of alerts raised to level 2, rate of false positives filtered  out downstream, etc.   This upgrade of the ongoing validation process thus requires from the teams in charge:   - At a mini mum, deploying and mastering tools for monitoring the operational behaviour  (in real  time if need be) of the algorithms.   - Building appropriate expertise and tradecraft so as to detect incidents upfront, and ideally to  diagnose and remedy them as well.     5.3.4.  The case of internal risk model updates   Whenever AI is used in the construction of internal models, an essential consideration in the validation  process is how to define triggering events for a model revalidation. I n this regard, AI -based internal  models differ from expert systems based on rules and various predefined configuration parameters,  which have to be revalidated each time those parameters are proactively updated or are deemed  obsolete: ML -based internal mod els become invalid following a major change in their input data. (It  should however be noted that those models are not always devoid of predefined configuration INITIAL FUNCTIONAL VA LIDATION   The impact of an AI algorithm on the initial validation process should be defined as early as the  design phase. Stakeholders involved should include technical staff from the design and validation  teams, domain experts, and transverse committees concerned with the business processes in  question.   ONGOING FUNCTIONAL VAL IDATION   Ongoing functional validation of AI algorithms requires both dedicated tools (such as dashboards  enabling the teams in charge to monitor their overall behavior) and closely  interacting  with the  technical experts who designed the m and performed the initial validation.  
  26   parameters, such as a learning algorithm’s hyper -parameters, in which case they must be subject ed to  the same treatment as traditional, rule -based models.)   In the particular case of internal risk models (referred to as “Basel models” in the banking sector), a  model update policy defined by the banking institution clearly documents a number of criter ia (such  as a specific threshold being exceeded upon a parameter adjustment) which trigger the requirement  to report the model update to the supervisory authority. This kind of parameter adjustment is typically  decided and performed by the human experts in  charge of the risk model, therefore one may ask what  should become of those triggering events if the model is based on ML.   In fact, “classical” internal models assume that parameters are calibrated against the data, not unlike  the training phase for an ML  model. Besides, current regulation requires a governance framework  which comprises the processes of back -testing and model parameter updating: parameter updates are  often the consequence of back -testing results and thus caused by changes observed in the data.   The internal model update must be reported as soon as the induced change is deemed material by the  institution, which should therefore define within its governance framework the process for evaluating  the materiality of a change – whether the model u ses ML or not.   Lastly, most actors in the banking sector opt for a scheduled calibration strategy; however some  classical internal models update their parameters on a periodic, systematic basis (for example their  volatility parameters in the case of a mark et model), also similarly to ML models. Therefore, from a  model update policy standpoint, it appears that ML -based models can be treated like traditional  internal models.   5.3.5.  Technical validation   Technical expertise is required for AI validation, typically thr oughout the Data Science spectrum:   - Data Owner and Data Steward are respectively responsible for the governance and for the  quality of data used by the algorithms.   - Data Engineers and Data Scientists are tasked with ensuring proper operational behaviour  of  software components which implement the algorithms.   - Lastly, in this context Data Analysts perform initial and ongoing validation of the algorithms’  output.   The AI engineering stages to be covered by an adequate technical validation process include:   - Model selection, training and tuning.   - Continuous monitoring of the model (non -regression of the algorithm, absence of model drift,  etc.).   - The higher -value task of detecting new data sources or variables to feed the algorithm.  
  27     5.3.6.  Management of AI -related risks   Internal control procedures are inevitably impacted by the use of AI and their evolution closely related  to the generated risks, depending on the type of integration with business pro cesses  (automated vs.  computer -aided decision -making) and on the nature of those risks (regulatory or legal, operational,  financial).   Let us consider  the example of AI used for assisting insurance claim processing: this downstream  process within the value chain of insurance product distribution – which is not part of the exploratory  works presented in this documented – has recently experienced an increased usage of ML. The typical  use case in this context is to perform algorithmic filtering of incoming dama ge reports, so as to detect  likely fraud cases or apply exclusion criteria. Cases having gone through those filters will result in a  compensation offer (automatically generated in some cases), whereas rejected cases will be routed  toward operators in charg e of insurance claim files. The main challenge here is the accuracy of the  claim management process:   - The insurance organization is exposed to financial risk if the rate of incoming claims resulting  in a compensation offer unduly increases.   - Operational risk  exists in case the volume of cases rejected by the algorithms increases to the  point of overloading level -2 teams who need to review them.   - Lastly, compliance risk appears if the overload is such that the rate of disputed claims itself  increases significan tly.  Conversely, the main stake for the workshop on customer  protection (section 8.5) is the explainability  of the advice given , in order for the consumer who is offered an insurance product to be informed  prior to making a decision.   Besides these various AI -related risks, a cross -cutting concern is the necessity for a dedicated safety  mechanism (as part of a fall-back  plan going a ll the way to business continuity planning) designed to  remedy an incident, major malfunction or failure of an AI component:   - If the integration into initial business processes  is sufficiently modular and robust, this safety  mechanism may simply consist of falling back onto the initial process for the time period  necessary to fix the failure.   - If the process  has been more structurally impacted by the integration of AI, the safety  mechanism will require more sophistication (and often proves more complex to imp lement as  it also needs to be officially validated).  TECHNICAL VALIDATION   Technica l validation of AI algorithms introduced in business processe s requires developing a broad  in-house Data Science experti se, be it spread across departments (within an organizational  structure built around multi -disciplinary profiles) or gathered within a s pecialized business unit.   This technical expertise should span the Data Science spectrum (from data engineering to state -ofthe-art ML techniques) and be multi -tiered: generalist skills, financial sector specialization, and  deep knowledge of business processes specific to the organization .  
  28     5.4. Security and outsourcing   Security of solutions relying on ML algorithms requires taking into account at least two types of risks  rarely – it at all – encountered in traditional solutions: specific algorithmic risk (in terms of availability  and integrity) and data processing risk. An additional consideration is the potential outsourcing of the  design, impl ementation or exploitation of those solutions, which bears ML -specific security risks.   5.4.1.  ML security   ML security challenges are similar to those of traditional IT systems: they typically pertain to  confidentiality, integrity, and availability. Their treatmen t, although it should be tailored to their exact  usage of ML, is in no way unique to the financial sector. This is even more so as the attack surface in  finance is narrower than in other sectors: IT security in finance is usually a well -funded and mature  area, furthermore exposure from things like open source code and use of public data has thus far  tended to be more limited than elsewhere14.  The way to make an ML model safe is different from the way in which a web service exposed through  a REST API15 – or th e underlying data sources for that matter – can be secured. These three potential  targets lie on three different architectural layers (while being mutually interwoven): respectively the  model layer, the application layer, and the data layer.   A comprehensiv e description of the potential flaws of an ML model and of the means to remedy them  is beyond the scope of this document. A categorization of the main possible attacks is however given  in appendix  12.                                                              14 The reversion of this trend is however already visible, as many actors rely heavily (and sometimes exclusively)  on open -source libraries and products implementing ML functionality so as to avoid “re -inventing the wheel”,  and as the use of so -called altern ative data (collected from the web or from other publically available sources)  becomes widespread among data -driven systems in finance.   15 REST (Representational State Transfer) is an architectural method commonly used to build applications  exposed on the w eb. A REST API (Application Programming Interface) is thus a simple, standard, easily secured  method to design and deploy a web service.  MANAGEMENT OF AI-RELATED RISKS   The nature of risks pertaining to AI’s role in the process should be carefully identified : operational  IT-related risk, financial risk, compliance risk, etc .  These risks should be included in the risk map required  by governance principles, up to and  including incident remediation and business continuity plans . 
  29     5.4.2.  Third -party and outsourcing risks   Financial actors rely on different types of third -party providers to develop their AI: design and  implementation may be outsourced, off -the-shelf software products and services are now a common  offering in AI, lastly hosting and administration of AI applications are  often delegated to a cloud or  hosting services provider.   Outsourcing -related risks   Risks classically generated by the outsourcing of software skills, design and implementation are  particularly acute in AI.   Those risks are difficult to mitigate in practice  if they have not been sufficiently anticipated. Hence a  good practice prior to any outsourcing decision is to perform an ex-ante  risk analysis taking into  account the following risks.   Reversibility   As reported by the outsourcing guidelines published by European control authorities (EBA, 2019;  EIOPA, 2020), reversibility of outsourced solutions constitutes a significant, non -AI-specific source of  vulnerability within financial institutions today.   Using AI may even further exacerbate those concerns. Contro lling the entire engineering workflow  when it is outsourced requires mastering a variety of skills, including:   - Data Science skills pertaining to advanced ML techniques.   - Software design and architecture tradecraft related to complex systems with multiple  integration points among components whose source code is not always open and well documented.   - DevOps expertise in order to manage a heterogeneous infrastructure, which often combines  dedicated hosting servers and cloud services.   Even when the entire skillset  is available, as is often the case in large banking institutions, those skills  may be scattered across departments whose technical teams are too “siloed ” to be able to re internalize what had been delivered (often as a monolith) by third -parties.   Outsourced AI development   As described in the workshop on probability of default (section 8.4), outsourcing the development of  an AI component induces many changes in  the business process . Resulting challenges are, among  others:  ML SECURITY   Assessing the security of processes involving AI should take into account both classical IT security  evaluation and the analysis of AI -specific risks and mitigation techniques.   In particular, any impact assessment , incident remediation plan or security audit mission should  consider potential attacks against ML models, against their underlying data, and against predictive  and decision -making systems as a whole.  
  30   - Developing and nurturing adequate in -house resources (human and technological) to validate  code written outside the organization.   - Ensuring the delivered software is thoroughly documented as pos sible and periodically  updated, so as to meet the criteria of internal control procedures.   - Planning for an audit of the deployed solution as early as in the AI design phase, which requires  thinking ahead in terms of software architecture and integration ca pabilities.   - Deploying sophisticated (and themselves well -documented) explanatory methods in order to  explain the results produced by a system whose development was outsourced.   Off-the-shelf AI software   The risks induced by an AI software acquisition strate gy are similar to those resulting from outsourcing  its development: dependency risk, non -reproducibility of results, lacking IT security by the software  provider, product support deficiencies, and audit capability (assuming audit operations are relevant,  i.e. when recurrently buying from the same provider).   The workshop on probability of default (section 8.4) has raised the issue of the dependency risk  towards an AI solution provider: in that specific case, the risk is controlled insofar as the provider  enables the customer to review all stages le ading to the delivered ML model. It remains nonetheless  the customer’s responsibility to master the technologies involved in order to mitigate the dependency  and vendor lock -in risks. Furthermore, caution is advised against acquiring software which does no t  sufficiently limit this dependency risk, for instance if the resulting model constitutes the only  deliverable, to the exclusion of upstream stages in the engineering workflow allowing to rebuild the  model – or alternatively, if that workflow is not adequ ately documented. This lack of information, both  on how the ML model was designed and how it can be expected to behave, may lead both to  operational risk and to difficulties in internal control and audit missions.   Lastly, it should be noted that a risk su ch as non -reproducibility of results is neither new nor AI -specific:  traditional models which AI aims to replace often share this characteristic with outsourced AI solutions,  especially if they rely on stochastic methods such as Monte Carlo simulations16.  Cloud hosting   Service and application hosting on a public cloud has become an outsourcing scenario commonly  encountered in the financial sector. To accompany this trend, an initial set of outsourcing guidelines  have been published by European control author ities in banking (EBA, 2019) and in insurance (EIOPA,  2020).   Those guidelines cover more or less the same ground in both domains, namely: assessing how critical  business processes are (and impact analysis), documentation requirements, duty to inform the  supervisor, access and audit rights by the financial institution but also by the supervisor, IT security,  the risks associated to data management, subcontracting, contingency planning (including business  continuity plans), and the exit strategy out of the ou tsourcing agreement.                                                               16 Monte Carlo simulations are a class of optimization methods which relies on randomness (more precisely,  repeated, com putationally -intensive random sampling) to emulate the behaviour of an often deterministic  process or model.  
  31     5.5. Audit of AI algorithms   The AI evaluation principles previously exposed in the context of initial and ongoing v alidation remain  valid for audit operations, be they performed by internal teams as part of periodic review or by the  control authority as part of its supervisory missions. Thus, an auditor will need to consider the precise  context in which the algorithm w as developed and, in particular, the business processes  into which it  is integrated or which are impacted by it in one way or another.   Based on this context and on their objectives, auditors will need to consider the aforementioned trade offs between the different evaluation criteria of AI algorithms, and the evaluation methods themselves  will need to be suitable for self -learning systems. The audit team’s AI skills must be sufficient to meet  these requirements – as do those of the teams in charge of ongoi ng review.   5.5.1.  A multi -pronged approach   A variety of situations can be encountered when evaluating an ML algorithm, due to the previously  mentioned parameters (combination of algorithm type, end users and application scenario) and to the  circumstances of the validation process itself (access to source code and underlying data may or may  not be possible, technical resources may be available or not, etc.). The necessity to handle these  different situations encourages the adoption of a multidimensional approach to the evaluation of ML  algorithms: th e one described in the following associates analytical and empirical methods.   Analytical evaluation   An AI algorithm can be characterized by its maximum explainability level, according to the four levels  identified in  section 3.4.4 .  Levels 1 (observation) and 2 (justification) do not make the algorithm’s internal behaviour  intelligible:  an explanation then cannot rely on the model architecture nor on the piecewise anal ysis of the  algorithm at various levels of granularity. Levels 3 (approximation) and 4 (replication) on the other  hand rely on a structural or detailed analysis of the algorithm – or more precisely analysis of its source  code and of the resulting model.  THIRD -PARTY AND OUTSOURCIN G RISKS   Any decision to outsource the design, implementation, hosting or operations of an AI system, or to  use third -party products or services, must be preceded by an ex-ante  risk analysis and take into  account its results, especially with regard to reversibility.   Some governance principles pertaining to the third -party relation should also be observed:   - Ensuring  proper documentation of deliverables and traceability of the process so as to  enable auditing  if necessary.   - Guaranteeing  the financial institution access (technical, practical, but also legal with regard  to IP – intellectual property – rights) to the source code and the ML models, even when  development or hosting of the latter has been outsourced.   - Offering the same guarantee to the supervisor so as to enable audit missions which cover  the AI systems, their source code, and the data  they use . 
  32   If the organizational risk policy has adequately determined the explainability level required by the use  cases of each AI algorithm, then audit missions pertaining to those algorithms should focus on high stake algorithms which have logically been assigned a  higher explainability level (3 or 4). In that case,  the analytical evaluation is feasible, assuming a few prerequisites are met – the most important one  being the accessibility of the source code, including its documentation.   In cases where an internal or  external audit mission addresses an algorithm which has been assigned  a lower explainability level (1 or 2), one of the first stages of the audit should consist of validating that  this level is compatible with the types of risks and the compliance require ments of the business process  in which the algorithm is integrated. The audit may then evaluate the algorithm and its impact on the  efficiency of that process via the empirical methods described in the following section.     Empirical evaluation   The ML system  is in that case treated as a “black box”  and is evaluated from the outside, i.e. by  observing its behaviour  based on various input data. Several approaches are feasible, of which three  are described in the following.   Post -modelling  explanatory methods.  These methods operate on pre -trained ML models and are  categorized as global or local depending on whether the y aim to explain a specific decision or the  algorithm’s overall behaviour . A number of such methods are called “black -box” explanatory methods:  they remain valid even when it is impossible to access the algorithm’s documentation or source code,  and are the refore particularly suitable for algorithms whose maximum required explainability is level  1 or 2.   An auditor can also use post -modelling  explanatory methods as a complement to any explanatory  method implemented by the algorithm’s designer (such as those described in appendix 11.2 ). Besides,  counterfactual explanations constitute a particularly interesting case of post -modelling  explanatory  method insofar as they can  contribute to assessing that the appropriate data management principle  described in this document has been followed, both in terms of regulatory compliance (specifically  with respect to GDPR) and in terms of ethics and fairness. A non -comprehensive list o f post -modelling   explanatory methods is provided as appendix 11.3 .   The workshops led by the ACPR  with financial actors, also detailed as appendix, have shown those  explanatory methods to be already in widespread use within the internal validation processes of ML  algorithms, mostly as a way to assess their proper behaviour  by means other than efficiency metrics.  It seems logical that the same methods should also be mad e available to external actors tasked with  evaluating those systems.  ANALYTICAL EVALUATION  OF ML  When  the stakes warrant it, a ppropriate analytical evaluation techniques and tools should be  implemented as early as the algorithm design stage . Those methods may rely on information sheet s  describing the algorithm, the model, and the data used, and whenever p ossible on the analysis of  the code and data themselves.  
  33   Benchmark datasets.  This method consists of providing test data designed to stress -test the algorithm.  That dataset may be either synthetically generated or composed of real -world data (a nonymized if  need be), or even hybrid (typically via a generative model which enables to augment an initial  “bootstrap” data sample).   From a technical standpoint, any empirical evaluation of this kind requires dedicated Data Science  resources, more specifically Data Engineering profiles who can build benchmark datasets and  frameworks.   Challenger models . This method consists of providing a challenger model, whose predictions or  decisions are to be compared against those produced by the model under eva luation.   A point of attention regarding this method is its practical feasibility: indeed, the development of  challenger models requires allocating significant resources (human and material) and time to the task.  Those constraints are also hardly compatible  with audit missions as currently performed by the  supervisor, which consist of analysing  the properties of the model in place and of checking their  consistency with respect to regulatory requirements. Thus they do not aim at building an alternative  ML mod el17. The next section suggests a few ways to implement this type of empirical  evaluation  method – which is both an ambitious and a complex task.   Lastly, it should be noted that multiple empirical evaluation metrics are available and that their choice  depen ds on the objectives, both when using benchmark datasets and for challenger models. Some  metrics focus on efficacy (in order to assess algorithmic performance) whereas others analyse  how  particular segments of the population are treated (in order to detect  discriminatory biases), others still  investigate decisions made by the algorithm on a particular data point, etc.                                                               17 To put in perspective the effort required for building an alternative model, implementing a credit model for a  banking institution typically invo lves tens of employees over a timespan of several years, even though its scope  is limited to the organization’s own data.  
  34     5.5.2.  Challenges for the supervisor   The previously described multidimensional approach to evaluating AI algorithms requires the  supervisory authority to adapt its tools, methods and data. Indeed, the analysis of an ML component  differs from that of a procedural algorithm (and even more so of a process performed by human  operators). It requires not only a certain level of expertise, but significant resources dedicated to  building tools – such as challenger models – and maintain datasets which enable efficient control  missions.   Besides, the variety of use cases for AI, and the variety of possible models for a given use case18, require  that the supervisor find a balance in the implementation of its evaluation methods: on the one hand  adaptability is necessary to support the inevitab le diversity of models encountered in different  organizations, on the other hand only a sufficient level of formalism enables a systematic approach  (e.g. to be able to plug a challenger model on the organization’s data, and conversely to test any  deployed model against a benchmark dataset).   Work on the tools   This line of work consists of building software and Data Science components in order to facilitate and  accelerate supervisory missions.   Those tools should enable producing challenger models as previousl y described, in order to compare  them against those provided by the supervised organizations. A specific obstacle for the supervisor is  the dependency on heterogeneous data models across financial actors: the evaluation method based  on challenger models im plies that those models be able to ingest data according to a structure specific  to each organization. The challenge is analogous at the output: for example, some of the transaction                                                              18 In the case of internal risk models, for instance, the diversity of models can be seen as a factor of prevention  against herd behavi our and hence against systemic risk (see discussion paper “Artificial intelligence: challenges  for the financial sector”  published by the ACPR in December 2018).  EMPIRICAL EVALUATION OF ML  Empirical evaluation methods should be implemented as early as the AI algorithm design stage, and  included in t he quality assurance process of the resulting models ( as part of the non-regression,  functional, and integration tests).   Explanatory methods should be viewed as an essential tool for evaluating an ML model. They may  be implemented at the design stage or o perate on previously trained models, besides some  methods apply to models that need to be evaluated as “black boxes”. The choice of the most  appropriate explanatory methods should take into account the algorithm type, the intended  audience of the explanations , and the risk associated to the process.   Internal audit and supervisory missions may also use empirical evaluation methods such as  benchmarking using their own test scenarios and datasets, or the comparison of challenger models  to the models deployed within the organization. In order to facilitate those missions, it is  recommended to design data models and algorithms to be as modular and well -documented as  possible – which constitutes good software engineering practice anyway.  
  35   monitoring m odels described in the workshop  on AML -CFT ( section 8.1) produce a categorical output  (low/medium/high risk level) whereas others produce a numeric suspicion score.   Work on the data   This line of work consists of enabling the supervisory authority to access and process data from various  open or closed sources (public data, regulatory data, supervisory mission reports, etc.) at different  levels (national, European or international control authorities).   Those data should allow to  build and maintain datasets for benchmarking the models deployed in the  industry: the goals are to measure those models’ performance, to assess their explainability on new  kinds of data, to detect their temporal drifts, and so on.   The main challenge prese nted by the benchmark dataset approach is closely related to that described  for challenger models: in the absence of standardization efforts, datasets must be produced according  to a format and semantics aligned with those in place within the supervised or ganizations, which here  also incurs an additional cost of technological and methodological adaptation.   Training   In order to enable and accompany this adaptation of the methods, tools and data available to the  supervisor, appropriate training is a clear req uirement, above all in the field of Data Science: such  training may find its place at the supervision authority – as is considered within the ACPR – or in  specialized institutions (for example pertaining to compliance in the banking sector).  
  36   6. Public consult ation   Respondents are invited to illustrate their answers to the following questions using the use cases of AI  – particularly of ML – implemented in their organization.    6.1. Context       6.2. Explainability principle   On the basis of exploratory works conducted on three topics along w ith a broader investigation of AI  in finance and of state -of-the-art ML in other domains, this document has outlined a number of  expectations pertaining to the explainability of AI algorithms.   The relevance of those guidelines needs to be confirmed on several points, which are the object of the  following questions.    QUESTION 1: EXPERIENCE WITH ML   What kind of knowledge or experience do you possess regarding AI in general and ML in  particular (R&D, Data Science, operational tradecraft, etc.)?    If you are answer ing on behalf of a financial institution, what is the level of familiarity with  AI within your personnel (both in technical and in busi ness roles)?   QUESTION 2: IMPLEMENTATION OF ML (QUESTION SOLELY FOR FINANCIAL CORPORATIO NS)   What are the ML algorithms implemented in your organization?    For each algorithm type, specify their use cases and the type of environment (development,  pre-production, production)?    For each use case, according to which  criteria and evaluation methods has the algorithm  been selected (raw performance, explainability/efficacy trade -off, etc.)?    What are the respective roles of the teams involved in the design  and implementation of  ML algorithms in your organization (Data Scientist, software architects, project  management, business experts, compliance officers, etc.)?    QUESTION 3: DEFINITION OF THE EXP LANATION LEVELS   Are the four explanation levels emerging from this analysis (1: observation, 2:  justification, 3:  approximation, 4: replication) clearly defined?  If not, indicate the points of misunderstanding.  
  37       6.3. Performance principle       6.4. Stability principle    QUESTION 4: ADEQUACY OF THE EXPLA NATION LEVELS   Do those explanation levels appear to represent an adequate scale in the following senses :   Do they span the entire spectrum of current and future applications of AI in finance, from  full transparency all the way to algorithms operating as “black boxes”?    Does the choice of four levels seem appropriate (if not, should there be fewer or more  levels)?  QUESTION 5: PRACTICAL EXAMPLES OF  EXPLANATION LEVELS   The table presented in section 3.4.1  suggests an appropriate explanation level for a few use cases  of AI in the financial sector.    How suitable are those suggested levels? If insufficiently, for what reason?    Are those suggestions  adapted to your own usage scenarios of AI (specify those scenarios)?  If not, in what sense?   QUESTION 6: TECHNICAL PERFORMANCE  METRICS   How do you view the technical performance metrics commonly used for ML (AUC or F1 score, GINI  score, etc.), specifically:    Their adequacy with respect to the various ML algorithms?    The availability of methods to choose between those metrics?    How those metrics are used (model validation, selection of its operating point, model drift  detection, etc.)?   QUESTION 7: FUNCTIONAL PERFORMANC E METRICS    Which functional metrics (KPI) seem relevant when evaluating an AI component? Do those  metrics account for compliance requirements specific to the processes considered?    Who should be responsible for defining functional metrics  (technical or domain experts,  with or without input from risk management and compliance teams)?   QUESTION 8: TEMPORAL DRIFT OF MOD ELS   What risks are, according to you, generated by the potential drift of models over time?    What methods are or should be used to remedy those risks , or at least circumscribe them  (out -of-time testing, alert triggering based on model drift detection, etc.) ? 
  38       6.5. Appropriate data management principle      QUESTION 9: MODEL  GENERALISATION    What limits to the generalization power of ML models have been identified, whether in  relation to overfitting or to intrinsic limits of the model?    How can those limits be handled (out -of-sample testing, etc.)?   QUESTION 10: RETRAINING AS A SOURC E OF INSTABILITY    Based on your experience, a re model retraining phases  (whether on a periodic or  continuous basis) a source of model instability?    What techniques are or could be used to limit this source of instability (no n-regression  testing with appropriate  datasets, etc. )?  QUESTION 11: REGULATORY COMPLIANCE  OF DATA MANAGEMENT   In your experience, which methods or techniques appear advisable in order to ensure compliance  with vario us regulatory requirements relative to data management:    The GDPR?    Other cross -cutting regulations?    Sector -specific regulations, such as the European I DD (Insurance Distribution Directive )?  Specify what stage(s) of the AI development process (design / training / prediction) involve these  methods and techniques.   QUESTION 12: BIAS DETECTION AND MI TIGATION   Which methods appear advisable in order to analyze biases in ML systems, for each of the following  types of bias:    Pre-existing b iases in the input data fed to the ML models?    Biases present in the algorithms themselves?    Biases with in the models produced by the algorithms – and in the ir decision s and  prediction s?  More precisely, which fairness metrics enable the identification of bi ases, for example those with a  discriminatory nature ?  Which methods can be used to mitigate the undesired biases thusly identified?  
  39   6.6. Integration in business processes       6.7. Internal control system        QUESTION 13: ROLE OF AI   Which are or should be, according to you, the outlines of a method to assess the integration  of AI components in business processes ?   What should such a method enable to evaluate: how critical the function of those  components is, how disruptive they are wit h respect to the traditional process , what  human -machine interactions are possible, etc.?    What are  your thoughts on maintaining “ parallel”  processes  assigned to human operators  so as to continuously evaluate and/or correct an algorithm’s results?   QUESTION 14: AI ENGINEERING METHODO LOGY    Should the engineering methodology used for AI differ from that used for traditional  models, and more generally from standard software engineering practices? If so, in what  way?    How should, according to you, the ML m odel -building process take into account the  integration of those models in business processe s?  QUESTION 15: RISK MANAGEMENT    How does the introduction of AI into business processes  impact risk management: does it  generate new risks or magnify pre -existing risks (specify the nature of those risks:  operational or financial, legal, etc.)?    Are new, AI -specific risk management methods called for (for example, calibration of ML  models in order to limit the exposure to a given type of  risk)?   QUESTION 16: FUNCTIONAL VALIDATION    What should be the initial functional validation process of an ML model (i.e. prior to  deployment in production)?    Should functional validation be re -iterated when deploying a new version? Specify if the  answer  depends on the type of update (patch, improvement, etc.).    How should ML components be continuously monitored for business risks?   QUESTION 17: INTERNAL MODEL UPDATE POLICY (INTERNAL RISK MODELS )   On what conditions may, according to you,  ML algorithms b e used within “Basel models”   in the banking sector , and within internal models in the insurance sector?    How should an organization’s internal model update policy take into account the us e of ML  in its internal models?  
  40     6.8. Security and outsourcing       6.9. Multi -pronged approach to evaluation   This document suggests implementing a multidimensional approach for auditing processes using AI.  The follo wing questions aim to further define this approach.      QUESTION 18: TECHNICAL VALIDATION    What should be the initial technical validation process of an ML model (i.e. prior to  deployment in production)?    What technical indicators and methods should be used to continuously monitor ML  components deployed in production?   QUESTION 19: OUTSOURCING   Does the use of AI generate specific challenges or risks when its development, hosting or  administration are outsourced? If so, which ones?   QUESTION 20 : SECURITY    What is the impact of using ML on IT security?    Which types of attack against ML models (causative attacks, surrogate model attacks,  adversarial attacks, etc.) appear the most important to you, both in terms of occurrence  likelihood and in terms of damage inflicted in case of success? Specify according to the type  of ML model, the use case, and the environment (dedicated hosting servers or cloud  services, etc.) .  QUESTION 21: ANALYTICAL EVALUATION    Which of the following elements are available for evaluating an AI algorithm  in the relevant  organizations : the source code? Its documentation? The resulting models? The training and  validation data? Specify if the answer depends on the algorithm type, the use case involved,  or the context of the evaluation (internal validation, external audit, etc.).    Do you use standardized documentation frameworks such as information sheets describing  the algorithm, the model, or the data used ? 
  41       QUESTION 23 : EXPLANATORY  METHODS    Which explanatory methods ( cf. appendix  11) are  currently implement ed among the use  cases of AI to your knowledge?    Do you know of any explanatory method for AI other than those described in this  document ? If so, which ones? Have they already been implemented and deployed?    Does the most appropriate explanatory method to use depend on the algor ithm type?    Does it depend on the intended recipients of the explanation , and  if so, in what way?    Does it depend on the level of risk associated with the business process, and if so, in what  way? QUESTION 22: EMPIRICAL EVALUATION    Which of the empirical evaluation methods suggested in section 5.5.1  (benchmark datasets  or challenger models) seems more appropriate in your opinion?    Is the architecture of data processing workflows and AI systems within the relevant  organizations sufficient ly modular and robust to enable this kind of functional testing at the  data or model level?    Are the data format and schema sufficiently standardized (or flexible ) to support a data  benchmarking method without incurring data integration costs by the supervisor?    Analogously, are they sufficiently documented and transparent to support the integration  of challenger models developed by the supervisor , without this appro ach being rendered  unrealistic by an information asymmetry?  
  42   Appendices  
  43   7. Technical scope   AI is an extremely broad field whose definition – based on academic work and industry practices –  evolves quickly over time. Within this discussion document, AI is considered solely in its embodiments  relevant to the financial sector, both in their current form and  in their likely evolutions over the near -  to medium -term horizon.   7.1. ML vs. AI   The scope of this document is restricted to ML (Machine Learning), which happens to be probably the  most intensely studied field within AI. Other forms of AI are not taken into co nsideration: robotics,  game theory, optimization under constraints, multi -agent systems, knowledge representation and  reasoning, or planning automation.   Among the ML methods used in the financial sector and considered in this document, the following  catego ries should be mentioned (without any comprehensiveness):   - Unsupervised learning methods (in particular clustering techniques), which are commonly  used in fraud and anomaly detection scenarios.   - Predictive models which may be called “traditional”, such as de cision trees and logistic or  linear regressions.   - More sophisticated yet also commonly implemented models such as decision -tree based  ensemble methods (Random Forests, Gradient Tree Boosting, etc.).   - NLP (Natural Language Processing), used to classify and analyse  all kinds of text data.   - Deep Learning (deep neural networks), used in various use cases including CV (Computer  Vision) where they particularly shine – although a less prominent use case in finance than in  other sectors.   7.2. Models vs. algorithms   Anothe r key point of terminology is the distinction between an AI algorithm and the model produced  by that algorithm. An ML algorithm (AI being, as indicated above, the field of AI considered in this  document) is an executable procedure represented as software c ode, just like any algorithm. Its  specificity with respect to other types of algorithms is to operate on input data (above all training data  but also validation data) and to produce an ML model as output. That model is, generally speaking,  itself composed of a predictive algorithm and of model data. The predictive algorithm is typically an  optimization procedure which minimizes an error metric for the model on training data.   A few examples shall illustrate the relations between ML models and algorithms:   - A linear regression algorithm produces a model composed of a vector of weights.   - A decision -tree construction algorithm produces a model which is a tree whose internal nodes  are logical conditions involving predictor variable s, and whose leaves are predicted v alues.   - A neural network algorithm (based e.g. on a back -propagation method and a gradient descent  algorithm) produces a model which is a graph structure whose nodes are weight vectors.   The terms model and algorithm  are sometimes used interchangeably within  the present document  when the context is unambiguous, or when the meaning refers both to the model building process  realized by the algorithm and to the prediction process realized by the already -built model.  
  44   8. Detailed description of the exploratory works   This appendix presents, for each exploratory work on the three topics selected:   - A description of the purpose and relevance of the exercise.   - The objectives of the algorithm presented by the financial actor involved.   - A few technical details on the method and  the implementation.   - The validation process adopted by the actor.   - The governance issues raised by the introduction of AI into the business process .  - The evaluation methods used and their implications, according to the four evaluation  principles exposed in t his document (appropriate data management, performance, stability,  explainability).   - The engineering methodology used to develop the AI system in question.   The following sections do not in any way constitute an evaluation of the algorithms studied during the  exploratory works, nor of the business processes in which they are used19. Their goal is to provide  contextual, factual information to the reader, so as to shed light on the lessons drawn by the ACPR in  this discussion document.   8.1. Topic 1: Anti -money laun dering and combating the financing of terrorism  (AML -CFT)   8.1.1.  Regulatory context   Current AML -CFT regulation requires financial institutions to implement risk management procedures  enabling them to detect PEPs (Politically exposed persons), the transactions inv olving individuals tied  to a high -risk country listed by the FATF (Financial Action Task Force) or the European Commission, as  well as the transactions, which are incoherent or anomalous with respect to the organization’s  knowledge of its customers, and ma y result in a SAR (suspicious activity report, or equivalently  suspicious transaction report).   European and national regulations pertaining to freezing of assets also require financial institutions to  set up a unit dedicated to implementing the relevant me asures – which include, in addition to asset  freezing, the prohibition of making funds available.   Those regulations do not require using a computer system to do so, but in practice most organizations  use software processes due to their size and their activ ity volume.   Lastly, those regulations do not contain any provision specific to the use of AI.   8.1.2.  Purpose of the exercise   The objectives of the primary AML -CFT workshop, augmented with a secondary  workshop, were the  following:   - Understanding the potential use cases of AI in AML -CFT.   - Gaining familiarity with the underlying AI techniques.                                                               19 The call for applications published in March 2019 stated that the works envisioned were in n o way related to  the ACPR’s supervisory procedures.    
  45   - Thinking about possible adjustments of supervisory processes in view of controlling AI -driven  AML -CFT processes.   8.1.3.  Objectives of the algorithm   The main project studied within this  topic consists of introducing ML models to aid the filtering of  transactional messages – in other words, design algorithms which can assist agents tasked with  distinguishing, among the list of alerts raised by a rule -based third -party monitoring tool, the  false  positives from the transactions concerning individuals who actually are on embargo or sanction lists.   In the process prior to introducing AI, operators review alerts issued by the screening mechanism in  order to determine whether they are physical o r moral persons targeted by restrictive measures. These  operators are organized according to two levels. A level -1 team is in charge of the initial alert  processing, on the basis of a decision -making matrix. The alerts which are not resolved at level 1 are   escalated to level -2 teams, which are authorized to release the payment, reject it, or file a homonymy  case with the administrative authority responsible for the freezing of assets.   The role of the ML model developed is to assist this decision -making pro cess and to route the  transactional messages to the appropriate level based on their relevance, i.e. the more sensitive  messages will be directly processed by level 2, which aims at streamlining and securing the overall  process. Level -1 teams, no longer in  charge of the initial processing of some of the alerts, will then be  able to absorb a volume increase. This model, developed by the participant to this workshop, was  dubbed TPA (True Positive Acceleration).   8.1.4.  Technical details   The ML algorithm is based on a  neural network which is fed features with varying levels of complexity:  message characteristics, phonetic distances between strings, address components (using NER, i.e.  named entity recognition), and semantic analysis of free -form text. Those variables ar e extracted from  transactional messages by the filtering tool and do not contain any personal data (unlike the original  messages).   AI contributes to rationalizing the filtering process. Indeed, by quickly and efficiently discriminating  between heaps of vol uminous messages not only frees up the analysts who can focus on tasks with  higher added -value. The analysis of results produced by the AI also gives them higher accuracy in their  daily job, since the risk forecasting process gains in precision as the volu me of data analysed  grows  over time. A reduced amount of routine, repetitive tasks, along with the opportunity to partake in  engaging strategic works, should also contribute to employee retention.   Lastly, the situation can also be considered where AI direc tly contributes to improving the decision making of human analysts by performing a post -hoc analysis of the abandoned or escalated alerts, so  as to give them a means to adjust their decisions on future alerts.   8.1.5.  Validation process   The starting point for the participant to this workshop was to build on existing validation methods used  for risk management models, which could be relevant to internal control procedures.  
  46   The usual frameworks for such risk management models are organized around a model validation t eam  and a model update team. Those two teams are mutually independent: an independent review tends  to increase the algorithm’s efficacy and to reduce its operational risk.   The goal is to perform a formal validation once a year and each time the model unde rgoes significant  change. Meanwhile, machine or human expert systems might use rule -based procedures in order to  build a reference dataset, which can then operate as a benchmark against which to compare the model  under development, so as to identify cases where AI -driven decisions deviate from expected norms.   The peculiarity of validation processes for ML is the lifecycle of the models:   - On the one hand, the integration of the ML component in the business process should be  performed on ce, according to validation methods in line with the organization’s governance  framework.   - On the other hand, the statistical validation of the model should be consistent with the first  kind of validation, and be repeated over time – ideally on a continuous basis.   In other words, the notion of a priori  validation should be re-examined , since shorter validation cycles  are necessary, which makes the dichotomy between initial validation and ongoing review less relevant  in the case of AI algorithms.   At any rate, the validation p rocess should be proportional to the risks, in particular in terms of  regulatory compliance.   8.1.6.  Governance issues   The governance schema chosen by the actor was to ensure a two -fold human role in the monitoring  of the algorithm: level -2 analysts are tasked to authorize or reject transactions, but also to guarantee  the algorithm’s proper behaviour , while level -1 analysts also annotate transactions in parallel with the  algorithm, which increases the amount of additional training data available to it. The latter a pproach  has not been retained by all actors who introduced AI into their AML -CFT filtering workflows ( cf.  section 8.1.9 ), however it enables to validate the performan ce and stability of the algorithm over time,  even in the presence of major changes in transaction profiles.   In terms of operational risk, a point of attention is the significant decrease of the level -1 workload (on  the order of 10%) due to the introduction  of the ML model. It is necessary to anticipate the operational  risk that would result either from an interruption of the algorithm’s operation or from a more general  system failure: this risk is critical because of its potential ramifications, given that the AI component  contributes here to a performance obligation. In particular, the organization needs to ensure that the  level -1 validation teams remain capable of absorbing, if necessary, the entirety of incoming  transactions without degrading the quality of service provided.   8.1.7.  Evaluation  methods and their implications   Explainability   The explainability requirements of the algorithm are different from the other workshops (which  pertain to credit granting models and to the construction of an insurance product).  
  47   Indeed, there is no requirement to motivate the decisions made by the algorithm which impact an  individual. Checking the relevance of an alert raised by the algorithm is also relatively simple for an  analyst: in order to do an efficient job of comparing t he alert to a sanction list, the operator does not  need to know the reasons why the alert was triggered.   The most important benefit of explainability in this case is its business value: it facilitates the analysis  of the patterns of filtering behaviour  captured by the algorithm (which also constitute its training data).  This assistance in understanding the operations performed by a human analyst is an additional help  for the ongoing review of the algorithm’s efficacy, which is a key asset in a domain subjec ted to a  performance obligation (otherwise put, false negatives are very costly and should be reduced to a strict  minimum).   Performance   The statistical performance of the predictive model, along with its operational impact on the alert  processing workflow,  have been evaluated with the following observations:   - Statistically speaking, the model exhibits a slight overfitting, which however does not appear  to induce any functional risk given how the algorithm is integrated in the overall process: in  the worst ca se, that process will not be automatically accelerated, nevertheless they will be  adequately processed by level -2 teams if need be.   - The algorithm’s impact on the business process manifests itself by a significant decrease of the  volume of alerts to be proc essed by level -1 teams, and by a marginal increase of the volume  of alerts to be processed by level -2 teams due to the improved recall of the model.   Stability   The model’s behaviour  appears to be stable over time, insofar as the relative impact of the accel eration  of message processing on the workload managed by level -1 and level -2 teams is itself stable over time.   However in this usage scenario, data quality and comprehensiveness are essential, and their  “freshness” is necessary to ensuring that the model which relies on them is operating properly. Two  approaches can be used to this aim:   - Making temporality explicit in the algorithm, since it plays a key role in the semantics of data:   in particular, datasets used in AML -CFT should be periodically reviewed in  order to take into  account new methods used by malicious individuals.   - Building generic, time -independent variables: for instance, instead of using a “ country”  variable , use “country belongs to a given sanction list” which is a time -invariant feature relat ed  to the issue considered.   Appropriate data management   This project directly stems from the compliance department, however as previously indicated, a  specificity introduced by the use of ML is that the responsibility for the validation process, in additio n  to the compliance team, also lies on domain experts and on technical experts.   8.1.8.  AI engineering methodology   The project was undertaken according to  an agile methodology, and was at the time of the workshop  still at the experimental stage. As suggested in se ction  8.1.5 , it appears sensible at this stage not to 
  48   demand – even on as sensitive a topic as AML -CFT – an excessively broad or cumbersome validation  process, which would involve other departments and hinder its deployment in production.   8.1.9.  Secondary workshop   A secondary workshop on the AML -CFT topic was conducted with another banking group20. This section  only summarizes the noteworthy differences with the primary worksh op.  Objectives of the algorithm   The business process in which ML was integrated in this case is the filtering of transactional messages,  not to screen them against sanction lists (which in the primary workshop led to a potential rejection  of the payment or  the assets being frozen), but to detect suspicious transactions and, when  appropriate, yield a suspicious activity report (SAR). This function is performed by enterprise software  specialized in filtering financial transactions, which uses preconfigured bu siness rules: those rules are  executed on each transaction to produce a suspicion score, which is then used to route transactions  above a threshold toward teams tasked with the analysis of alerts. Following the standard, those teams  are broken down into le vels 1 and 2: alerts above a first threshold are directed to level 1 (at the level  of the branch offices) while those above a second, higher threshold are directed to level 2 (the Tracfin21  correspondents of the banking group).   In the new approach, an ML mo del is trained on a training dataset composed of 50% manually issued  alerts which have been validated and 50% alerts generated by the rule -based software. It should be  noted that for a significant portion of the manually issued alerts, the suspicion score produced by the  business rules is zero.   The integration of ML into the process differs from the primary AML -CFT workshop in that the ML  model is here introduced as a complement to the enterprise software, with the following features:   - The function of the ML  model in the primary workshop was to escalate some of the alerts  trigged by business rules from level 1 to level 2. In this case, the ML model produces additional  alerts which are sent directly to level 2. The AI thus follows a parallel workflow, and not a serial  one where the execution of business rules would be followed by ML prediction. Thus, rather  than a classifier for previously -raised alerts, the banking group has deployed a detection tool  for validated alerts which is applicable to the entire trans action flow.   - Also, a filter has been introduced so that, when a transaction is assigned a high suspicion score  by the ML model, an alert will only be generated if no alert was raised by the business rule  engine on the same customer within the three preceding months. In other words, an alert  triggered by the ML corresponds to a customer which has been given a high score while having  stayed below the rule engine’s detection threshold for a while.   - Lastly, contrary to the business rule engine, the ML mod el takes into account information  beyond transactional data: statistical features of the transactions are combined with static  variables (either direct measures such as duration of the customer relationship or asset value,  or constructed variables such as the types of products and contracts) on a sliding time window.                                                               20 This workshop is presented as a secondary study as it was conducted belatedly, furthermore the use case for  AI and its technical implementation are relatively similar to the first workshop.   21 Tracfin ("Traitement du renseignement et action contre les circuits financiers clandestins ") is a service of the  French Ministry of Finances in charge of enforce AML regulation and coordinating its application.  
  49   Governance issues   Contrary to the primary AML -CFT workshop, level -1 teams do not annotate the transactions in parallel  with the ML model so as to detect false negatives: this is because accordi ng to the organization’s AI  team, any relevant sample (i.e. having a sufficient number of false negatives) would be too large. Two  methods could be considered for analysing  false negatives – namely either lowering the alert triggering  threshold or systemat ically sending the n most suspicious cases for review –, both of which would  likely induce an excessive additional workload for the operators. Besides, some false positives are  simply due to non -observable variables.   It should be noted that this way of introducing ML into the business process (i.e. as a complement to  the enterprise software), along with the routing of ML -generated alerts to level 2, result in additional  workload for level -2 teams. This is why a new AI project has been initiated with the goal of routing  certain alerts raised by the enterprise software from level 2 to level 1, in order to reduce that extra  workload.   Also in relation with those changes in the business process, the banking group has decided  to structure  the organization of its AML -CFT expertise around “dual skillsets”, i.e. employees who master both ML  (including data management issues) and possess business experience (including in risk management).   Those different governance choices between  both AML -CFT workshops are particularly interesting:  each option is probably suitable for its particular context, and the feedbacks gathered around both  projects will likely provide valuable know -how regarding the possible trade -offs between the  predictiv e power of an ML model, its temporal stability, and the workload dedicated to manual  annotation of data.   Explainability   Explainability requirements are aimed at different types of users. A joint effort within the banking  group, involving technical teams, t he compliance department and IT people, led to proposing  explainability forms adapted to what each user type wishes to observe and in what context (in line  with the approach described in section “ Recipients of the explanation ”):  - Technical teams (in particular Data Scientists) rely on the explanations during the model  construction phase – not for continuous monitoring. SHAP ( Shapley Additive Explanations )  values are the e xplanation form used in that case to understand the decision made on a  particular transaction.   - Compliance experts use explanations to support their decision to abandon or validate an alert.  Workshops were organized with these users in order to better defin e their needs (as simple  tabular representations of SHAP values were quickly deemed inadequate). This led to the  development of a GUI (graphical user interface) showing explanations which are still based on  SHAP values but easier to interpret and more acti onable.   - Lastly, the banking group also aims to provide relevant explanations to internal or external  auditors, including (as a complement to both previous explanation forms) a documentation  ensuring proper intelligibility of the algorithm.   Performance   The main performance indicator is the rate of alerts generated by the detection system which result  in a SAR. The introduction of ML according to the aforementioned architecture enabled the doubling  of this indicator.  
  50   Stability   A monitoring tool has been imple mented as early as the initial deployment of the ML model, so as to  detect any operational anomaly or model drift. That tool periodically checks several indicators  characterizing the model, the input data, the output score distribution, etc.   Technical tea ms indicated during the workshop that it was still too early to determine whether drifts  of the ML model were more or less frequent than the need to reconfigure the enterprise software.  Updating the ML model would nevertheless be simpler than updating the parameters of the business  rule engine for several reasons: it is a simple retraining phase without addition of new features, it is  also fully automatable, and the entirety of model parameters are adjusted without any manual  intervention. Besides, the ML m odel update – from retraining to deployment to production – would  not take longer than 2 to 3 days, which is significantly less than a reconfiguration of the enterprise  software.   8.2. Topic 2: Internal models in banking and insurance   The second topic for the ex ploratory works conducted by the ACPR pertained to internal risk and  capital requirements models. In fact, candidates on this topic suggested to study use cases in a slightly  different domain.   As a consequence, this topic pivoted toward risk credit modelli ng, considering both granted to  individuals and to businesses. It consisted of two distinct workshops:   - A workshop focusing on credit granting models:  those models usually compute a credit score.  The participant to this workshop is a banking group.   - Another workshop relative to so -called behavioural  credit models:  those models aim to  estimate a probability of default on a given time horizon for a current credit. The participant  to this workshop is a large consulting firm which provides to banking orga nizations an ML  model construction platform.   8.2.1.  Regulatory context   Both workshops shared the following initial observations:    Classical internal models are generally relatively easy to audit but perform poorly. More  advanced or more complex models should provide a performance improvement, albeit at the  cost of explainability.    Regulatory requirements are identified as hindrances to the implementation of innovative  algorithms, especially those based on ML: such requirements pertain to stability of the  result ing models, to their auditability, but also to the transparency and explainability of the  algorithms.    Additional challenges related to personal data protection, along with limitations inherent to  the data (in terms of access or completeness, for example), make it challenging to analyse   correlations among multiple variables characterizing customers and their behaviour .  8.3. Workshop on credit scoring   8.3.1.  Purpose of the exercise   The banking group in question has implemented methodological guidelines for credit scoring  models.  
  51   The workshop aimed to explain how credit modelling  teams took into account those guidelines – which  had been defined and refined over the course of many years – so as to build models in accordance.   8.3.2.  Objectives of the algorithm   This workshop involve d the analysis of several credit scoring models, all of which answered a dual  objective:   - To reduce the dependency toward third -party data providers (such as Credit Bureau) by  integrating additional internal data sources into the algorithms: for instance, behavioural  data  in addition to Credit Bureau scores and to traditional internal data such as credit history.   - A more classical objective was to improve the discriminating power of credit scoring models.   The three models studies were respectively about credi t for enterprises, credit for the purchase of  used vehicles, and credit for household equipment purchases.   The Household Equipment  model is described in further detail in this section. The other two models  present similar issues, both at a functional and a t a technical level. The business objective of the  Household Equipment  model is to make a decision on the credit request within 5 minutes.   8.3.3.  Technical details   The project relies on the following data sources:   - Data on credit applications   o Individual data on th e applicant (and co - applicant when appropriate)   o Information on the product (amount, credit terms, etc.)   - Data on contractual risk   o Data used for computing default states   o Data used for computing behavioural  variables   - External data   o Credit Bureau scores   o Data f rom central banks   Data Scientists met during the workshop insist specifically on the importance of enriching internal data  (which is typically the only kind used in such projects) using external data: the latter will be of various  types (text, time series, etc.) and sometim es collected from open data sources (obtained via web  scraping ). The strength of ML lies not only in using novel algorithms, but also in leveraging such data  sources - often called “alternative” data sources.   Most models implemented by the teams decided to  use a Gradient Tree Boosting algorithm (or variants  thereof) after comparing it to other algorithms commonly used in the organization (in particular, SVMs  were too demanding in computing resources, and neural networks were deemed unsuitable for this  use c ase).   8.3.4.  Validation process   The validation process within the banking group for any credit granting model developed using ML  prior to its deployment in production (whether a new model or a patch on an already -deployed model)  is as follows:  
  52   - Credit teams who de signed the model (usually located in the same country, or centralized  teams in cases where sufficient Data Science resources are not available at local entities) send  the Validation team a dossier comprising a technical documentation along with the entire  source code.   - The Validation team inspects the documentation (conceptual validation) and re -runs the  model generation code (training, test, validation) in order to verify its results and to bring a  critical look on the methods used. This is only possible be cause the Validation team possesses  all necessary skills to evaluate the model according to the principles described in this  document (data management, performance, stability, explainability).   - For certain entities of the banking group, credit granting mode ls are used in Basel models (i.e.  internal risk models in the banking sector): in such cases, the Validation team presents the  model to the group’s Risk Committee in order to get the strategy choice approved (e.g.  constant risk, decreased risk, hybrid stra tegy).   - When appropriate, the dossier – once validated at the group level – is sent to the ECB for  validation of prudential models.   The validation process thus comprises conceptual phases but also applied phases.   8.3.5.  Governance issues   This workshop described a  scenario where an ML component is introduced as a computer aid to a  decision -making process (and not as a fully automated process). Indeed, the component is part of a  multi -step process:   1. Execution of business rules (related to age, filters, over -indebtedn ess) previously defined by  domain experts jointly with the Validation team.   2. Automatic computation of the credit score (which is given a lower weight than business rules  in the overall decision -making process).   3. Possible intervention by a human agent, who ca n override the decision, both in cases of a high  score (credit granted by the system) and in cases of a score below the threshold.   8.3.6.  Evaluation  methods and their implications   Explainability   There are multiple objectives for explanations in this use case:   - Mod el designers need to guarantee the proper behaviour  of the algorithm and to facilitate the  validation process.   - Explanations are also aimed at the teams responsible for continuously monitoring the system.   - Lastly, they will in the future be useful to agent s who need to understand a negative result  produced by the algorithm before making a decision, i.e. either confirming the credit denial or  granting the credit through a manual override.   The SHAP method was retained for the three situations (LIME was also e valuated), for the following  reasons:   - It enables both global explainability (i.e. which type of information weighs on the model’s  decisions) and local explainability (i.e. which values taken by a specific data point impact the  decision positively or negati vely).   - The form of explanation provided by SHAP has been deemed by users to be the most  analogous to the traditional (logistic regression) model.   - The method was easy to implement in each of the three situations.  
  53   A counterfactual explanatory method ( cf. section 11.3.1 ) is however also being considered: it would  likely require a significant amount of UI work, especially if a large amount of information needs to be  presented to users. Besides, the explanation should be as intuitive as possible, which is not  straightforward in cases where the underlying decision tree has been split on criteria which are not  quite logical (e.g. “ age < 23.5 years ”).  Performance   The main methods and metrics retained to evaluate the model performance are the confusion matrix  or F1 score to assess recall and precision, the GINI score to evaluate its discriminating power, and the  Kappa coefficient for comparing the old and new scorin g models.   In particular, a GINI threshold is defined by the guidelines implemented throughout the organization,  both for all credit models (the current status being that this threshold is achievable for most re designed models except on certain population  segments such as younger age groups) and for all  regulatory models (with a higher threshold in that case).   The GINI gain obtained when going from traditional scoring models to the ML model produced by  Gradient Tree Boosting is rather small (a few percenta ge points) in  the case examined during the  workshop, i.e. the household equipment model. Nevertheless it can reach up to 23 percentage points  in som e models developed by the team,  namely those which initially had a low discriminating power.  Furthermore, e ven a seemingly marginal GINI gain generally represents a significant decrease in the  key business metric in this case, namely the expected credit loss.   Stability   The main stability metric retained in this project is based on cross -validation results (name ly some  checks on the standard deviation over the different folds).   Several indicators are also monitored:   - Mutation rate of the population (using the Population Stability Index).   - Evolution of the portfolio profile (credit application rate, acceptance rate , number of defaults  over the previous three months), in accordance with the monitoring practices described in  section  3.3.  - Evolution of business performance metrics .  In case an alert is raised on those indicators, an analysis if performed in order to find probable causes  for the corresponding statistical anomalies, and a remediation plan is produced, which may in some  cases include a model redesign.   Due to lack of hi ndsight on the operation of the new model thus far (which is done in parallel with the  traditional model still used in production), the teams were not able to estimate its stability nor its  appropriate update frequency.   8.3.7.  AI engineering methodology   The credi t granting models developed by the teams are not yet in production. A method to analyse   corporate credit risk, however, has been implemented and deployed: it leverages (mostly open) data  in order to estimate a company’s default risk.  
  54   8.4. Workshop on probability of default   8.4.1.  Purpose of the exercise   A workshop was conducted with the Credit department of a consulting firm, who offers its clients from  the financial sector an ML engineering solution which is applicable to building models to estimate  probabil ities of default. This workshop was quite complementary to the previous one which focused  on credit scoring models insofar as it relates to a generic, fully externalized AI solution. It thus  represents an interesting example of the adoption by a financial actor of an ML product developed by  a third -party.   The solution offered by the consulting firm is not an off -the-shelf product operating as a black box, but  a toolbox which enables to design and build a model while maintain ing a constant interaction betwe en  the solution provider and the customer. In practice, the resulting model is a hybrid one, partly based  on advanced ML algorithms during the design phase but then translated into simple and explainable  algorithms for the deployment phase. This choice app ears to have been motivated by the necessity to  deliver a well -documented model, along with an audit track.   The solution as currently available is designed to support credit scoring and probability of default  models, however the solution provider is workin g on applying a similar approach to internal risk  models, namely leveraging ML to yield corrections and improvements to currently used models in the  form of business rules.   8.4.2.  Objectives of the algorithm   The main objectives of the project were the following:    Increasing the performance of the models used for decision -making. In particular, an improved  risk discrimination through the identification of non -linear effects between risk factors, an  improved classification of individuals, and a faster identification of changes in the underlying  risk portfolio.    Improving data quality through the use of quality assessment and improvement techniques.    Refining the estimation of regulatory capital requirements through the use of more accurate  models.    Increasing the transp arency and auditability of the models.   Data availability is an essential issue in this case, since the volume of data that can be exploited varies  greatly with the use case: few data points for consumer credit, far more for housing credit.     8.4.3.  Technical details   The main stages of the nominal behaviour  of the solution are commonly encountered when adopting  advanced ML models, with the exception of the last one which makes the approach original. These  stages are:   1. Data quality control and data preparation  prior to modelling   2. Construction of a reference model (of a “traditional” type), in practice a logistic regression.   3. Construction of a challenger model (of an “advanced” type): more sophisticated, supervised  ML algorithms are used, typically random forests or neural networks . 
  55   4. Identification of the margin of improvement of the reference model: in the use case  considered, 80% of the prediction error can be attributed to 20% of the population, thus the  goal is to identify population segments which are incorrectly classified by the reference model.   5. Visual explanation of the decisions made by the ML model: the methods used are classical  ones (SHAP, LIME).   6. Extraction of simple, auditable business rules which explain the performance gap between the  ML m odel and the ref erence model : to this aim, population segments which are incorrectly  classified by the reference model are automatically identified, then business rules are  extracted by a domain expert (typically from risk management) so as to reduce as much as  possible t hat performance gap.   7. Definition of the final hybrid model, as a combination of the reference model and business  rules.   The solution is offered as “manage d services”: besides the hybrid -model -building workflow described  above, an information sharing platfor m enables the customer to review the entire design process  independently from the execution of that workflow.   To some extent, the model building approach adopted here relies on challenger models mentioned in  this document as a possible audit method ( cf. section 5.5): several hundreds of model exemplars are  compared against one another, then the best one is retained, following which the system will minimize  the performance gap between the reference model and that “top challenger”. In a nutshell, the  strategy is to try to replicate the performance of the best challenger models while remaining inside a  more controlled operational framework – which is guaranteed by combining an intrinsically  explainable model  (logistic regression) with a limited number of business rules.   The creators of the ML platform examined insist that the choice of avoiding “pure” ML models was  made early on in the project, firstly because ML is notoriously difficult to implement in this  type of  scenario, secondly because such a model would hide behaviour  inherent to the population considered,  such as the transition of individuals across population segments over time – which is essentially  observed in any credit model.   8.4.4.  Validation process   Initial functional validation relies on the documentation of the algorithm and on a presentation of its  results. It is performed by the solution provider in support of the customer, in an iterative mode which  is more specifically focused on aforementioned stages 4 to 7 (i.e. from the identification of margins of  improvement to the definition of the resulting hybrid model).   As for continuous functional validation, it is similar to back -testing  which is usually performed for credit  models, except that freque nt monitoring of population segments impacted by business rules is  required, the goal being to anticipate the detection of model biases. Interestingly, back -testing  results  are presented to the Risk Committee in order to assess the relevance of a model adj ustment.   8.4.5.  Governance issues   The solution design, which ultimately consists of tuning a reference model via business rules (stages 6  and 7), aims to make it compatible with governance frameworks common to most traditional models.  In particular, the hybrid mo del designed by the consulting firm can be assimilated to the classical  behaviour  of credit granting models, which follows an analogous business process: a regression model 
  56   – comparable to IRB (Internal Ratings -Based Approach) models – is first executed, t hen an override  (similar to the “notching” practiced by credit rating agencies) can be applied by a human agent if a  weakness has been identified in the model’s output. Besides its compatibility with a tried -and-tested  governance framework, the benefit exp ected from the approach is a hi gh model explainability (cf. next  section) . The choice of a hybrid model has also been made for various operational reasons: easier  implementation, stability and robustness.   Another governance issue raised by this use case is  however relatively common, namely the  outsourcing of the model design and implementation, and also of its maintenance .  8.4.6.  Evaluation  methods and their implications   Explainability   By choosing a hybrid model based on decision rules, the solution provider has p ut the emphasis22 on  the generation of convincing explanations – local and global – intended both for users and for  governance bodies.   For instance, an essential explainability criterion is that the aforementioned overrides of the model  decisions must be motivated. In particular, a user of the model (typically an account manager) must  understand why the model produced a given score. Besides, as explained in section  5.2.3 , the  intervention of a human agent introduces a risk of “explanatory bias” with respect to the more  objective result provided by the model. In the hybrid model, business rules have been pre -selected by  the algorithm: in essence, the model is first opti mized as a logistic regression, then the addition of  business rules aims to optimize the resulting hybrid model – in both cases in terms of global  performance.   As for local explainability, the use of SHAP enables to provide the reasons for a particular sco re given  by the logistic regression model. An explanation of the decision made by the overall hybrid model in  turn consists of augmenting those SHAP values by the motives of any overrides made by the business  rules. Those motives are quite simply the membe rship of the individual considered in one or more  population segments on which the model’s predictive performance had been optimized by the  algorithm.   Performance   The following metrics, which combine predictive performance and business efficacy ( cf. section  3.2),  are used to assess the relevance of the overall model construction workflow:   - As predictive performance metric, the GINI score gain is used (typically on  the order of 5% in  the cases studied).   - Two business performance indicators are computed: the gain in terms of returns measured  while keeping the risk appetite constant (around 50%) and the reduction in expected loss  (which is a standard computation in int ernal risk models).                                                               22 This concern is also evident in certain technical c hoices: for example a genetic algorithm was picked for hyper parameter tuning rather than e.g. a Bayesian optimization method, because it was deemed easier to explain even  to laypeople while offering comparable performance.  
  57   Furthermore, the replicability of the model has been studied: initial runs experienced a problematic  lack of reproducibility, which was later solved.   Stability   Firstly, this workshop illustrated the observation made in section  3.3.1 , namely that the temporal drift  of a predictive model may in most cases be due to a significant change in input data, without even  considering the impact of the ML algo rithm. Thus in the case of credit models, structural modifications  of the population considered may introduce model biases. Nevertheless the evolution of the client  database of a banking group, for instance, is rarely taken into account by IRB models. This  is why the  consulting firm which participated to the workshop advocates for the adoption of a portfolio  monitoring solution by banking institutions, wherein customer portfolios as well as credit and asset  portfolios are regularly analysed  to detect such s tructural changes.   Regarding the stability of the predictive model, a sub -project has been undertaken by the solution  provider in order to provide KPIs as the basis of a monitoring and back -testing  protocol of the hybrid  models, in order to identify deviations of the model itself.   The stability of the hybrid model in fact only differs from that of the logistic regression model by the  choice of the business rules embedded in it. That choice is made b y the customer in interaction with  the consulting firm, as both discuss the technical implications  together . The customer may also choose  during a model review to suppress a rule, for example to be more aligned with its risk appetite, or due  to data qualit y problems identified on a variable involved in that rule.   Furthermore, the analysis of the model stability has shown that introducing business rules does not  make the model less robust, provided those rules are guaranteed to apply only to the population  segments identified. They also enable a specific monitoring of those population segments.   Lastly, initial studies suggest that a periodicity of 6 months for model updates would be adequate, both  for credit scoring and for probability of default models.   Appr opriate data management   In this kind of outsourced model -building solution, the validation of the resulting models, as well as  that of the adequacy of data management, is ultimately the responsibility of the customer’s  compliance and risk departments.   Ther e is thus no delegation of responsibility, nevertheless the regulatory requirements imposed to the  end customer – particularly when it comes to explaining model predictions – are reported onto the  solution provided by the third -party. Interestingly, the wo rkshop participant has indicated that a  project had been undertaken to set up an Ethics Committee involving large banking institutions among  its customers, with the ultimate goal of producing an MRM ( Model Risk Management) framework.   8.4.7.  AI engineering methodo logy  The choice of an iterative model building workflow rather than a fully automated, single -step process  is deliberate. Indeed, the solution designed by the workshop participant involves human intervention  in the hybrid model optimization phase: this app roach makes end -to-end automation of the build  process impossible (while providing, according to the solution creators, benefits in terms of  explainability and stability of the resulting model – see previous section).  
  58   Besides this lack of end -to-end automa tion, the engineering methodology relies on two foundations:   - On the one hand, the hybrid model building workflow follows a systematic approach and is  developed according to industry standards.   - On the other hand, the tooling handed over to the customer will  take the form of an  information (model, data, and results ) sharing platform, which enables the customer to be in  the loop of all the decisions made and all the results obtained during model construction. The  objective of this platform, still under constru ction at the time of this writing, is to provide an  automated audit track of all exchanges with the customer.   The goal of this architectural choice is to make each stage of the model building traceable even after  the model has been deployed in production,  whether that stage has been automated or is performed  by a human agent.   As for the risks induced by outsourcing ( cf. section  5.4.2 ), they call for the following comm ents:   - The aforementioned model building method enables both reproducibility and auditability of  the models produced.   - The quality of service is the customer’s responsibility, as the customer ultimately decides to  deploy the models and is in charge of their operational maintenance.   - Continuity of service and reversibility do not raise major difficulties either since the customer  is able to revert to the regression model at any time, furthermore the evolution of business  rules can be monitored independently fro m the model construction having been outsourced.   - Lastly, the risk of dependency towards the solution provider remains, specifically in its most  fundamental aspect of technical knowledge: in this kind of situation, the end customer is  responsible for develo ping and maintaining its expertise and know -how in order to control  that risk.   8.5. Topic 3: Customer  protection   This workshop was conducted with an insurance institution around a project pertaining to sales  proposals: that project aims to produce prefilled quotes for home insurance.   8.5.1.  Regulatory context   As mentioned in section  3.1.1 , the  duty to advise as defined by the IDD (Insurance Distribution  Directive) imposes to se ll an insurance product in accordance with the client’s best interests.  Therefore, the goal of technological innovation in that domain should be to make an offer consistent  with the customer’s needs and requirements – not to contribute to the creation of a  demand.    8.5.2.  Purpose of the exercise   The main challenge of this workshop was to shed light – by focusing on a specific use case – on the  regulatory issues raised by the use of AI in the distribution of insurance products.   8.5.3.  Objectives of the algorithm   For a cus tomer who already subscribed to a contract, for example for an automobile insurance, the  system implemented attempts to prefill a home insurance quote, including a “starting at” price.  
  59   8.5.4.  Technical details   The specificity of this use case is its reliance on g eographical data directly linked to real -estate  sociology:   - Gridded data provided by INSEE (the French National Institute of Statistics and Economic  Studies), including information such as the ratio of houses vs. apartments, the rate of home  ownership, the average surface, the average household income (at the neighbourhood  and  commune levels).   - Data on buildings, acquired from a data provider, which gives the building surface and  perimeter, from which a building shape is determined, and then a probability of house vs.  apartment is estimated.   - The average number of rooms at the commune level.   - A postal address field, on which text analysis is performed in order to extract discriminating  features for the house/apartment prediction.   - An email field, used for the sam e prediction.     The quote is prefilled with the following target variables, which are predicted iteratively (i.e. the 2nd  one is predicted using the predicted value for the 1st variable, the 3rd using the first 2 predictions,  and so on):   1. Home type: house or  apartment   2. Customer status: owner or tenant   3. Number of rooms   4. Optional insurance of valuables   5. Year of construction     8.5.5.  Validation process   Validation involved mainly the Compliance department, who performs consistency checks between  the needs expressed by the c ustomer on the one hand, and the risks declared in prefilled (then possibly  amended) quote on the other hand.   8.5.6.  Governance issues   The prefilled quote produced by the algorithm examined is leveraged by the insurance institution in  several use cases:   - Sending v ia email a hyperlink to the quote.   - Processing incoming calls in order to perform portfolio cross -selling.   - Supporting outgoing telephone marketing campaigns.   The main governance issue is the respect of compliance requirements related to insurance product  distribution, notably the duty to advise, which imposes that the motives for offering a particular  product be exposed to the prospective customer, as well as the consistency between the product’s  characteristics and the customer’s needs and requirements.  
  60   Particular attention should be focused on the human -machine interactions so that the subscription  process based on prefilled information does not discourage the customer to express his or her needs23,  nor to verify the accuracy of the declared risks24. At the s ame time, regulation requires that any  amendments (checking or unchecking of an option, or changes indicated by the customer) to the  prefilled quote be faithfully reflected, as appropriate, in all other documents formalizing the gathering  of customer needs  and requirements and the insurance product offer.   These governance issues are illustrated by certain measures adopted during the system design and  development. In order to ensure proper information of the customer, upon opening the prefilled quote  a popup  window explicitly enjoins the prospective customer to verify the information and to correct it  if need be. The “insurance of valuables” option is particularly telling: it was initially checked in all  quotes produced by the system, but it turned out that 6 0% of customers unchecked the box. It was  therefore decided, again for the purpose of providing a quote as closely adjusted to the customer’s  intentions, to check or uncheck the box based on the model prediction on the “valuables” variable.   8.5.7.  Evaluation meth ods and their implications   Explainability   Explaining an individual prediction to the customer does not represent, according to the insurance  institution, a major issue in the case of predictive models intended for marketing: in the present case,  rather tha n an explanation, an explicit validation request should be provided to the customer.   On the contrary, it seems important to provide an explanation for the model predictions – and more  specifically its prediction errors – to the teams tasked with monitoring  the system and with ensuring  its compliance25. Indeed, a prediction error has a strong impact of the subscription process – a process  which must be correctly understood by the customer: indeed, a failure to advice (or even liability) can  be invoked when an  erroneous prediction is not corrected by the insured and thus becomes a false  declaration (albeit unintentional). Besides, prediction errors to the benefit of the insured generate – if  they accumulate – an additional risk, this one of a financial and oper ational nature.   Performance   The predictive performance of the model is trivial to assess: it is measured as the classification accuracy  according to each of the aforementioned target variables. By retaining the first three variables (with  an error margin of one unit on the number of rooms), the model produces 90% of correct predictions.   Stability   This use case does not present any stability challenge, since input data are relatively static and the  predictive power only has a minor business impact.                                                               23 In particular, an algorithm de emed efficient by the users may be endowed by them of a  “prescriptive”  power  even though it has not been designed to that aim.   24 At stake here are future po tential disputes in case of a “false statement”  whose origin would lie in the quote  prefilling.   25 However, the possibility to provide explanations to algorithmic decisions for purposes of internal control or  external audit has not been explored during this workshop.  
  61   Appropriate data management   In terms of data management, the domain of insurance pricing defines forbidden variables. The  absence of those variables from the resulting models should therefore be guaranteed, as well as the  practical infeasibility of their i nference from other predictor variable s used by the model.   8.5.8.  AI engineering methodology   The predictive models described here are deployed in production. Although they run in a production  environment, this use of AI is not for an automated decision -making pro cess: predictions are not  provided continuously, instead a manual collection stage of the algorithm’s output is necessary.   Thus, once the predictive model has been validated both functionally and technically, it is executed on  a periodic basis and its resu lts are used in the three situations previously described (email campaigns,  incoming calls, and outgoing telephone campaigns).  
  62   9. Explainability vs. interpretability   The distinction between these two concepts is a frequent topic within the scientific literatu re, however  there is no consensus on it.   Definition without a distinction   Burrell (2016) insists on the issue of the interpretability of algorithmic results, but without defining the  terms in question.  Doshi -Velez and Been Kim (2018) fail to distinguish th e two terms as they define  them in relation to each other. Nevertheless, their article strives to justify the necessity of categorizing  various forms of interpretability. Similarly, Biran and Cotton n (2017)  use a circular reasoning around  both concepts: “ Explanation is closely related to the concept of interpretability: systems are  interpretable if their operations can be understood by a human  […]”.  While pointing out the lack of any formal definition, Bogroff and Guéguan (2016) define interpretability  as the ability to explain or present stages using humanly understandable terms. For his part, Tim Miller  (2018)  offers a comprehensive analysis of both concepts. The introduction of the notion of degree  allows to define interpretabi lity as “ the degree to which an observer can understand the cause of a  decision. ” Unfortunately, explainability is not defined according to the same notion, but as a way to  obtain a human agent’s  understanding. Miller emphasiz es the necessity for the reade r to observe  similarities and differences between the two concepts… but only after stating five lines prior that they  would be used interchangeably.   Definition through distinction   Molnar (2019) lifts Miller’s definition of interpretability so as to attempt  distinguishing the two terms.  He defines explainability as explanations of predictions being provided to individuals, and introduces  the question of a “good explanation” in his book.   Bryce Goodman and Seth Flaxman, in European Union regulations on algorit hmic decision -making  and  a ‘’right to explanation’’ (2017), implicitly distinguish the two concepts in their reading of GDPR’s  articles 13 to 15. They mention in particular that an algorithm operates by correlation and association,  so that it performs pred ictions without providing any explanatory element of those correlations and  associations. The difficulty that arises is thus that interpretation becomes difficult insofar as the  algorithm works without having to explain its inner workings. The authors iden tify a tension between  the right to access personal information collected (articles 13 -15) and the right to collect data ( article  22). Giving article 22 a disproportionate weight would lead to the development of a “black -box” society   (Pasquale, 2015).   In his intervention at the Institut de Recherche en Informatique de Toulouse  (2018 ), Laurent Serrurier  links explainability to the technical characteristics of the algorithm, whereas interpretability is related  to an ethical dimension. Explainability is thus a  technical feature of the algorithm’s complex nature,  and interpretability refers to its social acceptability.   Likewise, in a talk given at the A CPR in 2019 , Louis Abraham tackles Biran et Cottonn ’s definition which  mixes both concepts (“Explanation is clo sely related to the concept of interpretability: systems are  interpretable if their operati ons can be understood by a huma n, either through introspection of through 
  63   a produced explanation.” ), relates interpretability to the question “why” and explainabilit y to the  question “how”.   Aurélien Garivier ’s 2018 article  “Toward a responsible artificial intelligence ” offers  an explicit  distinction when defining the two terms. As per the article “A Berkeley Vie w of Systems Challenges for  AI“, a decision rule is said to be interpretable if one can understand how it associates a response to  observations; it is said to be explainable if one can understand on which elements the decision is  grounded, possibly by using counterfactual reasoning.   Sub-distinction within interp retability   Lipton’s 2017 article gives the most satisfying meaning to the concepts of interpretability and  explainability. Rejecting the understanding of interpretability as a monolithic concept, Lipton  introduces a continuum based on a number of logical c riteria: trust in the algorithm’s results, causality,  transferability of knowledge, information contained in the decision, fairness of the decision. This  framework enables to propose a concrete representation of the continuum between intelligibility and  explanation.  
  64   10. Technical aspects of explainability   10.1.  Trade -offs  This appendix describes the technical choices which arise after the appropriate level of explainability  has been selected upon the introduction of AI in a business process. This description has a generic  scope since the elements considered are not restricted to the financial sector. Two trade -offs are  presented: between simplicity and efficacy of the ML algorithm on the one hand, between sobriety  and fidelity of the chosen explanatory method on the other hand.   10.1.1.  Simplicity/ efficacy trade -off  A given type of ML algorithm may be more or less complex, in th e sense of lending itself to an  inspection of its inner workings. They can also vary in efficacy, measured as previously indicated using  predictive performance or business performance metrics.   The following diagram attempts to illustrate the simplicity/ef ficacy trade -off among the most common  ML algorithms:     Among the numerous simpli fications and approximations operated by this diagram, the following  points should be underlined.   Simplicity and efficacy metrics   On the one hand, ordering ML algorithm types in terms of their simplicity is highly subjective. Indeed,  the size and structure of a model have a more significant impact on its explainability than the model  type does, because understanding only part of the model is useless: thus a random forest compri sing  thousands of trees will typically be much more difficult to understand than a single -layer neural  network composed of a dozen neurons.   
  65   On the other hand, the deterministic or stochastic nature of an algorithm is another essential criterion  to take int o account when assessing its efficacy. For instance, the results of a fundamentally stochastic  algorithm depend on random sampling - not only for building training and evaluation datasets, but also  within its procedure itself (for example, bootstrap method s contain re -sampling stages).   Lastly, it should be noted that the efficacy of a given algorithm type cannot be evaluated on a single dimension scale either, since it depends on the use case considered (nature and volume of the data,  choice of parameters, etc.)   Non-comprehensive t axonom y  It should also be emphasized that the representation of ML algorithms in the previous diagram does  not pretend to be comprehensive. In particular, categories such as Reinforcement Learning have been  excluded upfront because  they are – to the best of our current knowledge – absent from solutions  deployed as of today on the market.   On the other hand, unsupervised learning algorithms cannot be ignored. For example, graph analysis  based on factoring company features allows to mo del the interdependency network generated by  SMBs partaking in a P2P lending platform. The factoring technique used may be e.g. SVD ( Singular  Value Decomposition ) or Latent Factor Model , and in any case that type of modelling  demonstrated  not only its desc riptive value, but also its value as a predictor of credit default risk on such platforms  (Ahelegbey, 2019 ). Furthermore, credit default risk is not easily amenable to traditional, non -ML-based  model.   Decoupling between design and modelling   Lastly, the des ign of an algorithm can generally be decoupled from the structure of the resulting mode l:  this is the strength and the innovation brought by hybrid models such as the one described in the  workshop on probability of default (section 8.4).   This approach consists of building a simple, intuitive model through iterative optimization by  comparing it with a more efficient, often more complex model. The resulting model combin es the best  of both worlds, namely the performance (e.g. in terms of recall and precision) of a complex algorithm  and the explainability (in terms of its interpretability and limited size) of the final predictive model.    10.1.2.  Sobriety/fidelity trade -off  The exp lainability requirement induced by the introduction of ML into a business process is not limited  to a simplicity/efficacy trade -off pertaining to the algorithm: the explanation should itself be intelligible  and convincing to its intended recipients, suitab le for the use case considered, and proportionate to  the risk associated to the business process.   A trade -off is at play here as well. On the one hand, the explanation’s fidelity (with respect to the  algorithm which produced a given prediction) is imperfec t since the algorithm’s behaviour  is  necessarily simplified when its output is explained in terms of certain characteristics of the individual  or transaction considered. On the other hand, the sobriety of the explanation, that is its intuitiveness  and inte lligibility by a layperson, is both subjective and constrained in practice.   The following diagram attempts to represent the sobriety/fidelity trade -off for an explanation  according to the type of ML algorithm and the type of explanatory method. A few “corridors” are 
  66   drawn to show that for a given algorithm type, some explanatory methods will deviate slightly from  the general trends.     10.2.  Reaching a  high  explanation level   10.2.1.  Feasibility of replication   It should be noted that explanation level 4 (replication) aims to identically reproduce the model’s  behaviour , and not to understand its inner workings in their full est detail – which may prove impossible  for certa in models, typically deep neural networks.   Interestingly, some of the financial actors met during the exploratory works led by the ACPR   implemented a replication method as early as the design and initial validation phases of their  algorithms, hence upstream from the internal control or audit procedures. More specifically, they  opted for implementing their ML algorithms in multiple (in som e cases three) languages, which is a  software engineering technique classically used for particularly critical components of a system.   10.2.2.  The problem of software dependencies   Besides, a problem arises whenever a code review is in order (i.e. for levels 3 and 4). This problem is  not specific to AI algorithms and comes up in virtually any well -conceived software audit mission:  multiple external software libraries, tools and components are invoked by the code analysed , and their  review ranges from difficult (in t he case of open source software) to impossible (in the case of closed source code). Even a simple logistic or linear regression algorithm uses several third -party libraries,  and the problem is amplified for sophisticated algorithms, which incidentally also  require a higher  explanation level.   In conclusion, reaching a level -3 or level -4 explanation is challenging in most situations,  and the  challenge is made more difficult under certain circumstances: when the algorithm relies on third -party  
  67   libraries or pro ducts, and when the audit mission needs to cover the entire model building workflow  and not just the resulting model.   An approach sometimes mentioned to facilitate this kind of in -depth analysis consists of setting up a  certification process of off-the-shelf ML components, similarly e.g. to how software security  components must be tried -and-tested and officially approved prior to being embedded into critical  applications. At any rate, the detailed code analysis suggested for level -4 explanations (replication)  should also focus on the use of such off -the-shelf libraries, with a crucial point being the hyper parameter optimization stage insofar as it significantly impacts the algorithm’s accuracy.  
  68   11. Review of explanatory methods in AI   This review does  not pretend to be comprehensive: it is limited to the use of AI in the financial sector,  besides it aims to paint a picture of the use cases which are deployed in practice – whether currently  in production or simply at an experimental stage.   The frame of reference here is an ML algorithm. Explanatory algorithms can be classically grouped into  three categories according to where they come up within the model’s lifecycle:   1. Pre-modelling  explanatory methods aim to describe the data used when building the model s.  2. Explainable modelling  contributes to the production of intrinsically more explainable models.   3. Lastly, post -modelling  explanatory methods attempt to yield satisfactory explanations from  previously built and trained models .  11.1.  Pre-modelling  explanatory metho ds  Upstream from the learning stage of an ML model, a somewhat limited form of explainability can be  provided, whose goal is to illustrate the data used by the algorithm. The most common methods used  for this purpose are the following.   Exploratory data analysis   Exploratory data analysis often relies on data visualization, which enables to reveal characteristics of  the data – even when they are potentially hidden from descriptive statistics.   These methods are both model - and domain -agnostic. Within the fi nancial sector, they are particularly  useful for detecting and mitigating undesired biases (cf. section 3.1.2 ). Potential sources of such  problematic biases are numer ous (Kamishima, 2012) : direct, indirect or latent dependency on sensitive  variables, sampling biases (the most difficult case to detec t) or labelling  biases in the training data, or  imperfect convergence of the training stage.   Dataset d ocumentat ion  Severa l dataset documentation standards have been suggested, either applicable to AI models   (Mitchell, 2019) or to associated services (Hind, 2018).   This kind of approach, based on a thorough, concise and formal documentation of the datasets and  services, is suitable for level -1 explanations as described in this document ( cf. section 3.4.4 ).  However, the technical focus of the standards proposed thus far makes them ill -suited for the  customers and end users of AI algorithms: instead, they are intended for the creators of those tools,  or even for the individuals in charge of monitoring their operational behaviour . This standardization  effort is nevertheless recent and like ly to evolve in a near future.   Dataset summarization methods   In order to facilitate the mental representation and the interpretation of datasets, particularly the  voluminous and heterogeneous ones, certain dataset summarization methods may be used, ideally  as  a complement to the aforementioned exploratory analysis and documentation methods.   Examples of dataset summarization methods are :  - For textual data, automatic document summarization and classification.  
  69   - For images, visual scene synthesis.   - For any data type, the extraction of representative (or typical) exemplars from a dataset, and  of particularly atypical exemplars as well ( respectively  called prototypes and criticisms:  Kim,  2016) .  Explainable feature engineering   This last type of pre -modelling  explana tory method stems from the observation that an explanation  for a predictive model is only as good as the predictive features it relies on. Therefore, particular care  must be taken to the feature engineering stage when designing an ML system, i.e. to the co nstruction  of predictor variable s from original variables in order to adequately re -structure the training data for  the algorithm.   Two such methods should be mentioned  (Murdoch, 2019 ):  - The intervention of domain experts, who are sufficiently knowledgeable about the source data  to extract variables (combination of other variables, intermediate computation results, etc.)  which increase a model’s predictive accuracy while maintaining the interpretability of its  results. In other words, human expertise enables in certain cases to sidestep the usually  inevitable trade -off between efficacy and explainability of an ML model ( cf. section 10.1.1 ).  - A modelling -based, automated approach: usual data analysis techniques are then used, such  as dimensionality reduction and clustering, so as to extract predictor variable s as compact and  repr esentative as possible.   11.2.  Explainable modelling   Some methods enable simultaneously training the predictive model and building an associated  explanatory model. This category of explanatory method is referred to as explainable modelling .  Such methods are howev er far less frequently implemented than pre - and even more post -modelling     explanatory approaches, for several reasons:    - Explainable modelling  requires access to the source code which produces the predictive  model, and the possibility to modify the algorithm. On the contrary, access to the model itself  is sufficient for post -modelling  explanatory methods, which makes them much more widely  applic able.   - Explainable modelling  is useful when explanations are necessary as early as the design phase  of the ML algorithm, which demands a more mature engineering methodology and adequate  planning during the introduction of AI into a business process.   - Lastly , explainable modelling  is not very suitable for audit, all the more so when the predictive  model is only available as a black box, without a documentation of the algorithm itself.   The primary, highly ambitious goal of explainable modelling  is to avoid as m uch as possible the already  mentioned trade -off between efficacy and explainability, as they strive to provide additional  explainability without necessarily sacrificing predictive accuracy.   A few methods for explainable modelling  are described in what foll ows.  Intrinsically explainable models   An intrinsically explainable model can be chosen from the outset, for example linear models or  decision -tree-based models. This is the most trivial kind of explainable modelling  approach, assuming  that the simplicity/e fficacy trade -off is kept in mind, and that the specific model produced by the 
  70   algorithm is actually explainable. The latter point is not always guaranteed: in some cases, adopting an  explainable family of models is not sufficient since it may lead to a mo del with too many dimensions  to remain intelligible.   Hybrid explainable models   Hybrid explainable models are only applicable to a specific model type, namely neural networks. The  following types of models belong to this category:   - Deep k -NN (Papernot  and McDaniel , 2018) extracts the internal representation of a neural  network within each of its layers in order to illustrate how the final result is obtained (in the  last layer). A variant of this approach is the Deep Weighted Averaging Classifier .  - SENN (Self -Explaining Neural Networks:  Alvarez -Melis , 2018) uses neural networks to  simultaneously train the predictor variable s, the weights, and the aggregation method of a  linear model. A variant is the  Contextual Explanation Network ( Al-Shedivat, 2018 ).  Joint pr ediction and explanation   This approach consists of training the model to produce both a prediction and an explanation for this  prediction. It has recently received considerable attention,  despite two major limitations. Firstly, not  only does the ML algorit hm need to be modified, but explanations must be provided for the entire  training dataset, which is often unrealistic. Secondly, the explanations produced are only as accurate  and relevant as the information provided by human agents for training the hybrid  model, they do not  necessarily justify the genuine, internal workings of the predictive model.   The following methods fall under this joint prediction/explanation approach :  - TED (Teaching Explanations for Decisions : Hind, 2019 ) associated to each training data point  the motive behind the resulting prediction. A variant is the generation of multimod al  explanations (Park, 2018).   - Data -type -specific methods: these include Visual Explanations  (Hendricks, 2016) for object  recognition in images, or the generation of concise explanations in natural language (e.g.  English) for a predictive model using textual source data  (Lei,  2016 ).  Architectural adjustment methods   Methods relying on architectural adjustments are mostly specific to Deep Learning (which is as of toda y  relatively infrequent in the financial sector).   A few of them are nonetheless worth mentioning, such as Attention -Based Models  which aim to  identify the most important feature groups within input data, be they images, textual data, or – more  relevantly in the financial sector – time series. Some studies (Jain, 2019) however illustrate the limits  of this approach in terms of perfo rmance of the resulting model.   Regularization methods   Regularization methods are typically used to enhance the performance of an ML model, however some  kinds of regularization enable to improve model explainability.   For example, the decision boundary of a model may be constrained during training to be approachable   by a decision tree, which makes future predictions easily comprehensible by a human (Wu, 2017).   Another example are methods which orient model training to assign more weight to the predictor  variable s labelled  as most important by a domain expert  (Ross, 2017) . 
  71   Training/ modelling  decoupling   Of particular note are specialized approaches which decouple the training stage of an ML algorithm  from the structure of the resulting model.   An example of such approach is the hybrid method described in the workshop on probability of default  (section 8.4). An advanced model, with low explainability by natu re, is trained to achieve high  predictive accuracy, after which domain experts extract a number of business rules to augment an  intrinsically explainable model (e.g. a decision tree)  with a number of “overrides” . The resulting system  thus benefits both fro m the accuracy of a complex algorithm and from the explainability of a simple  predictive model.   11.3.  Post -modelling  explanatory methods   Methods operating on previously -trained ML models are de facto  the most commonly intended  meaning for explanatory methods in general. Their goal is to provide post -hoc explanation which justify  or illustrate a given result (or a set of results) produced by an ML model. The model is thus considered  as the object studied, on which changes can not be made (contrary to explainable modelling   approaches from section 11.2 ) and whose data can not be manipulated (contrary to pre -modelling   approaches from section 11.1 ).  Two main criteria are used to distinguish post -modelling  explanatory methods. Firstly, their local or  global scope:   - Local explanatory methods provide an explanation for a decision made on a particular input  data point (for instance, why a given credit application was granted to the applicant).   - Global explanatory methods attempt to simultaneously explain the entirety of possible  decisions (in this case, what are the general characteristics of the respective outcomes –  acceptance or denial – of credit applications).   The second criterion is whether a method is applicable to any type of ML model (model -agnostic  methods) or only to specific type of model or algorithm (model -specific methods).   11.3.1.  Local  explanatory methods   Black -box methods   Black -box methods, also called model -agnostic, are applicable to any type of model. They may consist  of a simple classifier (for example a Bayesian classifier trained on Parzen windows), or be more  sophisticated (a number of them operate by perturbing the model then observing the influence of  predictor variables).   The following techniques are among the most common model -agnostic explanatory methods:   - Naive Bayes Models, which are often crude in comparison to the next ones.   - LIME (Locally Interpretable Model -Agnostic Explanations ) works by constructing an  intermediate representation domain between the ML model and the “real -world” model so as  to find the optimal trade -off between fidelity of the model explanations and si mplicity of the  explanations (whose purpose is to be intelligible by domain experts who are not necessarily  technically savvy).  
  72   - SHAP combines game theory (Shapley values) with the optimization of credit allocation in  order to explain the influence of each predictor variable on the predicted values, also in a  model -agnostic manner (Lundberg, 2017) .  - Variants of the SHAP method, for example adapted to data structured as a network  (Chen,  2019) .  - Causal interpretation methods, which compute the marginal influence  of each predictor  variable and the joint influence of variable pairs (Datta, 2016) .  - SLIM (Supersparse Linear Integer Models) which selects decision rules so as to optimize the  accuracy of a binary classifier under constraints on the number of variables an d their relative  weights.   It should be noted that even the most commonly used local explanatory methods, such as LIME and  SHAP which are both based on model perturbations, encounter practical limitations in terms of  security  (Dylan , 2020). In particular, t hey are vulnerable to adversarial attacks ( cf. appendix 12) which  can produce models including discriminatory biases on which the explanations generated are  reassuring or even indistinguishable from the explanations produced on an unbiased model.   Some  black -box explanatory methods are also specific to models operating on NLP, and generally  provide either numeric explanations or explanations in the form of a textual example:   - An adaptation of the LIME method to NLP (Ribeiro, 2016) provides explanations a s the degree  of importance of each predictor variable.   - A generative method (Liu, 2018)  provides explanations as a simple textual example .  Model -specific methods    A number of local explanatory methods are specific to a type of ML model.    It should first be noted that some models are directly interpretable:   - Logistic regressions.   - Linear regressions and variants such as GLM (Generalized Linear Models) , provided their  density is limited.   - Additive models such as GAM (Generalized Additive Models) .  - Decision trees and random forests, at least when they are limited in depth and volume.   A number of explanatory methods are specific to Deep Learning models:   - Explanations in the form of surrogate models, particularly decision trees which approximate  the neural network  (Craven, 1995) .  - Explanations based on attention mechanisms (Choi , 2016) .  - Explanations which attribute decisions of the neural network to certain predictor variables  (Shrikumar, 2017) .  Lastly, the following  methods are domain -specific:   - Explanations for NLP algorithms based on Recurrent Neural Networks (Strobelt, 2018) .  - Explanations for CV algorithms, for example Interpretable Units  (Bau , 2017), Uncertainty Maps  (Kendall , 2017) , or S aliency Maps  (Adebayo , 2018) . 
  73   Counterfactual explanation s  Counterfactual explanations have their own place among methods aiming to explain an ML algorithm ,  insofar as they  are the only ones involving causal relations26 (and not just explanations grounded in  statistics or inferences generalized from large data vo lumes).   More precisely, a counterfactual explanation to prediction  Y, generated by a model from input data  X,  is given by input data  X’ as close as possible to X which would have resulted in prediction  Y’ different  from Y. In general , Y is an unfavourable  outcome (prediction or decision), for example a low credit  score computed from X resulting in the credit application being denied. A relevant explanation (to the  creator of the system, to an auditor, but above all to an individual impacted by the outcome,  in this  case the applicant who requested the credit) should then answer the question: what change as  minimal as possible in the credit application would have led to its acceptance? Thus, rather than a local  explanation which quantifies the influence of va rious predictor variables (age, income, credit history,  etc.) on the negative outcome, a far more useful, practical and simple explanation is obtained, for  example “ if the household income had been this much instead of that much, the credit would have been   granted. ”   Certain  methods for generating counterfactual explanations even goes beyond this definition   (McGrath, 2018 ):  - Some methods produce positive counterfactual explanations, i.e. which apply in cases where  the original decision  Y is favourable  to the individual considered. Using the previous example,  Y’ correspond s to the credit application being denied, thus the counterfactual explanation  indicates a safety margin for the favourable  outcome. This kind of explanation may be useful  to make an inf ormed decision e.g. to request another credit in the future given that the initial  application has been accepted.   - Another enhancement is achieved by weighing explanatory factors based on their variability.  Using yet again the credit scoring example, if the  individual has proven to be better able to  reduce their personal expenditures than to increase their revenue, then this enhancement  method would produce an explanation such as “if monthly expenses had been cut by half, then  the credit would have been gran ted.”  This specific explanation is indeed more useful than one  involving other features such as the household income, in that it is more directly actionable.   Ideally, counterfactual explanatory methods should be applicable to algorithms studies as black bo xes.  Certain methods do in fact satisfy this condition under well -determined situations  (Wachter, 2018).   11.3.2.  Global explanatory methods   Global explanatory methods provide an explanation to the entirety of decisions made by an ML model:  for example, what is the  contribution of the “age” variable to the decisions to accept or deny credit  applications  over the set of all applications.   Global explanatory methods may be useful to internal control teams or to an auditor in order to obtain  an understanding of the gene ral behaviour  of an algorithm, however they usually show their                                                              26 This ability to tackle causality is very promising for the deployment of AI in general,  and within the financial  sector in particular. For example, the explainability of internal risk models implemented by banking institutions  would be reinforced if those models enabled to assess causal relations. Causal inference is de facto  at the core  of the concerns in empirical economy since at least 25 years. Nevertheless it is missing from commonly -used AI  models, as well as from the more classical models currently deployed by banks.  
  74   limitations when compared to the study of a single, concrete use case (using a local explanation) or  several concrete use cases (for example to compare the algorithmic results on two individual s and  detect a potential inequality of treatment).   Global explanatory methods are also very difficult to materialize in practice. Such methods exist for  specific model types: for example it is possible to extract from deep neural networks a set of decision   rules which is easy to interpret and, according to the situation, relatively faithful to the Deep Learning  model considered (DeepRED  : Zilke, 2016).   In addition, few methods are able to provide a global explanation independently from the type of the  model being studied. This is however the case of Partial Dependence Plots (PDP), which show the  marginal effect of a given variable on the model prediction s (Friedman, 2001) . 
  75   12. Review of potential attacks against an ML model   ML security if a very recent field of study, but important enough to have been the object of a taxonomy.  This taxonomy is nevertheless in constant evolution, given the changing nature of t he field  (Papernot,  2018) . Among the most notorious attacks against ML models, the following categories can be  distinguished (with an example scenario is given for each type):    Causative attacks (a.k.a. Data Poisoning ): training data are altered (modified feature values, new  features created, etc.)   o Causative integrity attacks  are a subcategory of causative attacks. They are used e.g. to  grant generous loans or low insurance premiums to malicious individuals.   o Causative availability attacks are another subcat egory of causative attacks. They are used  e.g. to discriminate against a population group by denying them the same benefits as the  rest of the population.    Watermark Attacks:   a malicious actor modifies the code of the ML algorithm.   o Watermark integrity attacks: for example, conditions are introduced on certain input data  features so as to trigger advantageous outcomes in chosen cases.   o Watermark availability attacks:  for example, rules are injected into the code in order to  suppress favourable  outcomes fo r the target population group.    Surrogate Models  Attacks   o Inversion attacks are the equivalent of reverse engineering for ML model.   o Membership tests are another type of surrogate model attacks which operate by inferring  whether input data points belong to th e original training set.    Adversarial attacks  construct synthetic examples in order to avoid a detrimental outcome – or  alternatively to obtain a favourable  outcome.    Impersonation Attacks  work by injecting data corresponding to a real identity  (or a composi te of  several real identities) in order to usurp that identity.     A particularly interesting aspect of ML security is that defences  against them tend to have a beneficial  side effect  (Hall, 2019), namely that they bring the ML model closer to satisfying the  four design and  development principles mentioned in this document (section 3): appropriate data management,  performance, stability, and explainability.   To give but one example, a defence  against data poisoning attacks is the RONI method  (Reject On  Negative Impact ). It works by rejecting training data which decreases the model accuracy  (Barreno,  2010) , hence it also protects against degrading the model performance due to training data drift. As  an illustration, a facial recognition algorithm secured by RONI will exclude from its training set a series  of pictures, each associated to an ID document, which would significantly lower its precision: this  contributes to ensuring the integrity but also the performance of the model, which could for example  be used by a banking ins titution to remotely identify new customers (a use case commonly known as  “KYC at a distance”).     
  76   Bibliograph y  Louis Abraham. In Algorithms We Trust . ACPR (21 mars 2019).   Peter Addo, Dominique Guégan, Bertrand Hassani. Credit Risk Analysis using Machine and Deep  Learning models . ffhalshs -01719983f (2018).   Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, Been Kim: Sanity  Checks for Saliency Maps . NeurIPS 2018: 9525 -9536 (2018).   AEAPP. Final Report on public consultation No.  19/270 on Guidelines on outsourcing to cloud  service providers . EIOPA -BoS-20-002 (2020).   Daniel Felix Ahelegbey, Paolo Giudici, Branka Hadji -Misheva. Latent Factor Models for Credit  Scoring in P2P Systems . Physica A: Statistical Mechanics and its Applica tions No. 522 (10 February  2019): pp. 112 -121 (2018).   Maruan Al -Shedivat, Avinava Dubey, Eric P. Xing. Contextual Explanation Networks.   arXiv:1705.10301v3 [cs.LG] (2018).   David Alvarez -Melis, Tommi S. Jaakkola. Towards Robust Interpretability with Self -Explaining  Neural Networks.  arXiv:1806.07538v2 [cs.LG] (2018).   0 David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, Klaus Robert Muller . How to Explain Individual Classification Decisions.  J. Mach. Learn. Res. 11: 1803 1831 (2009).   Marco Barreno, Blaine Nelson, Anthony D. Joseph, J.D. Tygar. The security of machine learning.   Mach Learn (2010) 81: 121 –148 DOI 10.1007/s10994 -010-5188 -5 (2010).   Robert P. Bartlett, Adair Morse, Richard Stanton, Nancy Wallace. Consumer -lending  discrimination  in the FinTech era  (No. w25943). National Bureau of Economic Research (2019).   David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba. Network Dissection:  Quantifying Interpretability of Deep Visual Representations . CVPR 2017: 3 319-3327 (2017).   Roland Berger. The road to AI Investment dynamics in the European ecosystem . AI Global Index  (2019).   Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh  Ruchir Puri, José M. F. Moura, Peter Ecke rsley. Explainable Machine Learning in Deployment.   arXiv:1909.06342 [cs.LG] (2019)   Or Biran, C ourtenay V . Cotton. Explanation and Justification in Machine Learning: A Survey  (2017).  
  77   Alexis  Bogroff, Dominique  Guégan. Artificial Intelligence, Data, Ethics:  An holistic Approach for  Risks and Regulation , HAL (2019)   Jenna  Burrell. How the machine ‘thinks’: U nderstanding opacity in machine learning algorithms .  Big Data & Society (2016).   Cambridge Judge Business School. The Global RegTech Industry Benchmark R eport  (2019).   Cambridge Judge Business School, World Economic Forum. Transforming Paradigms A Global AI in  Financial Services Survey (2019).   Jianbo Chen, Le Song, Martin J. Wainwright, Michael I. Jordan. L-Shapley and C -Shapley: Efficient  Model Interpret ation for Structured Data.  ICLR (2019).   Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, Walter F.  Stewart. RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention  Mechanism. NIPS 2016: 3504 -3512 (2016).   Mark Craven, Jude W. Shavlik. Extracting Tree -Structured Representations of Trained Networks .  NIPS 1995: 24 -30 (1995).   Wei Dai, Isaac Wardlaw. Data Profiling Technology of Data Governance Regarding Big Data:  Review and Rethinking.  Information Technology, New Generations. Advances in Intelligent  Systems and Computing. 448. pp. 439 –450. ISBN 978 -3-319-32466 -1 (2016).   Anupam Datta, Shayak Sen, Yair Zick. Algorithmic transparency via quantitative input influence:  Theory and experiment s with learning systems . In Security and Privacy (SP), 2016 IEEE Symposium  on, pp. 598 –617. IEEE (2016).   Doshi -Velez,  Been Kim. Towards a Rigorous Science of Interpretable Machine Learning  (2017).   EBA. Draft recommendations on outsourcing to cloud servic e providers under Article 16 of  Regulation (EU) No 1093/20101 . EBA/CP/2017/06 (2017).   European Commission High -Level Expert Group on AI. Ethics Guidelines for Trustworthy Artificial  Intelligence  (2019).   Jerome H. Friedman. Greedy function approximation: A gradient boosting machine.  Annals of  statistics (2001).   Donna Fuscaldo. ZestFinance Using AI To Bring Fairness To Mortgage Lending  (2019).   Aurélien Garivier. Toward a responsible artificial intelligence . Institut Mathématique de Toulouse  (26 mars 2018).   Bryce Goodman,  Seth Flaxman. European Union regulations on algorithmic decision -making and a  ‘’right to explanation’’  (2017).  
  78   Dominique Guégan, Bertrand Hassani. Regulatory Learning: how to supervise machine learning  models? An application to credit scoring . ffhalshs -01592168v2f (2017).   Patrick Hall. Proposals for model vulnerability and security.  O’Reilly (2019).   Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, Trevor Darrell.  Generating Visual Explanations.  arXiv:1603.08507v1 [cs.CV] (2016).   Michael Hind, Sameep Mehta, Aleksandra Mojsilovic, Ravi Nair, Karthikeyan Natesan Ramamurthy,  Alexandra Olteanu, Kush R. Varshney. Increasing Trust in AI Services through Supplier’s  Declarations of Conformity.  CoRR abs/18 08.07261 (2018).   Michael Hind, Dennis Wei, Murray Campbell, Noel C. F. Codella, Amit Dhurandhar, Aleksandra  Mojsilović, Karthikeyan Natesan Ramamurthy, Kush R. Varshney. TED: Teaching AI to Explain its  Decisions.  arXiv:1811.04896v2 [cs.AI] (2019).   Sartha k Jain, Byron C. Wallace. Attention is not Explanation.  arXiv:1902.10186v3 [cs.CL] (2019).   Konstantinos Koutroumbas, Sergios Theodoridis. Pattern Recognition (4th ed.).  Burlington. ISBN  978-1-59749 -272-0 (2008).   C. Jung, H. Mueller, S. Pedemonte, S. Plances, O. Thew. Machine learning in UK financial services ,  Bank of England & Financial Conduct Authority (2019).   Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, Jun Sakuma.  Fairness -Aware Classifier with  Prejudice Remover Regularizer.  P. Flach et al. (Eds.): ECML PKDD 2012, Part II, LNCS 7524, pp. 35 – 50 (2012).   Alex Kendall, Yarin Gal. What Uncertainties Do We Need in Bayesian Deep Learning for Computer  Vision?  NIPS 2017: 5580 -5590 (2017).   Faye Kilburn. BlackRock to use machine learning to gauge liquidity risk  (2017).   Been Kim, Rajiv Khanna, Oluwasanmi Koyejo. Examples are not Enough, Learn to Criticize! Criticism  for Interpretability.  29th Conference on Neural Information Processing Systems (NIPS 2016),  Barcelona , Spain (2016).   KPMG. AI Compliance in Control  (2019).   Tao Lei, Regina Barzilay, Tommi Jaakkola. Rationalizing Neural Predictions.  EMNLP (2016).   Zachary C. Lipton. The Mythos of Model Interpretability  (2017).   Hui Liu, Qingyu Yin, William Yang Wang. Towards Explainable NLP: A Generative Explanation  Framework for Text Classification . CoRR abs/1811.00196 (2018).  
  79   Lloyd's, Taking control - Artificial intelligence and insurance . Emerging Risk Report (2019).   Scott M. Lundberg, Su -In Lee. A Unified Approach to Interpreting Model Predictions . NIPS 2017:  4768 -4777 (2017).   Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M. Prutkin, Bala Nair, Ronit  Katz, Jonathan Himmelfarb, Nisha Bansal, Su -In Lee . Explainable AI for Trees: From Loca l  Explanations to Global Understanding.  arXiv:1905.04610v1 [cs.LG] (2019).   MAS (Monetary Authority of Singapore). Principles to Promote Fairness, Ethics, Accountability and  Transparency in the Use of Artificial Intelligence and Data Analytics in Singapore ’s Financial Sector.   (2019).   Rory Mc Grath, Luca Costabello, Chan Le Van, Paul Sweeney, Farbod Kamia, Zhao Shen, Freddy  Lécué. Interpretable Credit Application Predictions With Counterfactual Explanations.  arXiv:1811.05245v2 [cs.AI] (2018).   Tim Miller. Explanation in Artificial Intelligence: Insights from the Social Sciences  (2018).   Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, BenHutchinson,  Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru. 2019. Model Cards  for Model R eporting.  In  Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ’19) (2019).   Christoph Molnar, Interpretable Machine Learning — A Guide for Making Black Box Models  Explainable  (2019).   W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi -Asl, Bin Yua. Interpretable machine  learning: definitions, methods, and applications.  arXiv:1901.04592v1 [stat.ML] (2019).   Nicolas Papernot, Patrick McDaniel. Deep k -Nearest Neighbors: Towards Con fident, Interpretable  and Robust Deep Learning.  arXiv:1803.04765v1 [cs.LG] (2018).   Nicolas Papernot. A Marauder’s Map of Security and Privacy in Machine Learning: An overview of  current and future research directions for making machine learning secure and  private.   Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security (2018).   Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell,  Marcus Rohrbach. Multimodal Explanations: Justifying Decisions a nd Pointing to the Evidence.   arXiv:1802.08129v1 [cs.AI] (2018).   Keyur Patel, Marshall Lincoln. It’s not magic: Weighing the risks of AI in financial services , Centre  for the Study of Financial Innovation (2019).   James Proudman. Cyborg supervision -the app lication of advanced analytics in prudential  supervision , Bank of England (2018).   PwC. Opportunities await:  How InsurTech is reshaping insurance.  Global FinTech Survey (2016).  
  80   Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. Model -agnostic interpretab ility of machine   learning.  ICML Workshop on Human Interpretability in Machine Learning (2016).   Marco Túlio Ribeiro, Sameer Singh, Carlos Guestrin. "Why Should I Trust You?": Explaining the  Predictions of Any Classifier . Explainable NLP KDD 2016: 1135 -1144 (2016).   Andrew Ross, Michael C. Hughes, Finale Doshi -Velez. Right for the Right Reasons: Training  Differentiable Models by Constraining their Explanations.  arXiv:1703.03717v2 [cs.LG] (2017).   Lukas Ryll, Sebastian Seidens. Evaluating the Performance of Machine Learning Algorithms in  Financial Market Forecasting: A Comprehensive Survey . arXiv:1906.07786 (2019).   Laurent Serrurier. Un point sur l’explicabilité et l’interprétabilité en (DEEP…) Machine Learning , IRIT  (12 novembre 2018).   Blake Shaw, Tony Jeb ara. Structure Preserving Embedding.  Proceedings of the 26 th International  Conference on Machine Learning, Montreal, Canada (2009).   Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov . Membership Inference Attacks  Against Machine Learning Models.  2017 IEEE Symposium on Security and Privacy (2017).   Avanti Shrikumar, Peyton Greenside, Anshul Kundaje. Learning Important Features Through  Propagating Activation Differences . ICML 2017: 3145 -3153 (2017).   Justin  Sirignano, A paar Sadwhani, Kay  Giesecke. Deep learning for mortgage risk  (2018).   Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, Himabindu Lakkaraju.  Fooling LIME and SHAP:  Adversarial Attacks on Post hoc Explanation Methods.  arXiv:1911.02508 [cs.LG] (2020).   Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, Alexander M. Rush. LSTMVis: A Tool for  Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks.  IEEE Trans. Vis. Comput.  Graph. 24(1): 667 -676 (2018).   Erik Štrumbelj, Igor Kononenko. An efficient explanation of individual classifications using game  theory . Journal of Machine Learning Research, 11:1 –18 (2010).   Tapestry Networks. Why banks can’t delay upgrading core legacy banking platforms  (2019).   Berk Ustun, Cynthia Rudin. Supersparse  Linear Integer Models for Optimized Medical Scoring  Systems . Machine Learning 102.3: 349 –391 (2015).   Sandra Wachter, Brent Mittelstadt, Chris Russell. Counterfactual explanations without opening the  black box : automated decisions and the GDPR.  Harvard Journal of Law & Technology Volume 31,  Number 2 Spring 2018 (2018).   Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich  Zemel, Yoshua Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual 
  81   Attenti on. Proceedings of the 32nd International Conference on Machine Learning, PMLR 37:2048 2057 (2015).   Jan Ruben Zilke, Eneldo Loza Mencía, Frederik Janssen. DeepRED – Rule Extraction from Deep  Neural Networks.  In: Calders T., Ceci M., Malerba D. ( Eds) Discovery Science. DS 2016. Lecture  Notes in Computer Science, vol . 9956. Springer, Cham (2016).     
  82   Thanks   The authors of this document wish to thank the participants to the exploratory works described herein  (domain experts, data scientists, validation and compliance teams, as well as their managers) for  responding to the call for applications and for their active contribution to the discussions,  presentations and workshops.   The authors also wish to thank their ACPR colleagues who contributed to the brai nstorming and  analyses which resulted in this document, and specifically : Jean -Philippe Barjon, Emmanuelle Boucher,  Nicolas Carta, Laurent Clerc ( AML -CFT Unit ), Richard Diani, Thierry Frigout, Cyril Gruffat, Gauthier  Jacquemin, Boris Jaros,  Matthias Laport e, Farid Oukaci, Jérôme Schmidt, and Pierre  Walckenaer.  
