                                   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –   FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI       Published 5 June 2018                                    
  2   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )  1. INTRODUCTION   Data is the basic building block of the digital economy. The exponential growth in data volume  and increasing computational power at decreasing cost work in tandem to promote data driven technologies such as Artificial Intelligence (AI). The b enefits and risks of AI have been  the subject of great public debate. On the one hand, AI has the ability to boost productivity,  transform businesses, grow the economy and enhance people’s lives. On the other hand, AI  may displace jobs and pose ethical cha llenges such as social profiling.   This paper presents the Singapore Personal Data Protection Commission (PDPC)’s preliminary  analysis of some of the issues pertinent to the commercial development and adoption of AI  solutions. The objective is to propose an  accountability -based framework for discussing  ethical, governance and consumer protection issues related to the commercial deployment  of AI in a systematic and structured manner. In a services -driven economy like Singapore, AI  will likely be deployed in i ntelligent systems that process personal data. Hence, this  framework is also relevant to personal data protection. Using this framework in the design of  systems or processes could also encourage “data protection by design.”1   The proposed framework aims to  encourage informed and constructive debate around this  complex issue. Ultimately, we hope it sow the seeds for the private sector to develop  voluntary governance frameworks, including voluntary codes of practice that can applied to  organisations, sectors,  or more generally across the digital economy .   (a) Striking the Right Balance in AI Governance   The nascence of AI development today presents a timely opportunity for key stakeholders  such as regulators, AI developers, AI user companies and consumers to discus s the need for  AI governance and regulations, as well as the form they might take. Key preliminary views  include:    Governance frameworks around AI should be technology -neutral and “light touch” so that AI technology can develop in a direction that is not h indered or  distorted by prescriptive rules that are laid down prematurely.      AI developers and user companies should be provided with regulatory clarity  when developing AI technologies and translating them into AI solutions .  Consumers benefit from choice a nd product differentiation arising directly from                                                              1 Data Protection by Design refers to the approach by which organisations consider the protection of personal  data from the earliest possible design stage, and throughout the operational lifecycle, of a new system, product  or service. This way, the appropria te safeguards to protect personal data would have been embedded within.  Extracted from the Guide to Data Protection Impact Assessments by the Singapore Personal Data Protection  Commission.   
  3   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )  the diffusion of AI technology into the marketplace. Regulatory clarity also  contributes to healthy market competition.      Policies and regulations that promote explainability, transparency and fairness,  as well as human -centricity, as clear baseline requirements can build consumer  trust in AI deployments. For example, explaining how AI -enabled decision -making  can lead to more  consistent decisions while providing more transparency in the  decision -making process can increase consumer confidence. In addition to the  Personal Data Protection Act (PDPA), sector -specific codes of practice can provide  assurance to consumers about the use of AI, especially when applied in decision making processes.   (b) The AI Value Chain and Deployment Process   This paper adopts the following model to describe the different stakeholders in the AI value  chain:     Fig. 1   The term “AI Developers” includes devel opers of application systems that make use of AI  technology. These may be commercial off -the-shelf products, online services, mobile apps  and other software that consumers can use directly. ”AI Developers” also include device and  equipment manufacturers th at integrate AI -powered features into their products, as well as  AI solution providers whose solutions are not stand -alone products but are meant to be  integrated into a final product.   Meanwhile, the term “User Companies” refers to companies that make use of AI solutions in  their operations. This could be a backroom operation (e.g. processing applications for loans)  or a front -of-house service (e.g. e -commerce portal or ride -hailing app). Equally, it can refer  to companies that sell or distribute devices or  equipment that provide AI -powered features  (e.g. smart home appliances).   
  4   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )  This paper adopts the following process model to describe the different phases in an AI  deployment:      Fig. 2   As different stakeholders have different roles in an AI value chain, the relevance and  applicability of the AI governance framework to each stakeholder may also be different. For  example, the deployment process is potentially applicable to AI Developers th at integrate a  machine -learning AI in their products. Equally, a User Company that makes use of commercial  off-the-shelf systems that employ supervised machine learning AI can also rely on this model  to better understand the risks (e.g. of unintended bias hidden in the training dataset) and the  need for ongoing maintenance (e.g. model tuning). Hence, it is necessary to consider both the  AI value chain and the technology deployment process in discussing the development of the  AI governance framework.   2. PRINCI PLES FOR RESPONSIBLE AI   In order for AI to benefit businesses and society at large, a set of principles needs to be  incorporated into the AI governance framework. These principles aim to promote trust and  understanding in the use of AI technologies. During  PDPC’s consultation, two main sets of  principles surfaced:    
  5   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )  (i) Decisions made by or with the assistance of AI should be explainable, transparent and  fair so that affected individuals will have trust and confidence in these decisions .   Explainable : How can automated algorithmic decisions and the data that drives such  decisions be explained to end -users and other stakeholders in non -technical terms?  The features that support explainability could possibly be designed  into either  the  intelligent  systems that de ploy AI engines or in the AI engines proper. If how the AI  engine works cannot be easily explained, can the ability to verify the automated  algorithmic decision be equally effective?      Transparent:  AI developers, data scientists, application builders and us er companies  should be accountable for the AI algorithms, systems, applications and resultant  decisions respectively in order to build trust in the entire AI ecosystem. What are the  measures and processes that stakeholders in the different parts of the val ue chain can  incorporate in order to be able to inform consumers or customers about how and  when AI technology is applied in decisions affecting them?      Fair:  AI algorithms and models embedded in decision -making systems should  incorporate fairness at their  core. This could include the training dataset, AI engine  and selection of model(s) for deployment in the intelligent system. What practices will  avoid unintentional discrimination in automated algorithmic decisions? Examples  include monitoring decisions t o detect unintentional discrimination and accounting  for how they were made.   (ii) AI systems, robots and decisions made using AI should be human -centric.  Human -centric  design refers to the design approach that puts the individual customer or consumer front  and centre of the design of the AI deployment. Organisations that are perceived to have  caused harm to consumers as a result of their AI deployments do not inspire consumer  trust and confidence. Beneficence or “Do no harm”  is a principle that can incorporate the  following:    Decisions should strive to confer a benefit on or provide individuals with assistance in  the performance of a task;    Decisions should not cause foreseeable harm2 to an individual, or should at least  minimise harm (in necessary circumstances, when weighed against the greater good);3   Tangible benefits to individuals should be identified and communicated in order to  build consumer understanding and confidence; and    AI systems and robots should be designed to avoid causing bodily harm or affecting  the safety of individuals.                                                               2 “Harm ” in this case  includes physical, psychological, emotional  and economic harm.   3 Adapted from UNICEF’s Humanitarian Principles.  
  6   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )    Illustrations    Automating the decision to approve an application for travel insurance to promote  consistency while catering for genuine differences  in individual circumstances.    Programming robot -assisted manufacturing such that the robotic arm will not swing  beyond a specific safety parameter or comes to a halt if someone steps into its  operational zone.    Including safety limits by design in the oper ation of self -driving vehicles (e.g. speed  limits, vehicle -to-vehicle safety limits for collision avoidance). Where collisions are  unavoidable, parameters should also be included to avoid or minimise harm to  humans .  3. EXPLORING A PROPOSED GOVERNANCE FRAMEWOR K FOR AI   Building on the AI Value Chain ( Fig. 1 ) and the aforementioned principles, this paper proposes  a governance framework for AI to encourage discussion around how the two sets of principles  could be adopted by different stakeholders. The proposed fra mework is generally applicable  to all sectors and provides options that may be adopted (fully or in part) by organisations,  depending on their needs and objectives.   Assumptions and Limits of the Proposed Framework    In exploring the proposed framework, the  following assumptions and limits were identified:   Assumptions    Technology Neutral : The proposed  framework focuses on the design,  application and use of technology in  contexts affecting individuals without  being specific to the AI technology.      Sector -agnostic : The proposed  framework should be applicable to all  sectors as a baseline standard. This does  not preclude specific sectors and  organisations to incorporate additional  standards above the baseline set out in  the proposed framework.  Limits     Legal Liability : The proposed framework  does not set out to address or resolve  specific questions of legal liability or  apportionment of damages or  restitution. However, some of the  practices advocated in this framework  are likely to assist in the manageme nt of  disputes and ensure the availability of  evidence that may be required to  resolve such questions.    
  7   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )  4. PROPOSED FOUR -STAGE GOVERNANCE FRAMEWORK   (A) IDENTIFYING THE OBJECTIVES OF AN AI GOVERNANCE FRAMEWORK   This section sets out several objectives for the proposed framework.   Proposed Objectives:   Explainability and Verifiability.  An organisation that employs AI in its decision -making  process should be able to explain how its AI engine functions. However, wher e this is not  possible for certain types of AI engines (e.g. neural networks), the organisation should  minimally be able to verify that the AI engine is performing to expectations and within the  technical and ethical parameters set. This would provide reas surance that the decision making process is supervised and not overly reliant on AI to suggest decisions or even  simply delegated to a set of software codes.   To achieve explainable AI, it is necessary to consider the roles of different stakeholders in  the AI value chain, for example:     The requirement of explainability may be catered for by AI Developers in their  design of AI engines or solutions. This will enable them to be in a better  position to explain to User Companies how their AI solutions function.    User Companies who are unable to explain how the AI engine functions can  design for verifiability4 of the decision -making process from the planning  stages. This will enable them to ensure that the necessary data points for  monitoring are catered for.     Good Data Accountability Practices. Organisations should put in place good data  accountability practices. These include the following:    Understanding the lineage of data. This means knowing where the data  originally came from, how it was collected, curated  and moved within an  organisation, and how its accuracy is maintained over time.    Minimising the risk of bias. Veracity or data quality refers to the risk of bias that  may be inherent or latent within a dataset. Organisations should adopt practices  that ena ble them to detect biases that may exist in their data so that they can  take steps to address them.    Maintaining data provenance records. This practice is important for  establishing data lineage in general, but separate provenance records can also                                                              4 Verification methods for AI deployments can as a baseline reference traditional software verification and  validation methods. These include testing, run -time monitoring, static analysi s, model checking and theorem  proving, and can be modified for the AI context. Human oversight is usually a core component of such verification  processes. Extracted from Menzies, T. and Pecheur, C. (2005) “ Verification and Validation and Artificial  Intelli gence ”, Advances in Computers , 65, 153 – 201.   
  8   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )  be maint ained to log the data that was used in the AI deployment process model  and also the entire AI value chain5.  Collectively, good data accountability practices provide reassurance to consumers that  downstream decisions or suggestions provided by intelligent s ystems are not false  (resulting in type I and/or type II errors6) and do not risk unintentional discrimination. It is  also important for User Companies to distinguish between data accountability, which aims  to ensure completeness and comprehensiveness in t he data preparation and model creation stages, and the responsibility of organisations to avoid making discriminatory,  unfair or unlawful decisions.    Transparency.  Open and transparent communication between stakeholders in the AI value  chain will be condu cive to building trust in the entire AI ecosystem. Examples of how this  principle may be implemented include :   Provision of information by different stakeholders should have a clear purpose, tailored  to the intended recipient’s interests and needs, and shou ld not be inadequate; and    Stakeholders are encouraged to explore and use a variety of communication channels  in order to ensure effective and clear communication with consumers and customers.   (B) SELECTING APPROPRIATE ORGANISATIONAL GOVERNANCE MEASURES   This section identifies accountability -based practices that can help organisations render an  account to a regulator, affected individual or interested stakeholder of how decisions are  made. Not all elements of this section are relevant in all cases: options may  be selected and  customised according to the AI engine that is adopted (e.g. rule -based, machine learning), the  sector that the organisation operates in, the type of decisions and degree of automation (e.g.  man -in-the-loop or man -out-of-the-loop), etc.  Governance    A. Internal governance. When AI systems are used for decision -making,  organisations should consider how their existing corporate governance or  oversight mechanisms could be adapted or new ones created. For example:     Ensuring that the departments und ertaking AI activities are aware of their  responsibilities ;    Introducing oversight mechanisms  for actions or decisions within the  responsible departments’ sphere of responsibilities, e.g. to review exceptions  identified by the automated decision -making pro cess, ensure verifiability of  automated decisions, or to review decisions in processes that have no human  oversight elements;                                                               5 Maintaining provenance records from data used to build models to the AI end -user input data could provide a  way to ascertain the quality of data used and trace back potential sources of errors .  6 In statisti cal analysis, type I errors refer to false positive errors and type II errors refer to false negative errors.  
  9   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )   Establishing monitoring or reporting  systems to ensure that information flows  to the right level within the corporate governance hierarchy; and     Periodically reviewing  the corporate governance or departmental oversight  mechanisms, especially when there are significant changes to the  organisational structure or key personnel involved in the governance or  oversight mechanisms, to ensu re their relevance.       B. Risk and/or harm mitigation. When  deploying intelligent systems for real world  uses, it is important to identify potential risk or harm that may foreseeably arise  in anticipated use -cases. A risk and impact assessment is a tool that can assist  with risk identification and harm mitigation, as follows:    When selecting candidate models  for AI deployments, risk and impact  assessments can assist in identifying and understanding the expected and  worst case implications, and i nform the crafting of mitigation processes;    Ethical considerations  should also be incorporated into the overall risk and  impact assessments; and    Documenting the risk and impact assessments can be helpful, e.g. to produce  audit trails for internal (or exter nal) accountability.       C. Ad hoc  and periodic  reviews of AI deployments and decisions. After initial  deployment, organisations should consider periodically reviewing their decisions  and processes to be satisfied that reliance on AI systems for decision -making  remains relevant and appropriate. In addition to periodic review, specific triggers  for reviews should also be defined, e.g. when there are significant changes to the  intelligent system or deployed models.  Operations Management & Systems D esign    D. Data Accountability. AI models deployed in intelligent systems that make or assist  the making of algorithmic decisions must operate on accurate information.  Accuracy of information is affected by:    The completeness of the data required;    How recentl y the data was collected and updated;    Whether the data is structured in a machine -understandable  form; and    The source of the data, as the context for initial collection may affect the  interpretation and reliance of the data for a secondary purpose.   Organisations should consider generating records or documenting processes to  mitigate potential issues with data accountability, including:    Keeping a data provenance record  or audit trail of the data that was used in  the model creation or decision making, which may help uncover any inherent  limits or errors;  
  10   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )   AI Developers can consider assigning a  veracity score7 to training datasets  during model creation that may be helpful during model selection to minimise  the risk of bias; and    User Companies can conside r assigning secondary veracity scores  for  operational (or input) data which may be helpful when it is used as feedback  data during model tuning.       E. Repeatability.  An intelligent system that is able to perform an action or make a  decision consistently within the same scenario will promote consumer  confidence. Practices that may be helpful include:    Repeatability assessments  for commercial deployments in live environ ments;    Where a decision is not repeatable, one possible design consideration is how  exceptions  should be identified and handled; and    Measures can also be put in place to identify and account for  changes over  time , especially if models are trained on time -sensitive data or are designed  to evolve.       F. Traceability  relates to how the AI module makes decisions or provides  suggestions. Practices that promote traceability include:    Building an audit trail  to document the decision -making process;    Implementing a black box recorder  that captures all input data streams. Data  relevant to traceability should be stored appropriately to ensure that there is  no degradation or alteration, and for retention durations relevant to the  industry; and    Access to and/or audits o f the AI algorithm, for organisational risk  management. Regardless of whether algorithm audits should be universally  practised, if executed well, algorithmic audits can foster consumer trust. In  considering algorithm audits, the following matters are relev ant:  o Expertise  required to effectively understand the algorithm, rules or  models;   o Considerations of commercial confidentiality  of the AI technology  provider, including mitigating measures; and   o The usefulness  of this information to assist in providing an ac count to  regulators, affected individuals and/or interested stakeholders.       G. Tuning of AI Models. The selection of model(s) for eventual deployment into the  intelligent system should be a considered decision and the process and reasons  for the choice should be documented. Moreover, models need to be periodically  updated. Relevant considerations include :                                                                7 In data science, d ata veracity refers to false or inaccurate data. A data veracity score refers t o the assignment  of a score to account for elements that could affect the accuracy of data , e.g. inconsistencies, contradictions,  or  “staleness” of data.  
  11   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )   Adopting internal governance processes and tuning AI models periodically  to  cater for changes to data and/or models over time;    Carry out active monitoring and tuning where AI  systems developed in a  comparatively static environment8 display model instab ility when deployed in  dynamic environments.9      (C) CONSIDERING CONSUMER  RELATIONSHIP  MANAGEMENT PROCESSES   The third step in the AI governance framework is the management of communications with  affected individuals and providing measures for recourse, which are important for building  consumer trust and confidence. The following measures can be incorporated into existing  consumer management processes. Depending on the nature of the AI deployment,  organisations may select measures that would best suit their nee ds. These measures include:      Transparency    A. Policy for disclosure.  Organisations should consider disclosing the use of AI in the  decision -making process (whether fully automated or to assist in decision  making), including how this disclosure can be made.     Increased transparency  would contribute to building consumer confidence and  acceptance by increasing the level of knowledge and trust in the customer  relationship. Organisations should also consider carrying out ethical evaluations  and making meaningful summaries of these evaluations available to their  customers.       B. Policy for explanation. Organisations should consider explaining how AI is  deployed in the decision -making process, and/or how a specific decision was  made including the reasons underpinning the decision where appropriate or  available.   Explanations could take the form of ex ante  information provided as part of  general communication by the organisation, or of specific information provided  by the organisation in respect of a decision affecting the individual making the  request. In the context of decisions made using profile informa tion, the affected  individual may be given information about how his personal data is associated  with user targeting profiles.                                                               8 Stoica , I. et al.  (2017)  A Berkeley View of Systems Challenges for AI. and Mishra , N. et al.  (2018)  Controlling AI  Engines in Dynamic Environm ents .  9 These are environments that change rapidly, frequently and in non -reproducible ways . 
  12   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 ) Interaction  C. Heuristic evaluation.  This ensures that usability problems are addressed, and that  user interfaces are tested, so th at the user interface layer serves its intended  purpose. Expectations of consumers will be a relevant area to address to preserve  the consumer experience. For example, with the increasing use of chat bots,  organisations should consider whether consumers sh ould be informed when they  interact with chat bots instead of a human agent.       D. Option to opt out. In a fully automated deployment scenario, an organisation  providing an option to opt out could lead to a decrease in operational efficiency,  may not be technically feasible, or lead to a process being commercially  uncompetitive. Nonetheless, providing an o ption to opt out could be beneficial,  particularly if it builds consumer trust, for example in deployments that could have  significant impact to the individual and risk to the organisation. Hence,  organisations should consider providing an option for opt o ut.    Communication    E. Feedback channel . Organisations should consider providing a channel (e.g. email  address) for affected individuals to provide any feedback or raise any queries they  may have for the organisation to address. In particular, where customers find  inaccuracies in the data, a ch annel that allows customers to access and correct  their own data will be useful to maintain data veracity.       F. Review of decision.  Organisations should consider providing the affected  individual with an avenue to request a review of a decision affecting h im. Relevant  considerations include the degree of automation and the extent to which AI is  deployed in decision -making, as well as the impact to the individual. In cases  where a fully automated decision is made, it could be reasonable to provide an  avenue for the affected individual to request a review of the decision.     (D) BUILDING A DECISION MAKING AND RISK ASSESSMENT FRAMEWORK   The final stage is to incorporate decision -making and risk assessment considerations into the  framework. The risk and severity of harm to the customer are factors that affect which  decision -making approach should be adopted, and in turn how organisations calibrate  governance and consumer management processes. The following decision -making  approaches can assist businesses in determini ng the appropriate method of deployment of AI  by maximising benefits while minimising the risks of harm:10                                                                10 Adapted from Citron, D. K. and Pasquale, F. A. (2014 ) “The Scored Society: Due Process for Automated  Predictions ”, Washington Law Review , 89 
  13   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )                      Fig. 3   To determine the appropriate decision -making approach, organisations can consider relying  on the proposed decision matrix (see Fig. 4 below), which takes into account the probability  and severity of harm to consume rs.  .Human in-theloop Human over the-loopHuman out-ofthe-loop Human -in-the-loop  models involve a human decision maker who relies on the intelligent  system to suggest one or more possible options, but who ult imately makes the final decision.  For example, an operational system that provides an employee with one or more options  customised for the case that he is handling.     Human -over -the-loop  models involve  a human decision maker who has  made a choice but relies on the  intelligent systems to suggest options  of how to perform the action. For  example, an individual specifies his  destination in a navigation system  which makes suggestions of one o r  more navigation routes.    Human -out-of-the-loop  models usually  involve automated decision making by  the intelligent system based on a pre determined set of scenarios. The  identification and handling of  exceptions will be important. For  example, an autonomous cleaning bot   can be left to map out the best  path for  cleaning a location, excluding “no -go  zones” which humans can pre -set.   
  14   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )    Fig. 4  Illustration of how to use the decision matrix   To illustrate how this proposed framework may be used, where an organisation assesses that  both the probability and severity of harm to a customer is high, it may decide that the human in-the-loop model may be the appropriate decision -making approach.11 Accordingly, given  the risk of harm, it is appropriate for the organisation to decide to also implement governance  processes focused on repeatability and traceability. Additionally, the organisation may decide  that since the algorithmic decision is not automated, there is no need to provide too much  information about its internal processes to its customers .   5. NEXT STEPS   This discussion paper is intended to promote healthy discussion on  promoting the responsible  development and adoption of AI solution and mitigating potential risks and negative impact.  PDPC invites organisations to use this document for internal discussion. Organisations are free  to adapt it for internal use. Trade assoc iations and chambers, professional bodies and  societies, and interest groups are encouraged to adapt this proposed framework for their  sectors in the form of voluntary codes of practice.                                                               11 However, where an organisation chooses to use human -in-the-loop model, it does not necessarily mean that  the probability and severity of harm is high.   
  15   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )  6. ACKNOWLEDGEMENTS   PDPC expresses its sincere appreciation to the follo wing organisations and individuals for  their valuable feedback to this discussion paper: (in alphabetical order)   AI Singapore   Attorney -General’s Chambers   Centre for Strategic Futures   Competition and  Consumer Commission of Singapore   Government Technology Ag ency   Infocomm Media Development Authority   Integrated Health Information Systems Pte Ltd   Inter -Agency Project Team on Ethics and Governance of Artificial Intelligence   Land Transport Authority    Ministry of Communications and Information   Ministry of Health   Monetary Authority of Singapore   Smart Nation and Digital Government Office       Adobe Systems, Inc.   Amazon Web Services   AsiaDPO   BSA | The Software Alliance   Cisco Systems, Inc.   Facebook   Microsoft   Symantec Corporation   Dr. Steven Tucker, Founder, Tucker Medical Pte Ltd and Chief Medical Officer, CXA Group       END OF DOCUMENT         
  16   DISCUSSION PAPER ON ARTIFICIAL INTELLIGENCE (AI) AND PERSONAL DATA –  FOSTERING RESPONSIBLE DEVELOPMENT AND ADOPTION OF AI   (published  05 June 2018 )  Copyright 201 8 – Personal Data Protection Commission Singapore (PDPC)     This publication is intended to foster responsible development and adoption of Artificial Intelligence. The contents herein  are not intended to be an authoritative statement of the law or a substitute for legal or other professional advice.     The contents of this publication are protected by copyright, trademark or other forms of proprietary rights and may not be  reproduced, republished  or transmitted in any form or by any means, in whole or in part, without written permission.    
