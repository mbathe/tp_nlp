1Executive Summary Artificial  Intelligence  and Childrens  Rights, This is an executive summary for the  research memorandum on artificial  intelligence and children's rights
2As Artificial Intelligence-based technologies become increasingly integrated into modern life, the onus is on  companies, governments, researchers  and parents to consider the ways in  which such technologies impact children’s human rights. The potential  impact of artificial intelligence on children deserves special attention, given  children’s heightened vulnerabilities  and the numerous roles that artificial  intelligence will play throughout the  lifespan of individuals who are born  in the 21st century. As much of the  underlying technology is proprietary  to corporations, corporations’ willingness and ability to incorporate human  rights considerations into the development and use of such technologies will  be critical. Governments will also need  to work with corporations, parents,  children and other stakeholders to create policies that safeguard children’s  human rights and related interests.1Executive  Summary In this memo, we briefly outline a  series of case studies to illustrate  the various ways that artificial intelligence-based technologies are beginning to positively and negatively  impact children’s human rights. W e  identify valuable opportunities to  use artificial intelligence in ways that  maximize children’s wellbeing, and  spotlight critical questions that researchers, corporations, governments,  educators and parents should be ask ing now in order to better protect children from negative consequences. W e  hope that this memo will help a range  of stakeholders better understand and  begin to lay a framework for addressing the potential impact of artificial  intelligence on today’s children, and on  future generations.
3At the request of UNICEF and its research partners, a team of students at  the Human Rights Center at UC Berkeley School of Law spent the Fall 2018  semester researching how artificial intelligence technologies are being used  in ways that positively or negatively  impact children at home, at school, and  at play.2 W e also reviewed and identified the disparate human rights that  may be disproportionately impacted, Methodology both positively and negatively, by its  use.3 Importantly, while any technology that affects adults will have secondary impacts on children, for the sake of  space we focused only on applications  that have been designed specifically  for children. In this summary, we spotlight three case studies that are particularly illustrative of emerging issues.  For other examples, please see the full  memorandum.
4What is  Artificial  Intelligence ? With the recent rise of and attention  given to deep learning technologies, the  terms artificial intelligence, machine  learning, and deep learning have been  used somewhat interchangeably by  the general public to reflect the concept  of replicating “intelligent” behavior in  machines. For purposes of this memo,  we use artificial intelligence to mean a  subfield of computer science focused on  building machines and software that can  mimic such behavior. Machine learning  is the subfield of artificial intelligence  that focuses on giving computer systems the ability to learn from data. Deep  learning is a subcategory of machine  learning that uses neural networks to  learn to represent and extrapolate from  a dataset. In this memo, we focus on the  ways that machine learning and deep  learning processes impact children’s  lives and ultimately, their human rights.
5What are   childrens  rights ?, The Convention on the Rights of the  Child (CRC) is the most comprehensive legal framework that protects  children--defined as human beings 18  years old and under--as rights bearers.4  The CRC aims to ensure children’s  equality of treatment by States.5 More  than a binding international document,  the Convention is an ethical and legal  framework for assessing states’ progress or regress on issues of particular  interest to children.6 Because of the  exponential advancement of artificial intelligence-based technologies  over the past few years, the current  international framework that protects  children’s rights does not explicitly  address many of the issues raised by  the development and use of artificial  intelligence.7 However, it does identify  several rights that may be implicated  by these technologies, and thus provides an important starting place for  any analysis of how children’s rights  may be positively or negatively affected  by new technologies, such as rights to  privacy, to education, to play, and to  non-discrimination.8 
6Children  s  Rights at Home  YouTube ::CASE  STUDY  ONE ,
7 1Social media platforms that rely on  streaming technologies are revolutionizing how adults and children consume  media content. Platforms are working  hard to ensure consumers maximize  their time on these sites. Y ouT ube9  stands out as the dominant player in  this space, especially when it comes  to today’s youth. In 2017, 80% of U.S.  children ages 6 to 12 used Y ouT ube on  a daily basis.10 Y ouT ube was the 2016  and 2017 “top kids brand” according to  Brand Love studies.11 In the 2017 study,  96% of children ages 6 to 12 were found  to be “aware of Y ouT ube,” and 94% of  children ages 6 to 12 said they “either  loved or liked” Y ouT ube.12 The Y ouT ube phenomenon isn’t just occurring  in the United States as Y ouT ube has  massive user bases in India, Moscow,  across Europe, and beyond.13 In 2015, Y ouT ube decided to launch  a dedicated platform called Y ouT ube  Kids as a means to provide safe, age  appropriate content for children.14  On both Y ouT ube and Y ouT ube Kids,  machine learning algorithms are used  to both recommend and mediate the  appropriateness of content.15 Y ouT ube  representatives, however, have been  opaque about differences in the input  data and reward functions underlying  Y ouT ube Kids and Y ouT ube.16 Lack  of transparency about the input data  used in algorithms makes it difficult  for concerned parties to understand  the distinction.17 More generally, the  issue of algorithmic opacity is of concern with both Y ouT ube and Y ouT ube  Kids, since Y ouT ube, and not Y ouT ube  Kids, continues to account for the  overwhelming majority of viewership  of children’s programming within the  Y ouT ube brand.18The machine learning algorithms –  primarily the recommendation engine  employed by Y ouT ube and Y ouT ube  Kids – are optimized to ensure that  children view as many videos on the  platform as possible.19 Children do not  need to enter any information or affirm  any acquired permissions to watch  thousands of videos on Y ouT ube and  Y ouT ube Kids.20 T ouchscreen technology and the design of the platforms  allow even young children substantial  ease of access.21 Unfortunately, neither  recommendation system appears to  optimize for the quality or educational  value of the content.22 Because companies developing children’s programming are similarly concerned about  maximizing viewers and viewer hours,  their posts are often designed around  Y ouT ube’s privileging of quantity  with little consideration for quality,  including educational value.23 There is  particular concern that with Y ouT ube  and Y ouT ube Kids’ algorithm-derived  “related-videos” recommendations  children can become easily trapped in  “filter bubbles” of poor-quality content.24  Filtering algorithms also raise other  problems, especially when a significant  number of external entities are able  to co-opt Y ouT ube and Y ouT ube Kids’  algorithmic discovery processes to  maximize viewer time with sometimes  startling consequences for children.25  For example, anyone over the age of  18 can create and upload content onto  Y ouT ube and their creations are not  regulated by professional protocols.  Y ouT ube and Y ouT ube Kids’ algorithmic discovery processes can be  manipulated to push content that the  pusher expects will perform well on  the platform’s “related-videos” engine, incentivizing sensational content.26  Prioritizing such content is one of the  critical impacts of Y ouT ube’s use of  machine learning algorithms.27 Kids  are particularly susceptible to content  recommendations, so shocking “related videos” can grab children’s attention and divert them away from more  child-friendly programming.28  Another challenge is children’s potential exposure to Y ouT ube and Y ouT ube  Kids-related advertising.29 Y ouT ube’s  business model relies on tracking the  IP addresses, search history, device  identifiers, location and personal data  of consumers so that it can categorize  consumers by their interests, in order  to deliver “effective” advertising.30  Some of the top advertising companies  pay Google vast sums to guarantee  that their ads are placed on Y ouT ube  channels with popular children’s programs.31 Advertisers also routinely employ keywords such as “kid,” “child,”  “toddler,” “baby” or “toy” in order to  better target children on Y ouT ube.32  Although Y ouT ube Kids claims to  prohibit “interest-based advertising”  and ads with “tracking pixels,” advertising disguised as programming is  ubiquitous on the Y ouT ube Kids application.33 Although Y ouT ube restricts  paid advertising of food and beverages  on Y ouT ube Kids, for example, food  companies may use their own branded channels to spotlight particular  food and beverages that they produce,  burying what are essentially ads  within programs, and thereby target  children with their products.34 Thus,  corporations are finding ways to target  minors in ways that uphold the letter  but not the spirit of the rules and in  ways that may be opaque to parents  and other concerned parties.35
Children  s  Rights at Play  Smart Toys  YouTube  8CASE  STUDY  TWO ::, 36
9Children’s leisure activities have  changed significantly over the last two  decades, from engaging with toys with  little interactive capacity to smart toys  that are capable of responding back.37  Through the use of weak artificial intelligence, these toys incorporate a set  of techniques that allow computers  to mimic the logic and interactions of  humans.38 Such toys raise a host of  human rights-related concerns. These  include potential violations of a child’s  right to privacy, and whether corporations have (or should have) a duty  to report sensitive information that is  shared with a toy and stored online— such as indications that a child might  be being abused or otherwise harmed.  39 There are three nodes involved in  smart toy processes, each of which  comes with a set of challenges and vulnerabilities: the toy (which interfaces  with the child), the mobile application,  which acts as an access point for Wi-Fi  connection, and the toy’s/consumer’s  personalized online account, where  data is stored. Such toys communicate  with cloud-based servers that store and  process data provided by the children  who interact with the toy. 40 Privacy concerns arising from this  model can be illustrated by the Cloud  Pets case, in which more than 800,000  toy accounts were hacked, exposing  customers’ (including children’s) private information.41 Another example  is that of the Hello Barbie doll, which  raised civil society concerns around the  interception of sensitive information  and whether the doll allowed for pervasive surveillance in ways that were  not transparent to users.42 In that case,  the toy’s manufacturer, Mattel – in collaboration with T oy T alk, Inc.– released an FAQ to try to address these  pressing questions.43 First, the document states that the conversations between the doll and the child cannot be  intercepted via Bluetooth technology  because the conversation takes place  over a secured TLS (HTTPS) network,  making it impossible to connect the  doll via Bluetooth. 44 The document  does advise against connecting the  doll to third party Wi-Fi, which may be  especially vulnerable to interception.45  Further, the document claims that the  Hello Barbie doll is not always listening but becomes inactive when not  expressly engaged.46According to the  document released by Mattel, the doll  has similar recognition technology  to Siri and is activated only when the  user pushes down the doll’s belt buck le.47 Finally, the company states that  the doll does not ask questions that  are intended to elicit personal information, in order to minimize the circumstances in which a child might divulge  sensitive information during his/her  conversation with the doll.48  Notably, parents can access their  child’s T oyT alk cloud account and  listen to what their child has said, deleting any personal information.49 As  a safeguard, T oyT alk also participates  in the FTC’s KidSafe Seal Program,  a compliance program for websites  and online services targeted towards  children. 50 There are two types of  certificates that a website or online  service can obtain: the KidSafe certificate and the KidSafe+ certificate. 51 The  KidSafe+ certificate requires additional  requirements and compliance with  COPPA.52 Because Hello Barbie targets  children in the age range protected  by COPPA, T oyT alk makes sure to comply with not only the basic KidSafe requirements but the additional  requirements for KidSafe+. 53 For example, the communications between  Hello Barbie and a child are encrypted  and stored on a trusted network.54 One emergent concern, despite these  safeguards, is whether a company has  a duty to report or otherwise “red flag”  sensitive information shared through  their toys—for example, children  who reveal they are being abused, or  children who share suicidal thoughts  or other self-harm related behavior.55  Existing privacy laws and common law  tort duties fall short of providing directly relevant protection.56 For example, while COPPA protects the privacy  rights of minors under the age of thirteen, requiring companies to obtain  parental consent and to disclose what  information is being collected about a  minor, it does not impose any reporting requirements regarding suspected  child abuse and neglect.57 Ultimately, most mechanisms for  tackling these challenges have been  designed by the corporations themselves.58 In the case of Hello Barbie,  T oyT alk has created automatic responses for serious conversations  such as bullying or abuse. Such  responses include “that sounds like  something you should talk to a grown-up  about.” 59 While an important step  towards addressing this issue, this  approach potentially pushes any responsibility for acting to the parents  or to the child herself. It is unclear how  many children would act on this response to report problems to a grownup or what it means for children if an  adult in their household is the one  perpetrating the harm. 2
10 Children  s  Rights at School  AI in EducationCASE  STUDY  THREE ::,
11 AI-based tools have three general  orientations in terms of their use in  schools: learner-facing, teacher-facing  and system-facing.60 Adaptive learning  systems that are learner-facing employ  algorithms, assessments, student  feedback and various media to deliver  material tailored to each student’s  needs and progress.61 For example, AI  may be used to enhance social skills,  especially for children with special  needs. One company that employs AI  for this purpose is Brain Power, which  addresses the issue of autism through  a wearable computer.62 Another example would be the deployment of  AI to help high school students build  career skills by using GPA calculators  and language learning applications.  Duolingo is one such language learning application which gives students  personalized feedback in over 300,000  classrooms around the globe.63 Under  the teacher-facing category, AI helps  teachers in administrative tasks such  as grading papers and detecting plagiarism. For example, Carnegie Learning  is working on a startup called Lumilo,  building an AI augmented reality  assistant that will keep teachers in  the loop as students work on assignments.64 In addition to the software and tools  touched on above, AI-incorporating robots are increasingly transforming educational methods and practices. Robots are being brought to classrooms  in a way that alters how students learn,  calling attention to a wide variety of  applications. Even though educational  robots promise great benefits to children—such as personalized learning,  helping kids develop social skills, enabling distance education for children  in remote regions, etc.—they also pose  risks.65 Human rights that may be positively or negatively affected by their  use include the right to education, as  well as the right to protection from  exploitation and abuse, and the protection of children with disabilities.  Surveillance of children is another use  case that is booming due to advance  machine learning and deep learning  techniques.66 Although some degree  of surveillance promises advanced  security, surveillance may also leave  children more vulnerable than previously.67 On the positive side, police  in New Delhi recently trialed facial  recognition technology and identified  almost 3,000 missing children in four  days.68 However, surveillance can also  create privacy, safety and security risks and limit children’s ability and  willingness to take risks and otherwise express themselves, especially  in educational contexts.69Always-on  surveillance practices that continuously monitor everything from children’s engagement in the classroom to  their emotional states throughout the  day threaten the creativity, freedom  of choice and self-determination of  children by potentially fostering an  overabundance of self-censorship and  social control.70 Once automated surveillance technologies are deployed at  schools and in classrooms, children’s  rights such as the right to privacy, the  right not to be subjected to discrimination, the right to flourish, and freedom  of expression may be compromised  due to the panopticon environment  in which children are confined.71 The  risks vary depending on who does the  surveilling (governments, teachers,  parents etc.) and for what purposes.72 However, the potentially chilling  effect of having cameras constantly  trained on children is undeniable.73 It  is important to consider and evaluate  the actors involved, their purposes,  the tools and methods they’ll use, and  the safeguards they’ll put in place, so  that the emerging trend of classroom  surveillance—and surveillance more  generally—helps children more than it  hurts.74 3
12Microsoft and Google have both established principles for the ethical use of  AI.75 However, neither has public-facing  policies specific to AI and children.76  Several technology centers, trade associations, and computer science groups  have also drafted ethical principles with  regards to AI.77 However, most have excluded explicit reference to child rights,  or discussion of the risks to children  on AI-incorporating technologies more  generally.78 Like corporations, governments  around the world have adopted strategies for becoming leaders in the  development and use of AI, fostering  environments congenial to innovators  and corporations.79 However, in most  cases, policymakers have not directly  addressed how the rights of children  fit into their national strategy.80 While  France’s strategy deals with the AI-related issues of achieving gender equality and implementing digital literacy through education, the broader scope  of impact on children is missing.81 An  example of a country that has taken  a more proactive look at the potential  benefits of AI for children is India,  whose AI initiative focuses on using  AI in education, such as creating  adaptive learning tools for customized learning, integrating intelligent  and interactive tutoring systems,  adding predictive tools to inform  pre-emptive action for students predicted to drop out of school, and developing automated rationalization  of teachers and customized professional development courses.82 Ultimately, both corporations and  governments would be well advised  to think through how their AI strategies can be strengthened to maximize the benefits and minimize the  harms of AI for children today, and in  the future.  How Corporations  and Governments  Can Help Mitigate  Harmful Impacts of  AI on Children
13 •Incorporate an inclusive design approach when developing child-facing products, which maximizes  gender, geographic and cultural  diversity, and includes a broad  range of stakeholders, such as parents, teachers, child psychologists,  and—where appropriate—children  themselves.   •Adopt a multi-disciplinary approach when developing technologies that affect children, and consult  with civil society, including academia, to identify the potential impacts of these technologies on the  rights of a diverse range of potential  end-users.   •Implement safety by design and  privacy by design for products and  services addressed to or commonly  used by children.    •Develop plans for handling especially sensitive data, including revelations of abuse or other harm that  may be shared with the company  through its products.  •Be aware of and consider using artificial intelligence-based tools that  may enhance learning for students,  such as specialized products that  can assist non-traditional learners  and children with special needs.   •Set up awareness campaigns that  help parents understand the importance of privacy for their children.  Parents should be aware of how  their children’s data is being used  and processed for diverse purposes,  including for targeted ad campaigns  or non-educative social media recommendations. They should also  be aware of the impacts of posting  pictures or other information about  their children to social media, and  the ways that what they post can  have a dramatic impact on their  children’s future.   •Adopt a clear, comprehensive  framework for corporations that  imposes a duty of care connected to  the handling of children’s data, and  provides an effective remedy (judicial, administrative or other) for  breach. This framework should incorporate human rights principles.   •Establish a comprehensive national  approach to the development of artificial intelligence that pays specific attention to the needs of children  as rights-bearers and integrates  children into national policy plans.A thorough set of recommendations is beyond the scope of this memo.   However, some initial suggestions are touched on below:Recommendations Corporations EducatorsGovernments •Carefully review and consider  avoiding the purchase and use  of products that do not have  clear policies on data protection, security, and other issues  that impact children.   •Incorporate children into the  decision-making process about  how their data will be used,  including whether to post their  information to social media  sites and whether to engage  smart toys, helping children  understand the potential short  and long-term impacts of that  use.   •Identify how schools might  be using artificial intelligence-based technologies to  assist or surveil children, and  raise concerns if some of the  policies or procedures are unclear or seem inappropriate— for example, by disincentivizing creativity and exploration.  Encourage the use of artificial  intelligence-based technologies  when they seem likely to enhance learning and that positive benefit has been confirmed  by peer-reviewed research.Parents •Avoid the overuse of facial and behavioral recognition technologies,  including for security purposes, in  ways that may constrain learning  and appropriate risk taking.
14The role of artificial intelligence in children’s lives—from how children play,  to how they are educated, to how they  consume information and learn about  the world—is expected to increase  exponentially over the coming years.  Thus, it’s imperative that stakeholders come together now to evaluate the  risks of using such technologies and  assess opportunities to use artificial  intelligence to maximize children’s Conclusion  wellbeing in a thoughtful and systematic manner. As part of this assessment,  stakeholders should work together to  map the potential positive and negative  uses of AI on children’s lives, and develop a child rights-based framework  for artificial intelligence that delineates  rights and corresponding duties for  developers, corporations, parents, and  children around the world. The authoring team of this memorandum are Mélina Cardinal-Bradette, Diana  Chavez-V arela, Samapika Dash, Olivia Koshy, Pearlé Nwaezeigwe, Malhar Patel,  Elif Sert, and Andrea Trewinnard, who conducted their research and writing under  the supervision of Alexa Koenig of the UC Berkeley Human Rights Center. The  authors thank all of members of the Child Rights W orking Group for their advice  and input, and especially thank Jennie Bernstein for her careful edits and patient  guidance throughout the process of researching and drafting this memo.
151 Cedric Villani, “For a Meaningful Artificial  Intelligence T owards a French and European Strategy,”  March 8, 2018, available at https://www.aiforhumanity.fr/ pdfs/MissionVillani_Report_ENG-VF.pdf.   2 “Office of Innovation, UNICEF Office of Innovation,”  UNICEF Innovation Home Page, available at https://www. unicef.org/innovation/. 3 “Human Rights Center,” Human Rights Center Home  Page, available at https://humanrights.berkeley.edu.  4 UN General Assembly, “Convention on the Rights of  the Child, 20 November 1989,” United Nations, Treaty  Series, vol. 1577, p. 3, Article 1. 5 United Nations News, “UN lauds Somalia as country  ratifies landmark children’s rights treaties,” 20  January 2015, available at https://news.un.org/en/ story/2015/01/488692-un-lauds-somalia-country-ratifieslandmark-childrens-rights-treaty. 6 UNICEF, “Convention on the Rights of the Child -  Frequently Asked Questions,” UNICEF,  30 November  2005, available at https://www.unicef.org/crc/index_30229. html. 7 Geraldine V an Bueren, “The International Law on the  Rights of the Child,” in International Studies in Human  Rights, (Dordrecht/Boston/London: Martinus Nijhoff  Publishers, 1995), 10-15. 8 UN General Assembly, “Convention on the Rights of the  Child,” United Nations, Treaty Series, vol. 1577, (November  20, 1989), p. 3, Article 20 and Article 22. 9 Y ouT ube is a subsidiary of Google, whose parent company  is Alphabet, Inc. 10 “2017 Brand Love Study: Kid & Family Trends,”  Smarty Pants: the Y outh and Family Experts (2017), 14. 11 Ibid. at 7. 12 Ibid. 13 Alexis Madrigal, “Raised by Y outube,” Atlantic 322, no. 4  (November 2018): 72–80. 14 “Introducing the Newest Member of Our Family, the  Y ouTube Kids App--Available on Google Play and the App  Store,” Official Y ouT ube Blog, https://youtube.googleblog. com/2015/02/youtube-kids.html; “Y ouT ube Kids,” https:// www.youtube.com/yt/kids/.  15 Karen Louise Smith and Leslie Regan Shade, “Children’s  Digital Playgrounds as Data Assemblages:  Problematics of Privacy, Personalization and  Promotional Culture,” Big Data & Society, V ol. 5 (2018),  at 5. 16 Adrienne LaFrance, “The Algorithm That Makes  Preschoolers Obsessed With Y ouTube Kids,” The  Atlantic, July 27, 2017, https://www.theatlantic.com/ technology/archive/2017/07/what-youtube-reveals-aboutthe-toddler-mind/534765/. 17 “T erms of Service - Y ouTube,” https://www.youtube. com/static?template=terms, (November 13, 2018); Matt  O’Brien. “Consumer Groups Say Y ouT ube Violates  Children’s Online Privacy,” Time.Com, April 10, 2018, 1. 18 Madrigal, “Raised by Y outube,” 80.  19 Matt O’Brien, “Consumer Groups Say Y ouTube  Violates Children’s Online Privacy,” Y ahoo! News,  April 10, 2018, https://news.yahoo.com/consumer-groupsyoutube-violates-children-012240300.html.  20 O’Brien, “Consumer Groups Say Y ouTube Violates  Children’s Online Privacy.” 21 Elias, Nelly, and Idit Sulkin. “Y ouTube Viewers in  Diapers: An Exploration of Factors Associated with  Amount of T oddlers’ Online Viewing.” Cyberpsychology,  November 23, 2017, at 2, available at https://cyberpsychology. eu/article/view/8559/7739. 22 Madrigal, “Raised by Y outube,” 79. 23 Adrienne LaFrance, “The Algorithm That Makes  Preschoolers Obsessed With Y ouTube Kids.” 24 Ibid. 25 James Bridle, “Something Is Wrong on the Internet,”  Medium, November 6, 2017, at https://medium.com/@ jamesbridle/something-is-wrong-on-the-internetc39c471271d2.  26 Elias, Nelly, and Idit Sulkin. 2.  27 Ibid. 28 Ibid. 29 Sarah Perez, “Over 20 advocacy groups complain  to FTC that Y ouTube is violating children’s privacy  law,” T echCrunch, April 9, 2018, https://techcrunch. com/2018/04/09/over-20-advocacy-groups-complain-to-ftcthat-youtube-is-violating-childrens-privacy-law/ 30 Ibid.31 “Request to Investigate Google’s Y ouTube Online  Service and Advertising Practices for Violating the  Children’s Online Privacy Protection Act,” 2, https:// www.law.georgetown.edu/wp-content/uploads/2018/08/FiledRequest-to-Investigate-Google%E2%80%99s-YouTubeOnline-Service-and-Advertising-Practices-for-ViolatingCOPPA.pdf 32 Ibid. 33 Sapna Maheshwari, “New Pressure on Google and  Y ouTube Over Children’s Data,” NY Times, September 20,  2018, https://www.nytimes.com/2018/09/20/business/media/ google-youtube-children-data.html 34 Cecilia Kang, “Y ouTube Kids App Faces New  Complaints Over Ads for Junk Food,” NY Times,  December 21, 2017, sec. T echnology, https://www.nytimes. com/2015/11/25/technology/youtube-kids-app-faces-newcomplaints.html 35 Smith and Shade, “Children’s Digital Playgrounds,” 5. 36 Laura Rafferty, Patrick C. K. Hung, Marcelo Fantinato,  Sarajane Marques Peres, Farkhund Iqbal, Sy-Y en Kuo, and  Shih-Chia Huang, “T owards a Privacy Rule Conceptual  Model for Smart T oys” in Computing in Smart T oys,  85-102, available at https://link.springer.com/content/ pdf/10.1007%2F978-3-319-62072-5_6.pdf. 37 Chris Nickson, “How a Y oung Generation Accepts  T echnology,” A T echnology Society, September 18, 2018,  available at http://www.atechnologysociety.co.uk/howyoung-generation-accepts-technology.html. 38 Rafferty et al., “T owards a Privacy Rule,” https://link. springer.com/content/pdf/10.1007%2F978-3-319-62072-5_6. pdf. 39 Benjamin Y ankson, Farkhund Iqbal, and Patrick C. K.  Hung, “Privacy Preservation Framework for Smart  Connected T oys,” Computing in Smart T oys, https://link. springer.com/content/pdf/10.1007%2F978-3-319-62072-5_6. pdf, 149-164. 40 Rafferty et al., “T owards a Privacy Rule,” https://link. springer.com/content/pdf/10.1007%2F978-3-319-62072-5_6. pdf. 41 Alex Hern, “CloudPets stuffed toys leak details of half  a million users,” The Guardian, https://www.theguardian. com/technology/2017/feb/28/cloudpets-data-breach-leaksdetails-of-500000-children-and-adults, (February 28, 2017). 42 Corinne Moini, “Protecting Privacy in the Era of  Smart T oys: Does Hello Barbie Have a Duty to Report,”  25 Cath. U. J. L. & T ech 281, (2017), 4. 43 “Hello Barbie FAQs,” at 4-5. 44 Corinne Moini, Protecting Privacy in the Era of Smart  T oys: Does Hello Barbie Have a Duty to Report, Catholic  University Journal of Law and T echnology, no.25 (2017): 4;  Hello Barbie FAQ 45 Vlahos, James, “Barbie W ants T o Get T o Know Y our  Child,” New Y ork Times, (September 16 2015). 46 Mattel, “Hello Barbie Frequently Asked Questions,”  (2015), http://hellobarbiefaq.mattel.com/wp-content/ uploads/2015/12/hellobarbie-faq-v3.pdf. 47 Ibid. 48 Ibid. 49 Ibid. 50 Federal Trade Commission, “KidSafe Seal Program:  Certification Rules V ersion 3.0 (Final),” https:// www.ftc.gov/system/files/attachments/press-releases/ ftc-approves-kidsafe-safeharbor-program/kidsafe sealprograms certification rules ftcapprovedkidsafe-coppaguidelinesfeb_2014.pdf [hereinafter KIDSAFE SEAL  PROGRAM], (2014). 51 Federal Trade Commission, “KidSafe Seal Program,”  (2014).  52 Corinne Moini, “Protecting Privacy in the Era of Smart  T oys: Does Hello Barbie Have a Duty to Report,” 25  Cath. U. J. L. & T ech 281(2017), 12-291.  53 Moini, Corinne, “Protecting Privacy in the Era  of Smart T oys: Does Hello Barbie Have a Duty to  Report,”4. 54 Hello Barbie FAQs, supra note 3, at 4-5. 55 CAL. PENAL CODE 11165.7 (2016). 56 “Phrasee’s AI Ethics Policy,” (2018), https://phrasee.co.  57 “Children’s Online Privacy Protection Rule,” 16 C.F.R.  312.1 (2001). 58 James Vincent, “Google Releases Free AI T ool to Help  Companies Identify Child Sexual Abuse Material,” The  V erge, https://www.theverge.com/2018/9/3/17814188/googleai-child-sex-abuse-material-moderation-tool-internetwatch-foundation, September 03, 2018.59 Mattel, “Hello Barbie Frequently Asked Questions.” 60 Anissa Baker Smith, “Educ-AI-tion Rebooted?,” Nesta,  https://www.nesta.org.uk/report/education-rebooted/ 61 Ibid. 62 Brain Power, “ About Us,” http://www.brain-power.com/ 63 Jackie Snow, “AI T echnology is disrupting the  traditional classroom,” https://www.pbs.org/wgbh/nova/ article/ai-technology-is-disrupting-the-traditional-classroom/ 64 Jackie Snow, “AI T echnology is disrupting the  traditional classroom,” https://www.pbs.org/wgbh/nova/ article/ai-technology-is-disrupting-the-traditional-classroom/ 65 Jon-Chao Hong, Kuang-Chao Y u, and Mei-Y ung Chen,  “Collaborative Learning in T echnological Project  Design,” International Journal of T echnology and Design  Education 21, no. 3 (August 2011): 335–47.; Mazzoni, Elvis, and  Martina Benvenuti, “ A Robot-Partner for Preschool Children  Learning English Using Socio-Cognitive Conflict,” Journal of  Educational T echnology & Society 18, no. 4 (2015): 474–85.;  Barak, Moshe, and Y air Zadok, “Robotics Projects and Learning  Concepts in Science, T echnology and Problem Solving,”  International Journal of T echnology and Design Education 19,  no. 3 (August 2009): 289–307.  66 Emmeline T aylor, “Surveillance Schools: A New  Era in Education,” in Surveillance Schools: Security,  Discipline and Control in Contemporary Education  (London: Palgrave Macmillan UK, 2013), 15–39,  https://doi. org/10.1057/9781137308863_2. 67 Joy Buolamwini, Timnit Gebru, “Gender Shades:  Intersectional Accuracy Disparities in Commercial  Gender Classification,” Conference on Fairness,  Accountability, and Transparency: Proceedings of Machine  Learning Research, (2018), 81.  68 Anthony Cuthbertson, “Police Trace 3,000 Missing  Children in Just Four Days Using Facial Recognition  T echnology,” The Independent, https://www.independent. co.uk/life-style/gadgets-and-tech/news/india-police-missingchildren-facial-recognition-tech-trace-find-reunite-a8320406. html, (April 24, 2018). 69 Article 19, “The Global Principles on Protection of  Freedom of Expression and Privacy,” op.cit. 70 Rich Haridy, “ AI in Schools: China’s Massive and  Unprecedented Education Experiment,” New Atlas - New  T echnology & Science News, https://newatlas.com/china-aieducation-schools-facial-recognition/54786/, (May 28, 2018). 71  Article 19, “Privacy and Freedom of Expression in the  Age of Artificial Intelligence,” https://www.article19.org/ wp-content/uploads/2018/04/Privacy-and-Freedom-ofExpression-In-the-Age-of-Artificial-Intelligence-1.pdf, (2018),  8.  72 William Michael Carter, “Big Brother Facial Recognition  Needs Ethical Regulations,” Phys.org, https://phys. org/news/2018-07-big-brother-facial-recognition-ethical. html#jCp. (July 23, 2018). 73 Ibid. 74 GDPR Article 5(1)(a). 75 “Microsoft Salient Human Rights Issues,” Report -  FY17, Microsoft. file:///Users/dreatrew/Downloads/Microsoft_ Salient_Human_Rights_Issues_Report-FY17.pdf; Google,  “Responsible Development of AI” (2018). 76 Microsoft, “The Future Computed: Artificial  Intelligence and Its Role in Society” (2018). 77 Alexa Hern, “Partnership on AI” Formed by Google,  Facebook, Amazon, IBM and Microsoft,” The Guardian,  (September 28, 2016). 78 John Gerard Ruggie, “Global Governance and New  Governance Theory,” Lessons from Business and Human  Rights, Global Governance 20, http://journals.rienner.com/ doi/pdf/10.5555/1075-2846-20.1.5, (2014), 5. 79 Council of Europe, “Recommendation CM/REC (2018)7  of the Committee of Ministers to member States on  Guidelines to respect, protect and fulfil the rights of the  child in the digital environment,” (July 4th 2018). 80 Cedric Villani, “For a Meaningful Artificial Intelligence  T owards a French and European Strategy,” (March 8th  2018).   81 Ibid. 82 NITI Aayog, “Discussion paper: National Strategy for  Artificial Intelligence,” http://niti.gov.in/writereaddata/files/ document_publication/NationalStrategy-for-AI-DiscussionPaper.pdf, (June 2018), 7.References
16 Attribution-ShareAlike 3.0 Read the full report here: unicef.org/innovation/  reports/memoAIchildrights
