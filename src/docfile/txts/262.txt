STUDY   Panel for the Future of Science and  Technology     EPRS | European Parliamentary Research Service   Scientific Foresight Unit (STOA)   PE 729.516 – May 2022 EN                                           AI and digital  tools in  workplace  management  and evaluation   An assessment of  the EU’s legal  framework   

       AI and digital tools  in workplace  management and  evaluation   An assessment of the EU 's legal framework     This  study focuses on  options for regulat ing the use of  AI enabled  and algorithmic management systems in the world of work  under  EU  law. The first part describes how these technologies  are already   being  deployed, particularly in recruitment, staff appraisal, task  distribution and disciplinary procedures . It discusses some  near term  potential development prospects and presents an impact  assessment , highlighting some of the se technologies ' most  significant implications .  The secon d part addresses the regulatory field . It examines  the  different  EU regulations and directives that are already relevant to  regulat ing the use of AI in employment.  Subsequently, it  analyses   the potential labour and employment implications of the European   Commission 's proposal for a regulation laying down harmonised  rules on artificial intelligence (AI act). Finally, it summarises  the other   ongoing  EU policy debates relevant to the regulation of AI at work .  The third and final part of this study  reflects in detail upon  the AI act  and its potential impact on  the existing  EU social acquis . On this  basis, it advances  potential  policy options across different  EU  legislative files , including but not limited to the AI a ct, to ensure that  regulation keeps pace with technological development . It also  argues that  the AI act should ' serve ' and complement  – rather than  over -ride – other regulatory standards  that can already govern the  introduction and use of AI -enabled  and algorithmic -management  systems at work .    
STOA  | Panel for the Future of Science and Technology     II AUTHOR S  This study has been written by Prof essor  Valerio De Stefano  of Osgoode Hall Law School, York University,  Toronto, Ontario and the KU Leuven (University of Leuven) , and Dr Mathias  Wouters  of KU Leuven (University  of Leuven)  at the request of the Panel for the Future of Science and Technology (STOA) and managed by the  Scientific Foresight Unit , within the Directorate -General for Parliamentary Research Services (EPRS) of the  Secretariat of the European Parliament. The r esearch leading to this study  was also carried out in the  framework of the 'Employment rights and labour  protection in the on -demand economy ' grant awarded by   the FWO Research Foundation – Flanders  to Prof essor  De Stefano . The authors  would  also  like to express their  gratitude to Prof essor  Frank  Hendrickx and Simon  Taes for their comments and suggestions.      ADMINISTRATOR RESPONSIBLE   Philip Boucher , Scientific Foresigh t Unit  (STOA)   To contact the publisher, please e -mail stoa@ep.europa.eu     LINGUISTIC VERSION   Original: EN   Manuscript completed in March 2022.             DISCLAIMER AND COPYRIGHT   This document is prepared for, and addressed to, the Members and staff of the European Parliament as  background material to assist them in their parliamentary work. The content of the document is the sole responsibility of its author(s) and any opinions expressed herein should not be taken to represent an official position of the Parliament.   Reproduction and translation for non -commercial purposes are authorised, provided the source is  acknowledged and the European Parliament is given prior notice and sent a copy.   Brussels © European Union, 20 22.   PE 729.516   ISBN: 978-92-846-9458-7  doi:10.2861/305539   QA-07-22-330- EN-N    http://www.europarl.europa.eu/stoa  (STOA website)   http://www.eprs.ep.parl.union.eu  (intranet)   http://www.europarl.europa.eu/thinktank  (internet)   http://epthinktank.eu  (blog)     
AI and digital tools in workplace management and evaluation     III Executive s ummary   Artificial intelligence (AI) is spreading into workplaces in many ways. W orkers  and employers will  increasingly be confronted with AI applications  and AI-induced software  that encroach on labour  relations. These AI systems could be introduced with an explicit aim to improve working conditions ;  however, this cannot be expected in gene ral, and evidence suggests that this will instead  be the  exception . Rather, for policy -makers and legislators, it will be a question of ensuring that the  introduction  and operation  of AI in the workplace are carried out  in a manner  that is mindful of  AI's  impact on working conditions . Furthermore  regulation will  have to force businesses to minimise  AI's  negative impact  on labour protections .  To this end, this study present s policy options intent on  preserv ing an adequate level of labour and  social protections in light of the introduction of AI  systems  at work . These policy options are limited  to reforms  that the European  Union 's institutions  can introduce . They are developed throughout  this study by , first , exploring  and assessing the impacts of how new robots, ' smart ' work equipment,  software, wearables and other technological tools will increasingly collect workers ' data  used by  various forms of AI to pursue their human -defined organi sational goals. The study then examines  the extent to which current European law s govern the use of AI at work  and analyses the interaction  between existing laws and the European Commission 's proposal laying down harmonised rules on  AI (AI act) and  the latter 's relevance concerning  working conditions , providing the building blocks  for the policy options.   The key findings of this study include the following .  AI will become prominent in the workplace – Over time, many forms of software , both industry specific and  generalist , will acquire AI functionalities that will impact working conditions. AI, as a  general technology, is likely to become a prominent feature in many people 's jobs in the next few  years. At the same time, a more limited group of AI technolog ies with particularly far-reaching  effects on workers will initially become prominent in some industries more than others.  At least  shortly , not all industries will be characterised by  equally intrusive AI systems.  Certain businesses,  for instance, engaged in lo gistics and stock management, seem to already extensively rely on AI enabled tools and applications to drive their economic activities. H owever, in other sectors,  developments are so far more limited, even if this could change in time . This will be made po ssible,  among other things, by the elaboration of generic AI products, which will be marketed as being  adaptable across industries. Likewise, c onsultancy companies will become more experienced in  proposing AI solutions to business challenges. Moreover, gen eric software packages, such as those  for human resources management (HRM), will also increasingly include some standard AI features.  Hence, AI will become more commonplace in managing  people at work  throughout the econom ies  of industrialised countries .  Introducing and operating  AI at work poses challenges for businesses  – Although it might seem  as if AI will eventually take the work floor by storm, some of our findings suggest the opposite. Firstly, employers will have to carefully reflect on whether and  how AI could best serve their needs by  considering  existing employment and labour protections and rights, including fundamental ones.  Leaving aside AI applications of minor importance, companies will have to think long and hard about how they can incorpor ate AI into their business structure to obtain the best results. Many of  the best tools will likely come with a hefty price tag, require additional investment in material and  personnel, or demand adjustments to companies ' work organisation. Therefore, before committing  to far -reaching AI -based systems, employers will have to examine the business case for it. This also  presents an opportunity for law and policy to intervene. As already happens in many EU countries,  the law can demand companies to fully consi der workers ' concerns, interests and rights before  implementing new technologies , something which  would be entirely justified by several reasons  discussed in the study.  
STOA  | Panel for the Future of Science and Technology     IV The functionality and utility of many forms of AI at work remain in doubt  – For example , many  of the AI tools currently deployed in recruitment remain untested. As the literature points out, there  are many valid reasons to adopt a cautious and critical attitude towards AI -powered recruitment  applications. Similarly, in the future, AI will li kely lead to more continuous staff appraisal. Instead of  evaluating workers ' performance only a couple of times a year, AI systems are projected to provide  more continuous feedback. Yet , despite the theoretical promise of AI in this area, it seems rather  complicated to effectively incorporate AI in staff appraisal in practice. For instance , how could  there   be any certainty that AI is obtaining and using all necessary data to reach a certain conclusion? Current systems effectively collect and compile  data, b ut interpreting such data is much trickier,  especially if humans are not allowed to correct a system 's inferences.   Furthermore, some companies already rely on algorithms and AI to distribute tasks, monitor workers  and evaluate their performance and attitu des. Reliance on such sweeping ' surveillance loops ' is still  somehow limited, but, in time, these may spread. Many more businesses will rely on AI to evaluate  workers, after which the AI 's findings will directly influence the decisions of managers. A criti cal issue  in this respect is that businesses risk trusting AI disproportionately and unwarrantedly. One crucial way to better assess whether these systems function adequately will be by informing, consulting  and negotiating with workers and their represent atives.   AI is promising for the world of work, but it is not a silver bullet. It requires a lot of effort to be  implemented successfully without producing excessive adverse effects that, among other things, are also highly detrimental to  workers.   Business es that lead in developing AI technologies will set the lines that other employers  follow; therefore, action is urgent ly needed  – Many employers still lack the hardware, data, staff,  etc., required  to rely on AI  purposefully . However,  some businesses at the forefront of this  technology are already pacing ahead, setting out the lines that many other employers will soon follow. Therefore, we must urgently pay attention to the businesses and the sectors most affected  by these developments and  have  broader discussions about what goals AI should  serve. The time  to set and enforce clear boundaries to dissuade investment in illegal, unethical and undesirable AI applications  is now . This is a challenge that European institutions c ould face immediately .   AI has evident  drawbacks in terms of its impact on working conditions  – AI will , among other  things, bolster the datafication of work, making data protection and privacy rights all the more  important as a counterweight. AI is likewise poised to ma ke work more precarious, boost businesses '  surveillance capabilities, and, in some instances, even frustrate union activities. Furthermore,  despite some proclaimed promises of AI in occupational safety and health  (OSH)  and nondiscrimination, these technol ogies also present clear threats in these areas. Overall, there are  genuine risks to the incorporation of AI at work.   Drawbacks that can be counteracted through regulation  – Despite the severe potential erosion  of working conditions, our review of the EU 's regulatory context indicates  that,  with the necessary  adjustments, much of the existing regulation can continue to function. Indeed, it can steer the  implementation of future AI at work in a d esirable direction.  For this to occur,  we outline here some  of the most relevant options  – a much more extensive list can be found in C hapter  5.  Based on the European Union 's primary law, t he European response to AI at work needs to, among  other things, pr imarily focus on fundamental and human rights. ' AI ethics ' is important; however,  the protection of established rights should not be watered down by generic discourses about ethics.  The European Union' s Charter of Fundamental Rights enshrines a crucial  range of personal, civil,  political, economic and social rights. Any rethinking of the EU 's secondary law in response to AI  should also pay due regard to the tension between certain AI applications and those rights. T he  potential benefits of AI do not justify  infringing these rights. It is up to the European  institutions and  the Member States to keep AI from endangering decent and just working conditions.  
AI and digital tools in workplace management and evaluation     V For example, the principles enshrined in the General Data Protection Regulation (GDPR) provide an  essentia l framework to mitigate the negative consequences of AI at work, especially but not solely  from a privacy perspective. Any AI 's goals and functioning inevitably depend on the data available.  Data protection laws can thus effectively  establish boundaries. A ssuming these rules are adequately  enforced, which is the real challenge, the GDPR 's principles of lawfulness, fairness, transparency,  purpose limitation, data minimisation and accuracy can significantly mitigate the risk that harmful  AI systems are elabor ated and implemented in the EU. At the same time, nothing in the GDPR  prevents AI developers and employers from carefully introducing AI at work as long as the  fundamental rights and interests of all the data subjects, particularly the workers,  are conside red.  Besides the GDPR, which is fundamentally important for the protection of workers against  undesirable  AI, EU law also provides workers ' representatives and social partners with a muchneeded legal basis to claim consultation rights when AI is introduced to the workplace. Most  notably, EU Directive  2002/14/EC  enshrines a right to information and consultation that can operate,  for instance,  when an AI or an algorithmic tool can lead to substantial changes in work organisation.  In this respect, however, Member States ' national law s need to be reviewed to ensure the directive  is adequately implemented. EU law provides some basis to subject AI to 'systematic ' social dialogue.  Yet, a t present, Member States often protect and promote information and consultation rights  stemming from EU law some what unsatisfactorily. Concerning AI, this could be addressed by better  clarifying the extent and scope o f these rights.   Similarly, under current rules, employers should already perform risk assessments to identify and  address OSH risks when introducing AI into the workplace. Some  analyses, however, have warned  that to the extent it merely concerns software, employers may underestimate the potential OSH  risks, including psychosocial risks, brought forth by AI. For this reason, future guidance on how to  assess AI from an OSH perspective, be it in the form of a soft law or a hard law instrument, could be highly valuable . The duty of employers to assess and react to the OSH risks associated with certain  forms of AI must , indeed, be stressed. It is a very reasonable objective for policy -makers to pursue  by building on the existing OSH framework.   EU non -discriminati on laws are likewise already somehow being used to vet AI systems in  the  workplace. However, t hese laws ' effectiveness  largely depends on the ease with which the burden  of proof in court is placed on the user of the AI  system instead of on the workers. In principle,  claimants must bring  prima facie evidence of discrimination before the burden of proof shifts to the  defendant. Legislatures and courts will play a vital  part in determining at which point the entity in  charge of the AI is obliged to disprove al legations of discrimination. Alternative procedures available  outside court to scrutinise AI systems, such as  certification or specialised auditing, will also be crucial  in combating discriminatory applications of AI and algorithmic -management systems. The  most  effective of such procedures could eventually find their place in the regulations. The burden of proof for claimants in court should , for example, be lowered in case an AI system is not certified. Various  such interactions are conceivable.   Regulatory  changes will be required in various fields of law  – By and large, across multiple areas   of labour and employment legislation, one could conclude that governing AI at work is not unfeasible, provided that the rules are adequately adapted to the challenges AI poses.   However , this somewhat positive overall assessment of current EU law does not  mean we can be  unconcerned  about any of these matters. In fact, without the necessary regulatory changes and  investments into effective enforcement mechanisms  in various areas , AI may well spiral out of  control at work and elsewhere. Moreover, even with s olid governance structures, things may still  become unpredictable. For instance, as noted in the study, in the long run, how AI systems will impact working time and workers ' contractual relations is still largely unknown. In this sense, even  though some ac tions, such as limiting the use of successive short- term employment contracts, can  also already be taken, policy -makers should arguably adopt a realistic approach  to the regulation of 
STOA  | Panel for the Future of Science and Technology     VI AI. Concrete measures should arguably be taken soon. This study argues t hat amendments are  possible and desirable to various labour laws; added together, they would result in a legal  environment that, taken as a whole, becomes much more capable of governing the use of AI at work.   The AI a ct should  bolster the effectiveness of other laws in governing AI  – With this in mind,  looking at the current situation from an employment perspective, we believe that the European Commission 's proposed AI act could strengthen  existing regulatory mechanisms to protect working  conditions . However, quite to the contrary, the current draft AI act arguably imposes a regulatory  'ceiling ', implicitly weakening the capacity of various existing regulatory mechanisms. Indeed, the  current draft would issu e an all -encompassing framework meant to determine what AI is  trustworthy, after which such 'trustworthy ' AI receives almost free reign to enter the workplace.  Many existing work protection  systems will be 'gutted ' rather than enhanced  by this approach. For  all the reasons  discussed in C hapter  4, this study argues that it is vital to put the AI act at the service  rather than above the other laws that could govern the introduction and use of AI and algorithmic   management systems in the work environment. This will require material amendments to the  current draft, something which warrants serious scrutiny of this proposed instrument.   Additionally, the AI act intersects with many other EU policy initiatives that will frame the future of  AI systems in European workplaces. Therefore, i t is important to coordinate these various initiatives.  In this regard, for example, the recent draft directive on platform work contains a chapter on  algorithmic management that offers valuable and yet improvable protection s for platform workers.  However, unless the AI act in cludes similar measures, all workers beyond platform work ers will  remain much more vulnerable to the risks posed by AI and algorithmic management at work. One  can also question whether the draft regulation is entire ly in line with the GDPR .  The AI a ct shou ld be a participant instead of  an arbiter in the field of labour protection  – The  AI act might succeed in regulating AI as such. Still  if the aim is to regulate ' AI at work ' adequately, we  argue that EU policy -makers should keep the many existing laws in mind and opt for a coherent set  of measures across different fields of law. Salvation will not come from the AI act alone. The  governance of AI at work could  draw on many more areas of law. Therefore, S ection  5.2 concludes  this study  by drawing  a material list of policy options in the context of different legislative files,  including but not limited to the AI a ct, that –  it is the opinion of the authors –  might  allow the  benefits of AI at work to be reaped by ade quately countering its relevant risks.     
AI and digital tools in workplace management and evaluation     VII Table of c ontents   1. Introduction: Setting the scene  __________________________________________________ 1  2. Methodology  _________________________________________________________________  4  3. Review of A I for HR management  _________________________________________________ 5  3.1. The technologies and tools  _____________________________________________________ 5  3.2. The application of AI in various contexts  __________________________________________ 10  3.2.1. Recruitment  _____________________________________________________________ 10  3.2.2. Performance management: Staff appraisal and professional development  ____________ 13  3.2.3. Task distribution, management and evaluation  __________________________________ 16  3.2.4. Retention, rewards and promotion ____________________________________________ 19  3.2.5. Disciplinary procedures  ____________________________________________________ 20  3.3. Near -term possible development prospects (next five years)  __________________________ 23  3.4. Assessment of impacts ________________________________________________________ 26  4. Review of the regulatory context  ________________________________________________ 33  4.1. Resolutions of the European Parliament  __________________________________________ 33  4.2. Current EU legislation  ________________________________________________________ 35  4.2.1. EU Primary law  ___________________________________________________________ 35  4.2.2. Secondary Law. The General Data Protection Regulation  __________________________ 35  4.2.3. Transparency and worker involvement  ________________________________________ 42  4.2.4. Occupational safety and health  ______________________________________________ 43  4.2.5. Working conditions  ________________________________________________________ 44  4.2.6. Anti -discrimination  ________________________________________________________ 47  4.3. Ongoing negotiations related to the AI Act ________________________________________ 49  4.4. Policy debates  ______________________________________________________________ 56  5. Policy options  ________________________________________________________________ 60  5.1. Critical points in the context of different legislative files ______________________________ 60 
STOA  | Panel for the Future of Science and Technology     VIII 5.2. An overview of policy options  __________________________________________________ 63  6. Conclusions  __________________________________________________________________ 70      
AI and digital tools in workplace management and evaluation     IX List of a bbreviations     AGI Artificial general intelligence   AI Artificial intelligence   AIaaS  AI as a service   AI act Regulation of the European Parliament and of the Council laying down harmonised  rules on artificial intelligence (Artificial Intelligence act) And amending certain union  legislative acts   AIDA  Special Committee on Artificial Intelligence in a Digital Age   BCI Brain -computer interface   CDEI  Centre for Data Ethics and Innovation   Charter  EU Charter of Fundamental Rights   CIPD  Chartered Institute of Personnel and Development   CJEU  Court of Justice of the European Union   CNIL  Commission Nationale de l 'Informatique et des Libertés   DPA  Data protection authority   DPIA  Data Protection Impact Assessment   DPO  Data protection officer   ECHR  European Convention on Human Rights   ECtHR  European Court of Human Rights   EDPB  European Data Protection Body   EDPS  European Data Protection Supervisor   EESC  European Economic and Social Committee   ETUC  European Trade Union Confederation   EU-OSHA  European Occupational Safety and Health at Work Authority   FLI Future of Life Institute   GDPR  General Data Protection Regulation   HBS  Harvard Business School   HRM  Human resource management   ICO Information Commissioner 's Office   ILO International Labour Organization   IP Internet protocol  
STOA  | Panel for the Future of Science and Technology     X ML Machine learning   NFC  Near -field communication   OECD  Organisation for Economic Co -operation and Development   OSH  Occupational safety and health   PwC  PricewaterhouseCoopers   SMS  Social media screening   TUC  Trade s Union Congress   WEF  World Economic Forum    
AI and digital tools in workplace management and evaluation     1 1. Introduction : Setting the scene  In 2016 Daniel Kahneman and colleagues  published a n article in the Harvard Business Review   detailing how unreliable humans are as decision- makers. As the authors contend ed, '[r]esearch has  confirmed that in many tasks, experts ' decisions are highly variable: valuing stocks, appraising real  estate, sentencing criminals, evaluating job performanc e, auditing financial statements, and more '  (Kahneman et al., 2016, 40). Th ese inherent errors in  human judg ements , leading to variability  between experts,  are an example of what the y refer to as  'noise '. In a subsequent book, t hese authors  argue d that alg orithms1 and artificial intelligence (AI)  may constitute a valuable improvement in this  respect  because  these tools  hardly  suffer from ' noise ' (Kahneman, Sibony and Sunstein, 2021).  In  principle, an  AI will reach the same outcomes based on the same inputs. Precise ly this is one of the  reasons why AI may become of crucial importance in businesses around the world . It promises  consistency , objectivity  and, in some instances, explicability. In this regar d, one could think of AI as  a brillia nt calculator: 'they are both machines designed to convert input into output in ways that  humans –who have minds –choose to interpret as meaningful ' (Heaven, 2021, 70). However, th is fact  also comes with its own set of ri sks.   A well -known example is COMPAS, which stands for ' Correctional Offender Management Profiling  for Alternative Sanctions '. Law enforcement used this  computer program to predict defendants '  likelihood of  committing a future crime. This AI tool was used  even though  '[t]he algorithm  underlying COMPAS is held as a trade secret by its manufacturer, Northpointe (now Equivant), which  means that we don' t know how COMPAS generates its predictions, nor do we have access to the  data the algorithm is trained on—so we cannot even inquire into its rationale ' (Babic et al., 2020, 65).  As it turns out, a ProPublica investigation discovered the algorithms were biased against African  American s (Angwin et al., 2016);  in other words,  this 'intelligent ' calculator was broken.   In a similar vein, as the upcoming chapters will illustrate, a wide range of AI  applications are being  used in the world of work that could – in theory  – improve human judg ement  and are purported to  do so. If the tools work ed, they could significantly improve working life. Along those lines, persons   could also be managed through AI -based systems . This would allegedly imply fewer  biases and  cronyism;  staff appraisals would be  entirely transparent and objective ; management could  also  be  warned by an AI when someone shows early signs of burnout ; staff scheduling could even take an  individual 's work -life balance into account . However , one of the biggest problems is that,  in reality,  most of these tools do not work . Some  tools might show too many false positives or negatives . For  example, it may falsely identify someone who is at risk of burnout while , at the same time, neglecting  a person who truly needs help but does not receive it since the AI is not picking up on the right  signals.  In other instances, AI -based  tools  may perform their task adequately but generate  undesirable effects . The risks connected to the use of AI as a managing tool may , in many cases ,  outweigh its opportunities.   As things stand, EU law will likely  play a significant role in shaping the future usage of AI at work .  European institutions have invested considerable resources in a  common  digital agenda and digital  single market. Recently, there have been various policy proposals in this field, covering, among  other things, AI , digital intermediation services, platform work and a right to disconnect. Many of  these legislative files  interlock  with each other . This is importan t because technological progress is  not being driven by just  one single technology,  such as AI. Instead, it is the interplay between several  mutually reinforcing technologies that drives  innovation  (Schwab, 2016 ). Still,  despite the need to   consider a rang e of technologies rather than just a  single one,  in order to provide this study with a  clear sense of direction , it will predominantly focus on AI . Other related developments, such as big    1 An algorithm is a ' set of rules  defining  how to  perform a  task  or solve a problem.  In the context  of AI, this  usually  refers to  computer  code defining how to  process data' (Boucher, 2020, VI).  
STOA  | Panel for the Future of Science and Technology     2 data, wearables  and people analytics, are also mentioned, but the research nonetheless remains  primarily focused on AI.   For the purpose of this study , AI is considered  as 'a system 's ability to interpret external data  correctly, to learn from such data, and to use those learnings to achieve specific goals and tasks  through flexible adaptation ' (Kaplan and Haenlein, 2019, 17). In that sense, AI is understood to be a  somewhat  precise operation performed by an ' intelligent ' system  that influences its environment  (OECD,  2019, 22) ; not all software should be understood to constitute an AI (Casilli, 2019) . The system  needs to have access to external data  (e.g., through sensors  or various datasets ), draw on that data  to improve its operations, and achieve its human- defined objectives  by executing tasks in an ever improving manner. Th is 'learning curve ', which  the system goes through , largely  distinguishes it  from 'standard ' automation, as practi sed for over a century.2 'Learning ', in this context,  can occur in  various ways. The more thought -provoking methods have to do with machine learning3, or also   deep learning, in which case the system does not require human  inputs  to improve  after it has been  put to work .4 Crucially, though, many f orms of AI  still rely on human inputs  to enhance  the  functioning  of algorithms . What is more, the people  engaged in these endeavours are not always  'experts '. As the World Economic Forum reminds us, '[o]ne of the challenges concerns the millions  of people who are contracted to collect and label the data used in machine -learning models. This  global workforce, sometimes referred to as the "invisible workers of AI, " is left largely in the shadows.  Ensurin g their ethical treatment requires more attention ' (WEF, 2021, 24).   Importantly , AI's 'intelligent ' features enable the system to operate in a more or less autonomous  manner . It also makes it possible  to replace forms of intellectual work.  The consequences  of this for  the labour market have already been described in the literature  (Frontier Economics, 2018; Martens  and Tolan, 2018; Lane and Saint -Martin, 2021; Gavaghan, Knott  and Maclaurin , 2021). Much of what  has been written focuses on the dangers of AI i n terms of un - and underemployment. In this regard,  it seems entirely plausible for unemployment issues  to arise in the long er run (Susskind, 2020) ;  hence, it is essential  to start thinking about how to deal with those issues  through policy and law   (Estlund, 2021). At the same time , it has been argued that for the time being, the impact of AI on the  economy and employment seems small  or even  negligible  (Littman et al., 2021, 59; Acemoglu et al.,  2021) . Therefore, w hat is arguably of more significant concern in the immediate future is not the  quantitative but the qualitative dimension of AI at work , and how to regulate AI systems ' impact on  working conditions here and now (De Stefano, 2019) .  This study explore s this question . It is, first and foremost,  a legal study , as set out in C hapter  2's  description of the methodology . However, labour law is m eant to be responsive to what happens  on the ground. As such, it is vital to understand  how AI already operates in the world of work. To this  end, Chapter  3 describes the various ways in which AI technologies and tools are already  employed  at work . It focuses , among other things,  on the use of AI in recruitment, staff appraisal , task  distribution  and disciplinary processes . Having  described how AI is already impacting the world of  work, Chapter  3 furthermore provides  a tentative picture of what can be expected to happen in the  next five years.  The chapter also  offers an impact assessment , highlightin g some of the most  significant impacts  of these developments and of the use of AI as a managing tool . Chapter  4 then  turns to the regulatory field with this assessment in min d. Its first section sets out what the European    2 'What may distinguish AI from other automation technologies, such as industrial robots and ot her automated machinery,  is its greater potential to expand the range of tasks that can be automated. This may be particularly the case with a  technology such as machine learning, which is specifically designed to self -improve ' (Lane and Saint -Martin, 2021, 20 21).  3 'Machine learning refers to second generation AI techniques whereby the algorithm is designed to find its own solution   to problems, rather  than following rules defined by  human experts ' (Boucher, 2020, VII).   4 For an extensive explanation of ho w AI works, see ( Lehr and Ohm, 2017;  Boucher, 2020; Devillé, Sergeyssels and Middag,  2021) . 
AI and digital tools in workplace management and evaluation     3 Parliament has achieved  so far  before its second and most extensive section discusses the various  EU regulations and directives that are already relevant to regulate the use of AI in employment. Its  third  section  analyses the Commission 's proposal for a regulation laying down harmonised rules on  artificial intelligence (AI a ct) and Section  4 summarises the other relevant policy debates ongoing at  the EU level. The fifth  and final chapter advances some  policy options  across the various legislative  files that could  be pursued to  improve the current state of the law.    
STOA  | Panel for the Future of Science and Technology     4 2. Methodology   The backbone of this study is a desk -based legal analysis of AI-enabled and algorithmic management  systems at work. Nonetheless, key insights were also provided by  sources different  from  legislation, case law and legal scholarship . Chapter 3  of the study especially draws on  management,  HR, economics , and  sociological  research. The aim of these chapters is not to provide  an exhaustive interdisciplinary literature review  on AI at work , which would go far beyond the scope  of this study.  It is, instead,  to give a  fair representation of what researchers in  these  fields have  argued about the potential impacts of AI at work  through academic publications or ' grey ' literature .  These account s of how AI -enabled and algorithmic management systems are  being implemented  in the world of work  inform s the legal analysis conducted in chapter 4  of this study .  This legal analysis primarily concentrates  on EU law. Therefore, it does not refer to  the domestic  legislation  of Member States  unless to give some specific examples,  even if the authors believe that  domestic labour laws  and industrial relations practices will be  essential  for the future regulation of  AI. Even without directly addressing domestic legislation, it emerges that t he areas of EU law  relevant to the regulation of AI in the work environment are already quite diverse. Some of these  areas of law have  arguably received more scholarly attention than others . This is also reflected in the  size of the relevant sub -sections  of this study . The legal analysis in chapter 4  aimed specifically at  identifying  the opportunities and pitfalls  in ex isting legislation by always paying heed to the   findings of  the abo vementioned  literature  on AI .  Subsequently, section 4.3. and, in particular, chapter 5 offer a general view on the potential impact   of the AI Act in light of the existing social acquis , based on the analysis carried out in the  previous   parts . At the end , various policy options are presented  and evaluated in terms of their potential to  counter the identified risks and enable beneficial application of AI to different elements of work .  Some of these  policy  options are based on arguments advanced  in the preceding  text of this study ,  while others can be found in different sources in literature, duly cited in the text . 
AI and digital tools in workplace management and evaluation     5 3. Review of AI for HR management   3.1. The technologies and tools   Before delving into the regulatory environment, section 3.1. of this chapt er mentions various AI induced technologies and tools that are already having an impact at work . Section 3.2 .  subsequently focuses on certain specific angles of AI's integration into the work  environment , such  as its use i n recruitment  and staff monitoring . Section 3.3.  discusses near term development  prospects . Lastly, section 3.4. contains an impact assessment, emphasising some of the major   impacts of the AI tools addre ssed in chapter  3.  Mapping all the existing AI applications on the  work floor is arguably impossible . Some sources  have  identified AI 'as the next ' general purpose technology ', the fourth since the beginning of the  industrial revolution after the steam engine, electricity and semiconductors ' (WEF, 2020, 7) .  Therefore, i t is reasonable to expect that this  technology  will influence the  world of work  in many  different ways.   One notable example is autonomous robots . These will be  used in both industry and services , and   are designed to function with little human interaction . In this regard, i t is best not to picture  the  stereotypical image of a human -like robot. Although these robots receive much  media and literary  attention, most ro bots will have a more application -oriented look, taking into account i.a. the  envi ronment and cost reduction strategies .   Many businesses already rely on semi- autonomous carts or drones to transport goods at the  workplace. Another clear example is the robots used in fruit cultivation to pick the fruits. Moreover,  robots must  not necessarily be mobile to be autonomous. At the German technology company  Bosch, if a certain welding station in one country works better than another , the other locations are  informed about how to make the necessary adjustments (Kreutzer and Sirrenber g, 2020, 97). Much  in the same way a modern car 's automatic driving features might learn how to avoid  a car crash on  the basis of data concerning an earlier accident elsewhere on the globe, the same may happen in  production processes.  These industrial tool s may change their operations based on the experience  gathered by similar tools elsewhere.   Compared to static  tools, such as a classic robot arm that continuously performs the same task on  the assembly line, many of these AI-enhanced  robots are projected  to operate  more autonomous ly  through space . Instead of having a fixed position at the assembly line, for example, a robot arm  could be mounted on the ceiling of an industrial kitchen so  that the device can manoeuvre through  the room. Businesses that rely on  autonomous robots are confronted with new risks  due to close  human- robot collaboration (Moore, 2019a , 5-7; Lane and Saint -Martin, 2021, 45- 46). Historically,  factories have increasingly adopted measures to shield workers from industrial robots to preserve  safety at work. AI 's ability to have robots perform  (semi -)autonomous actions  might make  interactions with workers  far less predictable, raising concerns from the point of view of labour law  (Taes, 2021).5 One of those concerns is that these devi ces may increasingly set the pace of work and   continuously  monitor the work productivity  of humans  in their vicinity.     5 Research has shown, for instance, that the more autonomous a robot is, the more people blame the robot for errors  instead of the co -worker operating  the robot (Furlough, Stokes and Gillan, 2019). With this in mind, one has to wonder  to what extent contempo rary rules that attribute liabilities among employers and employees , for instance, in case of a  work accident, can continue to operate. If people are generally inclined to blame autonomous robots for their ' errors ',  and implicitly the external company that  designed the robot , should rules be redesigned to also attribute liabilities  beyond the actors at the workplace itself?  
STOA  | Panel for the Future of Science and Technology     6 In addition to robots that operate (semi -)autonomously, there are also AI -driven co -bots  and  wearables . The latter can be  specifically designed to improve humans ' ability to perform work .  Exoskeletons and 'smart clothing ' that support the human body are examples ; besides supporting  muscles, they may likewise make workers aware of  unhealthy body posture s, inform ing them about  this through an app. Various other smaller 'wearables ' might  likewise be introduced into the  workplace. Think of s martwatches that some employers use  to incentivise  healthy living habits  (for  a critical assessment, see Ajunwa et al., 2017)  or 3D smart glasses provid ing workers with relevant  information while on the job  (PwC, 2016). S cholars  mention  at least six reasons for the adoption of  such technologies: (i) monitoring employees ' psychological and physiological state  (which also  poses  severe hazards  – Moore, 2020, 21- 26); (ii) enhancing operational efficiency; (iii) enhancing  collaboration; (iv) promoting a safe and secure work environment; (v) enabling  new industrial  practices ; and (vi) improving workers ' overall health (Khakurel, Melkas and Porras, 2018, 802 -803).   As these objectives indicate , wearable s may have positive implications for workers ; however,  besides the grave risks that we will analyse below in the chapter, their implemen tation can also lead   to a ' [f]eeling that one is being treated as a means to an end ' (Maltseva, 2020) 6.  In addition to wearables, there is also a whole range of other work tools  that are not worn by workers  but could  have an AI component in the future.  The Internet of Thing s, which allow s 'smart products '  to stay connected  and transfer  data , will make  it possible for daily work tools, such as an office chair,  to sense our use of that product . Furthermore, i n the case of office  work,  the computer, which is  often the primary  working tool, is already  digital. However, also workers engaged in industries such  as construction, that so far have required little digital tools compared to other sectors , might  increasingly see their activities tracked digitally  (Calvetti et al. , 2020) . Although the use of sensors in  that environment may be well -intentioned, most notably to improve health and safety  at work , their  implications for workers may be problematic.  Recent research in three high -tech Italian automotive  factories indicate s, for instance,  that the companies were more focused on digitalisation and  interconnection  than automation . Overall, according to the authors, the case study indicates that  'Industry 4.0 ' automation systems reduce  'room for employees ' autonomy and increases forms of  management control ' (Cirillo et al., 2021).   Having provided examples of how physical items such as robots, wearables and other objects can  collect or feed data to AI systems, we must also  reflect  on the implications of AI f or human resource  management  (HRM) . HRM has been described  as 'the management of work and people towards  desired ends ' (Boxall, Purcell and Wright, 2008, 1). In that sense, and based on the practices already  visible in our economies, it is reasonable  to as sume that  AI will increasingly feature in  many   enterprises ' HR polic ies. Inevitably, businesses will have to reflect on  how (semi -)autonomous robots  and other smart devices  can be implemented to reach the ir desired ends.  Arguably, even from a  purely managerial perspective, w orkers and their representatives should not be left out of th is  equation. A s economist Laura Nurski and others  have noted, ' [t]o make the most of AI, both  employers and workers need to be able to see its potentia l'. The speed with which new technologies  can be succe ssfully implemented at the work place  also  depends on workers ' cooperation . To  achieve this, technologies s hould arguably  be seen as working for workers, not against them  (Hoffman and Nurski, 2021; see also Tambe, Cappelli and Yakubovich, 2019, 34- 35), a concern that  is hardly adequately addressed in many current managerial practices.   To that extent, for example, it has been suggested that  management  at a hospital may want to frame   AI applications as tools to enhance doctors ' medical expertise. Instead of highlighting the  comparably better performance of AI in analysing medical images compared to human doctors, it  is preferable to stress how collaboration between AI and doctors  lead s to optimal results. In that  way, doctors feel supported by AI in pursuing their causes; such a narrative does not undermine    6 The author references (Twenge, Catanese, and  Baumeister, 2003)  and  (Bastian and Haslam, 2011) . 
AI and digital tools in workplace management and evaluation     7 their professional expertise, allowing them to remain  engaged in  'meaningful ' work (Smids, Nyholm  and Berkers, 2020, 511).  More broadly, beyond this one example, s ome argue that companies should  focus 'on creative new ways to configure groups of people and computers to accomplish valuable  tasks '. Such 'arrangements will enable computers to use their specialized intelligence to do what  they do best whil e people use their general intelligence and other skills to do what they do best '  (Malone, Rus and Laubacher, 2020, 21- 22)7. Some researchers claim this  type of approach will  lead  to better results than simply trying to replace  workers with  AI (Wilson and Daugherty, 2018; Babic et  al., 2020).   Therefore, many businesses will have to engage  in a thorough reflection about  if and how to use AI  at work , and all the more so if they will increasingly rely on AI -driven software  to perform HRM. As  researc hers from the International Labour Organization (ILO) have highlighted , in fact, ' AI is  replacing mental tasks rather than physical ones, which were the target of previous waves of  mechanization ' (Ekkehard, Merola and Samaan, 2018, 2; see also Prassl, 2019 ). Many white -collar  tasks  could, therefore, potentially be come  (partially) performed by AI -based software. A  distinction   can be made  in this respect between industry -specific and general software ; for instance, regarding  the former, AI-enabled  tools  aid lawyers in retrieving and classifying court cases or assist architects  in making design choices. Other AI -driven software  may, on the other hand, be more general,  becoming  utilised across many industries. A n example  would be incorporating  AI in packages  such  as widely used office suit es or other computer programs employed for administrative purposes.   Regarding industry -specific tools, AI seems to make significant strides in some areas.  Economists   David  Autor, David Mindell and Elisabeth Reynolds discuss a case in the field of auditing legal fees.   A big insurance company, for instance, has to hire legions of lawyers  each year. The ir legal bills must   be regularly audited . One such insurance company had AI developed to perform this auditing.  According to their study, ' [s]oon the system was yielding millions of dollars in annual savings, freeing  the auditors to move on to more complex work ' (Autor, Mindell and Reynolds, 202 0, 36; see also the  example of JP Morgan in Stancombe et al., 2018, 6).   In contrast to this somehow limited  task in the insurance sector , other businesses rely on much more  comprehensive information technologies . 'Warehouse management systems ' are already exerting  far-reaching influence over HRM in logistics . In research prepared for the MIT Task Force on Work of  the Future, 'an operations manager at a distribution center said: ' What I would really like is software  that keeps track of every person and every  robot on the floor and tells each of them what it should  do next ''. This kind of all- encompassing system has already become a reality  in some companies. As  the authors of th e report  stress, '[i]n fact, such software exists in a few large plants, but adapting it  more widely will require greater numbers of persons with technical skills ' (Mehta and Levy, 2020,  22). The same is arguably true in other parts of the supply chain, such as  trucking, where 'fleet  management systems ' perform  intrusive work perfo rmance monitoring (Levy, 2015). Hence, in some  sectors , AI-induced software  already exerts significant influence over  HRM and , consequently , over   working conditions .  Besides these industry -oriented software applications, s ome tools are specifically designe d to be  used across sectors by as many companies as possible . Most notably, scholars highlight the future  importance  of 'Artificial Intelligence as a Service (A IaaS) '. In this case, l arge cloud providers , such as  Amazon, IBM and Microsoft, provide their clients  with access to either  (i) 'technical environments  and resources to facilitate [them] in undertaking their own ML (sometimes called ' Machine Learning  as a Service '); or (ii) providing access to preb uilt models that [they] can essentially ' plug ' into their  applications ' (Cobbe and Singh, 2021, 4).  Tech c ompanies, in other words, can facilitate other  businesses in creating  their own AI -based systems or have them  rely on prebuilt models that can be    7 Reference is made to (Guszcza and Schwartz, 2020).  
STOA  | Panel for the Future of Science and Technology     8 fitted into their business environment.  In the latter case, client  businesses would decide  how to  operationalise this prebuilt model within the boundaries prescribed by the vendor.   Along similar lines,  tech companies  will also sell HRM software with AI fun ctionalities. For some time  now, companies have been digitalising  their HR practices. Paychecks are sent digitally instead of by  post. Employees  have to  apply for holidays  online. Expenses can also be submitted online. More  recently, however,  increasingly  complicated HR-related activities, such as recruitment, training and  performance appraisal , have likewise become digitalised.  The use of AI is reported to be promising   in this respect , especially for somewhat repetitive tasks (Hmoud and Laszlo, 2019, 26).  Management  scholars point to the potential value of AI , for example,  in: (i) deciding to who m jobs are offered; (ii)  recruiting suitable  candidates; (iii) ensuring recruits are up to speed  faster; (iv) managing  performance; (v) conceiving effective training methods; (vi) determining who is eligible for   promotions ; (vii) managing retention  and turnover ; (viii) and optimising employee benefits (Tambe,  Cappelli and Yakubovich, 2019, 19- 21).  While this impressive list seems to suggest AI will effectively dominate HR departments , those very  same authors also point out the limitations of these developments.  More specifically, s ome HRM   needs  might  limit AI 's potential . For one, it is observed t hat technology cannot answer the question  of who  is a 'good job applicant ' or a 'good employee '? Humans have to come up with the answer;  only then can technology  be put to work.  Likewise, o n what data points  should the algorithm base  its ev aluation to detect good employees ? Can all  the relevant factors even be captured in data  (Tambe, Cappelli and Yakubovich, 2019, 21- 23)? Research by Leighton Evans and Rob Kitchin , for  instance, has noted that the problem is not just that several work tasks , such as customer service,  can hardly be captured using employee performance metrics. That itself is already problematic . It  might mean workers decide to neglect customer service because it does not adequately count in  their performance statistics , or workers may start feeling  like they are being punished for doing what  is right  (taking good care of customers) . Another significant issue is that s ystems and equipment  meant to gather data  encounters frequent failures. 'These situations arise because although the  environment is highly data dependent, it relies in some cases on old digital technology that has  limited capability and lacks interoperability with other systems ' (Evans and Kitchin, 2018 , 51). In  other words, even if a business  had  unlimited resources , building a ' smart ' business would be far  from straightforward . Researchers  Prasanna Tambe, Peter Cappelli and Valery Yakubovich have  similarly observed how companies buy software from vendor X to track employee performance and  software from vendor Y to govern compensation and payroll. When sold by different vendors,  software are rarely compatible. Additionally, the authors note  how  'surprising ' it was  'to hear from  our respondents about the internal political battles over control over data. The payroll department,  for example, does not want to give its data to the talent acquisition department to let them see what  predicts which applicants are likely to take the most time off ' (Tambe, Cappelli and Yakubovich,  2019, 23 ; see also Vial et al., 2021) .  Bearing these experiences in mind, in contrast to its functioning under ' lab conditions ', adequate  functioning of AI in HRM in the real world is another matter entirely to achieve . Leaving aside  very  narrow AI  applications, such as AI-powered features in  office suit es, the effective  use of AI  technology would require  meticulous  planning , maintenance and implementation. This is true even  just by focusing on the operational problems for businesses ; Thorben Albrecht and Christian  Kellermann highlight that t he actual adoption of AI at the company level depends  on: (i) what it can  do, for example , given the amount of data available; (ii) what it is allowed to do in view of regulations  and ethics ; and (iii) what actual benefits it brings to the business considering its price tag . Since  many of these applications require a significant investment,  the return on investment must be  correspondingly high  (Albrecht and Kellermann, 2020, 5).  As such, one would  expect companies that  want to take full advantage of AI to thoroughly reflect on how to integrate it. This also  offers an  opportunity for law and policy . If, from a management perspective, businesses should, in any case,   carefully consider how AI serves their HRM, then why should the law not require  businesses to make  
AI and digital tools in workplace management and evaluation     9 certain considerations, for example, related to data protection , non -discrimination  or occupational  safety and health.     
STOA  | Panel for the Future of Science and Technology     10 To summarise:   • AI will be incorporated into businesses in various ways , both through hardware and  software applications .  • At least at first, f ar-reaching  AI technology may become more prominent in some  industries , such as logistics,  than others.   • Some businesses , such as tech companies, will develop AI features that other  businesses  will be able to incorporate in their organisations  (e.g. 'AIaaS').  • Businesses that wish to rely on AI will have to carefully reflect on how AI could  best  serve their HRM needs  without infringing workers ' fundamental rights . This presents  an opportunity for law and policy.  3.2. The application  of AI  in various contexts   3.2.1.  Recruitment   As a general rule, AI must have sufficient data  to make a meaningful contribution (Servoz, 2019, 22) .  One area of the labour market where such data is reportedly available is recruitment. Already in  2010, Kristie Ball reports that ' [i]n the US there are 20 million CVs stored in databases, and the US  Internet recruitment industry has attained the dubious accolade of being the second -largest source  of income for providers after pornography ' (Ball, 2010, 91).  Also, because of this copiousness of data,  AI-based recruitment tools have become increasingly common in our societies.   As one report remarks : 'General 'job websites ' such as Indeed, ZipRecruiter, and LinkedIn have  become large clearing -houses for job seekers and job advertisers ' (Gavaghan, Knott and Maclaurin,  2021, 13).  Websites like these allow employers to advertise their job postings to spec ific types of job  applicants. Employers are provided with targeting  options . Crucially, however, t his means that  some  jobseekers are  excluded from even seeing  particular  job opportunities . In this respect, wh ich  jobseeker – or group of jobseekers – is allo wed  to 'see' a specific job opportunity  is determined , on  the one hand,  by the employer 's choice to target certain categories  and , on the other, by  the  website 's algorithmic mechanisms underneath.8   Due to this process, even  this initial step – advertising jobs to particular sets of individuals – may  already lead to the unwarranted exclusion of jobseekers  and entire groups thereof  (Bogen and  Rieke, 2018, 17- 19; Kim and Scott, 2019).  Algorithms might, for example, disproportionally push job  advertisements  with a higher average income  towards men (Simons, 2020, 13- 16). This might not  even have only  to do with the employer 's targeting choices. S ome  computer scientists  observe  that  'during the ad delivery phase, advertising platforms can play an independent, central role in creating  skewed, and potentially discriminatory, outcomes ' (Ali et al., 2019, 23; see also Imana, Korolova and  Heidemann, 2021).   Besides using  AI in advertising job opportunities , AI systems  are also used to perform other tasks at  the early stage s of recruitment. For example, Johnson & Johnson and L 'Oréal have reportedly relied  on such systems to adjust their  language in job openings , significantly increasing interest  in their  positions  among wo men (Black and van Esch, 2020, 219).  Platforms , such as LinkedIn, Ziprecruiter  and CareerBuilder, might  eventually incorporate these features into their systems.   Besides the initial composition of the candidate s' pool , employers may use AI -enabled tools to  narrow it dow n, particularly when they  receive hundreds or even thousands of applications for    8 'For example, on ZipRecruiter, employers can opt to give incoming applicants a 'thumbs up. ' As ZipRecruiter collects  these positive signals, it uses a machine learning algorithm to identify other jobseekers in its system with similar  characteristics to those who have already been given a "thumbs up" —who have not yet applied for that role —and  automatically prompts them to apply ' (Bogen and Rieke, 2018, 20). 
AI and digital tools in workplace management and evaluation     11 specific  job profile s (Adler, Boyce and Caputo, 2017) .9 Tools such as  Recruitment Management  Systems and Applicant Tracking Systems c an be deployed to select job applications , allowing  recruiters to only pay attention to the most qualified candidates (Fuller et al., 2021, 8).  They allow  reducing significantly the time it takes for businesses to hire someone,  in some cases shortening it  from a n average of 24 to 9 days (Black and van Esch, 2020, 220). However, a s research conducted by  the Harvard Business School and Accenture highlights , these systems also have  significant  weaknesses.10 Too often, businesses continue to add requirements  that are needed  to pass th e initial  filtering. 'Over time, these requirements come to resemble the rings on a tree trunk; new  requirements are added to those accumulated over time. As the list of requirements and preferences  gets  longer, the number of applicants likely to qualify shrinks inevitably ' (Fuller et al., 2021, 22; see  also Cappelli, 2019, 52).   In other words, i f businesses rely on technology to automate these activities , the list of suitable   candidates risks being  artificially constrained  (as perfectly eligible candidates never make it to the  shortlist) . According to the same source , '[t]he net result: Millions of potential workers are effectively  ostracized from the workforce ' (Fuller et al., 2021, 36).  Other  scholars  have also noted that although  most stages of the recruitment process s till seem to involve human intervention, at this particular  stage of the process,  candidates that do not meet certain requirements may well be rejected on a  fully automated basis (Gavaghan, Knott and Maclaurin, 2021, 50).   Another issue  at this stage  relates to the reliance  on systems based on machine learning (ML)11 to  shortlist candidates. The goal is  to predict what persons will excel in a specific  function. However, a s  a theoretical case study from the Institute for the Future of Work indicates, these  systems tend to  reproduce the patterns found in the data on which they are trained. Consequently, ML may  disadvantage certain job applicants depending on the data used . Consider a call centre. Suppose  customer satisfaction ratings are used as a data source. In that cas e, the data  might show that  customers prefer workers that use language associated with t he (upper) middle class and, on  average, grant higher ratings to male workers . As those workers receive high ratings, the  recruitment system might disproportionally shortlist workers who  use that type of language  associated with a certain social background  or have a male -sounding voice (Simons, 2020, 6- 11). In  short, '[t]he process of ' cloning your best people ' can result in encoding human bias ', which, in turn,  could  constitute a discriminatory practice ( Ajunwa and Schlund, 2020 ). In this sense, based on the  idea  of 'AI as a Service ', some  companies will develop recruitment  software that any employer ,  willing to pay, can use. M uch in the same way various employers  might rely  on the same  recruitment  agency, these days , many employers  obtain  similar software aimed at selecting  job applicants from  business es such as Equifax or Kronos  – now called UKG  (Ajunwa and Greene, 2019, 62).  As multiple  employers use this technology , tackling the potential pitfalls mentioned above is increasingly  crucial .  Moreover, vendors  of recruitment software  may also advertise systems  with 'extravagant ' AIenabled tools  whose  functionality is  highly questionable . Some businesses  claim to identify human  traits and skills shared by the highest -performing people in different roles  through  AI-powered   chatbots  and phone calls , video games , and  AI-driven job interview s (Heilweil, 2019; Volini et al.,    9 This is actually not that uncommon. The intern et and all kinds of online labour market intermediaries have resulted in an  'avalanche in applications per position ' (Black and van Esch, 2020, 218).   10 For example, ' [a]lthough employing machine learning and semantics -based techniques have proved to assist  employers  in screening out irrelevant resumes, they still suffer from limitations, namely, semantic knowledge incompleteness and  limited domain coverage stemming from the resources (training data, ontologies and knowledge bases). This system is  evaluated in a real -world recruitment scenario by comparing manually calculated scores between resumes and job posts  with those produced by the system. The results have shown acceptable accuracy except for job offers that require special  skills ' (Maree, Kmail and Be lkhatir, 2019, 717).   11 'Machine learning refers to second generation AI techniques whereby the algorithm  is designed to find its own solution   to problems, rather  than following rules defined by  human experts ' (Boucher, 2020, VII).  
STOA  | Panel for the Future of Science and Technology     12 2019) . During the selection process  of a major food company, for instance, a pplicants have to play  online games  first, giving an  AI the chance  to identify traits like risk aversion. Subsequently ,  applicants hand in a video answering  questions relevant to the job position. That video is anal ysed  by an AI, which is supposed  to focus  on wording , body language, and tone  of voice . The 'best '  candidates are then invited for in -person interviews (Wilson and Daugherty, 2018, 122). Other   employers rely on AI  software to 'cybervet ' job applicants through social media screening (SMS). In  this respect , without even considering  privacy issues, 'researchers have cautioned against using SMS  in practice, citing a lack of a consistent research demonstrating SMS validity and utility ' (Hartw ell  and Eggli, 2020, 221; see also  Vaughn, Petersen and Gibson, 2019,  253- 254).   Many of the other assessment tools used in recruitment may also be flawed .12 Video games used to  select  job candidates might  be effective in measuring certain specific aspects. However, their  general  validity is questionable .13 Moreover, some remark that  the 'digital divide ' between citizens,  with some having better internet connections and ICT  devices or being more tech -savvy,  may lead  to unfairness  (Weidner and Short, 2019, 163- 164) , entailing  suboptimal selection . Furthermore ,  these tests may foster cheating. I n the past, all job applicants might have been invited on one single  day to come and conduct one single test resembling a n exam. As these tests and asse ssments  become digitalised and conducted every day of the week, it becomes more difficult to prevent  cheating and guarantee the integrity of the test' s results (Drasgow and Olson -Buchanan, 2017, 259 ;  see also Sche llmann and Strong, 2021b ). A relevant question is whether  AI integration can honestly  prevent this? If not , 'intelligent ' features would be built on top of very  shaky foundation s.  Other AI features  in recruitment  may likewise suffer from shortcomings. For instance, AI -driven job  interviews may attempt to assess an applicant 's personality based on infle xion and timber in their  voice . Yet, this can lead  to highly dubious  results (Wall and Schellmann, 2021 ; see also Schellmann  and Strong, 2021a )14. Facial recognition software is also sold to analyse job applicants ' facial  expressions;  there are many question s regarding the validity of any such findings as well (Raghavan  et al., 2020, 475).  Facial recognition software has, for in stance, been shown to  perform poorly on  dark er skin  tones  (Buolomwini, 2018).   Another significant issue might be the impact of automated selection processes on certain groups  of workers, such as people with disabilities. Some recruitment tools could potentially disadvantage    12 'With all this bounty come corresponding cautions and challenges, including the vastly increased access to a huge  number of psychometrically unsound or wholly untested tools now so easily available on the Internet and the greater  likelihood of assessments being misused b y untrained managers and HR staff ' (Adler, Boyce and Caputo, 2017 , 5).  13 'One of the games measured risk -taking. Candidates had 3 minutes to collect as much money as they could by clicking  ‘pump ' to inflate a digital balloon with air and money. Each click added 5 cents. At any point, the candidate could choose  to collect money to add the amount to his or her total and start with a new balloon. However, if the candidate waited  too long and the balloon popped, the candidate collected no money from that balloo n. Candidates could collect money  about as fast by clicking early and frequently or waiting –as long as they didn 't wait too long. The point of the game was  not really about the amount of money collected but identifying the individual 's risk propensity ' (Black and van Esch,  2020, 220). The problem with this kind of game, though, is that it measures someone's propensity to take risks in a game.  It might actually say very little about that person's inclination in real life. If a person is not very motivated to obtain the  job position, for example, that person may well take a lot of risk then and there. In contrast, if someone believes that  making a balloon pop is a horrendous mistake, then that person might be very cautious even if it really does not matter  all that much whether the balloons pop. Moreover, if that cautious person would have known someone at HR that told  him or her the employer was really looking for people who took risks regardless of how many balloons pop, it might  likewise change that person's  risk taking in the game. In short, does such a test actually have value?   14 In one instance a person obtained a reasonable English proficiency score by reading a German Wikipedia page to the AI.  Subsequently, the researchers decided to do the same exercise, this time speaking Mandarin. The hypothesis was that  German sounds more similar to English than Mandarin. Yet, the second person did not score any worse than the first.  
AI and digital tools in workplace management and evaluation     13 disabled jobseekers; in this respect, it is unclear how adequate accommodations can be made  (Kantrowitz and Gutierrez, 2017, 211; see also Schellmann and Strong, 2021a)15.  Taken as a whole,  despite the purported  benefits  of using all these AI tools  (Vrontis et al., 2021) ,  management scholars such as Peter Cappelli warn businesses not to get caught up in the prevailing  narrative surrounding AI and recruitment. First of all, i n a broader perspective, these recruitment  tools  prompt  businesses to find talent i n the labour market instead of developing it in -house.  That  might not be the right choice. Crucially, he also  remarks :  'In the end, the drawback to using algorithms is that we 're trying to use them on the cheap:  building them by looking only at best performers rather than all performers, using only measures  that are easy to gather, and relying on vendors ' claims that the algorithms work elsewhere rather  than observing the results with our own employees. Not only is there n o free lunch here, but you  might be better off skipping the cheap meal altogether ' (Cappelli, 2019, 57).   Indeed, multiple  authors call for more thorough evaluations of these tools ' overall  effectiveness,  utility and fairness ( Dattner et al., 2019; Morelli  and Illingworth, 2019, 86).16 Employers with massive  resources to invest in them  might, perhaps, find a way to make them work somehow satisfactorily  in their particular setting . Yet, that does not mean their effectiveness is guaranteed across the board.   3.2.2.  Performance management: Staff appraisal  and professional  development   Assessing work  performance has long posed a challenge  in many contexts  (see Buckingham and  Goodall, 2015). It is, therefore, unsurprising that also in this area,  tech solutionism is becom ing  prevalent (Kane, 2015) . In this respect, th e use of wearables devices and other work tools that  continuously collect workers ' data  is spreading . It raises legitimate concerns about the potential for  invasive surveillance ( Tomczak and Behrend, 2019; Ajunwa and Schlund, 2020). B esides wearables,  communication tools widely used at work , such as  Slack and Microsoft Teams , can also  function as  plentiful data sources . So do social media platforms.  Combining all these sources make s it possible  to obtain what is commonly referred to as ' big data ', which combines multiple data sources  and  large volumes of data  with  the capacity to generate and process such data at high velocity  (McAfee  and Brynjolfsson, 2012) . The creation and use of  big data for the purpose of HRM is an increasingly  common business practice  (Garcia- Arroyo and Osca, 2019).   Another relevant practice is ' data analytics ', whose  goal is to draw observations  and lessons  from  the (wealth of ) data available , and possibly predict what might happen or prescribe what ought to  happen  (see Rasmussen and Ulrich, 2015; Minbaeva, 2018) . Not all forms of data analy tics necessarily  require big data . At the same time,  the increasing relevance  of data analytics arguably  does trail t he  recent growth in data . Moreover , to the extent AI  is involved in this regard, one can expect the data  analysis to be oriented towards big data analytics rather than a limited setup.     15 One example is that of an AI recruitment test that might priorly ask whether a pers on is colour blind or has any trouble  hearing. This may well be relevant to take a suitable test. However, asking someone about a disability before the test is  problematic because it forces that person to admit he or she has this condition even though it m ight not actually be  relevant for the job. Admitting you have this condition at this early stage might not be in your best interest as a jobseeker   (and, in some countries, it may not even be lawful for the employers to become aware of it during recruitment ). This way  of working forces these individuals to either opt for an accommodation or to choose for the regular test out of fear that  opting for the accommodation will actually inappropriately disqualify them.   16 'The current  state  of affairs  in the develop ment  of AI/ML talent management tools reflects a yawning gap between I -O  psychologists  [i.e. industrial and organizational psychologists] —who study the science and practice  of personnel   selection  and have  learned  from  decades of organizational, legal, and ethical lessons in the selection and  workplace   contexts —and  computer  scientists and  applied  statisticians —who  implement AI/ML  talent  assessment  technologies '  (Gonzalez et al., 2019, 42).  
STOA  | Panel for the Future of Science and Technology     14 According to HR literature , however, compared  to data analytics in supply chain management,  logistics, finance etc., the HR profession has been quite hesitant to engage  in this direction. HR  personnel  might , among other things , lack training  in IT and  statistics.17 Furthermore, m any  businesses are li kely  to purchase  or lease  human resources information systems [HRIS] from external  providers.  Yet, this practice  is not without its dang ers. 'Rather than providing strategic and  predictive analytics that allow organisations to ask and answer big questions about how value can  be created, captured and leveraged, HRIS typically provide answers to a more limited set of  questions focused on operational reporting ' (Angrave et al., 2016, 5). This might have some value,  but the se authors believe the chanc es of this form of analytics significantly improving performance  are low.18  Complementarily, workforce analytics professor Mark Huselid argues  that the mo re effective HR  analytics systems are those  uniquely tailored to a given business and its environment  (Huselid, 2018,  682- 683) . However, it is extremely hard to reach the levels of expertise needed to develop  these   tools in -house.  Research from  McKinsey has identif ied five stages  a people analytics team should go  through : (i) poor data, (ii) good data, (iii) strong data, (iv) advanced analytics and (v) reliable  predictions . The  researchers stress  how difficult it is to reach the final stages  (Ledet et al., 2020) . Yet,  only in the later stages  could HR analytics  satisfactorily contribute to identifying and promoting  key  employee behavio urs, as well as relational dynamics (Leonardi and Contractor, 2018),  that would  help  the business achieve  its strategic and operational goals . Generi c metrics and analytics   purchased  from third -party vendors  are unlikely to provide high  level s of insight ( Minbaeva, 2018,  701).   This general observation about  HR analytics is, arguably , relevant when thinking about AI 's potential  impact on performance management . Indeed, i f, according to specialised HR literature about people  analytics,  generic HR information systems are unlikely to yield significant results, it is not sensible to  expect  generic  AI software from third -party suppliers  to function properly  across different firms in  diverse  settings . What is more , some evidence suggests  that in order for  AI to have a significant   impact, it is  important for developers to explain to users what the  AI model is doing, and, to t he  extent  possible, give those interacting with it in the field some control over what it does ( Dietvorst,  Simmons and Massey, 2018; Massey, 2019, 20- 21). These  findings also  conflict with the idea that  generic 'over the counter ' HR-focused AI will be capable of perfecting performance management  in  any business . As the McKinsey study  from 2020,  mentioned above , noted:   'No people analytics team we interviewed has been able to take a full fifth step to reach the top  level of the st airway: creating reliable, consistent, and valid predictive analytics. Reliable  predictions will enable people analytics teams to analyze and explore practical options for  management action. While some organizations have built fit -for-purpose predictive mo dels — mostly for workforce planning —implementing predictive analytics in the context of employee  selection, development, or engagement decisions requires a substantially scaled -up data -science  operation, massive amounts of highly accurate data ( 'very big da ta'), cutting -edge algorithmic    17 'The HR function will also require reskilling. It will need more expertise in IT support —especially given all the  performance data generated by the new apps —and deeper knowledge about teams and hands -on supervision '  (Cappelli and Tavis, 2018, 52). See in particular: Kryscynski et al., 2018.   18 There is also a c ounte rargument to this. For example, people analytics revealed that a manager meeting new hires on  their first day makes a significant difference in terms of these new hires' behaviour during the first couple of months.  When such rather obvious insights are con firmed by metrics and analytics, it becomes much easier to make the case in  the boardroom (Grant, 2019). Hence, also 'generic' analytical tools may actually succeed in improving individual and  team performance because management at the top level might be m ore easily convinced using data. At the same time,  it would also be true that people analytics sometimes requires a certain degree of contextualisation and granularity  (Rebele, 2019).  
AI and digital tools in workplace management and evaluation     15 technology, and organizational comfort with how to address the impact on fairness and bias '  (Ledet et al., 2020).   As the citation suggests , individual companies with  the necessary resources  to digitalise their  operations, imp lement workforce  analytics and comprehend what  the AI is doing in this setup may  obtain some  results. In theory,  the AI systems might  predict what is likely to happen or can even  suggest  to decision -makers what action they might want to take. Some researchers observe that , in  particular , these  tailored  predictive and prescriptive functions  of AI can contribute  to business   (Peeters, Paauwe and Van De Voorde, 2020, 208) ;19 yet, at the same time, these functions raise  significant legal and ethical questions concerning their impact on workers.  Some examples are  already known. IBM, for example, claims that an AI called Watson consistently made correct  predictions about employees likely to leave  the company, reportedly saving it 300 million U.S.  dollars. The company also uses an 'AI-powered learning platform ' that serves internal and external  courses, such as YouTube videos, Harvard Business Review  and MIT S loan Management Review   articles,  up for every single individual. The AI ' will say, ' Given what you 've taken so far and your career  goals, here are some recommendations, and here 's what people like you have taken and how  they 've rated it '' (Schrage et al., 2019, 3 -5).  This kind o f AI-supported  continual assessment  could materially  transform the field of performance  management. These systems are set to offer employees constant data -driven feedback , possibly  replacing periodical performance reviews . A continual feedback process  is created, meant to engage   worker s more . Some even picture the se e -process es as AI systems becoming  workers ' coach es  (Babic et al., 2020, 63- 64).  Overall, drawing on this literature, t echnology  seems destined to systematically collect and present  data about individuals ' and teams ' performance . Even if doing this right is already challenging,  the  real hard part is obtaining the data, metrics, and procedures needed to identify the real issues   through the data. In the long run, people analytics teams aim to  comprehend what is going on and   propose viable solutions. Along those lines, Microsoft' s MyAnalytics tool, for instance, tracks  workers ' email traffic, response time and time devoted to meetings , attempting  to optimise their  work rou tines (Giermindl et al., 2021, 11- 12). Likewise, Percolata claims to rely on sensor data and  information about sales per employee in retail to identify which colleagues work together best. T he  goal of its software is to generate the ' ideal'  constellation of workers for e very  fifteen minutes of the  day to optimise sales. Businesses that rely on Percolata 's software create a  highly flexible workforce ;  the schedule for each employee is sent to their smartphone , with good performers allegedly  receiving more hou rs (Giermindl et al., 2021, 15).  These  shift allocation tools , reliant on algorithms  and , possibly , an AI , are increasingly common in the retail and hospitality sectors (ACAS, 2020, 9).   Still, f or now, despite the real possibility of automatically scheduling workers through some of these  tools, Alex Wood remarks that  managers prefer mostly  to perform manual scheduling based on the  information derived from these tools ( Wood, 2021, 4).  Importantly, a study conducted by various  universities on retail workers shows that algorithms aimed at fostering business ' efficiency can lead  to suboptimal results . This is due to  these algorithms being based on a very limited notion of  efficiency and not taking into account the numerous hidden costs associated with schedule  instability (Williams et al., 2018). Workers subject to algorithmic scheduling risk paying the brunt of  these flaws by receiving irregular hours and seeing their work becoming even more precarious  at  the hands of a machine.   Again , echo ing our finding s in the previous section, the use of AI in the HRM framework presents  plenty of challenges and risks that , in many cases , outweigh its opportunities both at the societal    19 'After all, the ultimate purpose of using machine learning with employ ee data is to successfully predict behavioral  outcomes: Who will succeed if we promote them? Who is going to quit? How much return will we get in performance if  we implement this new training ?' (Rosett and Hagerty, 2021, 172).  
STOA  | Panel for the Future of Science and Technology     16 and business level. Labour market actors and policymakers should not f all into the tech -solutionist  fallacy whereby AI  in itself is believed to  solve  performance and business organisations '  shortcomings .  Lisa Marie  Giermindl et al.  highlight some  key consideration s in this respect. As they point out,  decision -makers would do well to  consider their choices carefully because:   'people analytics can promote a false sense of certainty regarding the data (Gal et al., 2017), which  can lead to two major pitfalls. First, people analytics ' illusion of objectivity can result in an overly  strong, possibly even a blind belief in the algorithms ' processes, results and capability to predict  reliable outcomes correctly (Leicht -Deobald et al., 2019; Mayer et al., 2020). […] Second, people  analytics might follow a reductionist logic which can mislead managers to postulating cause -and effect relationships that in fact do not exist (Bhattacharya et al., 2010; Khan & Tang, 2017; LeichtDeobald et al., 2019). […] By putting people into boxes, people analytics systems fail to consider  the complex, decisive nature of knowledge work and human interaction (Faraj et al., 2018; Gal et  al., 2017). ' (Giermindl et al., 2021, 11).   3.2.3.  Task distribution, management and evaluation   One of the scenarios depicted by Michael Schrage et al. is for  technology  to be introduced with the  aim of spurring workers ' productivity  (Schrage et al., 2019); this section  addresses  this scenario.   Businesses in various sectors are  already  moving along these lines.  For example , as mentioned in  the prior  section (e.g. Percolata), ' [m]ajor retail companies and restaurant chains have long been  using these methods, having internalised and updated the everlasting lessons of human resources  management: compartmentalise, measure, optimise, correct ' (Aloisi and De Stefano, forthcoming) .  In the transport industry , automated  processes  contribute to determining ho w vehicles are routed  (Abduljabbar  et al. , 2019). Uber , Lyft  and other transport platforms likewise rely on algorithms to  govern their fleet (Rosenblat, 2019 ). The European Commission 's second -phase consultation  document on platform work , moreover,  mentions that ' [w]hile the use of automated systems in the  work con text first gained prominence through its applications in the platform economy, algorithmic  management tools are spreading beyond platforms to traditional workplaces ' (European  Commission, 2021, 11; see also Spencer and Huws, 2021 ). Indeed, algorithmic mana gement was  probably already present in certain industries  before it gained prominence in the platform economy   (O'Neil, 2017) . Nonetheless, the platform economy has been a mass pilot test for automated  decision -making in the world of work before it widely s pread to other sectors.   As scholars note , businesses in other parts of the economy are increasingly  using algorithms to  direct and monitor the workforce  (see Bernhardt, Kresge and Suleiman, 2021 ; Wood, 2021) .  'Electronic Visit Verification ' system s, for example, are now  used to surveil home care workers   (Mateescu, 2021) . Likewise, remote teleworkers are being  increasingly  subjected to systems that  direct, monitor and report on their activities (Aloisi and De Stefano, 2021a).  Many other workers are  starting to similarly h ave their work constantly overseen  by an 'algorithmic boss ' (Adams -Prassl,  2019; Aloisi and De Stefano, forthcoming ). In a report commissioned by UNI Global, Alessandro  Delfanti and colleagues describe how Amazon workers  are guided and monitored , for example,   through AI -powered cameras20, barcode scanners21 and  computers at the workplace  both  to keep    20 'As the first wave of the C OVID -19 pandemic waned in 2020, Amazon unveiled Distance Assistant: an open source, AI driven system designed to monitor and enforce social distancing at its warehouses in the United States, and around the  world. The system also generates data that Amazon can use to modify its workplaces or, more accurately, surveil the  movements of its workers ' (Delfanti, Radovac and Walker, 2021, 8).   21 'Tools like the scanner assign them tasks (go to aisle X and pick item Y), read the information encoded in the barcodes  of the products Amazon sells, and are used to monitor workers ' every move inside the warehouse' (Delfanti, Radovac and  Walker, 2021, 6).  
AI and digital tools in workplace management and evaluation     17 up with production rates and to engage in  'political  control '22. For ins tance, a tool at Amazon, called  'Connections ', requires Amazon workers to respond to at least one survey question each day using  their computer, workstation device or hand scanner. ' The company states that Connections  'analyzes response data and provides insights to managers and leaders to review and take actions  as they uncover issues or see opportunities to improve. ' Many workers instead report feeling that  what is tested is their compliance with Amazon 's workplace culture, although the company frames  Connections and similar programs as tools for worker empowerment ' (Delfanti, Radovac and Walker,  2021, 7).   As the company tests these technologies for internal purposes, it is unsurprising that  Amazon also  sells  some of these tools to other business es. One example is the Amazon Web Services Panorama  Appliance. It is ' a hardware device that adds machine learning capabilities to standard internet  protocol (IP) connected cameras. Panorama is used to analyze videos within a company 's existing  network, in  real time, without the data ever leaving the premises ' (Delfanti, Radovac and Walker,  2021, 9).  Other companies likewise sell tools to keep tabs on employees  by bundling together  information to produce  'productivity scores ' (Heaven, 2020).  Troublingly, digital worker surveillance   seems to be  becom ing an industry in its own right (Steele, 2020) . Certainly, it is hard to deny that  digital surveillance tools appeal to more and more businesses.   To give some other examples, the  Korean e -commerce giant Coupang  is reported to heavily rely on  AI to shorten del ivery times . Notably , the company has recently come under media scrutiny as some  of its workers allegedly suffer ed severe injury and death from overwork ( Kim, 2021) . The gruelling  pace  in logistics businesse s, where the number  of units processed per hour  is tracked, can indeed   lead to dangerous working conditions  (EU-OSHA, forthcoming) . These findings contrast  sharply  with  the rather optimistic view s of some. Surveillance AI has , for instance,  been presented as an  opportunity to keep workers safe and improve their health (Chamorro- Premuzic, 2020). While this is  indeed a possibility,  in theory, this may be far from reality for many worker s. Along the lines of what  is happening in e -commerce warehouses, AI can be introduced in an attempt  to boost production   and productivity . If that is the primary thrust behind the introduction of AI -enabled management  systems, safeguard ing occupational saf ety and health might become a secondary objective at best .  One that may not be  ranked any higher  than  the disturbing tracking of  'unionisation risk'23 across  the business.   If used sensibly , AI-enabled tools  could , indeed, improve occupational health and sa fety conditions,  for instance , by helping lighten  worker s' workload (CIPD and PA Consulting, 2019, 26)24, and it can  offer insight s into measures that improve  work -life balance  (Deshpande et al., 2021, 20 ). Yet, another  side of the coin  is increasingly  show ing its face, most notably but not solely in platform work  and e commerce warehouses. Abigail Gilbert and her colleagues refer to this  as 'the Amazonian Era ', which  marks  not 'the replacement of humans by machines but the treatment of humans as machines '  (Gilbert et al., 2021, 4).     22 The authors use this term, illustrating this type of control by, for example, referring to job ads posted on the website of  Amazon in 2020 looking ‘ for analysts with prior experience in the military or law enforcement to gather intelligence on  'labour organizing threats against the company. '' (Delfanti, Radovac and Walker, 2021, 5).   23 ‘SPOC is designed to help Amazon monitor a wide range of potential threats to its operations, including severe weather  events, local crime rates, opioid usage and, especially, labour organizing. According to Recode, about half of the data  sets referenced in the February 2020 memo are related to unions: e.g., 'Whole Foods Market Activism/Unionization  Efforts, ' 'union grant money flow patterns, ' and 'Presence of Local Union Chapters and Alt labour Groups. '' (Delfanti,  Radovac and Walker, 2021, 12).   24 A worker survey from CIPD and PA Consulting found that, as a result of automation and AI, ' (24%) experienced a decrease  in their workload, with the same proportion experiencing an increase (23%; others noted no change). ' Additionally,  '[r]egarding the pac e of work, more respondents reported that AI and automation makes their work faster (45%) than  that the pace of work has slowed down (16%) ' (CIPD and PA Consulting, 2019, 26).  
STOA  | Panel for the Future of Science and Technology     18 Their case studies describe a supermarket worker whose till monitors the number of customers  standing at the checkout by using  heat sensors. Other sensors record the speed of this worker 's  scanni ng, leading to ' queue length reports ' that are used  to assess the worker 's performance. In  another example , an engineer is asked to record when she starts and completes a task in the  'Connected Worker app '. The system also relies on wearable headsets to mo nitor the engineer 's eye  movements, making it possible to detect when she is talking to colleague s. The data about  various  engineers is subsequently used to calculate how long it takes to complete a task, which, in turn,  leads the system to schedule the work for all workers ' according to 95% work optimisation of all staff  at all times ' (Gilbert et al., 2021, 12).    These algorithmic mechanisms, using sensors and other data sources  to track the productivity of  workers and profile  them , are spreading throughou t the economy. The level of sophistication varies.  Sometimes, the system s may  be rather simplistic , as is the above  case of the cashier in the  supermarket. It might  even  just be an algorithmic model without any real AI capabilities .  Nevertheless , what seems evident is that adding AI to the mix does not lessen the extent of control  in such a situation . One legitimate  concern is that AI could be put to the task of predictin g at which  performance levels  and paces  workers deliver the best results without being pushed over the limit  just yet .  Notwithstanding possible exceptions,25 however,  some scholars observe:  '[t]he application of AI  technology to human resources (HR) analytics is still in its infancy, even if one considers a generous  definition of what kind of technologies AI refers to. HR analytics software products rarely involve  automated decisions or even r ecommendations based on data- driven predictions. Rather, they  often develop and visualize an array of HR metrics leaving evaluations and decisions entirely to  human decision -makers ' (Loi, 2020, 4). In most instances , human decision -makers would rely on the   metrics presented to make specific  calls, such as  readjusting  performance thresholds  or not  renewing someone 's employment contract . Call centres,  for example,  are businesses generally  renowned for their intrusive surveillance and managerial practices towa rds workers (Moore, 2020,  32). These businesses, inter alia , rely on v oice -analysis software to detect both customers ' and  workers ' moods, offering motivational suggestions to the worker s about how to calm down the  customer ( Simonite, 2018). It is not hard  to imagine such tools being used to monitor workers . These   data  would  generally  also  be used to  advise  management decisions about who receives bonuses,  who is put on personal development plans, or who is sanctioned  (Bronowicka et al., 2020, 15).  Fully   automated decision -making by any form of AI  does not seem to be prevalent for now .26 However,   even if some  human inputs may still be required,  it should not be neglected that humans may not  be in the position, or not feel comfortable, to deviate from any option or choice suggested by machines – both because they would then likely be h eld to account for these d ecisions by their  supervisors and  because of ' automation biases '. The  latter are discussed in the literature as biases  that engender ' overconfidence in machine decisions, and an ensuing bias against challenges to  those decisions ' (Kaminski and Urban, 2021 , 1961).     25 Logistics companies, for example, might b e rather close to this kind of situat ion. ' Delivery drivers for some firms often  spend their entire working days following algorithmically generated instructions on what route to take to their  destination, in order to fulfil jobs that are themselves being allocated to them by an algorithm, by  an algorithmically  calculated target time that they are under huge pressure to beat, or see their performance downgraded by yet another  algorithm. Regardless of the classification of their employment status, such workers have very little contact with human  managers in the course of their jobs ' (ACAS, 2020, 9).   26 Colclough does describe a case in which the system might have done more than just present metrics to management.  She writes 'bank employees in a customer service centre are subject to a system that  measures the customers' and  workers' tone of voice and mood. It then advises the workers on what to say, sale and do and monitors them for  succeeding in doing the ‘right' thing' (Colclough, 2018, 6).  
AI and digital tools in workplace management and evaluation     19 3.2.4.  Retention , rewards  and p romotion   As the previous sections  have described, some companies  already follow a  'holistic ' digital  management approach aimed at governing all areas of people 's work experience (often presented  as a 'panopticon ' in the literature  (e.g. Hagen et al., 20 18; Woodcock, 2020) ). At the same time, rather   than investing  in all-encompassing algorithmic -driven information  systems , other  businesses may  opt to use digital tools to achieve  more limited goals  – for instance, reducing worker turnover. Some  argue that AI would be suited to make predictions about people inclined  to leave the company,  providing the business with an opportunity to retain them  (Jeske and Calvard, 2020, 249) . IBM's  above -mentioned  predictive attrition program is an oft used example in this regard (Kiron and  Spindel, 2019, 5) , one that could be of interest for industries with  high turnover  rates , such as the  hospital ity and tourism sector .  To that extent, one should immediately caution against being overly optimistic . It could be argued  that predictive systems that only spot employees willing to leave may be of  little  help. Tools that try  to obtain  a better fit between job applicants  and employer s at the time  of recruitment are likewise   considered key to employee retention (Johnson, Stone and Lukaszewski, 2021). A  tool that solely  tries to prevent turnover by predicting employees ' resignation risk may thus hardly make any real  difference . This would be true even if AI could truly predict and map this risk reliably, something that  seems already questionable (Albinus, 2021). One primary  reason is that most businesses  may simply  not have the data requir ed for AI to make accurate predictions  in this sphere , let alone the existence  of dependable technology able to make accurate predictions in this field if the data existed .  Furthermore, there are also other reasons why  it might be rather unlikely  for very specific  AI  applications to take hold  in HRM. Technology management professor P aul Leonardi  observes that  telework during the COVID -19 pandemic might have boosted the development of AI  HRM  applications because remote work creates considerable  'digital  exhaust '27:  'By themselves, individual particles of digital exhaust contain very little meaning. For example, if  an employee starts work late one day (as recorded by VPN login times), spends an unusually short  amount of time working with information in a portal (as recorded by server -side time -stamps), and  is unusually quiet in a meeting (as recorded by total seconds of talk -time in a Zoom session), none  of these pieces of digital exhaust by themselves tells us much. But when those pieces are  combined, exa mined over time to qualify a pattern of behaviour, and compared to other  employees ' patterns, they can start to create inferences that an employee is, for example,  disconnecting from the organization ' (Leonardi, 2021, 250).   Tracking all this 'digital exhau st' could form part of an employee retention program. Troublingly,  however, these data could also be used to ground other decisions, including disciplinary ones, once  again with no real oversight on the reliability of these systems or their transparency, and again without providers and employers worrying about privacy protection in collecting and processing  these data.   Earlier, we highlighted how  some HR analytic s applications aim to turn  periodical staff appraisals  into continuous feedback processes . Some observers suggest these processes should also be used  to determine workers ' compensation . They argue:  ' Companies should examine whether the metrics  they use to reward [sales representatives]  are aligned with their s trategic objectives. They should set  individual targets based not on past performance but on the potential of each rep 's customer    27 The meta -data we generate by using ICT technology is sometimes referred to as 'digital exhaust'. Usually, this term refers  to (meta -)data that is considered worthless at first sight, but might, in fact, be useful when combined with other (meta )data. For instance, ' [l]ogs of employee behaviour are called dig ital exhaust because they are by -products of other  activities, like setting up a meeting or running calculations. Although the term ‘exhaust ' may signal the inadvertent  nature of such digital records and connote worthlessness, nothing could be further from  the truth ' (Leonardi, 2021).  
STOA  | Panel for the Future of Science and Technology     20 portfolio. And they can boost reps ' motivation by setting and revising targets in line with customers '  purchasing cycles while  conducting experiments to arrive at the optimal frequency ' (Chung et al.,  2019).  In another article, some of these  same  authors argue  an AI can take into account a worker 's  behaviour as part of a ' behaviour -based compensation system ' (Chung, Kim  and  Syan,  2020, 43- 45).  Uber 's well -known 'surge pricing system '28 may  act as a blueprint in this respect .  Such developments – and their potentially detrimental implications in terms of wage instability and  inequality – cannot be ruled out unless regulation intervenes to limit or ban them. We might expect  to see these systems evolve in areas where laws and collectiv e agreements do not entirely fix wage s.  Furt hermore , in terms of rewarding workers, automated processes can also be used to achieve less  extreme goals , such as determining  what kind of end -of-year rewards work best for specific workers   in particular  situat ions  (Schweyer, 2018).   Lastly, we will briefly mention the subject of  job promotions . It would seem there are many different  ways of applying AI in this context. Still, t here are some reasons to believe that fully automated  decision -making in this field is unlikely to be widely deployed, at least  for the time being. For one , a  promotion might come with new duties, such as managing a small team or being required to  interact much more intensively with business clientele. Although it is conceivable for  an AI to be  programmed to predict how someone might perform in a new role, it seems unlikely that  management relinquish es to play a role in the final decision about promotions. Moreover , there is  also a human aspect that needs to be addressed.  When promoting someone who, for instance, has  far less experience at the company than a comparable worker, then, regardless of that person' s  merits, management might want to ensure this promotion is not perceived as unfair by colleagues .  The need to take ' fairness ' into account and manage interpersonal relationships when promoting  someone  might help explain why, relatively speaking, there seems to be little serious discussion  about the role of an AI in automating job promotions  (Köchling and Wehner, 2020, 836) .  3.2.5.  Discip linary procedures   As mentioned before, technological advancements have significantly expanded companies ' ability  to engage in worker surveillance (Ajunwa, Crawford and Schultz, 2017). Platform workers, in particular, have been at the receiving end of  automated or  quasi -automated  sanctions for m any  years . As Phoebe Moore discusses, digital labour platforms  have also long relied on c ustomer  feedback ratings to evaluate workers ' performance . Those ratings –  for instance, a customer giving  three stars out of five –  are a crucial component of the ' surveillance loop '. Workers are constantly   watched either  through the platform 's app, which relies ,  i.a., on GPS signals , or by outsourcing  supervision to  the clients (Moore, 2020, 28). Worldwide research by the I LO has found in this regard  that about 15 to 19 per cent of platform -based taxi and delivery workers  have experienced  deactivation (ILO, 2021, 182). Low ratings, in particular, can trigger deactivation . The impact of such  ratings means that  a cab driver may not ask a customer to stop smoking in the car out of fear of a  low rating . A food delivery worker may  likewise  feel pressured to unwillingly enter  into  someone 's  apartment out of the same fear of a low rating, even if traditional delivery compan ies have long  considered it reasonable for delivery personnel not to do this , also to avoid the risk of sexual or other  forms of harassment (Todol í-Signes, 2021b).     28 'Surge  pricing  is displayed  to drivers  through  a type  of heat map visualization, where the algorithmic assessment of  supply and demand will temporarily raise fares for a particular geographic location (see Figure 1).  Visible  to both riders  and drivers, the creation of such surge pricing zones is billed by Uber as a means  to ensure  positive  customer  experience   by enticing  new  supply  to an area of high  demand (Kedmey,  2014;  Uber  Technologies,  2015b).  Uber 's surge  pricing   patent  (Lin et al., 2014)  and its vernacular explanations contend that surge pricing prompts more drivers to get on the  road (Uber, 2014) when demand is high, but there is some evidence that surge primarily redistributes the existing supply  of drivers rath er than adding to it (Diakopoulos, 2015) ' (Rosenblat and Stark, 2016, 3765- 3766).  
AI and digital tools in workplace management and evaluation     21 Bearing platform workers ' experiences in mind , it should be emphasised that ratings might be come  increasingly important in the service economy at large. Suppose a customer downloads the app of  any service provider, such as an airline company.  From a technological viewpoint, i t is fairly easy to  enable passengers to rate the crew members after the flight. Those ratings, quite similar to what happens in the platform economy, could automatically trigger certain proceedings. They  provide  a  fertile sourc e of data that could, for example, affect per formance -linked remuneration or initiate a  performance review (Darrah, 2021).   As evidenced in the literature, digital management tools may also provoke disciplinary measures for  other reasons besides computing rating s. Some tools may be motivated by commendable goals.   For example, the Oxford Internet Institute refers to an AI bot that aims to detect bullying and  harassment by screening workplace emails  (Neff, McGrath and Prakash, 2020) , a practice that could  conflict with workers ' privacy . Algorithmic  systems could also be theoretically developed to mitigate  'like-me' and 'affinity ' biases at work  (Executive Office of the President, 2016, 14).   The disciplinary use of algorithmic management, however, raises important concerns. T hink of an  AI system that automatically attributes penalties to workers. In the platform economy, for example,  some businesses automatically charge fees and fines to workers for not meeting deadlines (Berg,  2019, 84- 85). 29 To combat similar practices in wa rehouses, the State of California has recently passed  an Assembly Bill to prohibit some  employer s from taking adverse action against workers  for failure  to meet a quota that has not been disclosed or for failure to meet a quota that does not allow worker s to take  a meal or rest break s or  otherwise comply with  occupational health and safety  laws. 30 Besides disciplinary sanctions, employers could  also engage in  automated or  semiautomated  dismissals. Amazon, for instance , is said to have automatically fired Flex delivery drivers .  Bloomberg  reports in this regard that: ' Amazon knew delegating work to machines would lead to  mistakes and damaging headlines, these former managers said, but decided it was cheaper to trust  the algorithms than pa y people to investigate mistaken firings so long as the drivers could be  replaced easily ' (Soper, 2021) . Important to note, such a utomated and semi -automated disciplinary  measures have, at least in one instance,  been found in court to result in indirect di scrimination .31  The legality of semi -automated dismissal s could also be  highly questionable under some national  unfair dismissal laws (Gaudio, forthcoming).   What is more, an AI does  not necessarily have to engage in direct sanctioning  to obtain a disciplin ing  effect . As professors Katherine Kellogg, Melissa Valentine and Angèle Christin write , algorithms may  also discipline workers by replacing and rewarding them. Firstly, t hey argue that organisations can  recruit workers on a massive scale in some digitalised businesses, such as platforms,  without this  being time -consuming. In this sense, the reliance on an AI to issue dismissals or non- renewals    29 'Imagine that the company you work for charged you a fee —$10, $20, or even $40—every time you showed up late for  work, left an hour early, or called in sick. Professional c leaners working for Handy, a cleaning and handyman service, don 't  have to imagine this scenario: They are living it ' (Van Doorn, 2018).   30 Assembly Bill No. 701 to amend Section 138.7 of, and to add Part 8.6 (commencing with Section 2100) to Division 2 of,  the Labor Code, relating to employment.   31 'The case, promoted by the most representative Italian labour union, Cgil, brought to light that Deliveroo' s riders were  evaluated primarily on two aspects: reliability and participation (we use the past tense because the company claims to  have «adjusted» the statistics used for its slots through its new contracts, which were signed in November and are anyway  widely contested). The combination of these metrics gave workers an internal ranking; by virtue of that rank ing they  were more or less likely to be offered new jobs or to be downgraded instead. Workers with good ratings were among  the first to be able to apply for the most coveted work shifts and could also turn down the most uncomfortable ones.  However, any wai ver in the 24 hours prior to the shift weighed against future calls. Upon returning from a period of  absence for various reasons (health problems, commitments related to the care of family members, or collective action),  workers could be automatically down graded and forced to start all over again, by climbing the ranking from the scratch '  (Aloisi and De Stefano, 2021b). Because the system did not pay any attention to the reason why workers were absent, it  negatively impacted workers who had a valid reason f or their absence. The Court in Bologna considered this to give rise  to indirect discrimination.  
STOA  | Panel for the Future of Science and Technology     22 cannot be dissociated from what is happening in  the business in terms of recruitment. Digitalisation  and AI enable some firms to access ' a reserve army of workers ready to take the jobs of those who  do not comply with managerial directives ' (Kellogg, Valentine and Christin, 2020, 380). From this  perspective , it is not only automated  firing that should be seen  as a form of algorithmic disciplining .  Especially in the context of rapid onboarding, the non -renewal of short -term contracts can also  fall  within that category.   Secondly, o n a different note , the authors  also  point to ' algorithmic rewarding ' as another  mechanism used to discipline worker behaviour ( Ibid. , 381). W e have noted  earlier how salespeople  might be encouraged through performance pay to take certain actions. However , it is not only  performance pay that might serve this purpose. Our previous discus sions have also covered the  example of an AI used to schedule work shifts, benefitting workers who perform well. Indeed, on call work  arrangements  provide businesses with the opportunity to offer work to some workers at  the expense of others. Platforms are again a clear example. Most platform workers in the taxi and  delivery sectors believe that their ratings and acceptance rates impact the amount of work they  receive (ILO,  2021, 152). As these workers are often (mis)classified as self-employed, this practice is  materially facilitated. It would become more problematic  to do so in many jurisdictions if these  workers were legally considered  employees . In that case, the employe r – i.e. the platform – might  have a legal duty to provide the employee with sufficient work, making it impossible  to withhold  work while the platform worker  is on the clock . Still, even when speaking of employees, an employer  might decide to offer worse t asks  or too much work  as a disciplinary tactic  (e.g. the excessive quota 's  in California) . In this respect, i n time , we may also see cases in which the operation of an automated  system is audited in constructive dismissal  case s.  Finally, it is appropriate to point out that AI  could also play a role in the degree of self- disciplining  in  which employees partake . The accounts described in this chapter  indicate that some workplaces  have turned into spaces of continuous surveillance. M oreover, the software does not forget. Human  supervisors  might not notice something because they are  busy doing something else  or may forget  someone made a similar mistake before . A computer system would  not. If the capacity to observe   all behaviours relen tlessly is combined with an  AI tailored to screen and monitor all that data,  workers may well feel like having entered an unforgiving environment , triggering what some call  'anticipatory surveillance fear ' (Samek Lodovici et al., 2021, 48). Although this i s an area that  demands more research, it might be the case that the combination of these factors spurs a higher  degree of self -disciplining , also triggering elevated pyschosocial risks for workers in terms of  heightened stress in addition to the risks of d iscrimination and privacy invasion that we already  mentioned,  and we will discuss more extensively below.   To summarise:   • Recruitment seems to be a sphere of HRM in which AI could achieve some degree of  generalis ation . Nonetheless, i t is worth emphasising that many of these tools remain   largely untested. Companies have valid reasons to ado pt a critical attitude towards  these applications.  It also requires expertise to interpret the findings of these  applications correctly.   • AI seems po ised to lead to more continuous forms of staff appraisal. However, it seems  quite challenging  to effectively incorporate AI in this area of activities  because of the  difficulties obtaining all necessary data  to make an accurate appraisal,  and issues  interp reting the results of the  AI's analysis.  AI does not reach any form of ' objective  truth '. HR staff has to learn  how to interpret AI  analysi s without losing sight of the  bigger picture.   • Some companies rely on  algorithms and AI to distribute tasks, monitor workers and  evaluate their performance and attitudes. Although the establishment of such  sweeping 'surveillance loops ' seem s limited so far , in time,  these may spread. In any 
AI and digital tools in workplace management and evaluation     23 event, many more businesses can be expected to rely on AI to  evaluate  workers  in the  future. Th ese AI -based evaluations  will directly influenc e managers ' decision s in terms  of who receives work and who is put out of work.  It is critical to follow up to what  extent AI -powered evaluations are becoming a driving force behind the automated  distribution of tasks; hence, taking managers out of the loop.   • Policymakers  should not indulge in wishful thinking. AI will only benefit wor kers if  designed to this end (e.g. the tension between higher productivity and workers ' OSH  as a guiding principle behind AI  operations ).  • In theory, AI could be used for  rather specific purposes . Yet, the data needed to make  even such limited decisions – e.g. which workers  may  leave the company – might just  as well serve as input for other AI decision -making tools. As such, once a business  meets the (digital) requirements for AI tools to be used  effectively , it seems likely  for  a business  to adopt multiple AI features  instead of just a few .  • It is important to acknowledge that AI can sanction workers in various ways.  Automated disciplinary sanctions based on evaluations are only one aspect.   3.3. Near -term possible development prospects (next five  years)   Looking into the future of AI and its various applications is a perilous  undertaking . Nevertheless, it  is possible to cautiously sketch some general lines based on current research . Firstly,  artificial  general intelligence (AGI) or 'strong AI ', which can at tempt to solve almost any problem you throw  at it, is not yet on the horizon. Concrete p redictions of when such a system  could come about  must  be viewed with a critical eye ( Malone, Rus and Laubacher, 2020, 10 and 16; Devillé, Sergeyssels and  Middag, 2021,  21). The literature is divided, with some authors believing incremental advances can  lead to AGI, others thinking that one or a few 'paradigm -shifting development s' are first needed,  and a third group remaining somewhat  sceptical about there  being such a possibility at all (Boucher,  2020, 14). In any case, f or the time being, AI  will still only be deployed to perform a limited range of  tasks  or pursu e a narrow set  of objectives ( 'specialised AI '). Speculating about AGI is not without its  meri ts, but it is superfluous for this study.   More relevant is to simply cover what tools are currently envisaged  to make a significant impact. For  example, Thomas Malone, Daniela Rus and Robert Laubacher emphasise that, under the right circumstances, contempo rary AI is good at sensing, deciding,  and creating. In the eyes of these  authors, examples of sensing  are AI systems  that are better at analysing medical imagery  than  physicians . Likewise, some  stores allow customers to walk through the checkout line as AI scans their  groceries, and AI software already  relies on records to predict when an Airbus aircraft require s  preventive maintenance. Besides detecting certain things, AI is already and will most likely continue  to be used to take (or assist in taking) a range of decisions, including extremely critical ones for  people. In the field of work, it is reasonable to expect that using AI in recruitment will expand .  Thirdly, AI systems might also be increasingly used to generate content . Whether or not to be later  vetted by human workers , routine news articles , simple legal texts , as well as , for instance , product  designs , could be developed by AI  (Malone, Rus and Laubacher, 2020, 19- 20).  As such, there are various ways in which AI may end up impacting our lives. W ith clock -like regularity,  there is news of a particularly successful application (e.g. Knight, 2018a ). At the same time, there are  also signals that the future of AI is sometimes portrayed as brighter than it actually is . In October  2021, for example,  The Wall Street Journal reported that internal documents at Facebook suggest  that the success rate of the company 's AI in removing content from Facebook can  be considered  dismal (Seetharaman, Horwitz and Scheck, 2021). Its performance in languages other than English is even worse. Similarly, Andrew Moore –  the head of Google Cloud AI –  has warned that  ' implementing artificial intelligence successfully is a slog.  […] Solving artificial- intelligence  problems involves a lot of tough engineering and math and linear algebra and all that stuff. It very  much isn 't the magic -dust type of solution ' (Knight, 2018b).  In this regard, it seems  unlikely to see 
STOA  | Panel for the Future of Science and Technology     24 major new develo pments within five years.  For instance, Luciano Floridi predicts that despite all the  hype of the last few years, a ' new [AI] winter is coming, we may try to learn some lessons, and avoid  this yo -yo of unreasonable illusions and exaggerated disillusions. L et us not forget that the winter  of AI should not be the winter of its opportunities ' (Floridi, 2021, 3).   Based on such accounts , it seems likely for AI to increasingly dominate those areas of the world of  work where it is already a n established  feature. As businesses, in general,  continue to digitalise their  operations and , for instance,  consultancy firms gain more experience  in implementing AI into  existing business es, the technology will likely spread to new enterprises and sectors.  Chipsets and  sensors will probably keep being added to v arious tools and appliances , increasingly  integrating  them into the Internet of Things. Along those lines,  as the work environment becomes more  digitalised , employers might  increasingly  rely on algori thms  to, for example,  direct  and manage   workers . Algorithms could also be programmed to restrict workers from acting in certain ways , such  as blocking emails  and messages  after  working hours . Furthermore, workers ' activities will be   increasingly logged  and rated through technology (Kellogg, Valentine and Christin, 2020). A more  digitalised work environment will facilitate  the use of AI in HRM.  Essentially, b usinesses across the  economy will increasingly meet the preconditions to effec tively  use AI , starting with the collection  of data and metadata.   What remains important to note in this respect , however,  is that , despite its benefits,  even in those  areas where AI  succeeds  – and even only focusing on its business case –  it will still ha ve certain  drawbacks . One of these is its  inability to identify causal links . This deficit  has been so exemplified :  'It's as if you knew that the presence of clouds made rain likelier, but you didn 't know clouds caused  rain ' (Bergstein, 2020). For such reasons, we need to put the potential impact of AI  at work  into  perspective. AI may contribute to teaching some  valuable lessons , but it will not manage to fully  grasp the comple x functioning of work environments in the near future  (and, quite possibly, it will  never be able to do so, as this would require something too close to AGI). To that extent, w hereas  there is plenty of speculation about the future impact of AI on HRM, it remains to be seen in which  areas the introduction of AI will actually be sustainable, functional and cost -effective. So far, the  most immediate impact of AI seems to be in recruitment. The technology leads to fairly evident developments in this area.  However, t here remains a pertinent need to assess whether these AI driven tools actually work or, instead, are ' 'pseudo -profound bullshit ' — the results sound inspiring  and meaningful, but they bear little resemblance to any objective truth ' (Rebele, 2019 , 11).   Moreover, even if these tools  do work, it is  still imperative to  regulate their use, among other things ,  by requiring audits of these systems to guarantee equal employment opportunities (Ajunwa, 2021) ,  and also by bringing them under the scrutiny of  social dialogue processes (De Stefano and Taes,  2021).   Besides AI 's use in recruitment , evidently,  some businesses have more profoundly embraced the  practices associated with the  'Amazonian Era ' (Gilbert et al., 2021) . They extensively rely on  technology to implement a form of ' digital Taylorism ' or 'new Taylorism ' (Eurofound, 2020, 3). Still,  what remains somewhat  unclear is how AI, in particular,  will intervene in this regard. As described,  even assuming a business has the necessary data o n workers to apply  detailed  HR analytics  – and  this is already a far -fetched assumption  if the data collection and processing is to comply with data  protection regulation , it is ultimately  up to humans  to decide for what purposes they want  an AI to  be leve raged . As hinted at by Albrecht and Kellerman, the practical  impact of AI in HRM will depend  on what AI ends up being good at, what makes economic sense for businesses,32 and what is legally  allowed. Research seems not yet to  have scoped out what is likely  to emerge along those lines.  Thus   far, n umerous factors seem to have hamper ed the implementation  of AI in HRM. For example,  'data    32 Kreutzer and Sirrenberg suggest that introducing AI -applications in HR requires a case by case analysis. Sometimes it is  worth it, at other times it is not. T his contrasts with certain other AI -applications, for example in the analysis and  predication of customer behaviour, which is a must do in their view (Kreutzer and Sirrenberg, 2020, 279).  
AI and digital tools in workplace management and evaluation     25 accessibility'  is seen as a significant  impairment  at the workplace (Vial et al., 2021) . Even if a business   does manage to obtain quality work -related data , it may still suffer from a lack of technical  knowledge about HR analytics and how AI fits into that picture, as well as difficulties in obtaining ' a  good understanding of how to use the results of the analy sis to improve performance ' (Nocker and  Sena, 2019).   This is not to say that HR analytics and AI -enhanced tools are not already having a major impact on  some workers. O f course, one large company where this is the case is  Amazon. Its recent patents  indicat e how, in the future, a supervisor could wear an augmented reality headset that recognises  individual workers and projects relevant information, such as ' demographic data about the user,  location data within the facility, relationships with other users, messages for the user, n avigation  paths through the facility, access permissions ' (Delfanti, Radovac and Walker, 2021, 14). A second  patent concerns a wristband or bracelet that would provide haptic feedback to monitor and direct  workers . A third patent aims to recognise individu al workers ' level s of frustration. Such patents seem  to further build on the idea that workers ' surveillance is essential  (Ibid ). The same could be said for  Walmart 's patenting of a microphone system to eavesdrop on its workers and shoppers (Hanley and  Hubbard, 2020, 13).  The food delivery service Eleme, owned by Alibaba, already uses  the Bluetooth  function in workers ' phones and beacons inside shops and restaurants to track their movement  (Ren, 2021).   Policymakers , trade unions and employers ' associations  must  carefully  follow these practices . Some  technologies are genuine ly sinister. For instance,  some scholars mention  the inject ion of  microchips  into an employee 's skin to transmit ' radio frequency identification ' signals. The microchip would rely  on near -field communication (NFC) for enabled devices, such as a photocopier, computer  or door,  to track when a n individual makes contact (Bales and Stone, 2020, 20). In another example from the  Harvard Business Review , AI consultant Alexandre Gonfalonieri asks readers to ' [i]magine if your  manager could know whether you actually paid attention in your last Zoom meeting. Or, imagine if  you could prepare your next presentation using only your thoughts. These scenarios might soon  become a reality thanks to the development of brain -computer interfaces (BCIs) ' (Gonfalonieri,  2020).   On the one hand, w e should not generalise such examples as if all businesses were to go along with  such trends . Some employers will implement digita l tools and AI  to assist  workers rather than ' hack '  them.  For instance , some researchers  describe a future vision of a ' coupled network of humans and  machines ' with 'fully integrated AI coachbots ' that improve people 's abilities to analyse and predict  what they are or should be doing  (Babic et al., 2020, 64). On the other hand, we should not  underestimate the appeal of the privacy -invasive AI tools  mentioned in the previous paragraph  either . As Tomas Chamorro -Premuzic and Ian Bailie contend, ' the temptation to force people into  certain behaviors, or to use their personal data against them, is more real than one would think '  (Chamorro -Premuzic and Bailie, 2020). It is a tendency  many  have also seen expanding  during the  pandemic. The transition from working in the office to working at home has been accompanied by  a sharp increase in the demand for online workplace surveillance tools ( Aloisi and De Stefano, 2021a;  Samek Lodovici et al., 2021, 55). Alongside , COVID -19 is said to have propelled the adoption of  automation and AI (Lund et al., 2021), among other reasons , because many businesses have  digitalised some of their activities , leading to more data that can be fed into AI  systems (Leonardi,  2021).   These r ecent developments arguably  foreshadow  a more intense recourse to electronic  surveillance , algorithmic management and (AI-driven ) performance in the years ahead. At the same  time, in many regards, the technology is still in its  infancy  (EU-OSHA, forthcoming) . Sometimes it  shows great potential , for better or worse ; at other times , it could more accurately be described as  'snake  oil' (O'Connor, 2021) . What is certain is that, at present, AI  usually still needs  a relatively  specific goal  to pursue , such as a camera  system noticing it when people are talking to one another ,  an AI  tool analysing communication between employees to identify harassment,  or an AI  system 
STOA  | Panel for the Future of Science and Technology     26 identifying  workers that have low -performance ra tings . Since some businesses are already deeply  commit ted to such  practices, and as it seems likely for other companies to follow suit, it is high time  to demarcate the legal boundaries of these and other  practices. In this vein,  solutions must be  'systemic and wide -ranging, encompassing complementary tools coming from different legal  domains, such as anti -discrimination law and occupational health and safety, based on the [AI's]  final use ' (Aloisi and De Stefano, 2021a ). The EU AI Act  – discussed be low – could become a crucial  piece in this wide -ranging regulatory paradigm.   To summarise:   • AI is not a magic -dust solution. It will probably never become one.   • Many b usinesses will increasingly meet the preconditions to implement AI at work  effectively .  • For the time being, AI serves specific human -defined goals . Policy d iscussions should  also concentrate on what goals it should serve .  • AI and digital technologies , more broadly , will enable businesses to  engage in certain  detrimental practices . The time is now to set and enforce clear boundaries  also to  dissuade investment s in illegal, unethical and undesirable  AI applications .  3.4. Assessment of impacts   Previous sections have highlighted how AI will engrain itself into people 's work ing life in various  ways  (section 3.1. ), that  AI will have  an impact in many  different fields of HRM  (section 3.2.), and that   it is unclear to what ends  AI at work will develop (section 3.3.) . In this respect, we will argue that  it is  not too late to influence the dev elopment of  AI at work , ensuring it is o perationalised to the benefit  of all . This section presents some of AI 's profound impacts on workplaces , highlighting  the  importance of regulatory  intervention.   As Irene Mandl , Head of Unit at the European Labour Authority,  highlight s, 'AI has the potential to  improve business performance, job creation, labour market access, workforce upskilling, and to  reduce physical hazards, but at the same time it may lead to offshoring, job loss, cybersecurity issues,  'management by AI ', platformisation of work, 24/7 on -call working time, and exploitation of  employee monitoring possibilities ' (AIDA, 2021a , 4). The authors of this report believe that we  should be cautious in outright dismissing the  potential benefits of AI for the labour market on the  whole. Nonetheless, we must also comprehend the profoundly negative changes AI may induce and  recognise  that AI systems will not automatically set their own goals. Their use may only be beneficial  for the many if they are des igned  for t hese  purposes . For the time being, it is possible to note that  algorithmic management and AI -enabled systems have been introduced along  with  the idea of  cutting HR costs and micromanaging and monitoring workers more closely, above all other  considerations  (Acemoglu, 20 21). Nor should  we underestimate the possibility that a technology  that could be put to good use could also be reverse -engineered for far from commendable purposes   (Kullmann and Cefaliello, 2022) . Imagine a system that aims at detecting the risk of burnout  of  workers, for instance, by scanning messages to track keywords that would point in that direction. Apart from the evident risk for privacy invasion, this system could also be employed to dismiss someone just before burnout occurs.   This section  does not plan  to provide an all -exhaustive overview. It  will focus on  certain  specific   implications  that seem relevant to carry into the regulatory discussion . With this in mind, first of all,  the risk of AI to the automation of jobs should be restressed. As said  in the introduction, in the short  term, it might be most important to focus on the impact of AI on the quality of jobs ; however, this  should in no way dissuade us from already preparing for the prospect of  technological  unemployment.  Indeed, t he COVID -19 pa ndemic is said to have further accelerated automation,  both through robots and software (Schwab and Malleret , 2020, 119- 121). A future of less work, at 
AI and digital tools in workplace management and evaluation     27 least 'for ordinary workers without scarce skills, and very possibly less work overall ', cannot be ruled  out. Policy responses  that reach beyond mere stilt calls  for lifelong  learning  are needed  (Estlund,  2021, 39).   Returning to the ' quality  side ' of the jobs,  an evident implication of AI  relates to its  need for data.  Once  digitisation became widespread in the 1990s, policymakers advanced a vision of employers  requiring  a legitimate reason to collect personal data. Many legal systems furthermore require a  sense of proportionality, in general, and purpose limitation, in part icular, meaning the personal data  should only be used for the purpose it was collected , as well as data minimisation  (Hendrickx et al.,  forthcoming). Overall, the relevant legal provisions on data protection in Europe could be described  'as rather individu alistic and defensive in nature against potential illegitimate uses of workers '  personal data ' (Dagnino and Armaroli, 2019, 179). Many of the d ynamics underpinning  HR analytics   are thus somewhat at odds with privacy standards . To obtain meaningful  results,  the company does  not just need basic data ; it requires 'strong data ' across the board, deriving from various sources  and  covering many employees , something that cannot be easily reconciled with data minimisation,  proportionality and purpose limitation . Arguably, many of the HR and computer science academic  debates as well as the few policy discourses about workplace analytics and ' AI at work ' seem to  neglect the structural irreconcil ability of these managerial practices with existing data protection  regula tion.   Indeed, efforts to introduce AI -enabled managerial practices at work  could boost a process of mass  'datafication'  that has been going on for much longer  (Sartor, 2020, 15- 16). As such, the broader  literature describing how datafication  or quantification  of employee s negatively affects human  beings is of crucial importance to any discussion of AI  at work  (Ball, 2021; Ajunwa, forthcoming) . For  example, i f it is indeed the case that governing workers by numbers, among other things, leads t o  an increase in mental illness at work (Supiot , 2017), it would  be essential to take this into account  not only by providers when designing an AI , or by employers when considering introducing  it, but  also when drafting any regulation to govern the introdu ction of AI -enabled tools at work . The  quantification  of work is furthermore alleged to lead to lots of  'meaningless ' work in the eyes  of  workers (Hoeyer and Wadmann, 2020).  As mentioned before, o ne example would be Amazon 's  demand  for workers to fill in a daily survey question. More broadly, w orkers are asked to log all kinds  of activities because the technology can otherwise not take them into account.  Although perhaps  essential to retain their job s, this can be  perceived  as a ' meaningless ' administrative burden .  Overall, a n important question is h ow AI  will fit into this  datafication  picture . Arguably, by adding  an additional layer of complexity through an AI  designed  for advanced data analytics, there is a risk  that too much importance will be attached to what the data indicates . A worker 's and supervisor 's  opinion s of the situation could be  perceived as subjective and flawed compared  to the system 's  assessment . In this sense, there is a  risk of forgetting that due to the sheer complexity of human  behaviour and interactions, especially at work, ' [a]nalytics and algorithms can never achieve true  'objectivity '' (Jeske and Calvard, 2020, 254).  They always turn  the situation  into a somewhat  reductioni st version suited to the AI . In this sense, n o matter how ingen ious AI might become, it will  always resemble a 'horse with blinders '33. Questioning the legitimacy of recourse to such  tools in  making or assisting decisions  having a substantial impact on people 's lives a nd work, thus, is  indispensable (Kaminski and Urban, 2021 ). Yet, current data protection rights do not seem sufficient  to really hold the entities in control of the automated system accountable  (Worker Info E xchange,  2021) .  In addition to  issues related to the increased reliance on data to govern the workplace , AI will also  impact  employment in various other ways . For example, in the view of ILO senior economist Janine  Berg, technological change will not bring about an end to work . '[W]hat we are witnessing instead    33 Like the horse, the AI sees what it is supposed to see. It is not expected to take broader contexts into consideration.  
STOA  | Panel for the Future of Science and Technology     28 is increased precariousness in the labor market. Technology is thus a tool used by enterprises to  'displace labor, ' to more precarious and invisible forms of work ' (Berg, 2019, 70).  Illustrative of her  argument is how  an AI is used in scheduling software to create an on- call workforce, granting most  working hours to the employees with the highest performance ratings. O f course, in theory, the  scheduling software could  also be developed to schedule hours to serve  workers ' work -life balance,  granting more working hours to those who want more hours  (Ibid. , 80-81). Yet, in most cases,  that  does  not seem to be the goal that programmers and users of AI pursue .  Similarly, suppose a business succeeds in performing perpetual  staff appraisals instead of doing  them every so often. T hat, too, would make it easier to shorten the duration of work contracts, with  detrimental implicatio ns in terms of job and income precarity for workers. The OECD also  mentions,  for instance, how '[a]dvances in information technology and artificial intelligence could enhance the  benefits of outsourcing ' for employers  (OECD, 2021) . Cutting a long and complicated story short, AI   may underpin and further enhance the  casualisation and destabilisation  of work , potentially  worsening social and economic inequalit ies. The technology in itself does not have to spur those  trends, but as thing s stand, it is likely  to do so if its use is not adequately governed .  The same is true for the degree of surveillance occurring at the workplace (Edwards, Martin and  Henderson, 2018). The many AI -driven surveillance practices described in the previous sect ions  have  'an impact on freedom, privacy, but also autonomy and moral reasoning, which is much more  relevant in a society in which the traditionally strict separation between private life and professional  life is dissolving ' (Aloisi and Gramano, 2019, 99- 100). When thinking about the potential harms of  workplace surveillance through (AI -induced) digital tools, we must indeed not overlook how some  of the technological developments  that enable greater surveillance, at the same time, undercut  the  separation between private and professional life  (De Stefano and Taes, 2021, 8) . For these  reasons,  Moore rightly emphasises that policy discussions should be oriented towards addressing excessive  surveillance, not just at the workplace but in people 's 'workspace ' (Moore, 2020, 1). Allowing AI to  analyse CCTV footage at  the work place  for OSH -related reasons  is one thing . Having it screen  individual desktop footage or using it  to check webcam footage at home  is quite another.   Nor it should be neglected that it is not just a matter of privacy. AI -enhanced algorithmic  monitoring  can jeopardise other fundamental rights at work, including collective ones.  The possibility of these  tools being used to bust unionisation is concrete  (Bernhardt, Kresge and Suleiman, 2 021, 16; Rogers,  forthcoming) . As Richard A. Bales and Katherine V. W. Stone remark, ' [a]fter all, a device that listens  in on conversations can pick up union talk more effectively than can any company spy. Moreover,  an Al algorithm that uses biomarkers and body language to identify which employees are  dissatisfied at work can predict which ones are likely to become union supporters or simply  troublemakers ' (Bales and Stone, 2020, 52).  Furthermore , AI-powered tools  pose certain problems  for wor ker representatives, who might find it hard to engage in informed consultation regarding the  use of such  systems . Collective rights can be seen as a fundamental tool to rationalise how AI  should  be used at work . As such, in order to preserve this tool 's utility to this end , a range of options for  quickly and effectively ad dressing t he use of technology to stifle workers ' voices should be  urgently   examined  (De Stefano, 2019, 41; see also De Stefano, 2020). Even pursuant to existing labour  regulation s in many  European countries (Aloisi and Gramano, 2019) , AI integration should already  be the subject of collective bargaining and consultation. If , despite these standards, AI technologies  are, in fact, designed to  undercut those very processes,  we run the risk of a deregulatory feedback  loop , further aggravating the other risks associated with AI at work .  Most of these adverse effects – workers suffering from datafication , the casualisation  of  employment, the incre ased levels of surveillance  – are not un avoidable, however. AI  can be oriented  towards improving working conditions , for instance , by keep ing workers safe r (Chamorro -Premuzic,  2020). This can occur in various ways, inter alia , by powering smart robots that replace humans in  the most hazardous work tasks, augmenting workers ' capabilities or identifying dangers.  In terms of  the latter, spotting  driver fatigue in transport occupations or implementing fall -detection systems 
AI and digital tools in workplace management and evaluation     29 in construction are early examples (Pishgar et al., 2021). At the same time, as is the case for many  other promises of AI at work, this will not just happen naturally or automa tically . It is legitimate to  wonder whether businesses  will invest in AI  applications solely designed to  increase  workers ' safety .  For example, Moore rightly remarks that ' [w]hile watching Deliveroo riders hurtle past her in the  rain, [mathematician  and data scientist]  Dr. O 'Neil34 considered the platforms directing the riders '  work, which operate on the basis of efficiency and speed, and thus instigate riders to cycle in unsafe  weather conditions. This clearly puts riders ' very lives at risk. Dr. O 'Neil ca lls algorithms 'toy models  of the universe, ' because these seemingly all -knowing entities actually only know what we tell them,  and thus have major blind spots ' (Moore, 2019b , 66). Adrián Todolí -Signes offers  a striking example  in this respect. In countries such as  Australia, delivery workers might want to reach their destination   by taking a longer route  through  shaded streets . However, delivery platforms ' systems do not know  which roads are shaded . Hence, the y direct workers to take the ' theoretically ' fastest route  even  though the sun might be beating down on them  in those locations, slowing down their delivery  (Todolí -Signes, 2021b ).  That narrow -mindedness is, indeed, a major challenge. It is possible to des ign AI  systems with  workers ' safety as their primary concern. However, to what extent is it realistic to  expect delivery  platforms  to automatically  suspend their  operations because the system finds weather conditions  to be unacceptably hazardous?  Consider  also current  investment s into AI  tools, such as cameras and  wristbands, that seek to ensure that workers keep a sufficient distance during the COVID -19  pandemic. Most likely, businesses will seek a return on s uch an investment also after t he health risks  associated with the pandemic have ceased . If, for instance, these systems will then be used to  enhance work monitoring , they could engender stress and prompt unsustainable work paces,  heightening psychosocial and physical occupational risks . The introduction of AI at work can indeed  cut both ways in terms of health and safety  (EU-OSHA, forthcoming).   The story is somewhat similar for many of the other potential benefits of AI at work. Since the beginning,  people analytics has been  hailed as a  means  to foster equality, overcoming human biases   (Bohnet, 2016). However, many applications of automation and AI to decision -making can lead to  discriminatory outcomes ( Kim, 2019; Köchling and Wehner, 2020, 830- 835) . One notable cause is  that  algorithms  may incorporate implicit and explicit biases present in society. A lgorithms are  trained on data that reflect pas t behaviours ; therefore , they might perpetuate  the same potentially  discriminatory patterns (De Stefano, 2019, 28- 29; Candelon et al., 2021, 106- 108) . The choice of  which data to use  and how to weigh and process them is paramount . Some seemingly neutral  characteristics and variables could, for instanc e, affect future performance.  An often -used example  is the distance prospective workers  have to travel to work; having to travel long distances may lead  to higher levels of turnover . Consequently , one can see why this would be a relevant variable to take  into account. The drawback is, however, that more remote areas, often with lower housing prices,  might correlate with racial origin  or other protected grounds . Similarly, ' many employers in the US  look for people who studied at famous and  expensive universi ties. But it might be relatively rare for  certain racial groups to study at those expensive universities. Therefore, it may have discriminatory  effects if an employer selects job applicants on the basis of whether they studied at a famous  university ' (Borg esius, 2018, 20).   The decision to cancel out such potential correlations is hard to make . AI can surely help to surpass  human biases, but businesses need to be aware that humans, in return, should actively keep an eye out and tackle bias in AI (Manyika, Silberg and Presten, 2019) . Something that risks rarely being the  case if automated decision -making is purported and perceived to be the panacea against  discrimination, as it often is.  To this extent, i t may even be ' easier to program bias out of  a machine    34 Phoebe Moore refers to Cathy O 'Neil. The author of Weapons of Math Destruction: How Big Data Increases Inequality  and Threatens Democracy and of The Shame Machine: Who Profits in the New Age of Humiliation. Both published by Penguin Random House.  
STOA  | Panel for the Future of Science and Technology     30 than out of a mind ', as management scholar Brian Uzzi puts it . Yet, human involvement in the  system 's operation  will always be critical (Uzzi, 2020). Without it, patterns of discrimination can well  be replicated or even bolstered by AI (Adams -Prassl, 2019, 139- 140).  Also , for this reason, much more  attention should be paid to the potential discriminatory effects in all instances of algorithmic  management  instead of only in the hiring process (Bales and Stone, 2020, 60).   As noted above, a nother potential  deployment  of AI is that it can turn  staff appraisals into a more  continuous process. Some , therefore,  see great promise in AI -driven reskilling and upskilling (Caine  and Firth -Butterfield, 2020). In a pilot program conducted by Accenture in partnership with the  World Economic Forum, Walmart and Unilever, an AI  tool called ' SkyHive ' is said to have broken  down several job roles into sets of skills that people themselves in those job roles would fail to recognise . They ' were then able to use the AI to develop specific learning pathways from declining  roles to emerging jobs, both internally and externally. In  some cases, we found that people were  already well prepared for new careers. They might only require a few months of training ' (Whittall  and Hinton, 2021).  As this example illustrates, AI  may well perform a valuable role in upskilling and  reskilling. Howe ver, as the ILO Global Commission on the Future of Work noted, the quality of these  digitally induced or sustained learning pathways needs to be assured. Consequently , the  Commission recommends that governments create quality assurance mechanisms, ensuring  skills  are portable , for instance  (ILO Global Commission on the Future of Work, 2019, 31).   This concern also intersects with another problem  raised by Bales and Stone. These authors expect  workers to have a ' boundaryless workplace ' in the future, working for multiple  employers. With this   premise  in mind, they imagine  workers to have an electronic resume produced by AI  that captures   workers ' information as they  move from one employer to the next. The problem with this , however,  is that the information on th at resume is derived from various companies. In their view, '[i]f  competing companies share information about employees and use that information to make hiring,  discipline, promotion or other decisions, they run into several potential antitrust issues. ' For  example, 'consider an HR services company that uses Al to conduct video job interviews of  prospective employees, uses data analytics to construct a personality profile and predict future  performance from those interviews, then sells that information t o all companies who pay for its  services ' (Bales and Stone, 2020, 34- 35). Such practices could be in restraint of trade a nd at odds with  privacy regulation. Nor should the risks of discriminatory outcomes of these automated processes be underestimated, bas ed on what we discussed above. This also gives the opportunity to remark  another severe danger of algorithmic discrimination: it risks propagating much beyond the single instance in which it occurs. A low rating assigned by a biased or flawed program durin g an interview  or a performance review, for example, could be recorded in the system and affect future recruitment or review processes for all the employers using the same program or even different programs sharing, even partially, a dataset. Similar to ho w AI -determined training opportunities might steer  workers in certain directions, also reliance on AI to generate electronic resumes may wire the labour market in completely undesirable ways.   Having discussed some of the more specific ways in which AI may impact the world of work, we will now turn our attention to some more overarching discussions. First, a broader critique regarding AI   and related technologies  is that it threatens to resemble  paternalism. As one of the authors of the  present report has arg ued elsewhere, ' [e]ven the most well -intentioned measures, including  wellness programs, risk turning into forms of dystopian and paternalist control, unless a serious  reflection on the use of technology at the workplace is carried out ' (De Stefano, 2019, 30 ; see also  Ajunwa, Crawford and Schultz, 2017 ). People analytics, as such, already  threaten s workers sense of  autonomy  as their behaviour is increasingly recorded both in career and personal realms (Bodie et  al., 2017, 1040- 1041). Adding an AI to the mix that is designed to predict what workers will do and  achieve, based on what they have done in the past, seems to constrain that sense of autonomy further . 
AI and digital tools in workplace management and evaluation     31 Second, a c entr al overarching theme in the literature on AI is its  potentially  opaque nature. Many  scholars have warned how certain digital developments, in particular machine learning,  can lead to  so-called 'black boxes ', in which  case  the system 's operations are not easily accessible or too  complex to understand , even for their programmers (Pasquale, 2016). Although not all AI  systems  are necessarily black boxes, machine learning algorithms  do perform 'millions of calculations  following their own internal logic. Even if the decisions are good quality, it is very difficult – often  impossible – to explain the decision or its logic in a way that makes sense to human experts, let  alone for users, policy -makers, judges and juries ' (Boucher, 2020, 19). Given the difficulties  encountered , a broad  field of  literature has developed regarding the need and means to open up  such black boxes. Beyond the vital need to make the systems compatible with the rule of law and  due process (Kaminski and Urban, 2021) , some additional reasons to pursue transparency are  that :  (i) an improv ed understanding of the AI allows humans to improve it; (ii) transparency could increase  the AI 's legitimacy, increasing its acceptance among stakeholders ; and (iii) transparency may make  it easier for user enterprises to tailor the AI -product to their nee ds. There are undoubtedly other  advantages as well.  Still, some scholars have also rightly pointed out that transparency  or  'explainability'  does have drawbacks  (Candelon et al., 2021, 109- 110) . It would, for example, make AI   systems more vulnerable to attack  and hacking (Burt, 2019). Consequently, intending  to strike a  balance, some have argued in favour of more limited forms of transparency that solely explain the reasons behind a decision rather than the actual decision -making process  (de Fine Licht and de Fine  Licht, 2020). T he way in which these broader discussions on explainable AI will unwind will  undoubtedly influence the degree to which end -users, workers and their representatives will be able  to exert agency in relation to AI at wo rk.  Furthermore, t he issue of AI transparency intersects with the many questions regarding who is  accountable when an AI is involved in decision -making. For example, on the one hand, a person may  want to blame  an AI for something go ing wrong at work , even if there is no objective proof it had  anything to do with the AI . On the other  hand , an AI might also make an unforeseeable mistake  once  in a while , yet it could seem  as if  a human is to blame ( Matthews, 2020, 86- 87). In both instances,  who is accountable when the blame is shifted towards the AI or the human? Such questions about  accountability become even more complicated because third -party suppliers  are frequently   involved  (e.g. 'AIaaS'), further complicating  the traditional instances of vica rious liabilities at work .  Suppose a business developed an AI  system that is indisputably the best on the market to detect a  disease . Therefore, it seems rather likely for multiple hospitals to rely on th at very same system. Well,  ' [i]f algorithmic predict ions were automatically turned into action without additional scrutiny or  human monitoring, who becomes accountable for ' mistakes, ' misdiagnoses ' (Faraj, Pachidi and  Sayegh, 2018, 66)? The doctor, hospital and manufacturer will each have their arguments to  place   the blame  on someone else.   Moreover , quite regularly , we can expect neither the worker/doctor nor the employer/hospital to  have a concrete view on  how the AI reaches its decision. Also , employers might essentially have little  understanding a bout how a third party 's AI completes the ta sk, simply relying on the fact that it  generally produces  satisfactory results. Important to note in  this regard  is that  ignorance is no t a free  pass. Contr ary to workers, employers are free to decide to introduce AI tools in the workplace . As  Giovanni Gaudio remarks, they  'remain free to decide whether to recur to algorithmic management  tools, and are therefore responsible for this choice, from both a manage rial and legal point of view,  towards their workforce ' (Gaudio, forthcoming).   What makes matters worse is that AI systems are not static. The European Commission 's white paper  on AI is right to highlight that AI systems can be modified throughout their lifecycle (European  Commission, 2020b, 14).  This is not without its risks. Tesla , for example,  recently recalled thousands  of vehicles because a n error in its full self -driving beta software could supposedly lead to the  unexpected activation of the a utomatic emergency brake system ( Alamalhodaei, 2021 ). What if  malfunctioning  is triggered by an update and occur s on the work floor? Furthermore, a s mentioned 
STOA  | Panel for the Future of Science and Technology     32 in a subsequent chapter, in anti -discrimination lawsuits, it could become  even more challenging to  determine whether an AI exhibited the same discriminatory patterns throughout i ts lifecycle ,  regardless of changes to algorithms or data. This ability to change must be taken into account when  reflecting on  transparency, accountability, and effective oversight.   To summarise:   AI-enabled tools and algorithmic management will have serious  implications for the workplace . This  section  has stressed  how :  • They  will bolster  the datafication of work , which has some underappreciated  downsides.  • They  could  make work more precarious.   • They  boost businesses' surveillance capabilities at and beyond the workplace , also  invading  workers' private lives and spaces .  • They  can be used to frustrate union activities or  to make it more difficult to collectively  govern working conditions by shielding 'proprietary' AI from collective bargaining  and consultation practices .  • They could  improve workers ' occupational safety and health, but only if this is a  primary goal for its application . On the contrary,  without sufficient safeguards,  these  technologies engender  OSH risks .  • They  can mitigate some  discriminatory biases. However, unless adequate safeguards  are taken, A I systems  can easily  slip in the opposite direction, creating discriminatory  patterns of their own.   • They  might  achieve good results in training,  reskilling and upskilling  workers ; yet, the  quality of AI  applications in this field needs to be  further  assessed.   • They  could provo ke undesirable effects for the labour market on the whole . For  example, different employers can use the same AI tools in recruitment  to make  decisions . Having employers rely on the same – possibly flawed – personal data of  workers (e.g. through electronic resumes) may raise problems under competition law.   It may, moreover, make it unnecessarily complicate d for some people to gain access  to jobs because too many employers adopt a similar hiring approach.   • They  may stimulate  a paternalistic attitude in employment relations , reducing  workers' autonomy.   • They raise  transparency and explainability issue s as well as accountability questions.   • It is challenging to maintain adequate  oversight because of AI 's continual u pdates.    
AI and digital tools in workplace management and evaluation     33 4. Review of the regulatory context   4.1. Resolutions  of the European Parliament   Since  AI will affect many spheres of work ing life, having broad implications  for both workers and  businesses , the review of the regulatory context likewise covers many different fields of regulation .  Section 4.1. first details the various resolutions adopted  by the European Parliament . Section 4.2.  subsequently assesses current EU legislation , including the GDPR  and multiple  instruments on  workers' representation, OSH, working conditions and non -discrimination. Section 4.3. discusses the  ongoing negotiations related to the European Commission 's proposal for a R egulation laying down  harmonised rules on AI.  The section focuses on the interaction of this draft proposal with current EU  labour and employment laws.  Lastly, s ection 4.4. details the various other policy debates at the EU  level that could impact  AI governance at work.   The European Parliament has engaged in the AI -debate since 2017. In February of that year, it issued  a resolution with recommendations to the Commission on C ivil Law Rules on Robotics.35 This  instrument adopts  'a gradualist, pragmatic and cautious approach ' to not stifle innovation, mainly  focusing on civil liability issues. It  argues that robots are likely to become increasingly autonomous,  taking quasi -independent decisions and interacting with their environment. Consequently, the  legal responsibility for robots ' harmful actions becomes crucial. In order to come up with solutions,  the resolution calls on the Commission to, inter alia , establish an EU Agency for Robotics and  Artificial Intelligence ' in order to provide the technical, ethical and regulatory e xpertise needed to  support the relevant public actors, at both Union and Member State level, in their efforts to ensure  a timely, ethical and well -informed response to the new opportunities and challenges, in particular  those of a cross -border nature, aris ing from technological developments in robotics, such as in the  transport sector '.  Notably, the resolution 's annex also contains a Code of Ethical Conduct for Robotics Engineers that  stresses, for instance, the engineers' accountability for the social, env ironmental and human health  impacts. One critique  of the instrument  regarded its narrow focus. Aída Ponce Del Castillo, a senior  researcher at the European Trade Union Institute, argued : 'rather than trying to define robots, which  can be divided into milli ons of sub -types depending on their functionalities, the Commission should  adopt a wider approach, encompassing algorithms and AI, and take into consideration machines  that have the capacity to learn, evolve and eventually become semi - or, maybe one day, f ully  autonomous ' (Ponce Del Castillo, 2017, 6). Additionally, also the European Parliament' s mention of   the possible attribution of  legal personhood to robots raised concern. It could, for example, lead to  owners and producers shielding themselves from leg al liabilities, and it may increase the chances of  the commodification of the people working alongside these types of machinery  (De Stefano, 2019,  19-20). Spurred by the EP resolution, other scholars have explored the benefits and drawbacks of  attributing legal personhood to AI applications (e.g. Bertolini, 2020).   In subsequent EP resolutions, the Parliament has repeatedly emphasised the importance of laying  down clear ethical guidelines. This has been the case, for instance, in its r esolution of 15 January   2019 on autonomous driving in European transport.36 In its resolution of 12 February 2019 on a  comprehensive European industrial policy on AI and robotics, the Parliament even asserted ' that  Europe should take the lead on the global stage by deploying onl y ethically embedded AI '.37 It    35 European Parliament resolution of 16 February 2017 with recommendations to the Commission on Civil Law Rules on  Robotics , P8_TA(2017)0051.  36 European Parliament resolution of 15 January 2019 on autonom ous driving in European transport , P8_TA(2019)0005.  37 European Parliament resolution of 12  February 2019 on a  comprehensive European industrial policy on artificial  intelligence and robotics, P8_TA(2019)0081. 
STOA  | Panel for the Future of Science and Technology     34 furthermore expressed great concern about the use of emotional surveillance programmes, such as  those monitoring the mental conditions of workers.   On 20 October 2020, the European Parliament issued three additional AI resolutions. The first  resolution offered recommendations to the Commission on a civil liability regime for AI . In it, since  the majority of  MEPs are convinced that the Digital Single Market needs to be fully harmonised and  that regulatory fragmentation should be avoided, they firmly believe 'that the new common rules  for AI -systems should only take the form of a regulation '.38 The second resolution of 20 October  governs the intellectual  property rights for the development of AI  technologies.39 The third  resolution, and the most relevant one  for this study , contains recommendations on a framework of  ethical aspects of AI, robotics and related technologies. It advances a proposal for a Regulation on  'ethical principles for the development, deployment and use of artificial intelligence, robotics and  related technologies '.40 This proposal considers ' employment ' a high -risk sector; additionally,  recruitment is considered a high -risk use or pur pose of AI.   The proposal suggests c onducting  an impartial, objective and external risk assessment by the  national supervisory authority regarding such high- risk activities. That risk assessment is meant to  ensure specific characteristics. Notably, high- risk technologies are subjected to complete human  oversight at any time. Furthermore, they need to be resilient, have an adequate level of security,  reliability, accuracy and be easily explainable. The AI must, moreover, be set up to disclose  information to developers, deployers and users, and capable of being disabled or downgraded. The  processes driving these high -risk tools need to be ' documented to the highest possible and  applicable standards '. The software, algorithms , and data used or produced must als o be unbiased,  leading to no discrimination. The proposal also stresses that AI systems have to respect workers '  rights. Importantly, it contains a right to redress for natural or legal persons suffering injury or harm  in these respects. The principal pill ar of the proposed R egulation seems to be that high -risk AI is  subjected to a compliance assessment, after which the respective national supervisory authority might issue a 'European certificate of ethical compliance '. 41  Overall, the Parliament 's actions have not been in vain; they have arguably influenced the European  Commission 's AI Act  (de Matos Pinto, 2021). This proposal was made in April 2021. Pr eviously , in  June 2020, the Parliament had decided to set up a special committee on Artificial Int elligence in a  Digital Age (AIDA).42 AIDA 's mandate includes  analys ing the future impact of AI on the EU economy,  particularly  in areas such as employment. The AIDA rapporteur published a draft report on AI  in a  digital age in November 2021. It sets out th e AIDA Committee 's views on the Commission 's proposal.  At the time of writing the present study, AIDA members were preparing amendments for  consideration in the Special Committee. The forthcoming chapters make use of the draft report.     38 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a civil liability regime  for artificial intelligence , P9_TA(2020)0276.   39 European Parliament resolution of 20 October 2020 on intellectual property rights for the development of artificial  intelligence technol ogies, P9_TA(2020)0277.   40 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical  aspects of artificial intelligence, robotics and related technologies, P9_TA(2020)0275.   41 European Parliament resol ution of 20 October 2020 with recommendations to the Commission on a framework of ethical  aspects of artificial intelligence, robotics and related technologies, P9_TA(2020)0275.   42 European Parliament decision of 18 June 2020 on setting up a special committ ee on artificial intelligence in a digital age,  and defining its responsibilities, numerical strength and term of office , P9_TA(2020)0162.  
AI and digital tools in workplace management and evaluation     35 4.2. Current EU legislatio n  4.2.1.  EU Primary law   Before delving into the various pieces of EU legislation relevant to the discussion of how AI could be  used in workspaces, we would like first to highlight a couple of points regarding EU primary law.  Most notably, the Charter of Fundamental Rights of the European Union (Charter) became legally  binding on 1 December 2009. It has the same legal value as the Treaties. As such, ' [g]iven that the  provisions of the Charter are primary EU law, secondary EU law, as well as national law implementing  EU law, must be interpreted in the light of those provisions ' (Lenaerts, 2020, 19). Additionally, Article  52(3) of the Charter states that the meaning and scope of the fundamental rights of the Charter shall be the same as those laid down by the European Convention on Human Rights (ECHR). ' This means,  in essence, that the level of protection guaranteed by the Charter may not disregard that  guaranteed by the ECHR ' (Ibid ., 26). Based on these provisions, any assessment of how current EU  legislation relates to AI at work should inherently consider EU citizens ' rights under the Charter.  Moreover, the European Court of Human Rights ' (ECtHR) case law becomes relevant to the extent it  concerns a right in the Charter that corresponds to a right in the ECHR.   4.2.2.  Second ary Law. The General Data Protection Regulation   Coming to secondary law, the EU 's General Data Protection Regulation is a most relevant instrument  in this field . The Regulation became applicable in May 2018 , and some scholars immediately  deemed it to ha ve significant implications for AI developers that wanted their tools to draw on  'personal data'  (Zarsky, 2017; Wallace and Castro, 2018). Others have since argued, though, that  '[t]he GDPR can be interpreted a nd applied in such a way that it does not hinder beneficial  application of AI to personal data, and that it does not place EU companies at a disadvantage in  comparison with non -European competitors ' (Sartor, 2020, 79). The European Commission 's  independent  High -Level Expert Group on Artificial Intelligence  furthermore  remarked that the  GDPR 's fundamental rights -based approach 'should be fostered and its enforcement should be  ensured ' (High -Level Expert Group on Artificial Intelligence, 2019b, 28). The following paragraphs  seem to corroborate the view  that the GDPR does not excessively constrain the use of AI at work.   As recalled  in the very first R ecital of the GDPR, pursuant to Article 8 of the Charter, everyone has the  fundamental right to the protection of personal data concerning them . Personal data must be  processed fairly, for specified purposes and only based on consent or another legitimate ground.  Therefore, to the extent a n AI tool requires using  personal data, the GDPR and Charter might impose  lega l safeguards . In this regard, what can be  considered 'personal data'  is essential . According to  Article 4 of the GDPR, personal data ' means any information relating to an identified or identifiable  natural person ( 'data subject '); an identifiable natural person is one who can be identified, directly  or indirectly, in particular by reference to an identifier such as a name, an identification number,  location data, an online identifier or to one or more factors specific to the physical, physiological,  genetic, mental, economic, cultural or social identity of that natural person '. Arguably, this  is a broad  definition  that  principally encompass es most of the information relevant for HR purposes. In many  instances, HR analytics or algorithmi c management seems to require the information to be  connected to an identifiable person. Moreover, Recital 26 of the GDPR adds that also  'pseudonymised '43 personal data is covered, and '[t]o determine whether a natural person is  identifiable, account shoul d be taken of all the means reasonably likely to be used, such as singling  out, either by the controller or by another person to identify the natural person directly or indirectly '.    43 ‘Contentious though it is, personal data, "which ha[s] undergone pseudonymisation, which could be attributed to a  natural person by the use of additional information," should be considered as falling within its scope –something that is  very relevant for AI applications '' (Aloisi and Gramano, 2019, 103).  
STOA  | Panel for the Future of Science and Technology     36 This is relevant because AI  systems could  make it possible to surreptitiou sly retrieve personal  information, for example, through ' data fusion '44.  Regarding the GDPR 's scope, some scholars  have  convincingly argued  that even inferred data is  'personal data ' under the Regulation (Sartor, 2020, 38 and 40; contra Wachter and Mittels tadt, 2019).  This is also the European Parliament 's viewpoint. The Parliament ' is concerned that too often  companies ignore the fact that inferred data is also personal data, subject to all safeguards under  the GDPR '.45 Consequently, for example, if an employer automatically generates an employee 's  'attrition score ', then the data subject 's right to rectification and erasure under the GDPR  would   'apply to both the ' input personal data'  (the personal data used to create t he profile) and the ' output  data ' (the profile itself or 'score ' assigned to the person) ' (Article 29 Data Protection Working Party,  2018a, 18). According to researcher Christina Colclough, whether or not data subjects have control  over these inferences un der the GDPR is a key topic (Colclough, 2020). Overall, then, it seems the  GDPR is widely applicable to AI -related interactions at work , leading  to a series of legal  consequences.   Firstly,  data processing will only be lawful if a legal ground exists . Usual ly, an easily applicable legal  ground is ' consent '. However, for employers wishing to deploy AI -enabled managerial tools,  generally, employees ' consent to data processing is not a valid legal ground. Inter alia , based on  Recital 43 of the GDPR, the Article  29 Data Protection Working Party  (WP29) , which is made up of  the heads of the national data protection authorities  (DPAs) , stated the following: ' In cases where an  employer says they require consent and there is a real or potential relevant prejudice that arises  from the employee not consenting (which can be highly probable in the employment context,  especially when it concerns the employer tracking the behaviour of the employee over time), then  the consent is not valid since it is not and ca nnot be freely given ' (Article 29 Data Protection Working  Party, 2017, 6).46 Since, typically, an employee cannot genuinely  provide free consent, another  possible legal ground could be met if the processing is  'necessary for the performance of a contract  to which the data subject is party or in order to take steps at the request of the data subject prior to  entering into a contract ' (Article 6 (1)(b)). Yet, the existence of this legal ground depends on what  can be considered as ' necessary '. No one disputes, for example, that a certain degree of managerial  control is necessary for the performance of an employment contract; however, this does not make  it 'necessary ' to implement a series of intrusive AI -driven surveillance tech. Therefore, this legal  ground cannot amount to an overall  free pass – the instances when an AI system is genuinely  'necessary ' will be the exception rather than the rule.   Another flexible legal ground is to show that the processing is appropriate in light of employers '  'legitimate interes ts' (Article 6(1)(f)). Employers may have some  legitimate interests related to their  payroll, diversity policies, their right to instruct the workforce, and so forth (Hendrickx, 2019b, 159).  However, when relying on this legal ground, employers will have t o engage in a ' test of the balance  of interests, which includes a fundamental rights assessment '.47 They  also  have to assess whether  the processing is necessary, fair, proportionate and transparent (Adams -Prassl, 2019, 141). This test    44 'While  in isolation,  individual  data sets  disper sed across thousands of servers may provide limited information insights,  this limitation can be resolved by a process known as ' data fusion, ' which merges, organizes, and correlates those data  points. Once data is collected, synthesized, and analyzed, thi rd parties create sophisticated profiles of their ' data subjects '  that offer a trove of useful intelligence to anyone who wants to influence or manipulate purchasing choices and other  decisions ' (Manheim and Kaplan, 2019, 120).   45 European Parliament resolu tion of 25 March 2021 on the Commission evaluation report on the implementation of the  General Data Protection Regulation two years after its application , P9_TA(2021)0111.   46 The Working Party adds that even in the rather exceptional case when an employee 's consent is valid, because it is  undoubtedly freely given, ' it needs to be a specific and informed indication of the employee 's wishes'  (Article 29 Data  Protection Working Party, 2017a , 7).  47 European Parliament resolution of 25 March 2021 on the Commissio n evaluation report on the implementation of the  General Data Protection Regulation two years after its application , P9_TA(2021)0111.  
AI and digital tools in workplace management and evaluation     37 would, overall, entail that if ' an application provides benefits that are outweighed by the  disadvantages imposed on the data subje cts, we should conclude that the application fails to have  a basis according to Article 6(1)(f) ' (Sartor, 2020, 50). For instance, keystroke logging or screen  capturing practices  are not grounded under this test (Adams -Prassl, 2019, 141). The need for a le gal  ground might, in other words, provide a significant limitation to the grounded use of AI -enabled  monitoring applications at work. Again, this also depends on consent not being a valid legal ground  in the context of work. The other aforementioned legal grounds, such as (b) and (f), as well as ground  (c), which allows processing that is necessary for compliance with a legal obligation,48 will allow for  some AI -applications to be l awfully installed; yet, the respective  'necessity test s' and 'balance of  interests test ' sever ely restrict the use of many of the most intrusive AI applications.   Crucially , the impossibility of relying  on consent as a legal ground does  not seem to be l imited to  employees only  but could apply regardless of employment status .49 For instance, also in the case of  a digital labour platform and a self -employed worker, a clear imbalance exists between the data  controller (platform) and the subject (worker). The same is true for the r elation ship  between many  principals and their (nominal) contractors. Hence, there might be a need for another legal ground,  such as  'legitimate interests ', also in these  cases. Furthermore , reliance on consent at the recruitment  stage could be questionable  as well . Much in the same way an employee cannot freely give consent,  due to the risk of negative consequences, a jobseeker may well experience similar pressure to consent to any AI -driven recruitment tool . Jobseekers have a  legitimate fear of not being s elected  when opposing to this , or also because the only way of applying , in practice, is through that tool . To  that extent, if consent is also an inconceivable legal ground for jobseekers (Kullmann, 2019, 50; Kelly Lyth, 2021b), it does seem probable for s ome AI -enabled recruitment tools to pass the ' test of the  balance of interests '.  Secondly, the GDPR adheres to a ' purpose limitation ' principle, meaning personal data should only  be collected for specified, explicit and legitimate purposes. Data is not to be processed in a manner incompatible with those initial purposes. In this sense, there seems to be a tension between this principle and big data  and  AI-related practices. This tension springs from ' the use of algorithms  (which means the purposes may not b e explicit when the data is collected), the tendency to collect  all the data (which means certain data may be collected for an unspecific, imprecise or inexplicit  purpose), the repurposing of data (whereby data collected to improve performance of one servi ce  may be used and re -applied to new or third party services) and the generation of new types of data  (again, where it is not clear to the individual what purpose the data will be used for from the outset) '  (Butterworth, 2018, 260). Regardless of the detai ls, one can immediately sense unbridled AI  processing at work  is incompatible with this principle . According to Giovanni Sartor, if personal data  is being used for individualised inferences, the following criteria need to be rigorously applied to evaluate the legitimateness of repurposing data: ' (a) the distance between the new purpose and the  original purpose, (b) the alignment of the new purpose with the data subjects ' expectations, the  nature of the data and their impact on the data subjects ' interests, and (c) the safeguards adopted  by the controller to ensure fair processing and prevent undue impacts ' (Sartor, 2020, 45- 46).50    48 Since employers have a duty to keep employees safe, they might be entitled to engage in processing of personal data  to that end under ground (c). Yet, again, the reliance on AI tools in this pursuit should be necessary to comply with this  legal obligation, which might generally not be the case.   49 Notably, in its Opinion 2/2017, whenever the Working Party uses the word 'empl oyee', it ' does  not intend  to restrict  the  scope of this term merely to persons with an employment contract recognized as such under applicable labour laws.  Over the past decades, new business models served by different types of labour relationships, and in particular  employment on a freelance basis, have become more commonplace. This Opinion is intended to cover all situations  where there is an employment relationship, regardless of whether this relationship is based on an employment contract '  (Article 29 Data Protection Working Party, 2017a , 4).  50 These criteria are found in O pinion 03/2013 on purpose limitation from the Article 29 Data Protection Working Party.  See also Article 6(4) GDPR.  
STOA  | Panel for the Future of Science and Technology     38 Besides the principle of purpose limitation, the GDPR also strives for quality data. It mentions that  'every reasonable step must be taken to ensure that personal data that are inaccurate, having regard  to the purposes for which they are processed, are erased or rectified without delay ' (Article 5(1)(d)).  This accuracy principle poses a challenge to AI practices because the data they run on may not  always be free of errors. As such, this principle offers data subjects a ground to intervene  (Article 18) .  Furthermore, the GDPR also sticks to a ' data  minimisation ' principle, implying only the necessary  personal data is collected. Accordingly, it is also prohibited to keep personal data when they are no  longer needed. In light of how AI operates, data minimisation might also raise some issues (Mitrou,  2019, 49- 51; Gaudio, forthcoming); for example, recruitment may involve the collection of all  available information on the web , covering also personal data that is irrelevant to someone 's  aptitude for the job (Article 29 Data Protection Working Party, 2017, 11). Likewise, if a worker is  equipped with wearables, is all the potential data that these wearables generate necessary? Since it  often concerns health data, there are additional guarantees, making it particularly dubious whether  using  these tools  in HR  analytics is allowable (Article 29 Data Protection Working Party, 2017, 18).   Indeed, another potential legal limitation  for some AI functionalities relates to Article 9 of the GDPR.  In principle, it prohibits processing sensitive personal data ' revealing racial or ethnic origin, political  opinions, religious or philosophical beliefs, or trade union membership, and the processing of  genetic data, biometric data for the purpose of uniquely identifying a natural person, data  concerning health or data concerni ng a natural person' s sex life or sexual orientation '. This general  prohibition has many exemptions; for instance, sensitive data that are manifestly made public by the data subject can be used. Nonetheless, Article 9 imposes an additional barrier that nee ds to be  considered when dealing with sensitive data despite these exemptions . This is most notably  relevant for what concerns ' biometric data ' 51. In this respect, an important open question is to what  extent Article 9 constrains employers from relying on a combination of proxies to infer sensitive  data.52  Subsequently, assuming an employer lawfully engages in AI -driven processing of workers ' data,  meeting  all the aforementioned requirements , the GDPR confers a broad range of rights to data  subjects. The fi rst notable right concerns access to information regarding the processing of the data  subject 's personal data (Article 15). This individual right comes on top of data controllers ' obligation  to provide certain forms of information as standard (Article 13, 14 and 3053). This ' principle of  transparency ' is crucial. The WP29 has given clear indications about what this principle implies. For  example, data controllers should 'separately  spell out in unambiguous language what the most  important consequences of the processing will be: in other words, what kind of effect will the  specific processing described in a privacy statement/ notice actually have on a data subject? ' (Article  29 Data Protection Working Party, 2018b, 7). In the case of automated decision -making, which an AI  could drive, the articles demand ' meaningful information about the logic involved, as well as the  significance and the envisaged consequences of such processing for the data subject '. While some  might want to describe this as a ' right to explanation ', there nevertheless remains significant    51 '‘biometric data ' means personal data resulting from specific technical processing relating to the physical, physiological  or behavioural characteristics of a natural person, which allow or confirm the unique identification of that natural person,  such as facial images or dactyloscopic data'. Article  4(14) GDPR.   52 'To take a pre -processing example, if an applicant 's CV states that they are the trustee of a charity that supports deaf  people, the CV is not ‘special category data' , even though the employer might infer that the applicant is deaf. Similarly,  even if one ca n infer ethnicity or religion from a name, the name is not generally ‘special category ' unless it is being  processed with the intention of inferring such information ' (Kelly -Lyth, 2021b).   53 Article 30 contains a duty for ea ch data controller and processor employing 250 persons or more to maintain a record  of processing activities  containing all the information mentioned in Article 30 GDPR. Controllers and processors with less  employees are in principle exempted. This is ' unless the processing it carries out  is likely to result in a risk to the rights  and freedoms of data subjects, the processing is not occasional, or the processing includes special categories of data'.  The question is whether a 'smaller' employer that relies on AI can still be exempted from the duties in Article 30, or will  generally have to comply.  
AI and digital tools in workplace management and evaluation     39 uncertainty about what information these articles precisely demand from data controllers (Turner,  2019, 329). What is clear, howev er, is that employers owe workers  extensive insight, and, moreover,  DPAs should  enforce these rules if  an employer refuses to comply.   In this regard, it is essential to note that access to information can be seen as an enabling right meant to buttress the effective use of other GDPR rights. Argu ably,  ' the data controller has to ' explain ' the  decision in such a way that enables the data subject to assess whether the reasons that led to a  particular outcome were legitimate and lawful ' (Bayamlıoğlu, 2021; see also Kaminski and Urban,  2021, 1980). The right to access information should allow workers to effectively assert their other rights, which, in fact, might restrict the use of  AI-driven recruitment or algorithmic management.  First of all, data subjects have  a right to rectification (Article 16). Yet, it is not easy to assert that right  if workers do not know on the basis of  what data the AI generates its assessments. One cannot rectify  the unknown. What seems more practical is that if there is a legitimate g round, the data subject can  ask to erase personal data ( 'right to erasure ') (Article 17). That right, moreover, implicitly provides  the data subject with the possibility to contest the legitimacy of ongoing data processing. A nother   approach in this regard is for the data subject to ask for a restriction of processing instead of erasure (Article 18). This could happen, for example, if the data controller successfully manages to oppose  the subject 's claim to erase the data.   Additionally, a central  right for w orkers in relation to AI -driven tools at work is Article 21 's right to  object to data processing, including profiling, if that processing is  based on an employer 's legitimate  interests. As mentioned above, ' legitimate interests ' will often be a legal ground referred to by  employers . Accordingly , even if the balance of interests initially allowed for the AI tool to be used in  the employer 's legitimate interest, the individual worker s can still object on grounds relating to their   particular situati on. The data controller has to demonstrate compelling legitimate grounds to  counter the objection. In addition , Article 22 provides for a  right for the data subjects  not to be  subjected to automated processing alone if this processing produces legal effect s concerning them   or significantly affects them . More specifically, it offers data subjects a right to human intervention, 54  a right to express their points of view and a right to contest the automated decision. Its guarantees  are of  the utmost relevance f or automated decision -making in the world of work.   Article 22 has, for example, already been invoked  in cases regarding digital labour platforms, such  as Uber and Ola. One notable observation about  these cases is that it has been difficult for workers  to p rove that the platform 's decision -making is truly fully automated.55 Oftentimes, platforms can  argue that a human is actually in the loop before making the decisions; hence , Article 22 would  not  apply.56 This is indeed a  gutting limitation on the applicati on of Article 22. Although disputed, some  have argued that any level of human intervention would entail the decision is no longer ' automated '  (Todol í-Signes, 2019, 471) – this results in the fear that companies may use a dressed -up 'straw  man '57 to neutralise these rights. Moreover, in one instance in which the automated nature of the  decision was not disputed, Uber 's decision to automatically suspend an account in response to a  'fraud alert ' was not even deemed  to have  significantly affect ed that worker, according to  a decision    54 'Any review must be carried out  by someone who has the appropriate authority  and capability  to change the decision.   The reviewer should  undertake a thorough assessment of all  the relevant data, including  any additional  information   provided  by the data subject' (Article 29 Data Protection Working Party, 2018a, 27).   55 In this regard, the cases illustrate the broader imbalance between data subjects and digital giants (Degli -Esposti and  Ferr ándiz, 2021, 11- 12).  56 Rechtbank Amsterdam 11 maart 2021, Case No.  ECLI:NL:RBAMS:2021:1019.   57 'The controller  cannot avoid  the Article 22  provisions  by fabricating  human  involvement.  For example, if  someone  routinely  applies  automatically  generated profiles  to individua ls  without  any actual influence  on the result,  this would   still be a decision  based solely  on automated processing. To qualify  as human  involvement,  the controller  must ensure  that any oversight  of the decision  is meaningful,  rather than just  a token gesture. It should  be carried out  by someone  who has the authority and competence to change the decision.  As part of the analysis,  they should  consider all  the  relevant data' (Article 29 Data Protection Working Party, 2018a, 20- 21). 
STOA  | Panel for the Future of Science and Technology     40 of the Tribunal of  Amsterdam.  The Tribunal , therefore, refused to apply  Article 22,58 a decision that  seems questionable, also considering a relevant opinion of the WP29.59 The same decision was made  for the allegedly auto mated way in which Uber connects drivers to passengers.  The Tribunal in  Amsterdam state d that it is evident that the ' batched -matching system ' and the ' upfront -pricing  system ' will have an  influence on the performance of the contract between Uber and the d river;  however, in the opinion of the Tribunal, the claimants had  not proven these systems ' legal effects or  any significant consequences for the data subjects.60 What is more, even if  Article 22 did apply,  according to this ruling, the platforms could still have tried to prove that the automated decision making was ' necessary ' for the performance of the contract. The (employment) contract could thus   provide for a justification to engage in profiling or fully  automated decision -making, further limiting  Article 22' s potential for workers (Hendrickx, 2019b, 168).   These three cases in which platform workers tried to rely  on the GDPR to make their case  about  automated management  are fairly illustrative of the broa der interaction between the GDPR and  algorithmic systems. The GDPR imposes many theoretical restrictions, and, in principle, it offers  valuable rights to data subjects. Yet, ultimately, the workers in question did not manage to gain  much, apart from being provided insight into their personal data and its use in certain respects   (Worker Info Exchange, 2021) . The rulings seem to show that the GDPR alone will not pose an  unsurmountable barrier against algorithmic management . Instead, ' it is possible – and inde ed likely  – that the GDPR will be interpreted in such a way as to reconcile both desiderata: protecting data  subjects and enabling useful applications of AI ' (Sartor, 2020, 76).   This process of interpretation, which is needed because of the often vague and open formulation in  the GDPR, is still in its infancy for what concerns automated decision -making and AI -induced  inferences at work.61 Although there is some degree of scepticism (Aloisi and Gramano, 2019, 108),  the GDPR seem s to have some potential to partially  prevent  the social harm related to these  practices  – much will, of course, depend on its interpretation.   One crucial issue is the extent to which employers will implement ' data protection by design and by  default ' (Article 25).62 Likewise, it rema ins to be seen how thoroughly businesses conduct ' Data  Protection Impact Assessments ' (DPIAs) because of the high risk of their AI practices to workers '  rights and freedoms (Article 35). Although DPIAs can make a critical contribution (Kaminski and  Malgier i, 2021), they suffer from some complications. For example, as data controllers, employers  are responsible for performing a DPIA even if they rely on recruitment software from a third- party  supplier . This is problematic  whenever suppliers  collaborate insufficiently. Employers often lack the  necessary technical expertise to make a meaningful assessment of the impact of algorithmic  systems (Kelly -Lyth, 2021b). Furthermore, most importantly, the effectiveness of DPIA self assessments will also depend on several  other factors. First, unions and workers ' representatives are    58 Rechtbank Amsterdam 11 maart 2021, Case No. ECLI:NL:RBAMS:2021:1018.   59 'It is difficult  to be precise about what would  be considered sufficiently  significant to meet the threshold,  although  the  following  decisions  could  fall into  this category: […] decisio ns that deny someone an employment  opportunity  or put  them at a serious disadvantage ' (Article 29 Data Protection Working Party, 2018a , 22).  60 Rechtbank Amsterdam 11 maart 2021, Case No. ECLI:NL:RBAMS:2021:1020.   61 The European Parliament ' calls on the EDPB  to issue guidelines that classify different legitimate use cases of profiling  according to their risks for the rights and freedoms of data subjects, along with recommendations for appropriate  technical and organisational measures, and with a clear delineation of illegal -use cases; […] encourages the EDPB to  clarify data processing for human resources purposes ' European Parliament resolution of 25 March 2021 on the  Commission evaluation report on the implementation of the General Data Protection Regulation two years after its  application , P9_TA(2021)0111.   62 'This means that data controllers have to be proactive and continually assess the privacy impact of technology '  (Hendrickx, 2019b, 161). For an example: (EDPB, 2020, 24).  
AI and digital tools in workplace management and evaluation     41 largely not involved in DPIAs, even though the WP29  calls upon employers to do so.63 Second,  external DPAs ' degree of active involvement  is essential (Article 36). These are meant to oversee  this  process of compulsory self -assessment and businesses'  overall compliance with GDPR (e.g. on H&M,  see Hamburg Commissioner for Data Protection and Freedom of Information, 2020). At present,  however, the European Parliament '[e]xpresses  its concern about the uneven and sometimes nonexistent enforcement of the GDPR by national DPAs more than two years after the start of its  application, and therefore regrets that the enforcement situation has not substantially improved  compared to the si tuation under Directive 95/46/EC '.64  With this in mind, it remains to be seen whether the GDPR 's tools can get a grip on the massive  amount of semi -automated data processing that is bound to arise in the world of work. A few  things  are important to mention in this regard. First, the GDPR is not being utilised fully. The Parliament is  convinced the D PAs are understaffed and underfinanced; it also regrets that the Member States  have not made recourse to  Article 80(2) of the GDPR.65 That provision could entitle trade unions to  lodge complaints and go to court without being mandated by data subjects, i.e . workers (Pato,  2019). Similarly, according to the European Economic and Social Committee, Article 88 of the GDPR  'gives Member States the option to establish more specific rules (through legislation or collective  agreements) to guarantee the protection of rights and freedoms with regard to the processing of  employees ' personal data within the framework of employment relationships, and this provides  genuine leverage that the states and social partners must use ' (EESC, 2018). So far, this advice does  not se em to be followed . Article 88 of the GDPR, which foreshadows an essential role of the social  partners in the governance of data collection and processing, including in the context of automated  decision -making, is still massively underutilised.  Furthermore , it is also important to keep thinking  beyond the GDPR  and privacy protections (De Stefano and Taes, 2021).  Some scholars , for instance,   argue in favour of an additional ' Privacy Due Diligence ' requirement embedded in a human rig htsbased corporate responsibility scheme (Ebert, Wildhaber and Adams -Prassl, 2021).   What should also not be overlooked is how the GDPR intersects with a broader field of privacy law ,  made up of, among other instruments,  the Council of Europe 's Modernised Convention for the  Protection of Individuals with Regard to the Processing of Personal Data . As noted, the GDPR 's  preamble explicitly references the Charter 's right to data protection and right to privacy. Bearing in  mind the Charter 's interconnectedness w ith the ECHR, it is important to highlight the role of the  ECtHR in, arguably, positioning privacy rights as a means to safeguard workers ' autonomy. The  Court 's case  law has firmly established the right to privacy in an employment context.66 Along those  lines, European courts seem to increasingly recognise, for example, a ' right to be let alone ' in some  areas of working life and a ' right to human interaction ' at work (Hendrickx, 2019a). Such  entitlements, flowing from the right to privacy, are fundamental to any discussion about the  legitimacy of AI -supported screening and surveillance of workers and jobseekers. The ECtHR  furthermore safeguards workers ' ability to develop a ' social identity ', providing a human rights  foundation for the humanisation of work ( Hendrickx, 2019b, 170- 172). These developments should    63 'The controller must  'seek  the views  of data  subjects  or their  representatives ' (Article  35(9)), 'where appropriate '. The  WP29 considers that: - those  views  could  be sought  through  a variety  of means,  depending  on the context (e.g.  a generic  study related to the purpose and means of  the processing operation, a question to the staff representatives, or usual  surveys sent to the data controller 's future customers) […] - if the data controller 's final decision differs from the views of  the data subjects, its reasons for going ahead or n ot should be documented ' (Article 29 Data Protection Working Party,  2017b, 15).   64 European Parliament resolution of 25 March 2021 on the Commission evaluation report on the implementation of the  General Data Protection Regulation two years after its applic ation , P9_TA(2021)0111.   65 European Parliament resolution of 25 March 2021 on the Commission evaluation report on the implementation of the  General Data Protection Regulation two years after its application , P9_TA(2021)0111.   66 European Court of Human Rights 16 December 1992, Case No. 13710/88, Nimitz v. Germany; European Court of Human  Rights, 5 September 2017, Case No. 61496/08,  Bărbulescu v. R omania; European Court of Human Rights 28 November  2017, Case No. 70838/13, Antović and Mirković v. Montenegro . 
STOA  | Panel for the Future of Science and Technology     42 be taken into account, for instance, to make widespread abstract discussions about  ethics and AI   more anchored to real -world fundamental rights . AI systems should indeed prevent harm to privacy  (High -Level Expert Group on Artificial Intelligence, 2019a, 17); beyond the GDPR, other sources  enshrining workers ' general right to privacy can offer additional protection .  4.2.3.  Transparency and worker involvement   As mentioned above, the GDPR obliges employers to share information about their data processing,  and it provides workers with a right to information. In the general literature regarding the GDPR 's  effects on AI systems, many scholars express scepticism abo ut the effectiveness of imposing such  obligations and granting such rights (Mitrou, 2019, 53- 59; Sartor, 2020, 53- 56). Some have even  suggested that relying on individual information rights to take control of machine learning systems may create a ' transpar ency fallacy ' (Edwards and Veale, 2017, 67; see also Schubert and Hütt, 2019,  12).  Labour law scholars, in particular, add that the GDPR focuses on individual data subjects ' rights to  information . In contrast,  labour and employment regulation s more largely  rely on collective rights  and the involvement of  workers ' representatives, which are supposed to have more expertise  (Hendrickx, 2019b, 169). In time, representatives could, for example, become more knowledgeable about how to effectively invoke the variou s rights on offer in the GDPR than an individual data  subject could. Furthermore, unions can engage in collective bargaining to move beyond mere  'transparency ' (De Stefano and Taes, 2021, 9). They could aim to establish a duty for the employer to  offer pra ctical explanations , consultations  and collective bargaining  regarding AI -driven systems.  They could demand comprehensible answers to questions, such as how does the system work? And,  who is responsible for the way it works (Floridi and Cowls, 2021, 12)? B ut also, importantly, should  that system be allowed in the first place (De Stefano, 2020)? That collective dimension of data  protection, reliant on collective actors instead of individual data subjects to make progress, is still  significantly underdevelope d (Dagnino and Armaroli, 2019; Todol í-Signes, 2019).   In this regard, a poignant question is how the already existing EU legal framework on collective  labour rights intersects with this finding. EU Directive 2002/14/EC establishes a general framework  for in forming and consulting employees in undertakings that employ at least 20 or 50 employees,  depending on the EU Member State' s choice. This duty to inform and consult covers any anticipatory  measure that forms a threat to employment, and any decision that le ads to ' substantial changes ' in  work organisation (Article 4(2)(c)). In certain instances, employees ' representatives might have to, in  other words, be informed and consulted about the implementation of new AI tools on an  ad hoc   basis. If it concerns ' substantial changes ' to work organisation, then those consultations are meant  to be conducted with a view of reaching an agreement (Article 4(4)(e)). More broadly, employees '  representatives are also to be informed, on a regular basis, about the recent and  probable  development s of the undertaking 's or the establishment' s activities and economic situation (De  Stefano, 2019, 43). The effectiveness of representatives ' right to information and consultation will  partially depend on how domestic legislation  and c ourts have  implemented the Directive. For  example, at what point does an AI lead to ' substantial changes ' to work organisation?  Should  the  'substantial ' nature of the change even matter ? For instance, in Spain, the recent ' riders law ' has,  more broadly,  advanced a ' right of workers ' representatives to be informed about the parameters,  rules and instructions on which the algorithms that may have an impact on working conditions are  based ' (Todolí -Signes, 2021c) .  The relatively scarce attention that is being p aid to Directive 2002/14/EC and its role in guiding AI  implementation is quite surprising. All the more because research suggests that worker representation ' has the potential to significantly facilitate the introduction of new technologies in  the workplac e', including AI  (Janssen, 2021, 6; see also Autor, Mindell and Reynolds, 2020, 73; Belloc, 
AI and digital tools in workplace management and evaluation     43 Burdin and Landini, 2020).67 As such, if one of the objectives of the EU is to foster the sustainable  introduction of AI in society, there is every reason to engage in a discussion on the role of unions,  workers ' representatives and employers ' organisations  in this regard. Moreover, in addition t o  economic considerations, participation by workers, their representatives and the social partners  is  also recommended from a human rights and ethical point of view (High- Level Expert Group on  Artificial Intelligence, 2019b, 13;  ILO Global Commission on th e Future of Work, 2019, 41- 42; United  Nations High Commissioner for Human Rights, 2021, 16).68 Thirdly, some argue that strong collective  bargaining frameworks will make it more likely for decent jobs to arise as a result of AI (Deshpande  et al., 2020, 29) . Social dialogue is, in other words, also important in terms of desirable job creation.   As the European Social Partners Framework Agreement on Digitalisation from June 2020 illustrates,  social partners acknowledge the benefits of AI. The document calls up on social partners at all levels  to 'pro-actively explore the potential of digital technology and AI to increase the productivity of the  enterprise and the well -being of the workforce, including a better allocation of tasks, augmented  competence developmen t and work capacities, the reduction of exposure to harmful working  conditions. ' Similar to other policy instruments ( EESC, 2017 , 11; ILO Global Commission on the  Future of Work, 2019, 43), the document  also  argues in favour of a ' human in control principl e'. The  European social partners furthermore refer to  principles of fairness (no unfair bias and  discrimination), safe deployment of AI through a risk assessment, and transparency and explicability  with effective oversight. Workers affected by AI systems i n HR procedures should be able to ' make a  request for human intervention and/or contest the decision along with testing of the AI outcomes. '69  Many of these principles and requirements already have a basis in existing EU legislation.   4.2.4.  Occupational safety an d health   The EU Framework Agreement on Digitalisation  calls  for 'trustworthy AI ' to be subjected to a risk  assessment, 'including opportunities to improve safety and prevent harm such as for human  physical integrity, psychological safety, confirmation bias or cognitive fatigue '. As mentioned earlier,  one of the great  yet largely unfulfilled  promises of new technologies has to do with their ability to  keep workers safer and healthier. Many existing safety and health directives already attemp t to  ensure workplaces develop in this direction.   Notably,  the Framework Directive 89/391/EEC imposes a duty upon employers ' to ensure the safety  and health of workers in every aspect related to the work. ' Employers must take measures, including  preventive  ones, to preserve workers ' safety and health, and, most importantly, they need to remain  alert, adjusting the measures as necessary (Article 6(1)). Article 11 obliges employers to consult  workers and/or their representatives, allowing them to engage ' on a ll questions relating to safety  and health at work. ' The preamble of the Directive clarifies this issue:   'employers shall be obliged to keep themselves informed of the latest advances in technology and  scientific findings concerning workplace design, accou nt being taken of the inherent dangers in  their undertaking, and to inform accordingly the workers ' representatives exercising participation    67 'The – rather negative – argument whereby management is supposedly investing in automation and robots to  circumvent  adversarial  labour  relations,  rigid  job protection  or the influence  of worker  representatives  on dismissals,  is  rejected  by the econometric  analysis.  Instead,  the empirical  evidence  points  at positive mechanisms explaining the link  between worker representation and investment in automation ' (Janssen, 2021, 6).   68 The High Commissioner recommends that States and businesses ensure ' participation of all relevant stakeholders in  decisions on the development, deployment and use of AI, in particular affected individuals and groups '. The Expert Group  is far more concrete . It recommends to inform and consult ' workers when developing or deploying AI, as set out in the  existing texts adopted by the European institutions and the social partners. Workers (not only employees but also  independent contractors) should be involved in discussions around the development, deployment or procurement of  algorithmic scheduling and work distribution systems, to ensure compliance with health and safety legislation, data  policy, working time legislation and work -life balance legislation. Soci al dialogue plays a key role to enable this. '  69 European Social Partners Framework Agreement on Digitalisation , June 2020.  
STOA  | Panel for the Future of Science and Technology     44 rights under this Directive, so as to be able to guarantee a better level of protection of workers '  health and safe ty'.  Since employers are responsible for every OSH aspect related to the work, they are also in charge of  preventing mental health risks (Stavroula and Aditya. 2014). As such, due to the psychosocial risks  that AI tools may induce, the Framework Directive becomes very important in obliging employers  to consider workers ' experiences (see also the 2004 EU Framework Agreement on Work -Related  Stress). Employers are effectively obliged to consider how algorithmic management might hurt their  workforce 's safety an d (mental) health (EU -OSHA, forthcoming). At the same time, some have  argued that this is not sufficient. Employers might be inclined to downplay the importance of 'invisible ' software on OSH. Adopting a separate, more detailed standard that stresses what is at  stake would probably incite  employers to address the risks more adequately (Todolí -Signes, 2021a ;  Cefaliello, 2021).  For example, the recent proposal of the European Commission for a Directive 'on  improving working conditions in platform work ' stresses the need for digital labour platforms to  evaluate their algorithmic systems  for OSH risks. 70  In addition to the Framework Directive, under Directive 2009/104/EC on Work Equipment,  employers must also ensure that work equipment is safe. This instru ment  likewise relies on  consultation with workers ' representatives. It contains various minimum requirements, among  other things, focusing on self -propelled work equipment. W orkers need to, among other things , be  appropriately trained to engage with these  tools . The instrument 's approach, with various annexes  detailing minimum requirements, is quite flexible . Adding AI -related minimum conditions could be  a viable option .  Furthermore, besides keeping employers to account, EU law can also hone in on businesse s that  provide  the tools workers use. For example, the Machinery Directive 2006/42/EC aims to ensure a  high level of safety and protection for persons exposed to machinery. In an evaluation of this  Directive, stakeholders mentioned that the existing instru ment might insufficiently cover machinery  with autonomous functions due to complex software systems (Simmonds, Brown and Rentel, 2017, 48-49). This was one reason why the European Commission advanced a proposal for a Regulation  on machinery products. This proposal is mentioned in section 5.4.   4.2.5.  Working conditions   As mentioned in previous chapters, AI -enabled practices at work  can drive workers to the breaking  point, which is one reason why OSH -related consultations are vital. An algorithm that principally  aims to optimise logistics might not account for its impact on working conditions; an autonomous  robot might operate at an unsustainable pace for workers  who are in the same environment ; an AI driven scheduling tool may jeopardise  people 's work -life balance  and job stability . In light of the   risks, beyond  OSH directives, other EU legislation aimed at protecting working conditions, such as  working time,  is relevant in governing AI algorithmic management at work .  For example, since robots can theoretically operate 24/7, increased robotisation may lead to increased working hours for workers that engage with such systems. On the other hand , assuming  robots and AI systems are displacing labour, policymakers may decide  to reduce overall working  hours or increase levels of part -time work (Servoz, 2020, 99). Future developments may, in other  words, have disparate consequences for working time arrangements. Working time is currently  primarily governed by the Working Time D irective and Part -Time Work Directive. However, so far,  the implications of AI -enabled tools and algorithmic management in the se respect s are still largely  unmapped. For a start, it is important to monitor whether AI -related developments put significant  stress on any of the rights attributed to workers in these Directives. It is not inconceivable that    70 Article 7 of the proposal for a Directive of the European Parliament and of the Council on improving working conditions  in platform  work, 9 December 2021.  
AI and digital tools in workplace management and evaluation     45 certain problems may arise. If workers have a rest break, for instance, are those workers truly left  alone by the AI systems with which they interact in the c ourse of work? AI systems may effectively  entice the worker to engage in training activities  while taking a break – also through ' gamification '  techniques  – or could de facto  penalise the worker for taking a break.71 Likewise, the Working Time  Directive co ntains annual leave entitlements for workers. Technology is frequently used to help  businesses decide about holiday allocation (TUC, 2020, 21). To that extent, as the Court of Justice of the European Union (CJEU) ruled, ' an employer that does not allow a w orker to exercise his right to  paid annual leave must bear the consequences. '72 In no way should an employer be allowed to hide  behind an algorithm in this respect.   Another issue that does seem to deserve some attention relates to standby time. The CJEU ha s had  to specify when standby times ought  to be considered 'working time ' under the Working Time  Directive.73 The question is important because if standby time is seen as spare  time instead of  working time, the workers ' standby time is left out of the equation for all rights under the Directive.  Based on the CJEU 's jurisprudence, by and large, if employers allow workers to remain standby  elsewhere than at the workplace, most notably, standby at home, then whether or not this standby time should be perceived as working time depends on the extent to which the workers can meaningfully devote time to their personal interests. Judges have to look at the facts. Do those facts  indicate that workers can genuinely spend time on  their personal interests, regardless of remaining   on standby? If not, the standby time counts as working time. The problem with this approach is that,  in the future, it is not inconceivable for AI in scheduling tool s to be relied upon to increasingly make  sure workers ' standby time should be legally classified as spare  time instead of working time.   Current regulations on working time are not airtight . Considerable debates have already arisen  regarding the abuse of on- call work. In that case, an agreement is signed without a clear  commitment from the employer about the amount of work supplied to the worker. The recent mass  adoption of ICT technologies has enabled constant connectivity between workers/jobseekers and  emp loyers/work providers (Spencer et al., 2021, 38). That digital environment enables businesses to  work towards a ' just-in-time workforce ', offering work as it arises (De Stefano, 2016). All of this leads  to variable working hours for employees  and  an unstab le income. One possibility could be for AI to  further bolster this development and  facilitate forms of ' contractual distancing ', driving  misclassification among remote workers ( Countouris and De Stefano, 2021) . Platform workers, such  as food -delivery bikers, are already experiencing great uncertainty with erratic earnings (Schor et al.,  2020). These platforms offer an extreme example because workers ' are almost invariably classified  as 'self-employed '; therefore, they are denied access to most labour protec tions.   Nor we should think  employees are spared from algorithmically- induced employment insecurity  (Kresge, 2020, 32). The relatively recent Directive (EU) 2019/1152 on transparent and predictable  working conditions contains provisions to address businesse s' abusive reliance on on- demand  employment contracts. Article 10 grants workers with an entirely or mostly unpredictable work pattern the right to refuse a work assignment without suffering  adverse consequences unless  employers ' on-call practices comply with that Article 's preconditions. Article 11 additionally calls on  the Member States to take measures to counteract the abusive use of on -demand employment    71 'A video screen leaderboard system for the housekeeping staff at Disneyland hotels in Anaheim, California generated  significant anxiety, embarrassment, and shame among workers, who labeled it ‘‘the electronic whip '' (Lopez 2011).  Seeing their performance ranked against that of coworkers on a large screen often caused some workers toskip  bathroom breaks and others to become panicked about losing their jobs ' (Kim and Werbach, 2016, 166).   72 European Court of Justice 29 November 2017, Case No. ECLI:EU:C:2017:914, Conley King v. The Sash Window Workshop  Ltd, Richard Dollar .  73 European Court of Justice 21 February 2 018, Case No. ECLI:EU:C:2018:82, Ville de Nivelles v. Rudy Matzak; European Court  of Justice 9 March 2021, Case  No. ECLI:EU:C :2021:182 , D.J. v. Radiotelevizija Slovenija; European Court of Justice 9 March  2021, Case No. ECLI:EU:C:2021:183 , RJ v. Stadt Offenbach am Main.  
STOA  | Panel for the Future of Science and Technology     46 contracts.74 As this Directive  is currently still being transposed into Member States ' domestic law, it  remains to be seen whether these measures will manage to constrain casual work arrangements  satisfactorily.   Furthermore, as mentioned earlier in section 3.2.2. , AI will probably be used to evaluate workers  more continuously. That might raise some issues concerning the excessive use of fixed -term  employment contracts. Employers have long used  fixed -term contracts to circumvent dismissal  protections provided for open -ended employment contra cts. EU Directive 1999/70/EC concerning  the framework agreement on fixed -term work contains provisions to prevent employers from  abusively relying on successive fixed -term contracts. Although these protections have their merit,  the system is far from being  without limits . Once a Member State has introduced  any measures to   notionally  limit this type of abuse, the CJEU ' does not very strictly check whether the measures are  sufficiently effective and have a sufficiently deterring effect ' (van der Mei, 2020, 88). As such, if AI  tools assist in generating continuous performance statistics, it might entice employers to maximally avail themselves of the possibility of using fixed -term contracts. Rather recently, Article 12 of the  Transparent and Predictable Working  Conditions Directive has provided workers with a right to  request a form of employment with more predictable and secure working conditions. However,  since the employer has only to provide a reasoned written reply to such a request, it is unclear  whether t his provision will make any real difference.   On an entirely different note, whereas some workers may wonder why an algorithmic system does  not grant them any work or fear that they will not receive a successive fixed -term contract, other  employees might fe el overwhelmed by the never -ending workstream. The COVID -19 pandemic has  made many aware of this risk, as surveys show how teleworkers ' working hours increased, causing   them to work during their free time (Eurofound, 2021, 4). In this respect, the 2002 EU Framework Agreement on Telework says very little about teleworkers ' risk of 'work overload '. Teleworkers are ,  in principle, asked to manage the organisation of their  working time. T he agreement also states that  teleworkers ' workload and performance standards should be the same as those of comparable  workers at the employer 's premises (Section 9). Yet, this does not seem to offer much effective  protection in practice .  Therefore, the 2020 EU Framework Agreement on Digitalisati on describes the modalities of  disconnection, calling for ' a culture that avoids out of hours contact ' and a clear understanding that  'the worker is not obliged to be contactable ' outside working hours .75 In a similar vein, the European  Parliament 2021 with recommendations to the Commission on the right to disconnect.76  Importantly , this right is not just relevant for teleworkers. Recital 16 points out that the right to  disconnect 'allows workers to refrain from engaging in work -related tasks, activities an d electronic  communication, such as phone calls, emails and other messages, outside their working time,  including during rest periods, official and annual holidays maternity, paternity and parental leave,  and other types of leave, without facing any advers e consequences '. Based on this description, the  right might just as well provide regular office workers with an opportunity for redress if their employer 's app continues to push notifications after working hours. Such a right to disconnect  might also mark a clear boundary that cannot be overlooked when designing work -related AI.  Most  notably , Portugal has recently adopted new rules that  oblige employers to refrain from contacting  employees outside of working hours, except in situations of force majeure . Vio lating this duty may  lead to administrative penal ties (Bateman, 2021) .    74 One possibility is 'a rebuttable presumption of the existence of an employment contract with a minimum amoun t of  paid hours based on the average hours worked during a given period ' (Article 11).   75 European Social Partners Framework Agreement on Digitalisation, June 2020.   76 European Parliament resolution of 21 January 2021 with recommendations to the Commission o n the right to  disconnect , P9_TA(2021)0021.  
AI and digital tools in workplace management and evaluation     47 Finally, regarding the enforcement of existing labour rights, Directive (EU) 2019/1937 on the  protection of whistle -blowers should also be highlighted. In its resolution of 20 October 2020, the  European Parliament proposed to amend the Directive to ensure whistle -blowers are protected  when reporting on breaches of Union law concerning the ' development, deployment and use of  artificial inte lligence, robotics and related technologies '.77 This idea has also been mentioned in  others ' policy documents (FLI, 2021). Considering how AI developers are most aware of AI 's ins and  outs, it would make sense to confer protection to them as whistle -blower s. This might be an  essential pathway to discover illegal or undesirable  practices. Along those lines, it is also worth  asking if this protection should also extend beyond developers, covering, for example, consultants that 'deploy ' it at the client 's ente rprise.   4.2.6.  Anti -discrimination   The ways algorithms and AI may impact bias es and discrimination has received widespread   coverage in the general literature on AI and EU non -discrimination law (Borgesius, 2020; Xenidis and  Senden, 2020). Some studies have explic itly focused on potential algorithmic discrimination in  recruitment or at the workplace more generally.   At the EU level, there is a significant number of non -discrimination instruments . Secondary  legislation is, moreover, overarched  by primary provisions l ike Article 21 of the Charter stating that  any discrimination based on any ground such as sex, race, colour, ethnic or social origin, genetic features, language, religion or belief, political or any other opinion, membership of a national  minority, propert y, birth, disability, age or sexual orientation shall be prohibited. 78 This article could  serve as an umbrella provision contributing to improved coherence between the separate directives  (Ward, 2018).   Looking at the various directives, first and foremost,  Council Directive 2000/78/EC of 27 November  2000 establishes a general framework for equal treatment in employment and occupation. It covers  discrimination on the grounds of religion, belief, disability, age and sexual orientation. Additionally, Directive  2000/43/EC advanced a framework for combatting discrimination on racial and ethnic  origin. Directive 2006/54/EC, subsequently, addressed gender discrimination in matters of employment and occupation. The latter has been supplemented by Directive 2010/41/E U, covering  equal treatment between men and women engaged in an activity in a self -employed capacity.   This body of EU non -discrimination law  has a relatively broad scope. This entails it does cover many  of the AI applications at work, albeit with one cavea t. Regarding self -employed workers, some  scholars warn:  ' it seems the [directives '] protection is limited to rights of access to a profession or  activity, not equal treatment once access has been obtained (i.e. during the exercise of the activity) '  (Barnard and Blackham, 2018, 206). Thi s implies , for instanc e, that  self-employed platform workers  will face an uphill battle when seeking  protections against platform users ' discriminatory treatment.  Unless it concerns false or dependent self -employed wo rkers, EU equality law, as such, arguably  provides  independent contractors with fairly marginal protections (Kullmann, 2018).   Comin g to the operation of EU anti -discrimination legislation, one of its essential elements is the  irrelevance of intent. It does  not matter whether the AI programmers intended to engage in  disparate treatment . Once an algorithmic model uses protected grounds directly as variables or  through proxies, the operation may constitute discrimination, regardless of anyone 's intention s    77 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical  aspects of artificial intelligence, robotics and related technologies, P9_TA(2020)0275.   78 Article 3 of the Treaty on European Union explicit ly states the Union shall combat social exclusion and discrimination.  Similarly, t he Treaty on the Functioning of the European Union  stresses that the Union shall aim to combat discrimination  in defining and implementing its policies and activities (Articl e 10), and contains various articles with more specific  instructions, such as on the principle of equal pay for male and female workers (Article 157).  
STOA  | Panel for the Future of Science and Technology     48 (Xen idis and Senden, 2020). That itself is helpful for claimants. For example, a Court in Bologn a ruled  that 'Frank ', the algorithm used by Deliveroo, indirectly discriminated against some courier s who  cancel led their shifts  within  24 hours prior to the se shift s.79 The reason underlying the courier 's  cancellation was irrelevant  from Deliveroo 's point of view ; the ranking of all workers who cancel too  late is lowered.  Hence, if couriers cancelled because they were sick or undertook collective action,  they w ould be treated exactly the same as persons who cancelled for no justifiable  reason.  In this  regard, e ven if Deliveroo may not have intended for this process  to have these indirectly  discriminatory effects , the Court nonetheless ruled in favour of the clai mants , who were supported  by Cgil, the most representative Italian labour union  (Aloisi and De Stefano, 2021b) .  Along these lines, mobilising the legislation in court seems feasible yet far from easy . The following  paragraphs  briefly discuss  why this appears to be the case. Notably, non -discrimination laws make  a distinction between direct and indirect discrimination. In the case of the former, one group of  workers is treated less favourably than another in a comparable situat ion based on  a discriminatory  ground, such as age , race or disability. Indirect discrimination instead requires an apparently neutral  provision, criterion or practice that puts persons having any protected feature at a particular  disadvantage compared with other persons.80  Sandra Wachter et al. raise a first legitimate point of concern. Arguably, the most significant pathway to enforce non -discrimination law in the EU is through individuals that file a claim in court. However,  as they point out, ' [c]ompared  to traditional forms of discrimination, automated discrimination is  more abstract and unintuitive, subtle, and intangible ' (Wachter, Mittelstadt and Russell, 2021, 5). This  is likely to affect jobseekers ' and workers ' propensity to take a case to court be cause they have to  first sense injustice before taking action. Subsequently, assuming individuals ' do for some reason  suspect discriminatory practices to occur , they have  to prove at least prima facie  discrimination, i.e.  facts such as a statistical dispar ity from which it may be presumed that there has been discrimination  (Gaudio, forthcoming).   That means claimants need to produce enough facts before the burden of proof shifts, after which the defendant has to show the processing happens based on objective  factors unrelated to any form  of discrimination. In this respect, even if the GDPR 's right of access to information and the ' right to  an explanation'  in case of automated decision -making may allow  data subjects to gain  relevant  information (Todol í-Signes,  2019, 478), it might nevertheless remain challenging to establish  enough  prima facie  evidence showing there is a disadvantage (Kelly -Lyth, 2021b). For example,  noticing that a manager discriminates against pregnant women is one thing, figuring out whether   AI-driven tech can be used to predict the likelihood that someone becomes pregnant in the near  future – and that this info is, in fact, used in employment -related decisions – is something else (HBS  Digital Initiative, 2021).81   Nonetheless, the prima facie  threshold is crucial to force those in possession of the AI – and who are  supposed to understand how it works , already a far- fetched assumption in most cases  – to come    79 Tribunale Ordinario di Bologna 31 dicembre 2020, Case No. 2949/2019.   80 A classic example is an employer 's policy that prohibits the wearing of political, philosophical or religious signs in a  general and undifferentiated way, or a policy that precludes any head covering, but most significantly impacts Muslim  women who want to wear a headscarf (Howard, 2021) .   81 'To give a hypothetical example: an organisation could discriminate against pregnant women, while that discrimination  would  be difficult to discover.  The US retail store  Target reportedly  constructed  a "pregnancy prediction" score,  based  on around 25 products, by analysing the shopping behaviour of customers. If a woman buys some of those products,  Target can predict  with  reasonable  accuracy  that  she is pregnant.  Target wanted to reach people with advertising during  moments in life when  they  are more likely  to change  their  shopping  habits. Therefore, Target wanted to know when  female customers were going to give birth.  "We knew that if we could  identify them in their second trimester, there' s a  good chance we could capture them  for years". Target  used  the prediction  for targeted marketing, but an organisation  could also use such a prediction for discrimination ' (Borgesius, 2018, 22- 23). 
AI and digital tools in workplace management and evaluation     49 forward with far more detailed evidence (Gaudio, forthcoming). In this respect, building on the  CJEU 's reasoning in Danfoss ,82 it is crucial to explore the extent to which the AI user 's unwillingness  to provide the claimant with the information necessary to build a non- discrimination lawsuit could  suffic e to shift the burden of proof to the defendant (Allen, 2020, 18). The effectiveness of individual  court cases in this area would indeed seem to largely hinge on how easily the burden of proof is shifted.   Additionally, policymakers should also think about other methods than simply relying on individual plaintiffs to halt potentially discriminatory AI (Wachter, Mittelstadt and Russell, 2021). An important  point of reference in this respect is the GDPR 's DPIAs, which tend to be obligatory in relation to AI at  work. Such assessments look at the risks to the rights and freedoms of data subjects, including the  risk of discrimination. They could develop into a tool offering the necessary transparency (Kelly -Lyth,  2021b). Another option is to create the capacity ne eded to issue certifications (Schubert and Hütt,  2019), or engage in algorithmic audits (Engler, 2021). Audits can serve various ends (Vecchione,  Barocas and Levy, 2021; Ajunwa, 2021 ). For instance, the auditing system could be set  to analyse an  employer 's practices in response to a credible complaint, e.g. a whistle -blower . Still,  it may just as  well serve as an official form of certification or become mandatory once an AI tool reaches a critical  mass of users.   Apart from these general –  'structural ' – issues, there are also more specific ones. People with  disabilities, for example, can expect reasonable adjustments to be made during in- person  interviews; yet, it remains unclear how AI tools will  be programmed to follow suit (CDEI, 2020). This  is a signific ant issue also in light of Article 26 of the Charter. 83 Another specific legal question  highlighted by Miriam Kullmann and others relates to protecting undisclosed know -how and trade  secrets. The algorithmic systems and AI used in HR management are probabl y covered under  Directive (EU) 2016/943 as a protected trade secret. Therefore, on the one hand, a  demand to  disclose the entire algorithmic system could meet obstacles of this nature ; on the other hand, given  how difficult it already is to prove algorithm ic discrimination, the relevant laws should arguably not  further complicate this task. It remains to be seen how this balance will be struck (e.g. ICO and The  Alan Turing Institute, 2020).   Moreover, there are also hurdles  of a more practical nature. As dis cussed above , some AI systems  are not inert ; they continue to change. This could potentially lead to discriminatory outcomes during  specific periods and not at other times. For this reason, it is essential to maintain detailed  documentation indicating how the algorithms have developed over time (Kullmann, 2019, 52- 53).  4.3. Ongoing negotiations related to the AI Act   Alongside  the European Parliament 's initiatives  (section  4.1.) , the E uropean  Commission has also  been engaged in the AI  debate  by developing an AI strategy .84 These efforts led to an AI package  published in April 2021. The centre piece of this package is the proposal for a n AI Act.  The purpose  of this Act, as explained in Recital 1 of the draft, ' is to improve the functioning of the internal market  by laying down a uniform legal framework in particular for the development, marketing and use of    82 'Directive 75/117 on equal pay for men and women must be interpreted as meaning that where an undertaking appli es  a system of pay which is totally lacking in transparency, it is for the employer to prove that his practice in  the matter of  wages is not discriminatory, if a female worker establishes, in relation to a relatively large number  of employees, that the  average pay for women is less than that for men. ' European Court of Justice 17 October 1989, Case No.  ECLI:EU:C:1989:383 , Handels - og Kontorfunktionærernes Forbund I Danmark v. Dansk Arbejdsgiverforening, acting on  behalf of Danfoss .  83 'The Union recognises a nd respects the right of persons with disabilities to benefit from measures designed to ensure  their independence, social and occupational integration and participation in the life of the community. '  84 European Commission, Communication from the Commission : Artificial Intelligence for Europe, European Union, 2018.  
STOA  | Panel for the Future of Science and Technology     50 artificial intelligence in conformity with Union values. This Regulation pursues a number of  overriding reasons of public interest, such as  a high level of protection of health, safety and  fundamental rights, and it ensures the free movement of AI -based goods and services cross- border,  thus preventing Member States from imposing restrictions on the development, marketing and use  of AI systems , unless explicitly authorised by this Regulation. '  Since the previous chapters have covered many relevant EU laws, it is evident that the AI Act does  not arise in a legal vacuum. Instead, there are already various ways in which AI is subjected to regulati ons, including many work -related instruments . Furthermore, although not covered above,   laws an d regulations  at the domestic level  also govern how new technologies have to be introduced  and used at the workplace (De Stefano and Taes, 2021). These domestic instruments provide employee representatives with rights to consultation, or, quite often, they even provide a right to co-determination (Moore, 2020, 87- 88 and 93- 94; see also Aloisi and Gramano, 2019 ). Drawing  inspiration from this type of regul ation , some consider it essential to create ' a new right for all  workers to have reasonable ' involvement ' in the design and deployment of algorithmic systems '  (All- Party Parliamentary Group on the Future of Work, 2021, 15).   Compared to these regulatory fra meworks that aim to safeguard workers ' health and safety, privacy,  personal data and voice at work in general, the AI Act is  arguably somewhat narrow . It advances a  legal framework for ' trustworthy AI '. The reasoning is as follows: if AI is trusted because  the risks  associated with its uses are addressed, its uptake is promoted. The overarching goal is, therefore, to stimulate the uptake and spread of AI in the EU . As part of this endeavour, the Commission aims to  create a single market for ' lawful, safe an d trustworthy AI ', countering  market fragmentation. T he  proposal 's explanatory memorandum expresses this so : ' an emerging patchwork of potentially  divergent national rules will hamper the seamless circulation of products and services related to AI  systems across the EU and will be ineffective in ensuring the safety and protection of fundamental  rights and Union values across the different Member States. '  The proposal 's goals are reflected in its  primary legal basis. Article 114 of the Treaty on the  Function ing of the European Union is used to harmonise or approximate the EU Member States '  laws so as to establish or ensure the functioning of the internal market  (see  for a detailed analysis,  Adams -Prassl and Veale, forthcoming) . Once a harmonisation measure li ke this is adopted, a  Member State can notify the Commission that the country deems it necessary to maintain national provisions relating to the protection of the working environment. If this happens, the Commission  must approve or reject the national prov isions involved. This decision is made after verifying  whether or not the provisions are a means of arbitrary discrimination or a disguised restriction on  trade between the Member States, and whether or not they would  constitute an obstacle to the  functioning of the internal market.  In practice, the Regulation will , in most cases , function as a  'ceiling ' instead of a 'floor ' for labour and employment protection (De Stefano and Taes, 2021).   Therefore, b earing in mind t hat the AI Act may under cut national provisions relating to the  protection of the working environment, it is not surprising trade unions are concerned (ETUC, 2021). As previous chapters have scoped out, AI will likely  permeate the world of work in various ways. It  might feature in work tools and HR management systems; in time, it could practically be integrated  into most objects  worker s interact with at work. The proposal 's idea is for AI in employment contexts  to be self -evaluated  by its providers  first, g uaranteeing it is trustworthy. Once this happens, it should  be given almost free rein. Ultimately, with very few exceptions, the proposal assumes the use of AI   tools  is to be fostered , and i ts implementation in the world of work is seen as desirable. The p roblem  with this assumption is that, as this report discussed at length,  even from a fundamental -rights  perspective, there are many instances in which AI 's functioning is far from desirable. For example,  Jennifer Cobbe and Jatinder Singh, computer scientis ts at the University of Cambridge, ' echo  assertions that introducing AI into video surveillance and other digital information- gathering  infrastructure alters power dynamics in favour of those controlling previously ' dumb'  systems,  requiring reconsideration of how to retain an appropriate balance of societal interests and 
AI and digital tools in workplace management and evaluation     51 fundamental rights and potentially limiting the expansion of digital infrastructure that might  otherwise seem appropriate ' (Cobbe and Singh, 2021, 24). Th e risk is that the proposed AI Act will  gut any regulatory or collective -based attempt to provide a counterweight, opening the door to the  'Amazonian Era'  described in section 3.2.3.   This is because, f irst of  all, the Act's definition of an AI system aims to be technology -neutral and  future proof, leading to a broad description (Kelly -Lyth, 2021a, 3; Veale and Borgesius, 2021). It  encompasses any  software that is developed with a technique or approach listed in annex I , pursues  human- defined objectives , and  generates outputs , such as content, predictions, recommendations,  or decisions , influencing the environments the system interacts with, be it in a physical or digital  dimension (Recital 6 and Article 3(1)). One could rightly wonder whether this covers about every  computer program (Ebers et al., 2021, 590) . As the R ecital clarifies, AI can function on a stand -alone  basis or be integrated into a larger product. In this sense, since AI is projected to ' quite possibly '  become a general -purpose technology along  the lines of steam, electricity and ICT (Crafts, 2021), it  might become a dominant component in any product. Coupled with the effects of the legal  harmonisation basis discussed above, t he broad description of AI combined with its potential future  omnipres ence, therefore, risks severely restricting worker voice regarding the future use of all  technologies at work.   Additionally, as noted, the AI Act favours the introduction and use of AI. In contrast, although  workers and their representatives are not princi pally always  opposed to AI ( supra section 4.2.3.), the  general sense is that they lack adequate legal tools to offset its risks. The AI Act is purported  to be of  help in this regard by, for example, banning specific AI practices and evaluating high- risk systems;  however, some fear the AI Act 's bar was set too low  and that the ' ceiling ' effect mentioned above  will not allow countering  this.   It thus remains  essential to ' break the ceiling '. There are various reasons why. Overall, since the AI  Act wants people to trust AI, it probably does not want to become scolded for its potential deregulatory effect. That would seem antithetical. In its current state, the  draft AI Act legitimises the  introduction of AI at work. To the extent AI systems comply with the applicable conditions under the Act, the systems will  be allowed to enter the market without, in principle, having to fulfil  additional requirements  determin ed by domestic laws and collective bargaining agreements. In this  regard, as current domestic laws and collective bargaining agreements may impose more stringent constraints than the AI Act, all such nationally determined constraints may come under scrutin y, also  because of the ' liberalising ' legal basis of this Act .   For example, the Belgian national collective bargaining agreement no. 81 clarifies that there are only four limited reasons why an employer may monitor workers ' online communications. F urtherm ore,  it attaches great importance to the principle of proportionality, meaning that the employer should  proceed in such way as to not impinge  in the worker 's privacy . If the e -monitoring does result in an  intrusion into privacy, this intrusion must be kept to a minimum.    Similar to this country example, other EU Member States have other laws and regulations that  constrain the employers ' ability to intrude on the privacy of workers in the digital realm  and to  introduce technology to monitor them . If the AI A ct allows for AI software to perform e -monitoring,  the question is  what room  this type of domestic legislation will continue to have. We could  end up  in a situation where employers that  do not use AI  are not allowed to read any e -mails of the workers   because , unless there are clear indications of malicious activity,  this is a disproportionate intrusion  into the ir privacy; however, employers that do use AI may be allowed to continuously screen  employee s' e-mails towards certain ends, as the AI system complies with all conditions prescribed  by the AI Act, and, hence, the domestic laws in place ought not to constrain the AI system 's operation  any further.  
STOA  | Panel for the Future of Science and Technology     52 Also, building on this example, most notably, the  AI Act claims to be without prejudice to the GDPR  and even to complement that instrument. As discussed , Article 88 of the GDPR explicitly allows the  Member States to provide more specific rules to protect the rights and freedoms regarding the  processing o f workers ' personal data in the employment context. The lack of any similar provision  in the AI Act, allowing Member States to take additional measures to guarantee trustworthy AI in  the workplace, seems to be at odds with the GDPR 's approach. Along those lines, what if a Member  State wants to make use of Article 88 but discovers that the AI Act severely limits the Member State 's  options in as far as an AI is involved (Aloisi and De Stefano, 2021c )? The interaction between the  GDPR and  the AI Act certainly deserves additional care, as also  mentioned by the European Data  Protection Board (Bergholm, 2021; EDPB and EDPS, 2021).85  On a more general note, another reason to break the regulatory ceiling is advanced  by Frederik  Borgesius. He remarks that, in the past, '[t]o mitigate problems caused by the industrial revolution,  we needed different laws for work safety, consumer protection, the environment, etc. In different  sectors, the risks are different, and different norms and values are at stake ' (Borgesius, 2020). This  sectoral approach has also been emphasised in, for instance, the discussions related to civil liabilities  for AI -induced harm  (Bertolini, 2020) .86 According to Borgesius and Bertolini , we might want more  sector -specific rules for algorithmic decision -making and AI. This, however, is not the  approach of  the AI Act, which seems to take a somewhat universalistic top -down course . The proposal does  identify high -risk areas for AI systems. Yet, it does not make further differentiation s, meaning a high risk AI system in education is treated quite similar to a high -risk AI system in workers ' management.  Most importantly, it seemingly discourages other regulators from acting upon sectoral concer ns. As  such, the Act risks overgeneralising its  regulatory solutions , neglecting to deal with particularities  that are at stake in different  sectors.   The low level of protection the AI Act provides  is also problematic . Along those lines, although the  Act a ttempts to somewhat resemble the GDPR by drawing on a fundamental rights narrative, it  seems much less focused on protecting citizens ' rights, including workers ' rights, than the GDPR (de  Matos Pinto, 2021).   Specifically, some observers argue that the list  of unacceptable risks, hence determining what AI  practices are prohibited, is too narrow (AlgorithmWatch, 2021, 9; Biber, 2021). The same critique  could be made when reflecting on the field of work . For what concerns this context, Article 5 of the  draft A I Act prohibits, most notably, AI systems that deploy ' subliminal techniques beyond a  person 's consciousness in order to materially distort a person 's behaviour in a manner that causes  or is likely to cause that person or another person physical or psychol ogical harm '. An example given  by an Officer of the European Commission is of an AI intent on finding the perfect frequency to create an inaudible sound that pushes truck drivers to drive longer than healthy (Veale and  Borgesius, 2021). Scholars have highl ighted how this example is highly improbable, and in any case,  unlawful (Kelly -Lyth, 2021a, 4). Indeed, the question should be whether there are no other, less    85 The European Parliament made some relevant comments in its resolution regarding the evaluation of the GDPR. It  regrets ' that the Commission itself does not always have a consistent approach to d ata protection in legislative proposals;  stresses that a reference to the application of the GDPR, or ‘without prejudice to the GDPR ', does not automatically make  a proposal GDPR compliant; calls on the Commission to consult the European Data Protection Su pervisor (EDPS) and the  EDPB where there is an impact on the protection of individuals ' rights and freedoms with regard to the processing of  personal data following the adoption of proposals for a legislative act; calls further on the Commission, when prep aring  proposals or recommendations, to endeavour to consult the EDPS, in order to ensure consistency of data protection  rules throughout the Union, and to always conduct an impact assessment '. European Parliament resolution of 25 March  2021 on the Commissi on evaluation report on the implementation of the General Data Protection Regulation two years  after its application , P9_TA(2021)0111.   86 'AI is pervasive and will be used in diverse fields – such  as consultancy,  consumer  products  and services,  mobility,  online   connectivity,  energy  production  and  distribution,  police  and  justice administration – , where EU and MS liability rules are  already sector -specific. The advent of AI does not justify a shift towards a universal regulatory approach ' (Bertoli ni, 2020,  122).  
AI and digital tools in workplace management and evaluation     53 hideous  AI applications that likewise deserve to be prohibited. For example, algorithmic work  surveillance is bound to become highly problematic if left unchecked. AI  can likely enable its most  invasive forms, as it allows compiling and analysing all the data to provide unprecedented levels of  insight. To that extent, some have called  to outlaw practices in which AI is used to create a  surveillance 'panopticon '87 (Ponce Del Castillo, 2021, 8). Indeed, a s mentioned above, there are  certain aspects of modern work surveillance that  deserv e a very critical stance. In particular, the use  of AI to assess the emotional state of workers –  and people in general –  is a contender to make it to  the list of unacceptable risks (Crawford et al., 2019, 50- 51; EDPB and EDPS, 2021, 12).88 The same  could be said, for instance, for facial recognition technologies (Bernhardt, Kresge and Suleiman,  2021, 22), or biometric identification systems (Ebers et al., 2021, 592- 593) . Among other reasons,  this  seems appropriate  because these practices  can hypercharge wo rker surveillance.   Moving on from the unacceptable risks, we should highlight that ' employment, workers  management and access to self -employment ' is considered a high -risk area  (De Stefano and Taes,  2021) . Annex e III more specifically points to high -risk 'AI systems intended to be used for recruitment  or selection of natural persons, notably for advertising vacancies, screening or filtering applications,  evaluating candidates in the course of interviews or tests ', and to high- risk 'AI intended to be used  for making decisions on promotion and termination of work -related contractual relationships, for  task allocation and for monitoring and evaluating performance and behavior of persons in such  relationships ' (Kelly -Lyth, 2021a).  These  systems need to fulfil the requirements of chapter 2 of the  proposal. This entails providers, i.e. the ones that develop AI systems or have them developed to  place them on the market, must establish and maintain a risk management system,  which, among  other things, performs regular analysis of known and foreseeable risks (Article 9). The chapter also makes demands regarding the data used to train models, the technical documentation needed, the record- keeping, the transparency of the system , the level of human oversight and the accuracy,  robustness and cybersecurity of the system (Articles 10- 15). Ultimately, it is the responsibility of the  provider of a high -risk system to engage in self -assessment, verifying whether all of the  requirements  are met (Article 16). Some have remarked that since it is likely for this list of high -risk  areas not to be entirely satisfactory, a light version of self -assessments would also be appropriate  for systems that, so far, are not classified  high risk but cou ld arguably qualify for this classification  (AlgorithmWatch, 2021, 4).   Regardless of this discussion about how to delineate what is high -risk, in practice, what is likely  to  happen is that in the years to come, the European Standardisation Organisations wi ll draft  harmonised standards, which transpose the AI Act 's minimum requirements for high -risk systems  into more technical provisions  (see Nativi and De Nigris, 2021). When providers follow these  standards, something  they can voluntarily opt for, they will  enjoy a presumption of conformity,  severely restricting their risks under the AI Act. If they do not, they will  have to self -assess their high risk AI system themselves; therefore, it might be the case that harmonised technical standards, instead of the AI Act itself, will end up playing a very significant role. This fact is not without controversy. As Michael Veal e and Frederik Borgesius highlight, ' the Commission 's long practice of  privately outsourcing complex negotiations [to standardisation organisations]  has been  controversial for years. The Draft AI Act may trigger more attention to this constitutional proble m'  (Veale and Borgesius, 2021; see also Ebers et al., 2021, 594- 595).     87 On the concept, see: (Manokha, 2018).   88 'Expresses great concern about the employment of AI applications, including facial and voice recognition, in ‘emotional surveillance ' programmes, i.e. monitoring the mental conditions of workers an d citizens in order to increase productivity  and preserve social stability, sometimes coupled with ‘social credit ' systems, as already seen in China, for instance;  stresses that such programmes are inherently at odds with European values and norms protecti ng the rights and  freedoms of individuals '. European Parliament resolution of 12  February 2019 on a  comprehensive European industrial  policy on artificial intelligence and robotics, P8_TA(2019)0081.  
STOA  | Panel for the Future of Science and Technology     54 Leaving aside th e question about the desirability of technical standardisation driving the field of AI,  more broadly, commentators contend that these internal ' conformity assessment proc edures ' for  high -risk systems are insufficient  (AlgorithmWatch, 2021, 5; Biber, 2021; EDPB and EDPS, 2021, 1213; Ebers et al., 2021, 595; Kop, 2021, 8). Likewise, scholars who focus on the potential impact of AI  at work observe that self -assessment could  easily  become a rubber stamp (Kelly -Lyth, 2021a, 7;  Ponce Del Castillo, 2021, 5). Many would prefer to see an external ex-ante assessment by third  parties in this respect. Unfortunately , the chances of this happening seem rather slim. For example,  the draf t report of the Parliament' s AIDA Committee mentions:   'that obligatory ex ante risk self -assessments, comparable with CE markings or data protection  impact assessments, combined with market surveillance based on clear rules and standards, and  complemented with ex post enforcement for high -risk AI systems, seem to be a sufficiently robust  governance approach for AI; warns that overly burdensome conformity assessment obligations  could create significant burdens that make the business models of AI developers a nd companies  economically unviable ' (AIDA, 2021b).   As such, since external control is likely to remain relatively limited, it would be crucial to ensure self assessment does  not descend  into  empty  formalities . This risk manifests itself  along at least two  lines. First, as mentioned above, self -assessments may not be implemented seriously enough,  resulting in a convenient way to present high- risk AI systems as trustworthy. Accordingly, there   might be a need for adequate ex-post  procedures in this respect to ensure compliance by   providers.89 Second, Article 14 demands human oversight, something relevant also in employment related AI systems. In this respect , it is also worth noting that the AI Act references the very real risk  of 'automation bias ' (Todol í-Signes, 2021a), implying that people are inclined not to override the  suggestions made by an algorithm. However, despite this awareness, it is uncertain whether the Act provides enough s afeguards to establish effective human oversight. Martin Ebers and others stress,  for instance, that Article 14 demands for the watchers to ' fully understand the capacities and  limitations of the high -risk AI system and be able to duly monitor its operation, so that signs of  anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as  possible '; that requirement is simply not realistic in their opinion (Ebers et al., 2021, 596).   To that extent, it seems opportune to revisit t he question of how human oversight can be effectively  and realistically ensured. Workers ' representatives, for example, often enjoy particular dismissal  protections because their function at the enterprise demands additional safeguards to ensure they can p roperly fulfil their job. In a similar vein, workers performing human oversight might need an  appropriate legal status to perform their function within the enterprise without constraint and fear  of retaliation if they decide to deviate from the course of a ction suggested by AI systems.  Such a  status would also make it relatively easy to provide them with specific rights to training.  In this  regard, a draft of the EU Act,  that was leaked to the press days before its official release, provided  that people in charge of human oversight were to be put in the position, among other things, to ' decide not to use the high -risk AI system or its outputs in any particular situation without any reason  to fear negative consequences .' Commenting on the draft, one of us arg ued that it was problematic  that it did not explicitly mention the need to provide managers and supervisors with the specialised training and powers to counter the specific implications of the use of these systems in the context of work  (De Stefano, 2021a) . He also stated that, without explicit workplace protection, this provision  might  not adequately prevent disciplinary actions from employers. The final proposal for the AI Act,  however, does not even mention anymore the need to attenuate  the fear of negat ive consequences    89 'As only those with obligations under the Draft AI Act can challenge regulators ' decisions, rather than those whose  fundamental rights deployed AI systems affect, the Draft AI Act lacks a bottom -up force to hold regulators to account for  weak enforcement. Data protection law where affected groups can raise com plaints is already characterised by inaction  and paralysis. Enforcement of the Draft AI Act therefore seems likely to play out in an even more lacklustre way than it  has with the GDPR to date ' (Veale and Borgesius, 2021).  
AI and digital tools in workplace management and evaluation     55 for the human supervisors who reverse or disregard the outputs of high -risk AI systems. Certainly,  this does not seem enough to ensure effective human oversight in the context of work .  Another crucial issue  is the lack of involvement by social partners in the decisions made in the  framework of this Act. For instance, some argue that revising the list of high -risk systems  and setting  technical standards at the European Standardisation Organisations should involve the public (Ebers  et al., 2021, 593- 595), including the social partners (ETUC, 2021). Another critical remark is that the  Act does not provide those subjected to AI who feel they have suffered harm with any possibility of  claiming redress (Ponce Del Castillo, 2021, 6; Veale and Borgesius 2021; AlgorithmWatch, 2021, 8).90  This lack of individual rights has been highlighted as one of the most critical elements of the  Act  (Ebers et al., 2021, 600). Furthermore, contrary to the GDPR, the Act does not provide individuals with significant access to personally relevant information (Kelly -Lyth, 2021a, 7- 8). 91 Only the users  are given information, which would mean employers in the context of work .  Additionally , there are some criticalities in how the self -employed are included in the provision of  Annex III concerning AI systems used at work. As discussed above , the provision mentions  'Employment, workers management and access  to self -employment '. It then lists two types of  relevant instruments: under letter a), systems used in recruitment; under letter b), systems used to, broadly speaking, manage people in ' work -related contractual relationships ' and monitor and  evaluat e thei r performance. While it is undoubtedly positive that the self -employed are included in  the protection of the AI Act, it should not be underestimated that some of the AI -enabled  management tools may  come  at odds with genuine forms of self -employment. In fac t, selfemployment is not compatible with intrusive and detailed monitoring of the work performance ,  which becomes possible when  powered by AI . A business 's reliance on c onstant tracking  of workers '  movement , strict monitoring of  the work pace, and tech- enabled control of messaging, browsing  activity and use of computers  contrasts with a worker 's self -employed status, especially when an AI  is used to combine information deduced from these features . Even if all the measures were taken to  comply with the AI Act and  workers ' fundamental rights , these systems, if put in place to monitor  the self -employed, may ground the reclassification of the working relationship into one of  employment. It is essential, therefore, to specifically clarify that the fact that a management system is allowable under the AI Act does not prevent  that the use of this system in relation to self-employed  persons could lead to the reclassification of those persons under the existing standards used to determine an emp loyment relationship.   Lastly , in principle, the users have few obligations under the current AI Act; in essence, they have to  follow the instruction manual handed by the provider of the AI system (Veale and Borgesius, 2021). Users, such as employers, may, however, become considered providers, hence subjected to the  corresponding obligations , if they modify the intended purpose of a high -risk AI system or make a  substantial modification. This shift in obligations might raise some issues, though, because ' users of  general -purpose AI -as-a-Service APIs, designed to be repurposed, changed and configured, may  find themselves with conformity assessment obligations without the capacity or expertise to carry  them out ' (Veale and Borgesius, 2021). To make matters even  more complicated, the users of an AI  system – the employers  in the world of work  – are already seen as the data controllers under  the  GDPR, meaning they have to conduct  DPIAs . Likewise, they might have to perform a risk assessment  under OSH regulations . However, under  the AI Act , it is the provider, not the user, that  would have  to perform the assessment unless this obligation shifts because the user  makes substantial changes  to the AI. Since  these obligations, deriving from the GDPR  – OSH laws  and AI Act, misalign, some  suggest making the users responsible for the conformity  assessment under the AI Act (Ebers et al.,    90 European Parliament resolution o f 20 October 2020 with recommendations to the Commission on a framework of ethical  aspects of artificial intelligence, robotics and related technologies, P9_TA(2020)0275.   91 'In this regard, we recommend that the AIA should be amended to include an obligation to explain AI -based predictions  or decisions to the affected persons in order to safeguard the rights and freedoms of individuals ' (Ebers et al., 2021, 596).  
STOA  | Panel for the Future of Science and Technology     56 2021, 597). While this might make sense in some cases, there is probably qui te often  also  a need to  involve the original developer. Policymakers may want to reconsider how to allocate obligations  between providers and users. This is probably one of those areas where the universalistic approach  of the AI Act conflicts with the vari ous ways in which ' users '92 will rely on AI. Developing one pattern  that justifiably allocates obligations across all provider -user relationships, ranging between  Microsoft licencing an AI to an SME and a niche software company selling an AI to a corporate  giant ,  is an arduous task;  even more , given the range of purposes  that AI system s might serve.   4.4. Policy debates   The AI Act is only one  piece of a larger regulatory scene . Another key component of the European  Commission 's recent initiatives on the digital market is the Digital Services Act  package . It is made   up of the Digital Services Act and Digital Markets Act. Similar to the AI Act, the former  aims to  lay  down harmonised rules in the internal market, this time in relation to the provision of  digital   intermedia tion  services. Initially, from a workers ' protection point of view, it was hoped the  instrument could spur some protections, most notably for platform workers (Ponce Del Castillo,  2020). However, the final EU  Commission 's proposal for a R egulation on a single market for digital  services (Digital Services Act) does not seem to deliver on that point . Article 27, for example, requires  'very large online platforms ' to put in place reasonable, proportionate and effectiv e mitigation  measures, counteracting specific systemic risks. This should be done, inter alia , in relation to the  platform 's content moderation or ' recommender ' systems and their decision -making processes. As  such, in as far as the platform relies on AI in these respects, the Digital Services Act would seem to  matter to AI governance because it imposes a nother  sort of risk assessment. Yet, the Act does  not  seem  oriented towards digital labour platforms , focusing, in particular, on services known as ' mere  conduit ', 'caching ' and 'hosting ' services. It  thus  targets social media and online marketplaces ,  among others.   Similarly, the Digital Markets Act – officially called the R egulation on contestable and f air markets in  the digital sector – does not seem to offer much recourse for workers either. The Act aims to regulate  platforms ' 'gatekeeper ' function. In 2019 the EU adopted a R egulation on promoting fairness and  transparency for business users of online intermediation services. As the Digital Market Act' s  explanatory memorandum mentions, the Act builds on this R egulation to establish 'clearly defined  obligations vis -a-vis a very limited number of cross -border providers of core platform services that  serve  as important gateways for business users to reach end users. ' The Act essentially aims to  preserve fair competition in the digital realm.   Besides the Digital Services Act package, the Commission is also moving ahead with a revision of  European rules  on pr oduct safety (European Commission, 2020a ). The proposal for a R egulation on  machinery products (Machinery Regulation), which would replace the 2006 Machinery Directive,  seems  most important for what concerns  workplace safety. As R ecital 11 of the draft Machinery  Regulation states:   'Recently, more advanced machines, w hich are less dependent on human operators, have been  introduced on the market. These machines, known as collaborative robots or cobots, are working  on defined tasks and in structured environments, yet they can learn to perform new actions in this  context and become more autonomous. Further refinements to machines, already in place or to  be expected, include real -time processing of information, problem solving, mobility, sensor  systems, learning, adaptability, and capability of operating in unstructured env ironments (e.g.  construction sites). '    92 ''user ' means any natural or legal person, public authority, agency or other body usi ng an AI system under its authority,  except where the AI system is used in the course of a personal non -professional activity ' Article 3 AI Act.  
AI and digital tools in workplace management and evaluation     57 The Regulation needs to account for close interaction between humans and robots in workspaces,  as mentioned above, as well as robots ' autonomous features . To that end, the  draft Regulation  suggests a sort of interplay between the risk assessment carried out pursuant to the AI Act and the  risk assessment needed for the machinery product under the proposed Machinery Regulation.   Furthermore, there is the enduring question about legal liabilities concerning the deployment of AI .  As the European Commission observed , some of the characteristics of AI ' could make it hard to trace  the damage back to a person, which would be necessary for a fault -based claim in accordance with  most national rules. This could significantly  increas e the costs for victims and means that liability  claims against others than producers may be difficult to make or prove ' (European Commission,  2020b, 15). This may call for additional rule -setting at the EU level, in addition to a revision of the  Product Liability Directive  85/374/EEC . The EU Parliament 's AIDA Committee  has taken a position on  this question. The Committee is:   'convinced that despite the legal challenges caused by AI systems, there is no need for a complete  revision of the existing liability rules; stresses that the Product Liability Directive and the national  fault -based liability regimes can in principle remain the centrepiece legislation for countering most  harm caused by AI; underlines that only in some cases could there be inapp ropriate outcomes, but  warns that any revision should take the existing product safety legislation into account and should  solely be based on clearly identified gaps ' (AIDA, 2021b).   Therefore, the Committee suggests : 'the introduction of a limited new liab ility mechanism for legal  claims against the operator, who controls the risks associated with the AI system and who also often  is the cheapest cost avoider; specifies that while high -risk AI systems should fall under strict liability,  combined with mandato ry insurance cover, victims of low -risk AI systems should only benefit from  a presumption of fault against the operator ' (AIDA, 2021b). The document lacks a ny clarification of  who can be considered an operator. However, the European Parliament resolution o f 20 October  2020 with recommendations to the Commission on a civil liability regime for AI  already ma de similar  suggestions.93 Recital 10 of the proposal for a Regulation on liability for the operation of AI  systems,  which can be found in the annex to the resolution, highlights that since there are often several  entities operating the AI -system in a meaningful way, both the ' frontend '94 and the ' backend '95  operator are targeted. The proposal  advances joint and several liabilit ies between the two  types of  operators , leaving them  the possibility to stipulate a  right to recourse against one another (Recital  12). This position , taken by the European Parliament and the rapporteur of the AIDA Committee, can  be expected  to feed into the European Commission 's broader consultation  on adapting liability  rules to the digital age and circular economy. The public consultation close d in January 2022, with  an outcome planned for the third quarter of 2022.   Additionally , as mentioned,  platform work  has, in many respects,  been a t the forefront of broader  labour market developments. As such, the European institutions have been reflecting on how to tackle it for quite some time. The most promising instrument  until  recently  has been the Transparent  and Predictable Working Conditions Directive, which regulates abusive on- demand work.  Importantly, h owever, due to the personal scope of application of the said Directive, it is doubtful it  will have any meaningful conseq uences for (nominally) self -employed platform workers  (Bednarowicz, 2019). Furthermore, although both the Council Recommendation of 8 November    93 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a civil liability regime  for artificial intelligence , P9_TA(2020)0276.   94 '(e) ‘frontend operator ' means any natural or legal person who exercises a degree of control over a risk connected with  the operation and functioning of the AI -system and benefits from its op eration ;' Article 3.   95 '(f) ‘backend operator ' means any natural or legal person who, on a continuous basis, defines the features of the  technology and provides data and an essential backend support service and therefore also exercises a degree of control  over the risk connected with the operation and functioning of the AI -system; ' Article 3.  
STOA  | Panel for the Future of Science and Technology     58 2019 on access to social protection for workers and the self -employed and the Regulation (EU)  2019/1150 of 20 June 2019 on promoting fairness and transparency for business users of online  intermediation services might be relevant for  platform  workers to some extent  (Schoukens, 2020 ),  it is the European Commission 's recent proposal for a Directive ' on improving working conditions in  platform work ' that might make a real difference.96   The proposed Directive has three main goals: resolve the employment status  of platform workers,  provide for fairness, transparency and accountability in platforms ' algorithmic management, and  improve the enforcement of existing rules (De Stefano and Aloisi, 2021). Especially the second goal is relevant to any discussion on AI. T he draft proposal obliges the digital labour platform to inform  workers and their representatives  thoroughly  about how the algorithmic system s operate  (Article  6). Furthermore, workers ' representatives must  be consulted on decisions likely to lead to the  introduction of or substantial changes in the use of automated monitoring and decision -making  systems (Article 9). The proposed Directive,  moreover, limit s the personal data these algorithmic  systems can process (e.g. data related to emo tions, psychological state, health  and  private  conversations ) (Article 6). Digital labour platforms must also evaluate the  OSH risks , including  psychosocial ones,  of their algorithmic systems (Article 7) , and offer a mechanism to overturn  automated decisions (Article 8). Arguably, many of the articles included in this draft proposal can inspire  much -needed changes to the AI Act. As a matter of fact, similar to the need to bring the AI  Act into line with the GDPR, the former should likewise align itself with the go als of the platform  work in itiative. Beyond the platform economy , workers and their representatives should als o be  informed about how AI  systems impact their working conditions, be consulted about these same   systems and be allowed to question  the outcomes that AI produces. Furthermore, all employers, not  just digital labour platforms, should keep an eye on the potential OSH risks of their AI systems, and  guarantee that certain forms of personal data are not  processed through automated (AI) systems.   Having said that, the proposed Directive on platform work seems to take for granted that  algorithmic management should, in principle, be allowed, provided that it meets the Directive 's  requirements. It could instead  be argued that algorithmic management should not be assumed as  a 'given '. Its introduction should be –  at the very least –  a matter for negotiation with the social  partners, sometimes also subject to public authori sation. This has been the approach taken in the  past by some European national legislation concerning the use of technology , such as cameras,  that  may allow monitoring the work performance (Aloisi and Gramano, 2019). It is hard to see why  algorithmic management – which relies on technologies that  could be much more invasive than  those more severely scrutinised in the past –  should be held to lower regulatory standards.   Finally, in the wake  of the GDPR 's adoption, the European Commission also proposed the ePrivacy  Regulation in 2017. The proposal  had been sidelined for a long time until the EU Council adopted  its position  in early 2021.  This is significant because the instrument aims to preserve the  confidentiality of electronic communications. As noted earlier, electronic communications are a rich  source of data  for AI systems. Consequently,  it is not surprising the ePrivacy proposal has also been  the subject of fi erce lobbying  (Nemitz, 2018) . As Giovanni Buttarelli,  a former European Data  Protection Supervisor, remarked  in 2018:   ' Older arguments opposing the GDPR are being rehashed: the argument, for example, that  freedom to harvest personal data is needed in order  to develop new technologies, such as Artificial  Intelligence tools. It is posited that the lack of general data protection and privacy rules in the US    96 Proposal for a Directive of the European Parliament and of the Council on improving working conditions in platform  work, 9 December 2021.  
AI and digital tools in workplace management and evaluation     59 and China will allow businesses in those countries to take advantage and increase their  competitive edge . This argument is flawed97 in several ways ' (Buttar elli, 2018).   Considering these remarks, the ePrivacy Regulation is undoubtedly of relevance to debates on AI  governance  (Czarnocki, 2020) .  To summarise:   • Notwithstanding  the importance of ethics, the response to AI at work also needs to  be based on established fundamental and human right s.  • The GDPR 's data protection principles provide a crucial  framework to mitigate the  negative consequences of AI at work.  A vital concern  is to ensure adequate  enforcement.   • Workers' and social partners ' involvement in introducing and operating  AI at work is  essential . EU Directive 2002/14/EC offers starting points to spur  such involvemen t. It  is necessary to examine  how Member States can adequately implement the Directive  so that AI is the subject of ' systematic ' social dialogue.   • When bringing AI into the workplace, e mployers have to perform risk assessments to  identify and address OSH risks. This obligation must be more adequately considered.   • How AI systems will exactly impact  people 's working time  is still largely unknown.  Nevertheless, the current  instruments that regulate working time will continue to be  important both to ensure that some workers  receive enough wo rking hours  and to  preserve a tenable work -life balance for others.   • EU non -discrimination laws have some capacity to allow vetting  AI systems a t the  workplace. These laws ' effectiveness, however, will also depend on the ease with  which the burden of proof in the C ourt s is placed on the user of the AI system  instead  of on the workers . Moreover , the number  of alternative procedures available outside  the court s to scrutinise AI systems  will be determinative . Such external procedures for  auditing and certification need to be explored.  • The draft AI Act intervenes in a setting where various EU and domestic laws already  govern the use of AI at work. As such, despite the Act 's proclaimed  intentions, it risks  imposing a regulatory 'ceiling ', undercutting regulatory mechanisms  that are vital to  adequately govern AI and algorithmic management at work . In this respect, i t is  essential  to put the AI Act at the service rather than above the oth er laws  governing  AI's implementation in the work environment.   • In addition to the draft AI Act, m any other EU policy discussions might likewise frame  the future use of AI systems in European workplaces. T he many initiatives  need to be  coordinated . For exam ple, the recent proposal for a Directive  on platform work  contains a chapter on algorithmic management . It offers some valuable  and yet  improvable  protections for platform workers; unless the AI Act adopts a  corresponding stance, workers outside of the platform economy will most likely be  much less protected than platform workers.       97 Namely, according to the author, a ' technology which is developed under the control of a surveillance system such as  exists in China cannot be the solution for a democratic society based on the rule of law and respect of fundamental rights.  Meanwhile, there is clear momentum growing for general rules and limits in the United States as well as most countries  around the world, as concern increases about the unlimited use and abuse of personal dat a' (Buttarelli, 2018).  
STOA  | Panel for the Future of Science and Technology     60 5. Policy options   5.1. Critical points  in the context of different legislative files   The GDPR is an instrument with great promise. However, its principles and criteria are quite flexible,  making it challenging to apply the instrument consistently. As mentioned, the European Parliament has alre ady suggested that the European Data Protection Board would adopt guidelines related to  AI, profiling and fully automated decision -making in a n HR environment. This tool could indeed be  momentous for the world of work. Moreover, Article 88 of the GDPR enab les the Member States to  adopt more specific rules, also by means of collective bargaining, to ensure the protection of the rights and freedoms in respect of the processing of employees ' personal data in the employment  context. This article, as already mentioned, has yet to be substantially used by lawmakers and social  partners.   In addition to bolstering the application of the GDPR in the field of employment, the most critical concern is about adequate enforcement. National DPAs  are, in theory , the most cru cial actors in this  respect. Some authorities have acted against employers ' surveillance practices in the past. For  instance, t he French DPA imposed a fine of € 400 000 on the  Régie autonome des transports  parisiens , among other reasons, because it appeared  excessive and contrary to the principle of data  minimisation to record the number of strike days of individuals instead of just the total number of  days of absence for the purpose of career advancement (CNIL, 2021). The Italian DPA issued a  €2.6 million s anction against a food -delivery platform for alleged regulatory violations also linked to  algorithmic discrimination  (GPDP, 2021) . More systemic interactions between the DPAs and civil  society, including unions, are crucial in this respect to bring issues to the attention of DPAs (Degli Esposti and Ferrándiz, 2021). However, employment has been a strategic and operational topic for  only a very few countries, including France and Italy, so far (Barros Vale, Zanfir -Fortuna and van Eijk,  2021, 10). This  subject would benefit from more scrutiny also elsewhere.   Nonetheless, even if that were to happen, it arguably remains unrealistic to expect DPAs to enforce the GDPR on a large scale in relation to employment matters. As such, it is worth exploring other ways of buttressing enforcement. As mentioned, DPIAs  should involve workers or their  representatives. Despite the provisions of GDPR and the WP29 's guidelines, this is currently not  happening, albeit it would put the social partners in a better position to scrutinise certain AI  applications. Moreover, as noted, Article  80 enables Member States to allow trade unions to file  GDPR proceedings irrespective of any individual claimant, a provision largely unutilised for the moment.   Similar to the GDPR, many other  existing instruments are not currently used to their full potential .  Regarding Directive  2002/14/EC on the information and consultation of employees ' representatives,  in the 2009 report on its implementation, Jean -Louis  Cottigny (S&D, France, 2012- 2014) agreed ' with  the Commission that in some Member States the transposition of Directive  2002/14/EC has taken  place in a minimal or deliberately vague manner or not at all ' (Cottigny, 2009). The inadequate  implementation of this directive is also manifest in t he AI debate. In theory, t he directive might open  the door to consultation on AI -related matters in several instances; yet, systematic engagement by  workers ' representatives in this field is lagging behind. It is thus urgent to revisit the debate on how  to make this directive more effective.   Concerning the EU instruments on OSH, the impact of AI software on workers ' health and safety,  including its psychosocial elements, appear unlikely to receive the necessary attention in employers '  risk assessments. In r esponse, m ore awareness c ould thus be raised around some specific AI -related  OSH risks, such as technostress (e.g. Cadieux et al., 2021). For instance, a social partners ' agreement  could be pursued, resembling the EU Framework Agreement on Work -Related Str ess. The annexes  in the Work Equipment Directive could also be revised. The European Agency on Occupational 
AI and digital tools in workplace management and evaluation     61 Safety and Health (EU -OSHA) could likewise stress the issue, which could even feature in the  discussions on the right to disconnect. In addition, a right to disconnect might serve to limit  excessive surveillance, as it could pose clearer and urgently needed boundaries to limit AI 's reach  into workers ' lives (EU -OSHA, forthcoming).   Another essential legal file is anti -discrimination law. It is urgent to review the options that could  guarantee its application to algorithmic management and AI at work. Individual lawsuits are key elements of anti -discrimination enforceme nt. The claimants ' success, as already mentioned, seems  largely to hinge on how easily the burden of proof switches, i.e. requiring the employer to prove no  discrimination occurred or that it can be justified instead of the claimant proving the opposite.  Reaching the threshold of enough  prima facie evidence is vital . The GDPR contains several  mechanisms related to the principle of transparency that can enable claimants to meet this  threshold (Gaudio, forthcoming). Nonetheless, many scholars argue that individual enforcement,  albeit valuable, will not suffice. I ndeed, i t is also paramount to develop  regulatory responses driven  by public bodies to combat algorithmic discrimination, including by means of audits. Computer  scientists should arguably be more involved in this regard.  Policy -makers must reflect on how such  alternative means can interact with court proceedings.   Lastly, the AI act has to be considered from this holistic perspective. It has long been clear that  algorithms do not operate in a legal vacuum in the work context. The same is true for AI; however,  the le gal machinery needs to be reinforced to cope with its integration into the world of work.  Currently , the draft AI act does not achieve this. In response , an explicit provision that the act is  without prejudice to the GDPR and other labour laws, along the l ines of what Article  88 of the GDPR  prescribes, could be added to ensure that the AI act does not preempt any domestic legislation that  can steer the use of AI towards sustainable  ends  (Aloisi and De Stefano, 2021c) .98  Furthermore, as mentioned above, the leaked draft of the AI a ct contained a provision that people  in charge of AI oversight needed to be able to ' decide not to use the high -risk AI system or its outputs  in any particular situation without any reason to fear negative consequences ' (De Stefano,  2021a).  Although this initial phrasing was arguably too vague to begin with, the regulation actually  proposed no longer mentions anything of this sort. Provisions could be added to ensure that  persons in charge of AI oversight obtain  adequate protections to guarantee they can fulfil their  tasks. Also, as part of this endeavour to truly put humans in command, one possibility would be to  adapt the Whistle blower Directive to cover employees engaged with AI that report violations of EU  and national regulation.   Additionally, the AI a ct seems to mostly rely on  the fact that it imposes duties at the source of the AI  (the provider). This duty on the provider comes in addition to the user/employer 's duties under the  GDPR, OSH and collective legal frameworks to engag e in risk/impact assessments and consultation.  Options should be explored to link up these various assessments, forming a coherent evaluation  chain.  In this vein, to the extent a thorough assessment is conducted at the source under the AI act,  the employer , as the user of the AI system, could be in a better position to pass the DPIA, risk  assessment and preliminary consultation. Instead , if these (self -)assessments  by or for providers are  of poor quality, the employer might struggle for the AI application to satisfy the requirements under  the data protection, OSH, anti -discrimination and collective laws applicable to the employer. In  other words, the employers ' obligations under these respective regulations may  also pressure the    98 We can think of examples such as the Spanish riders law, which c ontains the ' right of workers ' representatives to be  informed about the parameters, rules and instructions on which the algorithms that may have an impact on working  conditions are based ' (Todolí -Signes , 2021c). On another note, a new act in Portugal const rains an employer's possibilities  of controlling teleworkers through ICT technology (Bateman, 2021). To the extent the AI Act considers an AI application  trustworthy and approved through self -assessment, such laws would constrain employer's use of these AI  systems;  hence, it is not inconceivable these laws would violate the AI act because they impose additional roadblocks to the  uptake of AI.  
STOA  | Panel for the Future of Science and Technology     62 (self -)assessments under the AI a ct to remain of high quality with sufficient transparency,  and not to  becom e a rubber stamp.   In addition, as mentioned above, it has been argued that the AI act should go further and ban certain  hazardous and invasive AI applications . Recital  41 merely mentions how '[t]he fact that an AI system  is classified as high risk under this Regulation should not be interpreted as indicating that the use of  the system is  necessarily lawful under other acts of Union law or under national law compatible with  Union law, such as on the protection of personal data, on the use of polygraphs and similar tools or  other systems to detect the emotional state of natural persons .' Instead of pointing this out,  however, the act could arguably be more daring in its own approach. For instance, i nferring a natural  person 's emotions through AI  should arguably be prohibited in principle (EDPB and EDPS, 2021, 12) .  Other AI -related practices that are  evidently  at odds with the fundamental rights of workers,  including the many examples  we discuss above,  could likewise  be directly prohibited.   This would spare  labour inspectors, data protection and other enforcement authorities the work  of  having to argue why these tools violate privacy, OSH and other standards . Similarly, options should  be explored to ensure that employers are not left alone in assessing the  OSH  risks of such tools  and  their compatibility with GDPR standards . If abusive practices are more proactively banned ,  authorities and employers  could  devote their time to assessing other AI applications , such as AI  monitoring of fatigue,  that , sometimes,  might likewise unwillingly  infringe  fundamental rights but  can, at other  times,  be implemented  in a proportionate  and  justifiable manner.  The AI act could take  a clearer and more comprehensive stance on unacceptable uses of AI, or at least indicate which  authority can make such a determination.   Furthermore, policy -makers shoul d reflect on options for the enforcement of the act. For example,  Aislinn  Kelly -Lyth highlights the potential of Article  60. It creates a n EU database for stand -alone  high -risk AI systems , providing publicly available  information about these individual AI systems  (Kelly -Lyth, 2021a , 8). This  system could increase transparency, but w hy should individuals with a  legitimate interest not be allowed  to request additional  information beyond what is publicly  available ? The wor kers  of an employer who  relies on a third -party AI system could benefit from  obtaining such information directly from the AI act's authorities . Likewise, i magine that hundreds of  employers use an AI system,  and , at some point, the system  is subjected to legal scrutiny  in some of  these enterprises. It is entirely plausible  for this EU database to serve as a  central registry ,  documenting these  issues.  Article  62 already demands that providers of high -risk AI systems report  any serious incident or any malfunctioning of AI systems that  constitute a breach of obligations  under Union law intended to protect fundamental rights . It would be helpful to spread such  information beyond just the national public authorities that 'supervise or enforce the re spect of  obligations under Union law protecting fundamental rights ' (Article  64).  In 2020,  the European Parliament proposed to hand natural or legal persons a right to seek redress  for injury or harm caused by the development, deployment and use of high- risk AI in breach of  Union law and the obligations set out in th e proposed  regulation  on ethical principles for the  development, deployment and use of artificial intelligence, robotics and related technologies .99 The  current AI a ct does not live up to that promise  as information is largely shielded from those  subjected to AI. Moreover, there is no single right to seek re dress  for AI subjects . This type of bottom up accountability , however, would likely be of great support for the enforcement of the AI a ct. For   example,  by making it possible to file a negligence claim if  a provider or user  repeatedly  violates the  AI act or other forms of Union law , all parties could be pressured to take self -assessments more  serious ly. Options should be explored to ensure that the AI act creates a positive dynamic in which   existing laws benefit from increased transparency  obtai ned through the act. In turn , providers  and    99 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical   aspects of artificial intelligence, robotics and related technologies,  P9_TA(2020)0275.  
AI and digital tools in workplace management and evaluation     63 users  would feel compelled to take their self- assessments and reporting duties  under the AI act more  serious ly because of their accountability under existing anti -discrimination, privacy and other laws   if problems do arise  at a later stage , and, potentially , some form of redress  under the AI a ct in the  event of flagrant or persistent violations with harmful consequences for individuals . Should such a  redress mechanism be introduced , further articles could allow business es to commence   regularisation process es – within a short period of time – without being penalised under the  law,  provided that full disclosure is made  and the relevant authorities and the social partners are involved   (De Stefano, 2021b) .   5.2. An overview of policy options   Along the lines of the arguments presented so far, the following sections set ou t a range of possible  policy options for a range of policy files :  Concerning the proposed AI act:  • Break the ' regulatory ceiling '. Allow Member St ates to impose additional  requirements in some areas, such as most notably employment (Kelly -Lyth, 2021a;  Veale and Borgesius, 2021), similar to what has been provided for in the GDPR (Aloisi  and De  Stefano, 2021a).   • Reconsider the scope of the AI a ct. Better define the act's scope, for example, by  focusing on the impact of the technology rather than the specific type of technology (AlgorithmWatch, 2021), or by focusing on certain kinds of practices or uses (Biber,  2021).   • Expand what are considered unacceptable hazards. B etter define and reconsider  the list of AI systems that are banned, raising the bar of acceptability . For example, the  use of biometric identification systems should be prohibited to a broader extent  (AlgorithmWatch, 2021). The same should  be true for all forms of emotional  surveillance at work (EDPB and EDPS, 2021). 100  • Acknowledge the imbalance of power. E xplicitly acknowledge that AI systems will  tend to exacerbate the imbalance of power between certain actors, notably between  employers and workers  (Kullmann and Cefaliello, 2021) . Therefore , it is paramount to  involve workers, their representatives and unions in the introduction and use of these technologies at work (AlgorithmWatch, 2021; ETUC, 2021).   • Introduce preliminary impact assessments for any AI -based application.  Reconsider the approach to risk categorisation and require a preliminary impact  assessment for any AI -based application, not only high- risk systems, allowing to  determine the system' s respective risk level on a case- by-case basis (AlgorithmWatch,  2021).   • Redefine what are high -risk systems. R econsider the criteria that categorise an AI  system , including  at work , as a high -risk system, by, for example, pointing out t hat it  is sufficient fo r the AI system to potentially  have a significant harmful impact on  health and safety, instead of an actual effect along those lines ( Kullmann and  Cefaliello, 2022).   • Demand third -party assessments. I mplement third -party assessment for all high risk AI systems as standard (Biber, 2021; Ebers et al., 2021, 595; ETUC, 2021; Kelly -Lyth,  2021a; Kop, 2021; Ponce Del Castillo, 2021). 101 Alternatively, an enforcement    100 See also European Parliament resolution of 12  February 2019 on a  comprehensive European industrial policy on  artificial intelligence and robotics, P8_TA(2019)0081.  101 See also European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework  of ethical aspects of artificial intelligence, robotics and related technologies, P9_TA(2020)0275. 
STOA  | Panel for the Future of Science and Technology     64 mechanism could be conceived that would subject problematic self -assessed high risk systems to a third -party assessment at a later stage.   • Acknowledge the value of existing laws. A cknowledge the merits of existing  privacy, non -discrimination and labour laws in dealing with algorithmic and AI  systems. Safeguard  existing and future legislat ion in these fields relative to the  harmonisation aims of the a ct. Call on the Member States to evaluate that legislation  to make sure it can be more effectively enforced in the AI context.   • Stress that the use of AI may lead to employment reclassification.  Specify that the  fact that a management system is allowable under the AI a ct does not exclude that  the use of this system vis- à-vis self -employed persons could lead to the reclassification  of those persons under the existing standards used to determine th e existence of an  employment relationship.   • Demand transparent and democratic standardisation processes. Guarantee that  the European standardisation organisations ' processes are transparent and  democratic, also by including the social partners (AlgorithmWat ch, 2021; ETUC, 2021).   • Protect the workers ' supervising AI systems.  Provide workers who are meant to  perform AI oversight with the protections necessary for them to be able to perform  their work and deviate from the course set by an AI without any fear of adverse  consequences.   • Reinforce non -discrimination safeguards.  Reconsider the requirements imposed  by the AI a ct to make sure the risk of discrimination is adequately assessed in practice  instead of under 'lab conditions ' (AlgorithmWatch, 2021). In this re spect, it might be  necessary to assess certain requirements again once the system is introduced to the market.   • Favour auditability and explainability.  Demand for AI systems in certain high- risk  areas to be developed using techniques that favour ' auditabili ty' and explainability  (Borgesius, 2020, 1583).   • Demand Fundamental Rights Impact Assessments.  Oblige providers of high -risk  AI systems to make Fundamental Rights Impact Assessments that vary depending on the area in which the system is operationalised. 102 This would necessitate going  beyond the procedural checklist already established under C hapter  2 of the AI act.  • Expand the EU database. The EU database  prescribed in Article  60 of the draft AI act  could become a much more important mechanism  to achieve greater levels of  transparency than it currently is (AlgorithmWatch, 2021; Ebers et al., 2021, 597;  Kelly -Lyth, 2021a).   • Identify the enforcement bodies. C larify the role of the enforcement bodies  provided for in the AI act, for instance, in relation to the GDPR 's enforcement  mechanism, labour inspectorates and equality bodies in charge of enforcing non discrimination law (AlgorithmWatch, 2021). Data protection authorities could be  designated as national supervisory authorities pursuant to Article  59 of the draft AI  act (EDPB and EDPS, 2021).   • Increase coherence between the AI a ct and GDPR.  More broadly, pursue coherence  between the AI act and the GDPR, for example, for what concerns the certification of   AI systems under the AI a ct and GDPR, respectively (EDPB and EDPS, 2021).   • Increase coherence with other EU policy initiatives.  Make sure the AI act aligns  with other EU policy initiatives, such as the proposal for a directive on platform work.   • Provide AI subjects with access to information.  Provide not only users but also the  subjects of AI with access to information (AlgorithmWatch, 2021; Kelly -Lyth, 2021a).     102 For a discussion of such F RIAs, see: FRA, 2020.  
AI and digital tools in workplace management and evaluation     65 • Introduce (collective) redress mechanisms.  Introduce an effective (collective)  redress mechanism f or individuals subjected to AI (AlgorithmWatch, 2021; Ebers et al.,  2021).103  • Make it possible for businesses to commence a regularisation process.  Establish  the possibility to issue a regularisation period during which businesses can correct  their practices without being subjected to penalties under the act, subject to full  disclosure and involvement of public authorities and the social partners.   • Adequately fund and staff market surveillance authorities.  Guarantee that, in so  far as market surveillance auth orities become the main enforcers of the act, these  authorities are well funded and staffed.   Concerning  the GDPR:   • Introduce comprehensive compliance schemes.  Prompt the European Data  Protection Board (EDPB) and other EU authorities to formulate a ' comprehensive  compliance scheme ' to ensure  that automated decisions, in or  outside of the field of  employment, comply with the GDPR ( Bayamlıoğlu, 2021).  If the EDPB neglects to do  so, national Data Protection Author ities (DPAs) can provide the guidance needed   (Sartor, 2020, 80).   • Stimulate systematic interactions between DPAs and civil society.  Push for more  systemic interactions between the DPAs and civil society, including unions. This is crucial to bring issues to the attention of DPAs (Degli -Esposti and Ferrándiz, 2021).   • Prioritise AI in employment as a topic for DPAs.  Since data protection in the field  of employment does not seem to feature as a priority on the agenda of many DPAs, these public authorities could be stimulated to devote more time and resources to this end.   • Issue guidance to employees.  Have the  competent authorities  provide concrete  guidance  about  what the rights of data subjects in relation to AI systems at work  entail ; for example, what does it mean to provide information about the 'logic ' of an  AI system , or what rights do data subjects have in relation to ' inferred data ' (Sartor,  2020, 81).   • Issue guidance to employers. Inform employers about the implications of the GDPR 's principles for the implementation and operation of AI at work.   • Clarify the test of the balance of interests. The competent authorities should indicate what it means for employers to conduct a ' test of the balance of interests,  which includes a fundam ental rights assessment ' when introducing and running AI at  work.104 Accordingly, it should be clear how this intersects with workers ' right to  object to data processing when it is grounded on the employer 's legitimate interests.   • Explore complaint and notification mechanisms.  Envision new and more effective  complaint and notification mechanisms with a view to better enforce the GDPR (Deg li-Esposti and Ferrándiz, 2021).   • Enable collective enforcement.  Enable collective enforcement in this field  of law , for  instance , through class actions ( Sartor, 2020, 81; Moore, 2020).   • Provide trade unions with the ability to bring claims.  Enable trade unions to bring  cases under the GDPR without having to represent an individual claimant (Pato, 2019).   • Make use of Article 88 GDPR.  Spur Member States to use Article  88 and issue more  specific standards for personal data protection in employment (EESC , 2018) , also by    103 See also European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework  of ethical aspects of artificial intelligence, robotics and related technologies, P9_TA(2020)0275.  104 European Parliament resolution of 25 March 2021 on the Commission evaluation report on the implementation of the  General Data Protection Regulation two years after its application , P9_TA(2021)0111. 
STOA  | Panel for the Future of Science and Technology     66 encouraging collective bargaining in this field . This article could  also be used to  establish joint data protection committees , for instance (Todol í-Signes, 2019).   • Expand the importance of Data Protection Impact Assessments. E xplore the  potential role  of employers ' Data Protection Impact Assessments  (DPIAs) , inter alia , in  addressing the risk of discrimination through AI  systems  at the workplace  (Kelly- Lyth,  2021b) . Note the importance of incorporating workers' representatives and unions in  these DPIAs (Moore, 2020, 91).   • Confirm that employers must keep record s of processing activities.  Unambiguously confirm that, regardless of the amount of staff, an employer using a  high -risk AI system must keep a record of pr ocessing activities in the sense of  Article 30 GDPR.   • Explore the meaning of ' personal data protection by design '. Issue clarifications  about what it means for AI  system s to engage in  'personal data protection by design '  (Mitrou, 2019; Sartor, 2020, 81).  • Make a data protection officer mandatory. E valuate requiring that employers  relying on high- risk AI systems designate  a data protection officer  (DPO) (Article 37  GDPR) . Worker representative s could be added to cross -check the DPO 's work  (Moore,  2020, 89 -90).  • Establish a 'p rivacy due diligence' requirement.  Embed a privacy due diligence  requirement into the human rights -based corporate responsibility scheme (Ebert,  Wildhaber and Adams -Prassl, 2021).   Concerning the proposed EU directive on platform work:   • Algorithmic management should not be seen ' as a given '. Provide that algorithmic  management is not taken ' as a given' and is not regulated to a lesser extent than what  European national legislation already provide s for other technological tools and  applica tions that allow monitoring the work performance.   • Maintain a presumption of employment that sufficient ly covers  workers. Stress  the need for the legislative initiative on platform work to have a broad personal scope of application to ensure (bogus) self -employed platform workers are not without any  legal coverage (De  Stefano and Aloisi, 2021).  Consider strengthening the  presumption, since the draft indicators may currently be exceedingly narrow and risk excluding some of the most vulnerable platform workers  from the operation of the  presumption.    • Protect the workers ' supervising platforms ' algorithmic systems.  Ensure that  workers involved in overseeing digital labour platforms ' automated systems,  including their impacts on OSH, can  perform their work and de viate from the course  set by an AI without any fear of adverse consequences.   Concerning  the EU 's anti -discrimination laws:   • Ascertain whether and how claimants can bring sufficient prima facie evidence.   Investigate the difficulties for claimants in bringing sufficient prima facie evidence of  discrimination . Alternative methods might have to be developed that enable  claimants to  bring statistical evidence  that shows it is indeed likely the system  discrimina tes (Wachter, Mittelstadt and Russell, 2021).   • Make sure employers are cooperative.  Specify employers ' obligations in terms of  providing all relevant information once jobseekers or workers plan on lodging a non discrimination claim in court, as well as what  happens when an employer refuses to  do so.   • Introduce out -of-court mechanisms. Introduce  out-of-court  mechanisms to detect  discrimination in algorithms, such as certification or specialised auditing. Also, 
AI and digital tools in workplace management and evaluation     67 consider  clarifying the role of such processes in existing anti -discrimination  regulations. For instance, the burden of proof for a claimant could be higher when a  system has been certified in advance, or a court could decide in an interlocutory  judgment to have an algorithmic system audited before making  a final ruling.   • Develop the concept of ' equality by design '. Introduce the obligation for AI systems  to implement an 'equality by design ' approach , similar to the GDPR 's 'data protection  by design ' approach (Xenidis and Senden, 2020 ).  • Consider the potential role for e quality bodies and trade unions.  Reflect on how  equality bodies and trade unions  can intervene to  help certify  AI systems , audit  them   or act against them in the court s, taking into consideration the connection  with data  protection laws (Kelly- Lyth, 2021b; Schubert and Hütt, 2019).   • Highlight non- discrimination considerations in Data Protection Impact  Assessments and balance of interest tests.  Clarify the importance of considering  algorithms ' and AI 's potential discriminatory effects during the employer 's DPIAs of  such tools, and whenever the employer has to conduct a ' test of the balance of  interests ', taking into account data subjects ' interests and fundamental rights.   • Protect people with disabilities.  Pay special attention to how people with  disabilities could be provided with reasonable adjustments when subjected to AI in  recruitment and the world of work at large.   • Strike a human  right s-based balance between protecting trade secrets and  scrutinising AI systems.  Search for be st practices and reflect on how to strike a  human rights- based balance between the protection of AI as a protected trade secret  and the rights of individuals, unions and public authorities to scrutinise such systems, especially to discover discriminatory e ffects.   Concerning the EU 's OSH acquis :  • Negotiate a f ramework agreement. Ask European social partners to negotiate a  framework a greement on the OSH implications of AI  to include this aspect under the  Framework D irective.   • Amend the annex of the Work Equipment Directive . Amend the annex of the Work  Equipment Directive to address the impact of AI in this field.  • Provide adequate training.  Ensure that workers receive adequate training  to deal  with more autonomous tools  and to spot the dangers of revising software in  production processes ( e.g., excessive quotas).   • Follow -up on the draft m achinery regulation.  Evaluate the changes being made to  the machinery directive, bearing in mind the impact on workers ' health and safety.   Concerning  the Working Time Directive:   • Evaluate AI 's impact on the effective enjoyment of rights in this d irective.  Remain  vigilant to ensure AI  systems do not undermine the effective enjoyment of  workers '  rights, for example, to rest breaks , holidays or their stand -by time being considered  working time.   • Futureproof this d irective.  Engage in discussions on future changes to the directive  in response to technological unemployment . Some suggestions have been to lower   the maximum weekly working time and remove the opt- out clause  (Spencer et al.,  2021, 53).   Concerning the Fixed -Term Work Directive:   • Evaluate protections against successive fixed -term employment contracts. Be  aware that AI might make it feasible to  have a more continuous understanding of  workers' performance levels, inciting employers to rely on fixed- term employment 
STOA  | Panel for the Future of Science and Technology     68 contracts. Evaluate whether the implementation of this d irective has led to adequate  restraints on the successive use of fixed -term contracts that could resist this  development.   Concerning  the Transparent and Predictable Working Conditions Directive:   • Follow -up on the implementation of this directive.  Verify whether Member States '  implementation of the directive 's articles regarding the abuse of on -call work is   sufficient to constrain AI -enhanced scheduling.   • Maintain the effectiveness of Article 12.  Ensure that  the reliance on AI systems to  schedule work is not a ground to decline  the workers ' right to request a form of  employment with more predictable and secure working conditions under Article  12.  It must be possible to readjust the AI along those lines or to overrule its decisions.   Employers should not be able to use the argument that AI and algorithmic  management tools are technologically unable to grant such a request to avoid the  application of this provision.  Concerning  the EU instruments on workers ' information and consultation:  • Evaluate the implementation of Directive 2002/14/EC – general framework for  informing a nd consulting employees in the EU . Explore whether  Member States  have adequately implemented the framework directive in a way that requires worker  consultation whenever AI systems  with significant effects for workers  are  implemented at work  and wh ether these consultations meaningfully occur in  practice.   • Reassess at what point employers must provide information about an AI or  involve workers ' representatives. Along the lines of the Spanish ' riders law ', urge  Member States to draft domestic laws so as to oblige employers to inform workers  and their representatives about algorithms and AI at work, regardless of these tools making 'substantial changes ' to the work organisation. Workers and representatives  could even obtain  a right to involvement rather than just information (All -Party  Parliamentary Group on the Future of Work, 2021, 15).   • Demand mandatory consultation for high -risk AI systems. Aim at making  consultation mandatory between the employer and workers ' representat ives  whenever a high -risk AI system, as currently described in the AI a ct, is introduced to  the workplace or is altered throughout its lifecycle.   • Guarantee a broader technology information and consultation duty.  Reemphasise the Member States ' obligation to frame their domestic legislation in a  way that the directive becomes effective in this context, or, if necessary, revise the  directive  to guarantee a technology information and consultation duty concerning  AI  systems that can  have a significant effect on work organisation (Spencer et al., 2021,  55).  Concerning other  pending legislative files:   • Support the right to disconnect.  Support the introduction of a right to disconnect   at the EU level (Spencer et al., 2021, 52), or any other appropriate level,  which could  also serve to limit the invasion of workers ' private lives  by AI tools  (EU-OSHA,  forthcoming).105    105 European Social Partners Framework Agreement on Digitalisation, June 2020; European Parliament resolution of  21 January  2021 with recommendations to the Commission on the right to disconnect , P9_TA(2021)0021. 
AI and digital tools in workplace management and evaluation     69 • Pay attention to the ep rivacy regulation. Consider  the e privacy regulation as an  opportunity to safeguard workers ' communication from excessive AI monitoring.   • Provide self -employed workers with a right to collective bargaining. A dopt  solutions that allow  self- employed workers to bargain collectively  without violating  anti -trust laws  (Countouris, De Stefano, Lianos, 2021; Servoz, 2019, 128). These  workers will be subjected to AI  systems; they currently lack the ability to restrain these  systems  collectively .  In terms of  potential future i nitiatives:   • Draft  an ad hoc  directive on AI in employment . This instrument would be separate  from the AI a ct. It would aim to govern the risks of AI in an employment context (Ponce  Del Castillo, 2021).   • Introduce new reporting duties.  Issue a di rective requiring  firms to report on the  impacts of digital technologies on jobs, wages and the quality of work ( Spencer et al.,  2021, 54- 55).  • Draft a d irective on the safety of algorithms. A dvance a specific directive on the  safety of algorithms to reduce  the occupational risks suffered by workers subject to AI  (Todol í-Signes, 2021a).   • Draft a d irective on psychosocial risks. A dvance a directive on psychosocial risks  that could, among other things, highlight the relation between automated systems,  with or without AI features, and psychosocial risks (Cefaliello, 2021).   • Amend the Whistle blower Directive. A mend the Whistle blower Directive to  unambiguously  protect developers of AI and potentially other persons involved in the  implementation thereof.106    106 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of  ethical aspects of arti ficial intelligence, robotics and related technologies, P9_TA(2020)0275. 
STOA  | Panel for the Future of Science and Technology     70 6. Conclusions   This study first argued that the introduction and operation of AI -enabled recruitment and  managerial tools as well as any other form of algorithmic management at work require significant  care and should not be implemented lightly. From the busi ness perspective , this is also because data  accessibility in many workplaces remains poor, particularly when it comes to quality data.  Furthermore, existing devices and work tools may poorly support the incorporation of AI features,  potentially demanding s ignificant additional investment. AI tools from various suppliers are,  moreover, not always compatible, which can lead to unexpected issues. Various  departments with  varied interests  in one and the same company may even  refuse to cooperat e by sharing relev ant  data for one another 's AI. Therefore, even simp ly from the business standpoint, mainstream  discussions on AI seem to excessively present the situation as if AI lends itself to a ' plug and play '  routine. At the same time, as discussed above , these discu ssions also tend to overemphasi se the role  that AI -enabled tools and algorithmic management may play in addressing or solving complex  societal problems, including discrimination in employment and occupation as well as occupational  health and safety.   While some of these practices may help in some instances, we also discussed how in other cases ,  the outcomes could be quite the opposite. We also argued that the main ideas and objectives  behind the introduction of these practices and how these are incorporated in the design of software would significantly influence those outcomes. We have reported that a very significant thrust  behind the introduction of AI -enabled and algorithmic -management systems currently seems to  follow objectives of standardisation, enhanc ed monitoring and disciplining of the workforce more  than anything else (see also Acemoglu , 2021). Some AI applications, such as scheduling tools that  draw on sensor data or CCTV cameras that apply AI analysis, will critically influence working conditions,  and not for the better.   This warrants regulatory mandates to fully consider OSH, data protection, and other labour and  employment rights  when designing these AI systems. Ex -ante  risk and impact assessments by  employers, as well as consultations , are already obligatory in some of these instances. The existing  EU legislation could arguably be revisited to better clarify and reinforce such duties, leading to a  precautio nary  attitude when introducing and implementing AI -enabled manageri al tools at work.  In addition, too often, the domestic implementation and enforcement of already existing EU laws  leave much to be desired. The European Parliament might want to take action to address these  enforcement gaps.   The current proposal for an AI a ct does not reflect all these criticalities. Of course, the final a ct could  add valuable elements to the existing regulatory spectrum. For example, it could increase transparency not just for users but also for AI subjects, and it could force providers to make a  thorough assessment of their AI 's potential discriminatory effects and broader consequences for  fundamental rights before bringing it to the market. The act could also contain a vi tal right to redress  if providers neglect their duties under the reg ulation. However, as it stands, the proposed a ct  refrains from doing so, broadly opting for a deregulatory approach that presents serious risks to the  already existing regulations ' ability to govern the introduction and use of AI at work. This outcome  could be avoided by permitting the Member States and social partners to adopt additional rules  related to AI in the working environment as well as by better coordinating its provisions with other  EU legislation concerning employment and labour protection.     
AI and digital tools in workplace management and evaluation     71 References   Abduljabbar, Rusul, Dia, Hussein, Liyanage, Sohani and Saeed Asadi Bagloee, ' Applications of Artificial  Intelligence in Transport: An Overview ', Sustainability , 2019.   ACAS, My boss the algorithm: an ethical look at algorithms in the workplace , ACAS, 2020.   Acemoglu, Daron, Harms of AI , Nation al Bureau of Economic Research, 2021.   Acemoglu, Daron, Autor, David, Hazell, Jonathon and Pascual Restrepo, AI and Jobs: Evidence from  Online Vacancies , National Bureau of Economic Research, 2021.   Adams -Prassl, Jeremias, 'What if Your Boss Was an Algorithm? The Rise of Artificial Intelligence at Work ',  Comparative Labor Law & Policy Journal , 2019.   Adams -Prassl, Jeremias and Michael Veale, ' Towards an European Approach to AI Regulation? Critical  Less ons from the EU AI Proposal ', forthcoming.   Adler, Seymour, Boyce, Antony S. and Pat M. Caputo, 'Employment Testing ' in Next Generation  Technology -Enhanced Assessment: Global Perspectives on Occupational and Workplace Testing ,  Cambridge University Press, 20 17.  Alamalhodaei, Aria, ' Tesla recalls 11,704 vehicles after identifying Full Self -Driving Beta software error' ,  Tech Crunch , 2 November 2021.   Albinus, Phil, ' Can AI help reverse the Great Resignation? ', Human Resource Executive , 7 September 2021.   Albrecht,  Thorben and Christian Kellermann, Artificial Intelligence and the Future of the Digital Work oriented Society: An outline for a holistic technology impact assessment , Friedrich Ebert Stiftung, 2020.   AlgorithmWatch, Draft AI Act: EU needs to live up to its own ambitions in terms of governance and  enforcement , AlgorithmWatch, 2021.   All-Party Parliamentary Group on the Future of Work, The New Frontier: Artificial Intelligence at Work , UK  Parliament, 2021.   Aloisi, Antonio and Elena Gramano, 'Artificial Inte lligence Is Watching You at Work: Digital Surveillance,  Employee Monitoring, and Regulatory Issues in the EU Context ', Comparative Labor Law & Policy Journal ,  2019.   Aloisi, Antonio and Valerio De Stefano, 'Essential jobs, remote work and digital surveillan ce: addressing  the COVID -19 pandemic panopticon' , International Labour Review , 2021 a.  Aloisi, Antonio and Valerio De Stefano, ''Frankly, My Rider, I Don 't Give a Damn '', il Mulino , 7 January  2021b.   Aloisi, Antonio and Valerio De Stefano, ' Artificial Intell igence and workers ' rights ', Social Europe , 8  November 2021c.   Aloisi, Antonio and Valerio De Stefano, Your Boss is an Algorithm: Artificial Intelligence, Platform Work  and Labour , Bloomsbury, forthcoming.   Ajunwa, Ifeoma, ' An Auditing Imperative for Automated Hiring Systems ', Harvard Journal of Law &  Technology , 2021.   Ajunwa, Ifeoma, The Quantified Worker , Cambridge University Press , forthcoming.   Ajunwa, Ifeoma, Crawford, Kate and Jason Schultz, 'Limitless Worker Surveillance ', California Law Review ,  2017.   Ajunwa, Ifeoma and Daniel Greene, 'Platforms at Work: Automated Hiring Platforms and Other New  Intermediaries in the Organization of Work ' in Work and Labor in the Digital Age , Emerald Publishing,  2019.   Ajunwa, Ifeoma and Rachel Schlund, ' Algorithms and the Social Organization of Work ' in The Oxford  Handbook of Ethics of AI , Oxford University Press, 2020.   Allen, Robin, Artificial Intelligence, Machine Learning, Algorithms and Discrimination Law: The New  Frontier , Cloisters, 2020.  
STOA  | Panel for the Future of Science and Technology     72 Ali, Muhammad, Sapiezynski, Piotr, Bogen, Miranda, Korolova, Aleksandra, Mislove, Alan and Aaron Rieke,  'Discrimination through Optimization: How Facebook 's Ad Delivery Can Lead to Biased Outcomes ',  Proceedings of the ACM on human- computer interaction , 2019.   Angrave, David, Charlwood, Andy, Kirkpatrick, Ian, Lawrence, Mark and Mark Stuart, ' HR and analytics:  why HR is set to fail the big data challenge ', Human Resource Management Journal , 2016.   Angwin, Julia, Larson, Jeff, Mattu, Surya and Lauren Kirchner, ' Machine Bias ', ProPublica , 23 May 2016.   Article 29 Data Protection Working Party, Opinion 2/2017 on data processing at work , European  Commission, 2017 a.  Article 29 Data Protection Working Party,  Guidelines on Data Protection Impact Assessment (DPIA) and  determining whether processing is ' likely to result in a high risk ' for the purposes of Regulation 2016/679 ,  2017b.   Article 29 Data Protection Working Party, Guidelines on Automated individual decision- making and  Profiling for the purposes of Regulation 201 6/679 , European Commission, 2018 a.  Article 29 Data Protection Working Party, Guidelines on transparency under Regulation 2016/679 , 2018b.   Autor, David, David Mindell and Elisabeth Reynolds, The Work of the Future: Building Better Jobs in an  Age of Intelligent Machines , MIT Task Force on the Work of the Future, 2020.   Babic, Boris, Chen, Daniel L., Evgeniou, Theodoros and Anne -Laure Fayard, ' A Better Way To Onboard AI ',  Harvard Business Review , 2020.   Bales, Richard A. and Katherine V. W. Stone, 'The Invisible Web at Work: Artificial Intelligence and  Electronic Surveillance in the Workplace ', Berkeley Journal of Employment and Labor Law , 2020.   Ball, Kristie, ' Workplace surveillance: an overview ', Labor History , 2010.   Ball, Kristie, Electronic Monitoring and Surveillance in the Workplace. Literature review and policy  recommendations , Publications Office of the European Union, 2021.   Barnard, Catherine and Alysia Blackham, 'Discrimination and the Self -Employed: The Scope of Protection  in an Interconnected Age ' in European Contract Law and the Charter of Fundamental Rights , Cambridge  University Press , 2018.   Barros Vale, Sebastião, Zanfir -Fortuna, Gabriela and Rob van Eijk, Insights into the Future of Data  Protection Enforcement: Regulatory Strategies of European Data Protection Authorities for 2021 -2022 ,  Future of Privacy Forum, 2021.   Bastian, Brock and Nick Haslam, 'Experiencing Dehumanization: Cognitive and Emotional Effects of  Everyday Dehumanization' , Basic and Applied Social Psychology , 2011.   Bateman, Tom, ' Portugal makes it illegal for your boss to text you after work in ' game changer ' remote  work law ', euronews , 11 November 2021.   Bayamlıoğlu, Emre, ' The right to contest automated decisions under the General Data Protection  Regulation: Beyond the so -called 'right to explanation '', Regulation & Governance , 2021.   Bednarowicz, Bartłomiej, 'Delivering on the European Pillar of Social Rights: The New Directive on  Transparent and Predictable Working Conditions in the European Union' , Industrial Law Journal , 2019.   Belloc, Filippo, Burdin, Gabriel and Fabio Landini, Robots and Worker Voice: An Empirical Exploratio n,  Institute for Labor Economics, 2020.   Black, J. Stewart and Patrick van Esch, ' AI-enabled recruiting: What is it and how should a manager use  it?', Business Horizons , 2020.   Berg, Janine, 'Protecting Workers in the Digital Age: Technology, Outsourcing, and the Growing  Precariousness of Work ', Comparative Labor Law & Policy Journal , 2019.   Bergholm, Jenny, ' The GDPR and the Artificial Intelligence Regulation – it takes two to tango? ', KU Leuven  Centre for IT & IP Law , 6 July 2021.   Bergstein, Brian, ' AI Still Gets Confused About How The World Works ', MIT Technology Review , 2020.   Bernhardt, Annette, Kresge, Lisa and Reem Suleiman, Data and Algorithms at Work: The Case for Worker  Technology Rights , UC Berkeley Labor Center, 2021.   Berto lini, Andrea, Artificial Intelligence and Civil Liability , European Parliament, 2020.  
AI and digital tools in workplace management and evaluation     73 Bhattacharya, Sukanto, Wang, Yonggui and Dongming Xu, 'Beyond Simon 's Means -Ends Analysis:  Natural Creativity and the Unanswered ' Why ' in the Design of Intelligent System s for Problem -Solving ',  Minds and machines , 2010.   Biber, Sümeyye Elif, ' Machines Learning the Rule of Law: EU Proposes the World 's first Artificial  Intelligence Act ', Verfassungsblog , 13 July 2021.   Bodie, Matthew T., Cherry, Miriam A., McCormick, Marcia L. and Jintong Tang, ' The Law and Policy of  People Analytics ', University of Colorado Law Review , 2017.   Bogen, Miranda and Aaron Rieke, Help Wanted: An Examination of Hiring Algorithms, Equity, and Bi as,  Upturn, 2018.   Bohnet, Iris, What Works: Gender Equality by Design , Harvard University  Press, 2016.   Borgesius, Frederik Zuiderveen, Discrimination, Artificial Intelligence, and Algorithmic Decision- Making ,  Council of Europe, 2018.   Borgesius, Frederik J.  Zuiderveen, 'Strengthening legal protection against discrimination by algorithms  and artificial intelligence ', The International Journal of Human Rights , 2020.   Boucher, Philip, Artificial intelligence: How does it work, why does it matter, and what can we  do about  it?, Scientific Foresight Unit, European Parliament, 2020.   Boxall, Peter, John Purcell and Patrick M. Wright, ' Human Resource Management: Scope, Analysis, and  Significance ' in The Oxford Handbook of Human Resource Management , Oxford University Press, 2008.   Bronowicka, Joanna, Ivanova, Mirela, Klicki, Wojciech, King, Seán, Kocher, Eva, Kubisa, Julia and Justyna  Zielińska, 'Game that you can 't win' ? Workplace Surveillance in Germany and Poland , European  University Viadrina, 2020.   Buckingham, Marcus and Ashley Goodall, 'Reinventing Performance Management ', Harvard Business  Review , 2015.   Buolomwini, Joy, 'When the Robot Doesn 't See Dark Skin ', The New York Times , 21 June 2018.   Burt, And rew, 'The AI Transparency Paradox ', Harvard Business Review , 2019.   Buttarelli, Giovanni, The urgent case for a new ePrivacy law , European Data Protection Supervisor, 19  October 2018.   Butterworth, Michael, 'The ICO and artificial intelligence: The role of fairness in the GDPR framework ',  Computer Law & Security Review , 2018.   Cadieux, Nathalie, Fournier, Pierre -Luc, Cadieux , Jean  and Martine Gingues, 'New Techno -Stressors  Among Knowledge Professionals: The Contribution of Artificial Intelligence and Websites  that Misinform  Clients ', International Journal of Electronic Commerce , 2021.   Caine, Mark and Kay Firth -Butterfield, 'How AI can train workers for the jobs of the future ', World  Economic Forum , 22 October  2020.   Calvetti, Diego, Pedro Mêda, Miguel Chichorro  Gonçalves and Hipólito Sousa, 'Worker 4.0: The Future of  Sensored Construction Sites ', Buildings , 2020.   Candelon, François, Charme di Carlo, Rodolphe, De Bondt, Midas and Theodoros Evgeniou, ' ai Regulation  Is Coming ', Harvard Business Review , 2021.   Cappel li, Peter, ' Your Approach to Hiring Is All Wrong: Outsourcing and algorithms won' t get you the  people you need. ', Harvard Business Review , 2019.   Cappelli, Peter and Anna Tavis, ' HR Goes Agile ', Harvard Business Review , 2018.   Casilli, Antonio A., En attendant les robots  : Enquête sur le travail du clic , Seuil, 2019.   CDEI, Review into bias in algorithmic decision- making , Centre for Data Ethics and Innovation, 2020.   Cefaliello, Aude, Psychosocial risks in Europe: National examples as insp iration for a future directive ,  European Trade Union Institute,  2021.   Chamorro -Premuzic, Tomas, ' Can Surveillance AI Make the Workplace Safe?' , MIT Sloan Management  Review , 2020.  
STOA  | Panel for the Future of Science and Technology     74 Chamorro -Premuzic, Tomas and Ian Bailie, ' Tech Is Transforming People Analytics. Is That a Good Thing? ',  Harvard Business Review , 2020.   Chung, Doug J., Huber, Isabel, Murthy, Vinay, Sunku, Varun and Marije Weber, 'Setting Better Sales Goals  with Analytics ', Harvard Business Review , 2019.   Chung, Doug J., Kim, Byungyeon and Niladri B. Syam, A Practical Approach to Sales Compensation:  What  Do We Know Now? What Should We Know in the Future? , Harvard Business School, 2020.   CIPD and PA Consulting, People and machines: from hype to reality , Chartered Institute of Personnel and  Development , 2019.   Cirillo, Valeria, Matteo Rinaldini, Jacopo Staccioli  and Maria Enrica Virgillito, ' Technology vs. workers: the  case of Italy 's Industry 4.0 factories ', Structural Change and Economic Dynamics , 2021.   CNIL, Délibération de la formation restreinte n°SAN -2021- 019 du 29 octobre 2021 concernant la Régie  autonome des transports parisiens , Commission Nationale de l 'Informatique et des Libertés, 2021.   Cobbe, Jennifer and Jatinder Singh, ' Artificial intelligence as a service: Legal responsibilities, liabilities,  and policy challenges ', Computer Law & Security Review , 2021.   Colclough, Christina, 'When Algorithms Hire and Fire ', International Union Rights , 2018.   Colclough, Christina, 'Workers ' rights: negotiating and co -governing digital systems at work ', Social  Europe , 3 September 2020.   Cottigny, Jean Louis, The explanatory memorandum to the report on the implementation of Directive  2002/14/EC establishing a general framework for informing and consulting employees in the European  Community (2008/2246(INI)) , European Parliament, 2009.   Coun touris, Nicola and Valerio De Stefano, ' The 'long Covid ' of work relations and the future of remote  work ', Social Europe , 14 April 2021.   Countouris, Nicola, Valerio De Stefano and Ioannis Lianos, ' The EU, Competition and Workers ' Rights ',  Centre for Law, E conomics and Society Research Paper Series  2/2021.   Crafts, Nicholas, ' Artificial intelligence as a general- purpose technology: an historical perspective ', Oxford  Review of Economic Policy , 2021.   Crawford, Kate, Dobbe, Roel, Dryer, Theodora, Fried, Genevieve, Green, Ben, Kaziunas, Elizabeth, Kak,  Amba, Mathur, Varoon, McElroy, Erin, Sánchez, Andrea Nill, Raji, Deborah, Rankin, Joy Lisi, Richardson, Rashida, Schultz, Jason, West, Sarah Myers and Meredith Whittaker.  AI Now 2019 Report , AI Now Institut e,  2019.   Czarnocki, Jan, ' Electronic Communication Confidentiality in ePrivacy Regulation: How to Protect Privacy  and Personal Autonomy Without Hampering AI Solutions Development ', Studia Iuridica , 2020.   Darrah, Dan, ' How Customer Service Surveys Are Erodi ng Workers ' Rights ', Jacobin , 20 April 2021.   Dattner, Ben, Chamorro -Premuzic, Tomas, Buchband, Richard and Lucinda Schettler, 'The Legal and  Ethical Implications of Using AI in Hiring ', Harvard Business Review , 2019.   De Fine Licht, Karl and Jenny de Fine L icht, 'Artificial intelligence, transparency, and public  decision‑ making: Why explanations are key when trying to produce perceived legitimacy ', AI & Society ,  2020.   Degli -Esposti, Sara and Ester Mocholí Ferrándiz, ' After the GDPR: Cybersecurity is the Elep hant in the  Artificial Intelligence Room ', European Business Law Review , 2021.   Delfanti, Alessandro, Radovac Lilian and Taylor Walker, The Amazon Panopticon: A Guide for Workers,  Organizers & Policymakers , UNI Global, 2021.   Dellot, Benedict and Fabian Wallace -Stephens, The Age o f Automation: Artificial intelligence, robotics  and the future of low -skilled work , RSA Future Work Centre, 2017.   De Matos Pinto, Inês, ' The draft AI Act: a success story of strengthening Parliament' s right of legislative  initiative? ', ERA Forum , 2021.   Deshpande, Advait, Picken, Natalie, Kunertova, Linda, De Silva, Annemari, Lanfredi, Giuli and Joanna  Hofman, Improving  working conditions using Artificial Intelligence , European Parliament, 2021.  
AI and digital tools in workplace management and evaluation     75 De Stefano, Valerio, 'The Rise of the Just -in-Time Workforce: On -Demand Work, Crowdwork, and Labor  Protection in the Gig -Economy ', Comparative Labor Law & Policy Journal , 2016.   De Stefano, Valerio, ''Negotiating the Algorithm ': Automation, Artificial Intelligence, and Labor  Protection ', Comparative Labor Law & Policy Journal , 2019.   De Stefano, Valerio, '' Masters and Servers ': Collective Labour Rights and Private Government in the  Contemporary World of Work ', International Journal of Comparative Labour Law and Industrial Relations ,  2020.   De Stefano, Valerio, 'The EU Proposed Regulation on AI: a threat to labour protection? ', Regulating for  Globalization , 17 April 2 021a.  De Stefano, Valerio, 'The experts ' perspective: Valerio De Stefano ', Presentation at the European Agency  for Safety and Health at Work, 2021b.   De Stefano, Valerio and Antonio Aloisi, ' European Commission takes the lead in regulating platform  work ', Social Europe , 9 December 2021.   De Stefano, Valerio and Simon Taes, Algorithmic management and collective bargaining , European  Trade Union Institute, 2021.   Devillé, Rembrandt, Nico Sergeyssels and Catherine Middag, 'Basic Concepts of AI for Legal Scholars ' in  Artificial Intelligence and the Law , Intersentia, 2021.   Dietvorst, Berkeley J., Simmons, Joseph P. and Cade Massey, ' Overcoming Algorithm Aversion: People  Will Use Imperfect Algorithms If They Can (Even Slightly) Modify Them ', Management Science , 2018.   Drasgow, Fritz and Julie B. Olson -Buchanan, 'Technology -Driven Developments in Psychometrics ' in Next  Generation Technology -Enhanced Assessment: Global Perspectives on Occupational and Workplace  Testing , Cambridge University Press, 2017.   Ebers, Marti n, Hoch, Veronica R. S., Rosenkranz, Frank, Ruschemeier, Hannah and Björn Steinrötter, 'The  European Commission' s Proposal for an Artificial Intelligence Act—A Critical Assessment by Members of  the Robotics and AI Law Society (RAILS) ', J, 2021.   Ebert, Isab el, Wildhaber, Isabelle and Jeremias Adams -Prassl, 'Big Data in the workplace: Privacy Due  Diligence as a human rights -based approach to employee privacy protection ', Big Data & Society , 2021.   EDPB, Guidelines 4/2019 on Article 25 Data Protection by Design and by Default Version 2.0 , European  Data Protection Board, 2020.   EDPB and EDPS, Joint Opinion 5/2021 on the proposal for a Regulation of the European Parliament and  of the Council laying down harmonised rules on artificial intelligence (Artificial Intell igence Act) ,  European Data Protection Board, 2021.   Edwards, Lilian and Michael Veale, 'Slave to the Algorithm? Why a 'Right to an Explanation ' Is Probably  Not the Remedy You Are Looking For ', Duke Law & Technology Review , 2017.   Edwards, Lilian, Martin, Laura and Tristan Henderson, 'Employee Surveillance: The Road to Surveillance is  Paved with Good Intention ', SSRN , 31 August 2018.   EESC, Opinion of the European Economic and Social Committee on ' Artificial intelligence –  The  conseq uences of artificial intelligence on the (digital) single market, production, consumption,  employment and society , European Economic and Social Committee, 2017.   EESC , Opinion of the European Economic and Social Committee on ' Artificial intelligence: anticipating its  impact on work to ensure a fair transition ', European Economic and Social Committee, 2018.   Ekkehard, Ernst, Rossana Merola and Daniel Samaan, The economics of artificial intelligence: Implications  for the future of work , International Labour Organization, 2018.   Engler, Alex, Auditing employment algorithms for discrimination , The Brookings Institution, 2021.   Estlund, Cynthia, Automation Anxiety: Why and How to Save Work , OUP, 2021.   ETUC, Commission' s proposal for a regulation on Artificial Intelligence fails to address the workplace  dimension , European Trade Union Confederation, 28 Mai 2021.  
STOA  | Panel for the Future of Science and Technology     76 EU-OSHA, New forms of worker management based on Artificial Intelligence: definitions, mapping of  uses, and overview of policies, strategies and initiatives , European Agency for Safety and Health at Work,  forthcoming.   Eurofound, Employee monitoring and surveillance: The challenges of digitalisation , Publications Office  of the European Union, 2020.   Eurofound, Right to disconnect: Exploring company practices , Publications Office of the European Union,  2021.   European Commission, Report on the safety and liability implications of Artificial Intelligence, the  Internet of Things and robotics , European Union, 2020 a.  European Commission, White Paper on Artificial Intelligence –  A European approach to excellence and  trust , European Union, 2020b.   European Commission, Second -phase consultation of social partners under Article 154 TFEU on possible  action addressing the challenges related to working conditions in platform work , European Union, 2021.   Evans, Leighton and Rob Kitchin, 'A smart place to work? Big data systems, labour, control and modern  retail stores ', New Technology, Work and Employment , 2018.   Executive Office of the President, Big Data: A Report on Algorithmic Systems, Opportunity, and Civil  Rights , The White House, 2016.   Faraj, Samer, Pachidi, Stella and Karla Sayegh, ' Working and organizing in the age of the learning  algorithm ', Information a nd Organization , 2018.   Floridi, Luciano, ' Introduction –The Importance of an Ethics -First Approach to the Development of AI ' in  Ethics, Governance, and Policies in Artificial Intelligence , Springer, 2021.   Floridi, Luciano and Josh Cowls, 'A Unified Framewo rk of Five Principles for AI in Society ' in Ethics,  Governance, and Policies in Artificial Intelligence , Springer, 2021.   FRA, Getting the Future Right: Artificial Intelligence and Fundamental Rights , European Union Agency  for Fundamental Rights, 2020.   Fron tier Economics, The Impact of Artificial Intelligence on Work: An evidence review prepared for the  Royal Society and the British Academy , Frontier Economics, 2018.   Fuller, Joseph B., Raman, Manjari, Sage -Gavin, Eva and Hines, Kristen, Hidden Workers: Untap ped Talent ,  Harvard Business School Project on Managing the Future of Work and Accenture , 2021.   Furlough, Caleb, Stokes , Thomas  and Douglas J. Gillan, ' Attributing Blame to Robots: I. The Influence of  Robot Autonomy ', Human Factors , 2019.   FLI, FLI Position Paper on the EU AI Act , Future of Life Institute, 2021.   Gal, Uri, Jensen Tina B. and Mari- Klara Stein, ' People Analytics in the Age of Big Data: An Agenda for IS  Research ', Proceedings of the International Conference on Information Systems , 2017.   Garcia -Arroyo, José and Amparo Osca, ' Big data contributions to human resource management: a  systematic review ', The International Journal of Human Resource Management , 2019.   Gaudio, Giovanni, ' Algorithmic Bosses Can 't Lie! How to Foster Transparency and L imit Abuses of the New  Algorithmic Managers ', Comparative Labor Law & Policy Journal , forthcoming.   Gavaghan, Colin, Knott, Alistair and James Maclaurin, The Impact of Artificial Intelligence on Jobs and  Work in New Zealand , University of Otago, 2021.   Gierm indl, Lisa Marie, Strich, Franz, Christ, Oliver, Leicht -Deobald, Ulrich and Abdullah Redzepi, ' The dark  sides of people analytics: reviewing the perils for organisations and employees ', European Journal of  Information Systems , 2021.   Gilbert, Abigail, Thomas, Anna, Pissarides, Christopher, Al -Izzi, Hana, Miller, Catherine and Emma Burnell,  The Amazonian  Era How algorithmic systems are eroding good work , Institute for the Future of Work,  2021.   Gonfalonieri, Alexandre, ' What Brain -Computer Interfaces Could Mean for the Future of Work ', Harvard  Business Review , 2020.  
AI and digital tools in workplace management and evaluation     77 Gonzalez, Manuel F., Capman, John F., Os wald, Frederick L., Theys, Evan R. and David L. Tomczak,  ''Where 's the I -O?' Artificial Intelligence and Machine Learning in Talent Management Systems Talent  Management Systems ', Personnel Asses sment and Decisions , 2019.   GPDP, 'Riders: Italian SA says no t o algorithms causing discrimination –  A platform in the Glovo group  fined EUR 2.6 million' , Garante per la Protezione dei D ati Personali, 5 July 2021.   Grant, Adam, ' The Surprising Value of Obvious Insights ', MIT Sloan Management Review , 2019.   Guszcza, James and Jeff Schwartz, ' Superminds, not substitutes: Designing human- machine  collaboration for a better future of work ', Deloitte Review , 2020.   Hagen, Christina S., Bighash, Leila, Hollingshead, Andrea B., Shaikh, Sonia Jawaid and Kristen S. Ale xander,  'Why are you watching? Video surveillance in organizations ', Corporate Communications , 2018.   Hamburg Commissioner for Data Protection and Freedom of Information, Hamburg Commissioner Fines  H&M 35.3 Million Euro for Data Protection Violations in Ser vice Centre , European Data Protection Board,  2 October 2020.   Hanley, Daniel A. and Sally Hubbard, Eyes Everywhere: Amazon 's Surveillance Infrastructure and  Revitalizing Worker Power , Open Markets, 2020.   Hartwell, Christopher J. and Regan Eggli, ' Social Media Screening in Employee Selection'  in Encyclopedia  of Electronic HRM , De Gruyter, 2020.   HBS Digital Native, ' Ifeoma Ajunwa on the limitless boundaries of employee surveillance ', Harvard  Business School,  8 February 2021.   Heaven, Will Douglas, 'This startup is using AI to give workers a ' productivity score '', MIT Technology  Review , 2020.   Heaven, Will Douglas, ' A Mind of Its Own: If We Build Consciousness, What Will it Be Like –And How Will  We Know? ', MIT Technology Review , 2021.   Heilweil, Rebecca, 'Artificial intelligence will help determine if you get your next job ', Vox/Recode , 12  December 2019.   Hendrickx, Frank, ' From Digits to Robots: The Privacy -Autonomy Nexus in New Labor Law Machinery ',  Comparative Labor Law & Policy Journal , 2019a.   Hendrickx, Frank, ' Privacy 4.0 at Work: Regulating Employment, Technology and Automation ',  Comparative Labor Law & Policy Journal , 2019b.   Hendrickx, Frank , et al. , Protection of workers ' personal data: G eneral principles , International Labour  Organi zation,  forthcoming .  High- Level Expert Group on Artificial Intelligence, Ethics Guidelines for Trustworthy AI , European  Commission, 2019 a.  High- Level Expert Group on Artificial Intelligence, Policy and Investment Recommendations for  Trustworthy AI , European Commission, 2019b.   Hmoud, Bilal and Varallyai Laszlo, ' Will Artificial Intelligence Take Over Human Resources Recruitment  and Selection? ', Network Intelligence Studies , 2019.   Hoeyer, Klaus and Sarah Wadmann, ''Meaningless work ': How the datafication of health reconfigures  knowledge about work and erodes professional judgement ', Economy and Society , 2020.   Hoffman, Mia and Laura Nurski, ' Work ers can unlock the artificial intelligence revolution ', Bruegel Blog ,  30 June 2021.   Howard, Erica, ' Headscarf -wearing employees and the CJEU: what employers can and cannot do ', ERA  Forum , 2021.   Huselid, Mark A., ' The science and practice of workforce analytics: Introduction to the HRM special issue ',  Human Resource Management , 2018.   ICO and The Alan Turing Institute, Explaining decisions made with AI , Information Commissioner 's Office,  2020.   ILO, World Employment and Social Outlook: The role of digital labour platforms in transforming the world  of work , International Labour Organization, 2021.  
STOA  | Panel for the Future of Science and Technology     78 ILO Global Commission on the Future of Work, Work for a brighter future , International Labour  Organization, 2019.   Imana, Basile al, Korolova, Aleksandra and John Heidemann, Auditing for discrimination in Algorithms  Delivering Job Ads , University of Southern California, 2021.   Janssen, Ronald, A business case for social dialogue: How workplace representation and collective   bargaining  deliver better business performance , Global Deal, 2021.   Jeske, Debora and Thomas Calvard, ' Big data: lessons for employers and employees ', Employee Relations ,  2020.   Johnson, Richard D., Stone, Dianna L. and Kimberly M. Lukazewski, ' The benefits of eHRM and AI for talent  acquisition' , Journal of Tourism Futures , 2021.   Kahneman, Daniel, Andrew M. Rosenfield, Linnea Gandhi and Tom Blaser, ' Noise: Inconsistent Decision  Making is a Huge Hidden Cost for Many Companies. ', Harvard Business Review , 2016.   Kahneman, Daniel, Olivier , Sibony and Cass R. Sunstein, Noise: A Flaw in Human Judgment , Brown Spark,  2021.   Kaminski, Margot E. and Gianclaudio M algieri, 'Algorithmic impact assessments under the GDPR:  producing multi- layered explanations ', International Data Privacy Law , 2021.   Kaminski, Margot E. and Jennifer M. Urban, 'The Right to Contest AI ', Columbia Law Review , 2021.   Kane, Gerald C., ''People Analytics ' Through Super -Charged ID Badges ', MIT Sloan Management Review ,  2015.   Kantrowitz, Tracy M. and Sara L. Gutierrez, ' The Changing Landscape of Technology -Enhanced Test  Administration'  in Next Generation Technology -Enhanced Assessment: Global Perspectives on  Occupational and Workplace Testing , Cambridge University Press, 2017.   Kaplan, Andreas and Michael Haenlein, 'Siri, Siri, in my hand: Who 's the fairest in the land? On the  interpretations, illustrations, and implications of artificial intelligence ', Business Horizons , 2019.   Kellogg, Katherine C., Valentine, Melissa A. and Angèle Christin, ' Algorithms at Work: The New Contested  Terrain of Control ', Academy of Management Annals , 2020.   Kelly -Lyth, Aislinn, ' The AI Act and Algorithmic  Management ', Comparative Labor Law & Policy Journal  Dispatches , 2021a.   Kelly -Lyth, Aislinn, ' Challenging Biased Hiring Algorithms ', Oxford Journal of Legal Studies , 2021b.   Khakurel, Jayden, Helinä Melkas and Jari Porras, ' Tapping into the wearable device revolution in the  workenvironment: a systematic review ', Information Technology & People , 2018.   Khan, Shaji A. and Jinton Tang, 'The Paradox of Human Resource Analytics: Being Mindful of Employees ',  Journal of general management , 2016.   Kim, Max  S., 'This company delivers packages faster than Amazon, but workers pay the price ', MIT  Technology Review , 9 June 2021.   Kim, Tae Wan and Kevin Werbach, ' More than just a game: ethical issues in gamification' , Ethics and  information technology , 2016.   Kim, Pauline T.,  'Big Data and Artificial Intelligence: New Challenges for Workplace Equality ', University of  Louisville Law Review , 2019.   Kim, Pauline T. and Sharion Scott, ' Discrimination in Online Employment Recruiting ', Saint Louis  University Law Journal , 2019.   Kiron,  David and Barbara Spindel, ' Rebooting Work for a Digital Era: How IBM Reimagined Talent and  Performance Management ', MIT Sloan Management Review , 2019.   Knight, Will, 'Google just gave control over data center cooling to an AI ', MIT Technology Review , 2018 a.  Knight, Will, 'AI is not 'magic dust ' for your company, says Google 's Cloud AI boss' , MIT Technology  Review , 2018b.   Köchling, Alina and Marius Claus Wehner, 'Discriminated by an algorithm: a systematic review of  discrimination and fairness by algorithmic decision- making in the context of HR recruitment and HR  development ', Business Research , 2020.  
AI and digital tools in workplace management and evaluation     79 Kop, Mauritz, EU Artificial Intelligence Act: The European Approach to AI , AIRecht, 2021.   Kresge, Lisa, Data and Algorithms in the Workplace: A Primer on N ew Technologies , UC Berkeley Labor  Center, 2020.   Kryscynski, David, Reeves, Cody, Stice -Lusvardi, Ryan, Ulrich, Michael and Grant Russell, ' Analytical  abilities and the performance of HR professionals ', Human Resource Management , 2018.   Kullmann, Miriam, ' Platform Work, Algorithmic Decision- Making, and EU Gender Equality Law ',  International Journal of Comparative Labour Law and Industrial Relations , 2018.   Kullmann, Miriam, ' Discriminating job applicants through algorithmic decision- making ', Ars Aequi , 2019.   Kullmann, Miriam and Aude Cefaliello, ' The Interconnection between the AI Act and the EU 's  Occupational Safety and Health Legal Framework ', Global Workplace Law & Policy Blog , 24 January 2022.   Lane, Marguerita and Anne Saint -Martin, The impact of Artificial Intelligence on the labour market: What  do we know so far?, Directorate for Employment, Labour and Social Affairs , OECD, 2021.   Ledet, Elizabeth, McNulty, Keith, Morales, Daniel and Marissa Sh andell, How to be great at people  analytics , McKinsey, 2020.   Lehr, David and Paul Ohm, 'Playing with the Data: What Legal Scholars Should Learn about Machine  Learning ', U.C. Davis Law Review , 2017.   Leicht -Deobald, Ulrich, Busch, Thorsten, Schank, Christoph, Weibel, Antoinette, Schafheitle, Simon,  Wildhaber, Isabelle and Gabriel Kasper, 'The Challenges of Algorithm -Based HR Decision -Making for  Personal Integrity ', Journal of business ethics , 2019.   Lenaerts, Koen, ' The Role of the EU Charter in the Member Sta tes' in The EU Charter of Fundamental  Rights in the Member States , Hart Publishing, 2020.   Leonardi, Paul and Noshir Contractor, 'Better People Analytics: Measure Who They Know, Not Just Who  They Are. ', Harvard Business Review , 2018.   Leonardi, Paul M., 'COVID -19 and the New Technologies of Organizing: Digital Exhaust, Digital Footprints,  and Artificial Intelligence in the Wake of Rem ote Work ', Journal of Management Studies , 2021.   Levy, Karen E. C., ' The Contexts of Control: Information, Power, and Truck -Driving Work ', The Information  Society , 2015.   Littman, Michael L., Ajunwa, Ifeoma, Berger, Guy, Boutilier, Craig, Currie, Morgan, Dos hi-Velez, Finale,  Hadfield, Gillian, Horowitz, Michael C., Isbell, Charles, Kitano, Hiroaki, Levy, Karen, Lyons, Terah, Mitchell,  Melanie, Shah, Julie, Sloman, Steven, Vallor, Shannon and Toby Walsh, Gathering Strength, Gathering  Storms: The One Hundred Ye ar Study on Artificial Intelligence (AI100) 2021 Study Panel Report , Stanford  University, 2021.   Loi, Michele, People Analytics must benefit the people. An ethical analysis of data- driven algorithmic  systems in human resources management , AlgorithmWatch, 20 20.  Lund, Susan, Madgavkar, Anu, Manyika, James, Smit, Sven, Ellingrud, Kweilin, Meaney, Mary and Olivia Robinson,  The future of work after COVID -19, McKinsey, 2021.   Kreutzer, Ralf T. and Marie Sirrenberg, Understanding Artificial Intelligence: Fundamentals, Use Cases and  Methods for a Corporate AI Journey , Springer, 2020.   Malone, Thomas W., Rus , Daniela  and Robert Laubacher, Artificial Intelligence and the Future of Work ,  MIT Task Force on the Work of the Future, 2020.   Maltseva, Kateryna, 'Wearables in the workplace: The brave new world of employee engagement ',  Business Horizons , 2020.   Manheim, Karl and Lyric Kaplan, ' Artificial Intelligence: Risks to Privacy and Democracy ', Yale J ournal of  Law & Technology , 2019.   Manokha, Ivan, 'Surveillance,  Panopticism,  and Self-Discipline in the Digital Age ', Survaillance & Society ,  2018.   Manyika, James, Silberg, Jake and Brittany Presten, 'What Do We Do About the Biases in AI? ', Harvard  Business Review , 2019.  
STOA  | Panel for the Future of Science and Technology     80 Maree, Mohammed, Kmail, Aseel B. and  Mohammed Belkhatir, 'Analysis and shortcomings of e recruitment systems: Towards a semantics -based approach addressing knowledge incompleteness and  limited domain coverage ', Journal of Information Science , 2019.   Martens, Bertin and Songül Tolan, Will this time be different? A review of the literature on the Impact of  Artificial Intelligence on Employment, Incomes and Growth , Joint Research Centre, European  Commission, 2018.   Massey, Cade, ' How You Can Have More Imp act as a People Analyst' , MIT Sloan Management Review ,  2019.   Mateescu, Alexandra, Electronic Visit Verification: The Weight of Surveillance and the Fracturing of Care ,  Data & Society, 2021.   Matthews, Jeanna, ' Patterns and Antipatterns, Principles, and Pitf alls: Accountability and Transparency  in Artificial Intelligence ', AI Magazine , 2020.   Mayer, Anne -Sophie, Strich, Franz and Marina Fiedler, ' Unintended Consequences of Introducing AI  Systems for Decision Making ', MIS quarterly executive , 2020.   McAfee, Andr ew and Erik Brynjolfsson, ' Big Data: The Management Revolution ', Harvard Business Review ,  2012.   Mehta, Arshia and Frank Levy, Warehousing, Trucking, and Technology: The Future of Work in Logistics ,  MIT Task Force on the Work of the Future, 2020.   Minbaeva, Dana B., 'Building credible human capital analytics for organizational competitive advantage ',  Human Resource Management , 2018.   Mitrou, Lilian, Data Protection, Artificial Intelligence and Cognitive Services: Is the General Data  Protection Regula tion (GDPR) ' Artificial Intelligence -Proof '?, University of the Aegean, 2019.   Moore, Phoebe V., OSH and the Future of Work: Benefits and Risks of Artificial Intelligence Tools in  Workplaces , European Agency for Safety and Health at Work, 201 9a.  Moore, Phoe be V., 'The Mirror for (Artificial) Intelligence: In Whose Reflection' , Comparative Labor Law &  Policy Journal , 2019b .  Moore, Phoebe V., Data subjects, digital surveillance, AI and the future of work , Scientific Foresight Unit,  European Parliament, 2020.   Morelli, Neil A. and A. James Illingworth, 'The Next Wave of Internet -Based Recruitment'  in The  Cambridge Handbook of Technology and Employee Behavior , Cambridge University Press, 2019.   Nativi, Stefano and Sarah De Nigris, AI Watch: AI Standardisation Landscape –  State of play and link to  the EC proposal for an AI regulatory framework , Publications Office of the European Union, 2021.   Neff, Gina, McGrath, Maggie and Nayana Prakash, AI @ Wo rk, Oxford Internet Institute, 2020.   Nemitz, Paul, ' Constitutional democracy and technology in the age of artificial intelligence ', Philosophical  Transactions of the Royal Society of London , 2018.   Nocker, Manuela and Vania Sena, 'Big Data and Human Resourc es Management: The Rise of Talent  Analytics ', Social Sciences , 2019.   O'Connor, Sarah, 'AI at work isn' t always intelligent' , Financial Times , 26 April 2021.   OECD, Artificial Intelligence in Society , OECD Publishing, 2019.   OECD, OECD Employment Outlook 2021: Navigating the COVID -19 Crisis and Recovery , OECD Publishing,  2021.   O'Neil, Cathy , Weapons of Math Destruction , Penguin, 2017.   Pasquale, Frank, The Black Box Society , Harvard University Press, 2016.   Pasquale, Frank, New Laws of Robotics: Def ending Human Expertise in the Age of AI , Harvard University  Press , 2020.   Pato, Alexia, ' The National Adaptation of Article 80 GDPR: Towards the Effective Private Enforcement of  Collective Data Protection Rights ' in National Adaptations of the GDPR , Blogdro iteuropeen, 2019.  
AI and digital tools in workplace management and evaluation     81 Peeters, Tina, Paauwe, Jaap and Karina Van De Voorde, ' People analytics effectiveness: developing a  framework ', Journal of Organizational Effectiveness , 2020.   Pishgar, Maryam, Issa, Salah Fuad, Sietsema, Margaret, Pratap, Preethi and Hous hang Darabi, ' REDECA: A  Novel Framework to Review Artificial Intelligence and Its Applications in Occupational Safety and Health ',  International Journal of Environmental Research and Public Health , 2021.   Ponce Del Castillo, Aída, A law on robotics and arti ficial intelligence in the EU? , European Trade Union  Institute, 2017.   Ponce Del Castillo, Aída, The Digital Services Act package: Reflections on the EU Commission' s policy  options , European Trade Union Institute, 2020.   Ponce Del Castillo, Aída, The AI Regulation: entering an AI regulatory winter? Why an ad hoc directive on  AI in employment is required , European Trade Union Institute, 2021.   PwC, The Wearable Life 2.0: Connected living in a wearable world , PricewaterhouseC oopers , 2016.   Raghavan, Manish, Kleinberg, Jon, Barocas, Solon and Karen Levy, 'Mitigating Bias in Algorithmic Hiring:  Evaluating Claims and Practices ', Proceedings of the 2020 Conference on fairness, accountability, and  transparency , 2020.   Rebele, Reb, ' Can We Really Test People for Potential? ', MIT Sloan Management Review , 2019.   Ren, Yuan, 'How Alibaba tracks China 's delivery drivers ', MIT Technology Review , 27 October 2021.   Rogers, Brishen, Rethinking the Future of Work , MIT University Press, forthcoming.   Rosenblat, Alex , Uberland: How Algorithms Are Rewriting the Rules of Work , University of California Press,  2019.    Rosenblat, Alex and Luke Stark, ' Algorithmic Labor and Information Asymmetries: A Case Study of Uber 's  Drivers ', International Journal of Communication , 2016.   Rosett, Christopher M. and Austin Hagerty, Introducing HR Analytics with Machine Learning:  Empowering Practitioners, Psychologists, and Organizations , Springer, 2021.   Samek Lodovici, Manuela, Ferrari, Elena, Pa ladino, Emma, Pesce, Flavia, Frecassetti, Pietro, Aram, Eliat and  Kari Hadjivassiliou, The impact of teleworking and digital work on workers and society , European  Parliament, 2021.   Sartor, Giovanni, The impact of the General Data Protection Regulation (GDP R) on artificial intelligence ,  European Parliament, 2020.   Schellmann, Hilke and Jennifer Strong, 'Podcast: Playing the job market' , MIT Technology Review , 2021a.   Schellmann, Hilke and Jennifer Strong, ' Podcast: Beating the AI hiring machines, MIT Technolog y Review ,  2021b.   Schor, Juliet B., Attwood -Charles, William, Cansoy, Mehmet, Ladegaard, Isak and Robert Wengronowitz,  'Dependence and precarity in the platform economy ', Theory and Society , 2020.   Schoukens, Paul, 'Digitalisation and social security in the EU. The case of platform work: from work  protection to income protection? ', European Journal of Social Security , 2020.   Schrage, Michael, Kiron, David, Hancock, Bryan and Raffaele Breschi, 'Performance Management 's Digital  Shift ', MIT Sloan Management Review , 2019.   Schubert, Claudia and Marc -Thorsten Hütt, ' Economy -on-demand and the fairness of algorithms ',  European Labour Law Journal , 2019.   Schwab, Klaus, The Fourth Industrial Revolution , World Economic Forum, 2016.   Schwab, Klaus and Thierry Malleret, Covid -19: The Great Reset , Forum Publishing, 2020.   Schweyer, Allan, The Impact and Potential of Artificial Intelligence in I ncentives, Rewards, and  Recognition , Incentive Research Foundation, 2018.   Seetharaman, Deepa, Horwitz, Jeff and Justin Scheck, 'Facebook Says AI Will Clean Up the Platform. Its  Own Engineers Have Doubts. ', The Wall Street Journal , 17 October  2021.   Servoz, Michel, The Future of Work? Work of the Future! , European Commission, 2019.   Simonite, Tom, ' This Call May Be Monitored for Tone and Emotion' , Wired , 19 March 2018.  
STOA  | Panel for the Future of Science and Technology     82 Simons, Josh, Machine learning case studies , Institute for the Future of Work, 2020.   Simmonds, Paul, Brown, Neil and Maike Rentel, Evaluation of Directive 2006/42/EC on Machinery ,  technopolis group, 2017.   Smids, Jilles, Sven Nyholm and Hannah Berkers, ' Robots in the Workplace: a Threat to —or Opportunity  for—Meaningful Work? ', Philosophy & Technology , 2020.   AIDA, AIDA Working Paper on 'AI and the Labour Market' , European Parliament, 2021a .  AIDA, Draft report on artificial intelligence in a digital age (2020/2266(INI)) , European Parliament, 2021b.   Spencer, Neil H. and Ursula Huws, 'Platformis ation and the pandemic: changes in workers ' experiences  of platform work in England and Wales, 2016 -2021'  in Seven ways platform workers are fighting back ,  Trade s Union Congress , 2021.   Soper, Spencer, 'Fired by Bot at Amazon: 'It's You Against the Machine '', Bloomberg , 28 June  2021.   Spencer, David, Cole, Matt, Joyce, Simon, Whittaker, Xanthe and Mark Stuart, Digital automation and the   future of work , European Parliament, 2021.   Stancombe, Christopher, Tolido, Ron, Buvat, Jerome, Thieullent, Anne -Laur e, Khadikar, Amol, KVJ,  Subrahmanyam and Apoorva Chandna, Turning AI into concrete value: the successful implementers '  toolkit , Capgemini, 2018.   Stavroula, Leka and Jain Aditya, Interpretative Document of the Implementation of Council Directive  89/391/EEC in relation to Mental Health in the Workplace , European Commission, 2014.   Steele, Chandra, 'The Quantified Employee: How Companies Use Tech to Track Workers ', PCMag , 14  February 2020.   Supiot, Alain, Governance by Numbers: The Making of a Legal Model of Allegiance , Hart Publishing, 2017.   Susskind, Daniel, A World Without Work: Technology, Automation and How We Should Respond ,  Penguin, 2020.   Taes, Simon, ' Robotisation and Labour Law: The Dark Factory: the Dark Side of Work?'  in Artificial  Intelligence and the Law , Intersentia, 2021 .  Tambe, Prasanna, Peter Cappelli and Valery Yakubovich, ' Artificial Intelligence in Human Resources  Management: Challenges and a Path Forward ', California Management Review , 2019.   Todol í-Signes,  Adrián, 'Algorithms, artificial intelligence and automated decisions concerning workers  and the risks of discrimination: the necessary collective governance of data protection ', Transfer , 2019.   Todol í-Signes,  Adrián, 'Making algorithms safe for workers: occupational risks associated with work  managed by artificial intelligence ', Transfer , 2021a .  Todolí -Signes, Adrián, 'The experts ' perspective: Adrian Todolí ', Presentation at the European Agency for  Safety and Health at Work, 2021b.   Todolí -Signes, Adrián, 'Spanish riders law and the right to be informed about the algorithm ', European  Labour Law Journal , 2021c.   Tomczak, David L. and Tara S. Behrend, ' Electronic Surveillance and Privacy ' in The Cambridge Handbook  of Technology and Employee Behavior , Cambridge University Press, 2019.   TUC , Technology managing people: The worker experience , Trade Union Congress, 2020.   Turner, Jacob, Robot Rules: Regulating Artificial Intelligence , Palgrave Macmillan, 2019.   Twenge, Jean M., Kathleen R. Catanese and Roy F. Baumeister, 'Social exclusion and the deconstructed  state: Time perception, meaninglessness, lethargy, lack of emotion, and self -awareness ', Journal of  Personality and Social Psychology , 2003.   Uzzi, Brian, 'A Simple Tactic That Could Help Reduce Bias in AI ', Harvard Bu siness Review , 2020.   Van der Mei, Anne Pieter, 'Fixed -Term work: Recent developments in the case law of the Court of Justice  of the European Union' , European Labour Law Journal , 2020.   Van Doorn, Niels, ' Late for a job in the gig economy? Handy will dock yo ur pay ', Quartz , 3 October 2018.   Vaughn, Daly, Petersen, Nicole and Carter Gibson, 'The Use of Social Media in Staffing ' in The Cambridge  Handbook of Technology and Employee Behavior , Cambridge University Press, 2019.  
AI and digital tools in workplace management and evaluation     83 Veale, Michael and Frederik Zuiderveen  Borgesius, 'Demystifying the Draft EU Artificial Intelligence Act ',  Computer Law Review International , 2021.   Vecchione, Briana, Barocas, Solon and Karen Levy, ' Algorithmic Auditing and Social Justice: Lessons from  the History of Audit Studies , Equity and Access ' in Algorithms, Mechanisms, and Optimization , 2021.   Vial, Gregory, Jiang, Jinglu, Giannelia, Tanya and Ann- Frances Cameron, 'The Data Problem Stalling AI ',  MIT Sloan Management Review , 2021.   Volini, Enrica, Schwartz, Jeff, Roy, Indranil, Hauptmann, Maren and Yves Van Durme, Accessing talent: It 's  more than acquisition 2019 Global Human Capital Trends , Deloitte Insights, 2019.   Vrontis, Demetris, Christofi, Michael, Pereira, Vijay, Tarba, Shlomo , Makrides, Anna and Eleni Trichina,  'Artificial intelligence, robotics, advanced technologies and human resource management: a systematic  review ', The International Journal of Human Res ource Management , 2021.   Wachter, Sandra and Brent Mittelstadt, 'A Right to Reasonable Inferences: Re- Thinking Data Protection  Law in the Age of Big Data and AI ', Columbia Business Law Review , 2019.   Wachter, Sandra, Mittelstadt, Brent and Chris Russell, 'Why fairness cannot be automated: Bridging the  gap between EU non -discrimination law and AI ', Computer Law & Security Review , 2021.   Wall, Sheridan and Hilke Schellmann, ' We tested AI interview tools. Here 's what we found. ', MIT  Technology Review , 2021.   Wallace, Nick and Daniel Castro, The Impact of the EU 's New Data Protection Regulation on AI , Center for  Data Innovation, 2018.   Ward, Angela, ' The Impact of the EU Charter of Fundamental Rights on Anti -Discrimination Law: More a  Whimper than a Bang? ', Cambridge Yearbook of European Legal Studies , 2018.   WEF, Markets of T omorrow: Pathways to a New Economy , World Economic Forum, 2020.   WEF, The AI Governance Journey: Development and Opportunities , World Economic Forum, 2021.   Weidner, Nathan and Elizabeth Short, 'Playing with a purpose: The Role of Games and Gamification in  Modern Assessment Practices ' in The Cambridge Handbook of Technology and Employee Behavior ,  Cambridge University Press, 2019.   Whittall, Nicholas and Sean Hinton, ' Can we trust AI more than ourselves?' , Accenture Business Functions  Blog , 23 April 2021.   Willi ams, Joan C., Lambert, Susan J. , Kesavan, Saravanan, Fugiel, Pieter J., Ospina, Lori Ann, Rapoport, Erin  Devorah, Jarpe, Meghan, Bellisle, Dylan, Pendem, Pradeep, McCorkell, Lisa and Sarah Adler -Milstein,  Stable Scheduling Increases Productivity and Sales: The Stable Scheduling Study , Worklife Law, 2018.   Wilson, H. James and Paul R. Daugherty, ' Collaborative Intelligence: Humans and  AI Are Joining Forces ',  Harvard Business Review , 2018.   Wood, Alex J., Algorithmic Management: Consequences for Work Organisation and Working Conditions ,  European Commission, 2021.   Woodcock, Jamie, 'The algorithmic panopticon at Deliveroo: Measurement, pre carity, and the illusion of  control ', Ephemera , 2020.   Worker Info Exchange, Managed by Bots: Data -Driven Exploitation in the Gig Economy , 2021.   Xenidis, Raphaële and Linda Senden, ' EU Non -discrimination Law in the Era of Artificial Intelligence:  Mapping the Challenges of Algorithmic Discrimination'  in General Principles of EU Law and the EU Digital  Order , Kluwer, 2020.   Zarsky, Tal Z., 'Incompatible: The GDPR in the Age of Big Data ', Seton Hall Law Review , 2017.  
     This study focuses on options for regulating the use of  AI enabled and algorithmic management systems in the  world of work under EU law. The first part describes how these technologies are already being deployed, particularly in recruitment, staff appraisal, task distribution and disciplinary procedures. It discusses some near -term potential development prospects and  presents an impact assessment, highlighting some of these technologies' most significant implications.   The second part addresses the regulatory field. It  examines the different EU regulations and directives  that are already relevant to regulating the use of AI in employment. Subsequently, it analyses the potential  labour and employment implications of the European  Commission's proposal for a regulation laying down  harmonised rules on artificial intelligence (AI act).  Finally, it summaris es the other ongoing EU policy  debates relevant to the regulation of AI at work.   The third and final part of this study reflects in detail  upon the AI act and its potential impact on the existing  EU social  acquis . On this basis, it advances potential  polic y options across different EU legislative files,  including but not limited to the AI act, to ensure that regulation keeps pace with technological development. It also argues that the AI act should 'serve' and complement – rather than over -ride – other regu latory  standards that can already govern the introduction and use of AI -enabled and algorithmic -management  systems at work.     This is a publication of the Scientific Foresight Unit (STOA)   EPRS | European Parliamentary Research Service   This document is prepared for, and addressed to, the Members and staff of the European  Parliament as background material to assist them in their parliamentary work. The content of  the document is the sole responsibility of its author(s) and any opinions express ed herein should  not be taken to represent an official position of the Parliament.      ISBN  978-92-846-9458- 7  | doi: 10.2861/305539   |  QA -07-22-330- EN-N    QA-07-22-330-EN-N 
