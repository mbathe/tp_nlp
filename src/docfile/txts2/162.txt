EUROPEAN COMMISSION FOR THE  EFFICIENC Y OF JUSTICE (CEPEJ)E uropean ethical Charter  on the use of A rtificial Intelligence in judicial syst ems and their environmentA dopted at the 31st plenary meeting  of the CEPE J (Strasbourg, 3-4 December 2018)
Adopted at the 31st plenary meeting  of the CEPE J (Strasbourg, 3-4 December 2018)C ouncil of EuropeEUROPEAN COMMISSION FOR THE EFFICIENC Y OF JUSTICE (CEPEJ)E uropean Ethical Charter on the U se of Artificial Intelligence in J udicial Systems and their environment
French edition:C ommission européenne pour l’efficacité de la justic e (CEPEJ) – Charte éthique eur opéenne d’utilisation de l’intelligence ar tificielle dans les systèmes judiciair es et leur environnementT he opinions expressed in this work are the r esponsibility of the authors and do not nec essarily reflect the official policy of the C ouncil of Europe. A ll requests concerning the r eproduction or translation of all or par t of this document should be addr essed to the Directorate of C ommunication (F-67075 Strasbourg C edex or publishing@coe.int). All other c orrespondence concerning this documen t should be addressed t o European Commission for the Efficienc y of Justice (CEPEJ) c epej@coe.intC over and layout:  D ocuments and Publications P roduction Department (SPDP),  C ouncil of Europe P hoto: ShutterstockT his publication has not been  c opy-edited by the SPDP Editorial Unit t o correct typographical and g rammatical errors.© C ouncil of Europe, February 2019 P rinted at the Council of Europe 
► Page 3ContentsI NTRODUCTION 5  T HE FIVE PRINCIPLES OF THE ETHICAL CHARTER ON THE USE  O F ARTIFICIAL INTELLIGENCE IN JUDICIAL SYSTEMS AND THEIR  E NVIRONMENT 7  1. P rinciple of respect for fundamental rights: ensure that the design and implemen tation of artificial intelligence tools and services are compatible with fundamen tal rights8 2. P rinciple of non-discrimination: Specifically prevent the development or in tensification of any discrimination between individuals or groups of individuals 3. P rinciple of quality and security: With regard to the processing of judicial decisions and da ta, use certified sources and intangible data with models c onceived in a multi-disciplinary manner, in a secure technological en vironment4. P rinciple of transparency, impartiality and fairness: Make data processing methods ac cessible and understandable, authorise external audits5. P rinciple “under user control”: Preclude a prescriptive approach and ensure tha t users are informed actors and in control of their choicesA PPENDIX I – IN-DEPTH STUDY ON THE USE OF AI IN JUDICIAL SYSTEMS,  N OTABLY AI APPLICATIONS PROCESSING JUDICIAL DECISIONS AND DATA 13 1 . S tate of the use of artificial intelligence algorithms in the judicial systems  o f Council of Europe member States 1 6 2 . O verview of open data policies relating to judicial decisions in the judicial  s ystems of Council of Europe member States 1 8 3 . O perating characteristics of artificial intelligence (machine learning)  a pplied to judicial decisions 2 9 4 .   Can artificial intelligence model legal reasoning in advance? 3 5 5 . C an AIs explain judges’ behaviour in retrospect? 3 9 6 . H ow is AI to be applied in civil, commercial and administrative justice? 4 1 7 . I ssues specific to criminal justice: prevention of offences, risk of recidivism  a nd assessment of the level of danger 4 8 8 . S pecific questions relating to the protection of personal data 5 6 9 . T he potential and limitations of predictive justice tools 5 7 1 0.  The need for an in-depth public debate on these tools prior to the  i mplementation of public policies for their development. The urgent need  f or cyberethics to provide a framework for the development of artificial  i ntelligence algorithms while respecting fundamental rights 5 9 
Page 4 ► European Commission for the Efficiency of Justice (CEPEJ)APPENDIX II – WHICH USES OF AI IN EUROPEAN JUDICIAL SYSTEMS? 6 3 U ses to be encouraged 6 4 P ossible uses, requiring considerable methodological precautions 6 4 U ses to be considered following additional scientific studies 6 6 U ses to be considered with the most extreme reservations 6 6 A PPENDIX III – GLOSSARY 6 9 A PPENDIX IV – CHECKLIST FOR INTEGRATING THE CHARTER’S  P RINCIPLES INTO YOUR PROCESSING METHOD 7 6 C HECKLIST FOR EVALUATING YOUR PROCESSING METHODS 7 7 
► Page 5IntroductionA cknowledging the increasing importance of artificial intelligence1 (AI) in  our modern societies, and the expected benefits when it will be fully used  at the service of the efficiency and quality of justice, the CEPEJ for-mally  adopts the 5 fundamental principles entitled “European Ethical Charter on the use of AI in the judicial sy stems and their environment” . T he Charter is intended for public and private stakeholders responsible for the d esign and deployment of artificial intelligence tools and services that involve the d esign and deployment of artificial intelligence tools and services that involve the p rocessing of judicial decisions and data (machine learning or any other methods p rocessing of judicial decisions and data (machine learning or any other methods d eriving from data science).d eriving from data science).I t also concerns public decision-makers in charge of the legislative or regulatory I t also concerns public decision-makers in charge of the legislative or regulatory f ramework, of the development, audit or use of such tools and services. f ramework, of the development, audit or use of such tools and services. T he use of such tools and services in judicial systems seeks to improve the effi-T he use of such tools and services in judicial systems seeks to improve the effi-c iency and quality of justice, and should be encouraged. It must, however, be car-c iency and quality of justice, and should be encouraged. It must, however, be car-r ied out with responsibly, with due regard for the fundamental rights of individu-r ied out with responsibly, with due regard for the fundamental rights of individu-a ls as set forth in the European Convention on Human Rights and the Convention a ls as set forth in the European Convention on Human Rights and the Convention o n the Protection of Personal Data, and in compliance with other fundamental o n the Protection of Personal Data, and in compliance with other fundamental p rinciples set out below, which should guide the framing of public justice policies p rinciples set out below, which should guide the framing of public justice policies i n this field.i n this field.J udicial decision processing by artificial intelligence, according to their develop-J udicial decision processing by artificial intelligence, according to their develop-e rs, is likely, in civil, commercial and administrative matters, to help improve the e rs, is likely, in civil, commercial and administrative matters, to help improve the p redictability of the application of the law and consistency of court decisions, sub-p redictability of the application of the law and consistency of court decisions, sub-j ect to compliance with the principles set out below. In criminal matters, their use j ect to compliance with the principles set out below. In criminal matters, their use m ust be considered with the greatest reservations in order to prevent discrimina-m ust be considered with the greatest reservations in order to prevent discrimina-t ion based on sensitive data, in conformity with the guarantees of a fair trial. t ion based on sensitive data, in conformity with the guarantees of a fair trial. W hether designed with the aim of assisting in the provision of legal advice, help-W hether designed with the aim of assisting in the provision of legal advice, help-i ng in drafting or in the decision-making process, or advising the user, it is esseni ng in drafting or in the decision-making process, or advising the user, it is essent ial that processing is carried out with transparency, impartiality and equity, certit ial that processing is carried out with transparency, impartiality and equity, certifi ed by an external and independent expert assessment.  fi ed by an external and independent expert assessment.  1 . For the definition of artificial intelligence, see the Glossary appended. 
Page 6 ► European Commission for the Efficiency of Justice (CEPEJ)Application of the CharterT he principles of the Charter should be subject to regular application, moni-t oring and evaluation by public and private actors, with a view to continuous impr ovement of practices. I n this respect, it is desirable that a regular review of the implementation of  the principles of the Charter be made by these actors, explaining, where appr opriate, the reasons for non-implementation or partial implementation, ac companied by an action plan to introduce the necessary measures. T he independent authorities mentioned in the Charter could be responsible t o periodically assess the level of endorsement of the Charter’s principles by all  actors, and to propose improvements to adapt it to changing technolo-g ies and uses of such technologies.
► Page 7The five principles of the E thical Charter on the Use of A rtificial Intelligence in Judicial S ystems and their en vironment1 Principle of respect for fundamental rights:  ensure that the desig n and implementation of artificial intelligence tools and ser vices are compatible with fundamental rights.2 Principle of non-discrimination:  specifically prevent the  development or intensification of any discrimination bet ween individuals or groups of individuals. 3 Principle of quality and security : with regard to the process-ing  of judicial decisions and data, use certified sources and in tangible data with models elaborated in a multi-discipli-nar y manner, in a secure technological environment.4 Principle of transparency, impartiality and fairness : make da ta processing methods accessible and understandable, author ise external audits. 5 Principle “under user control” : preclude a prescriptive appr oach and ensure that users are informed actors and in c ontrol of the choices made.
Page 8 ►1Principle of respect for fundamental rights:  ensure that the desig n and implementation of artificial intelligence tools and ser vices are compatible with fundamental rights■ T he processing of judicial decisions and data must serve clear purposes, in  full compliance with the fundamental rights guaranteed by the European C onvention on Human Rights (ECHR) and the Convention on the Protection of  Personal Data (Convention for the Protection of Individuals with regard to A utomatic Processing of Personal Data, ETS No. 108 as amended by the CETS amending pr otocol No. 223). ■ W hen artificial intelligence tools are used to resolve a dispute or as a t ool to assist in judicial decision-making or to give guidance to the public, it is  essential to ensure that they do not undermine the guarantees of the right of  access to the judge and the right to a fair trial (equality of arms and respect f or the adversarial process). ■ T hey should also be used with due respect for the principles of the rule of la w and judges’ independence in their decision-making process.■ P reference should therefore be given to ethical-by-design2 or human-rights -by-design approaches. This means that right from the design and learning phases , rules prohibiting direct or indirect violations of the fundamental values pr otected by the conventions are fully integrated. 2. The ethical choice is made upstream by the program designers and is therefore not left t o the user.
► Page 92Principle of non-discrimination: specifically prevent the  development or intensification of any discrimination bet ween individuals or groups of individuals■ Giv en the ability of these processing methods to reveal existing discrim-ina tion, through grouping or classifying data relating to individuals or groups of  individuals, public and private stakeholders must ensure that the methods do  not reproduce or aggravate such discrimination and that they do not lead t o deterministic analyses or uses.■ P articular care must be taken in both the development and deployment phases , especially when the processing is directly or indirectly based on “sen-sitiv e” data. This could include alleged racial or ethnic origin, socio-economic backg round, political opinions, religious or philosophical beliefs, trade union membership , genetic data, biometric data, health-related data or data con-c erning sexual life or sexual orientation. When such discrimination has been iden tified, consideration must be given to corrective measures to limit or, if possible , neutralise these risks and as well as to awareness-raising among stakeholders .■ Ho wever, the use of machine learning and multidisciplinary scientific analy ses to combat such discrimination should be encouraged. 
Page 10 ►3Principle of quality  and security : with regard to the pro-c essing of judicial decisions and data, use certified sources and  intangible data with models conceived in a multi-disci-plinar y manner, in a secure technological environment■ D esigners of machine learning models should be able to draw widely on the  expertise of the relevant justice system professionals (judges, prosecutors, la wyers, etc.) and researchers/lecturers in the fields of law and social sciences (f or example, economists, sociologists and philosophers).■ F orming mixed project teams in short design cycles to produce functional models  is one of the organisational methods making it possible to capitalise on this multidisciplinar y approach.■ Existing ethical safeguards should be constantly shared by these project t eams and enhanced using feedback.■ Da ta based on judicial decisions that is entered into a software which implemen ts a machine learning algorithm should come from certified sources and  should not be modified until they have actually been used by the learning mechanism.  The whole process must therefore be traceable to ensure that no  modification has occurred to alter the content or meaning of the decision being pr ocessed. ■ T he models and algorithms created must also be able to be stored and  executed in secure environments, so as to ensure system integrity and in tangibility.
► Page 114Principle of transparency, impartiality and fairness:  make da ta processing methods accessible and understandable, author ise external audits■ A balance must be struck3 between the intellectual property of certain pr ocessing methods and the need for transparency (access to the design pr ocess), impartiality (absence of bias)4, fairness and intellectual integrity (pr ioritising the interests of justice) when tools are used that may have legal c onsequences or may significantly affect people’s lives. It should be made clear tha t these measures apply to the whole design and operating chain as the selec tion process and the quality and organisation of data directly influence the lear ning phase.■ T he first option is complete technical transparency (for example, open sour ce code and documentation), which is sometimes restricted by the pr otection of trade secrets. The system could also be explained in clear and familiar  language (to describe how results are produced) by communicating, f or example, the nature of the services offered, the tools that have been dev eloped, performance and the risks of error. Independent authorities or e xperts could be tasked with certifying and auditing processing methods or pr oviding advice beforehand. Public authorities could grant certification, to be r egularly reviewed. 3. Of interest in this connection is the suggestion made on page 38 of the Council of Europe’s MSI-NE T study on “Algorithms and Human Rights”: “The provision of entire algorithms or the  underlying software code to the public is an unlikely solution in this context, as private c ompanies regard their algorithm as key proprietary software that is protected. However, ther e may be a possibility of demanding that key subsets of information about the algorithms be  provided to the public, for example which variables are in use, which goals the algorithms ar e being optimised for, the training data and average values and standard deviations of the  results produced, or the amount and type of data being processed by the algorithm. ” Or ev en the suggestions appearing on page 117 of the “AI for humanity” report drafted by Mr C édric Villani, a member of the French National Assembly as part of a mission assigned to him  by the Prime Minister of the French Republic: “The auditors may be satisfied with simply check ing the fairness and equity of a programme (doing only what is required of them), by submitting  a variety of false input data, for example, or by creating a large quantity of system user  profiles according to precise guidelines. ” In addition, there are also the statements in the r eport by the House of Lords, “AI in the UK: ready, willing and able?” , paragraphs 92, 96-99.4. In this connection, it is interesting to note the solutions generally considered for ensuring the  neutrality of algorithms in the above-mentioned report by the House of Lords (para-g raphs 114, 115, 116, 119, 120): more diverse datasets, more diversity and multidisciplinary appr oaches, more auditing of aspects such as data processing and the manner in which the machine is c onstructed.
Page 12 ►5Principle “under user control”: preclude a prescriptive appr oach and ensure that users are informed actors and in c ontrol of their choices■ U ser autonomy must be increased and not restricted through the use of ar tificial intelligence tools and services.■ P rofessionals in the justice system should, at any moment, be able to r eview judicial decisions and the data used to produce a result and continue not  to be necessarily bound by it in the light of the specific features of that par ticular case.■ T he user must be informed in clear and understandable language whether or  not the solutions offered by the artificial intelligence tools are binding, of the  different options available, and that s/he has the right to legal advice and the  right to access a court. S/he must also be clearly informed of any prior pr ocessing of a case by artificial intelligence before or during a judicial process and  have the right to object, so that his/her case can be heard directly by a c ourt within the meaning of Article 6 of the ECHR. ■ G enerally speaking, when any artificial intelligence-based information sy stem is implemented there should be computer literacy programmes for users and deba tes involving professionals from the justice system. 
► Page 13Appendix II n-depth study on the use of AI in judicial sy stems, notably AI applic ations processing judicial decisions and da tapr epared by Mr Xavier Ronsin, First President of the Court of Appeal of R ennes, scientific expert (France), and  M r Vasileios Lampos, principal research fellow at the Computer Science depar tment of University College London (UCL), scientific expert (United K ingdom), and  with the contribution of Ms Agnès Maîtrepierre, judge, member of the C onsultative Committee of the Convention for the Protection of Individ-uals  with regard to Automatic Processing of Personal Data of the Council of E urope (France)T he following experts also contributed to fintune the Study:M r Francesco Contini, Senior Researcher at the Research Institute on Judicial S ystems – National Research Council (IRSIG-CNR), Bologna (Italy)M r Francesco De Santis, Professor of Human Rights Procedures, University of Naples (I taly)M r Jean Lassègue, philosopher and epistemologist, research fellow at the Cen-tr e National de Recherche Scientifique (CNRS) and associate researcher at the I nstitut des Hautes Etudes sur la Justice (IHEJ) (France)Ms  Dory Reiling, Honorary Senior Judge, Independent Expert on Information T echnology and Judicial Reform (Netherlands) M r Aleš Završnik, Chief Researcher at the Institute of Criminology, Associate  P rofessor at the Faculty of Law, University of Ljubljana (Slovenia) and EURIAS  R esearcher 2017-18 at Collegium Helveticum in Zürich (Switzerland) 
Page 14 ► European Commission for the Efficiency of Justice (CEPEJ)INTRODUCTION1. T he wave of digital transformation in our societies still has an uneven eff ect on the judicial systems of Council of Europe member States. Many E uropean countries seem to have already developed an extremely advanced appr oach to using practical applications (in terms of both technology and legal  support), while for others, this is still just an emerging issue and the f ocus is solely on effective IT management.2. A mong the technologies at work in this great digital transformation, ar tificial intelligence (AI) appears to be both the most spectacular and the most  striking. In the United States, “robot lawyers” are already at work and seem  to converse in natural language with humans. Legal tech start-ups spe-cialising  in the design of new legal services offer new applications to legal pr ofessions, mainly lawyers, legal services and insurers, allowing in-depth ac cess to judicial information and case law. These private companies even aim  to predict judges’ decisions with “predictive justice” tools, although we will see tha t this may not be the best description for them5.3. A n initial examination of this phenomenon, however, prompts us to diff erentiate between this commercial discourse and the reality of the use and  deployment of these technologies. For the time being judges in the C ouncil of Europe member States do not seem to be making any practical and  daily use of predictive software. Local tests6 and academic work7 have been  carried out to explore the potential of these applications, but they have not  yet been applied on a wide scale. The initiative for the development of these  tools comes largely from the private sector, whose clientele so far has been  made up mostly of insurance companies, lawyers and legal services w anting to reduce legal uncertainty and the unpredictability of judicial deci-sions . Nevertheless, public decision-makers are beginning to be increasingly solicit ed by a private sector wishing to see these tools – which are some-times  “beta” versions, i.e. they will evolve over time – integrated into public policies .5. See in particular the frame opening Chapter 9.6. For example, the Douai and Rennes Courts of Appeal in France conducted a three-month tr ial in 2017 with a software programme labelled “predictive” by a panel of judges.7. Work on a sample of 584 decisions of the European Court of Human Rights: Nikolaos A letras, Dimitrios Tsarapatsanis, Daniel Preoţiuc-Pietro, Vasileios Lampos, “Predicting judicial  decisions of the European Court of Human Rights: a Natural Language Processing perspec tive” , published on 24 October 2016, [Online], https://peerj.com/articles/cs-93/
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 154.I n the line of the thought process initiated in its “Guidelines on how to dr ive change towards Cyberjustice” ,8 the CEPEJ proposes to provide public decision-makers  and justice professionals with keys for a better understand-ing of the  “predictive justice” phenomenon.5. T he first task will be to clarify the questions relating to the intrinsic na ture of these mass case-law data processing systems, along with their t echnical and theoretical limitations. These aspects have not often been men tioned in the debate on this subject in the judicial sphere but they are v ery well known and discussed by specialists in these technologies (mathe-ma ticians, statisticians and computer scientists) and deserve some attention.6. S econdly, this document will analyse the benefits and risks of these t ools. While their supporters highlight their assets in terms of transparency, pr edictability and standardisation of case-law, their critics point to the lim-ita tions and the reasoning bias of the software currently on the market. The inher ent risks in these technologies may even transcend the act of judging and  affect essential functioning elements of the rule of law and judicial sys-t ems, to which the Council of Europe is particularly attached.7. T hese include principles such as the primacy of law. The effect of these t ools may be not only to provide incentives but to be almost prescriptive, cr eating a new form of normativity, which could supplement the law by r egula ting the sovereign discretion of the judge, and potentially leading, in the  long term, to a standardisation of judicial decisions based no longer on case -by-case reasoning by the courts, but on a pure statistical calculation linked t o the average compensation previously awarded by other courts.8. T here is also a need to consider whether these solutions are compatible with  the individual rights enshrined in the European Convention on Human R ights (ECHR). These include the right to a fair trial (particularly the right to a na tural judge established by law, the right to an independent and impartial tr ibunal and equality of arms in judicial proceedings) and, where insufficient car e has been taken to protect data communicated in open data, the right to r espect for private and family life.9. W hile taking these issues into consideration, the document highlights the  great potential of AI to help legal professionals in their work. There is no doubt  that some AI applications which are still under development or test-ing , such as those designed to improve legal research, could be very useful in r endering processing of the judicial workload faster and more efficient. The 8. See in particular paragraph 51 of the document CEPEJ(2016)13, Guidelines on how to drive change t owards Cyberjustice.
Page 16  ►  European Commission for the Efficiency of Justice (CEPEJ) document highlights these positive examples and generally advocates the use  of AI by legal professionals according to their needs, provided that due r egard is shown for the individual rights guaranteed by the ECHR and Coun-cil  of Europe standards, particularly in criminal matters. Far from being a simple  instrument for improving the efficiency of judicial systems, AI should str engthen the guarantees of the rule of law, together with the quality of public justic e.10. Lastly , the document suggests means of monitoring this phenome-non  in the form of an ethical charter, emphasising the need for a cautious appr oach to the integration of these tools into public policies. It is essential tha t any public debate involves all the stakeholders, whether legal profes-sionals , legal tech companies or scientists, to enable them to convey the full  scope and possible impact of the introduction of artificial intelligence applica tions in judicial systems and devise the ethical framework in which they  must operate. Subsequently, this debate could go beyond a pure “ business” framework, involving citizens themselves and hence contribut-ing  to some extent to general computer literacy, as has been achieved in C anada.91. S tate of the use of artificial intelligence algorithms in the judicial sy stems of Council of Europe member StatesI n 2018, the use of artificial intelligence algorithms in European judicial systems r emains primarily a private-sector commercial initiative aimed at insurance com-r emains primarily a private-sector commercial initiative aimed at insurance com-p anies, legal departments, lawyers and individuals.p anies, legal departments, lawyers and individuals.11. T he use of AI in the judicial field appears to be quite popular in the Unit ed States, which has invested in these tools in a fairly uncomplicated w ay, both in civil and criminal matters.1012. P inpointing instances of AI algorithm initiatives in the judicial systems of  Council of Europe member States is a more difficult task, as most of the initia tives come from the private sector and are not often integrated into public policies .13. T he question of the use of AI in judicial systems was dealt with in a s pecific online survey, launched in April 2018 for representatives of the  9 . Declaration of Montreal, available at https://www.declarationmontreal-iaresponsable. c om/demarche, accessed 16 July 2018. 1 0. See COMPAS algorithms or tools such as RAVEL LAW or ROSS chatbot 
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 17CEPEJ member States and civil society. The response level was relatively low and  did not allow clear trends to be identified. Some private operators did not  seem very receptive to this survey and the members of the CEPEJ, who belong  for the most part to ministries of justice or higher councils of justice, w ere able to quote only the tools currently used by the public sphere.14. A s a result, the inventory below is only partial and is based solely on r esearch conducted by experts and the secretariat using publicly available lit erature.1115. Classifica tions can be made according to the service offered. The in volvement of AI can vary greatly according to the applications. For illustra-tiv e purposes, the main categories are as follows:► Advanced case-law search engines ► Online dispute resolution ► Assistance in drafting deeds ► Analysis (predictive, scales)► Categorisation of contracts according to different criteria and detection of div ergent or incompatible contractual clauses► “Chatbots” to inform litigants or support them in their legal proceedings16. La tvia stated that it was exploring the possibilities of machine learning f or the administration of justice. The main purpose would be to process court sta tistics to draw up provisional estimates of human and financial resources t o be allocated.17. O ther activities carried out by legal tech companies have not been included  in this classification because they involve little or no artificial intel-ligenc e processing: some sites offer access to legal information, “cloud” solu-tions , electronic signatures, etc.18. A non-exhaustive list of legal services making use of artificial intelli-genc e in their operations is set out below:11. See summary bibliography in Appendix IV – substantial contributions from Benoît Charpentier as  well as Giuseppe Contissa and Giovanni Sartori (h ttps://media.wix.com/ugd/c21db1_14b -04c49ba7f46bf9a5d88581cbda172.pdf ) and Emmanuel Barthe (h ttp://www.precisement.org/blog/I ntelligence-artificielle-en-droit-derriere-la-hype-la-realite.html#nb14) (F rench only)
Page 18  ►  European Commission for the Efficiency of Justice (CEPEJ) SoftwareS tateT ypeD octrine.frF ranceF ranceS earch engineS earch engineP rédicticeF ranceA nalysis (except criminal cases)C ase Law AnalyticsFranceA nalysis (except criminal cases)A nalysis (except criminal cases)Jur isData Analytics (L exisNexis)FranceS earch engine, Analysis (except cr iminal cases)L uminanceU nited Kingdom AnalysisU nited Kingdom AnalysisW atson/Ross (IBM) USAA nalysisH ARTU nited Kingdom Analysis (criminal, risk of U nited Kingdom Analysis (criminal, risk of r eoffending)r eoffending)L ex Machina (L exisNexis)USAA nalysis2. O verview of open data policies relating t o judicial decisions in the judicial systems of C ouncil of Europe member StatesT he availability of data is an essential condition for the development of AI, ena-b ling it to perform certain tasks previously carried out by humans in a non-au-b ling it to perform certain tasks previously carried out by humans in a non-au-t omated manner. The more data available, the more AI is able to refine models t omated manner. The more data available, the more AI is able to refine models i mproving their predictive ability. An open data approach to judicial decisions is i mproving their predictive ability. An open data approach to judicial decisions is t herefore a prerequisite for the work of legal tech companies specialising in search t herefore a prerequisite for the work of legal tech companies specialising in search e ngines or trend analysis (“predictive justice”).e ngines or trend analysis (“predictive justice”).P rocessing of these data raises a number of issues, such as changes in the for-P rocessing of these data raises a number of issues, such as changes in the for-m ation of case-law and protection of personal data (including the names of m ation of case-law and protection of personal data (including the names of p rofessionals).p rofessionals).19. C omputer-raised data are said to be the “oil” of the 21st century as their use  and cross-referencing are producing a whole new wealth. Even though some  stakeholders and authors dispute this argument, the global successes of  the digital industry over recent decades have confirmed the enormous g rowth potential of this field of activity.2 0. The quantification of human activities, now on a global scale, could  n ot fail to touch on the data produced by the public sector. This is what has  p rompted the movement to open up public data, based on much older  i mperatives which are the founding principles of our constitutional states. 
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 1921.T he major change in recent years has been brought about by the emer-genc e of downloadable public data (open data), notably in the context of the  “Partnership for Open Government” (OGP). The OGP is a non-governmen-tal  organisation bringing together nearly 70 member States (including many of  the Council of Europe member States) with representatives of civil society and  digital giants. The aim of this openness is to improve the transparency of public  activities, encourage citizens in the development and assessment of public  policies and guarantee the integrity of public service and those who per form it by processing considerable amounts of information, organised in to databases (big data).2.1. Definition of op en data on judicial decisions22. F irst of all, let us redefine the notion of open data before dealing with the  question of the impact of allowing open data on judicial activity. Firstly, ther e is often confusion between access to information and access to data (mor e precisely, access to information in the form of database)12.23. A certain amount of public information, requiring wide publicity, is alr eady disseminated using information technology. In France, the govern-men t site Légifrance.fr is the main online source of certified public informa-tion,  comprising not only legislative and regulatory texts but also case-law and  information on appointments to public posts. This unitary information, although  available on the Internet, differs completely from direct access to da ta organised and included in a database that can be downloaded and pro-c essed by a computer.24. Open data therefore only involves the dissemination of “raw” data in struc tured computer databases. These data, aggregated in whole or in part with  other structured sources, constitute what we call big data. The Coun-cil  of Europe Convention 108 Consultative Committee defines big data as “ the growing technological ability to collect, process and extract new and pr edictive knowledge from great volume, velocity, and variety of data. In t erms of data protection, the main issues do not only concern the volume, v elocity, and variety of processed data, but also the analysis of the data using sof tware to extract new and predictive knowledge for decision-making 12. The data are meaningless letters and numbers. Information is data included in a context. I t is the context that gives meaning to the data. We can guess that 2005 is a year, but with-out  context, we do not know. But in “in 2005, we completed 3 projects” the context gives meaning  to the number. Therefore, ‘open data’ is not data in the sense of the definition, but inf ormation. Similarly, large data are also large amounts of information, not data.
Page 20 ► European Commission for the Efficiency of Justice (CEPEJ)purposes regarding individuals or groups. For the purposes of these Guide-lines , the definition of Big Data therefore encompasses both Big Data and Big Da ta analytics” .1325. A s this definition shows, open data should not be confused with their  means of processing. Some of the discourse on this issue actually r elates to processing carried out by various advanced methods which are gener ally defined as data science. Predictive justice using artificial intelli-genc e, advanced search engines applying extremely precise criteria and legal  robots are all algorithmic applications which are fed with data but have nothing t o do with the policy of open data itself.26. Ho wever, this policy must be examined in the light of the possibilities it  offers for further processing, whatever its nature. If certain data are filtered upstr eam, taking account for example, of the need for confidentiality and r espect for privacy, subsequent risks of misuse appear to be reduced.2.2. S tate of development of open data on judicial decisions in C ouncil of Europe member States and c onsequences for the development of case law27. W hat is the situation of the Council of Europe member States as regards open  data on judicial decisions? The 2016-2018 CEPEJ evaluation cycle f ocused for the first time on the question of court decisions being provided in  open data, for which some AI processing is used. The issue of data anony-misa tion or pseudonomysation within the European data protection frame-wo rk14 provided by the General Data Protection Regulation (GDPR, EU Regu-la tion 2016/679) and Council of Europe Convention No. 108 was the subject of  a specific question designed to identify the measures implemented by member S tates and observers in this particularly sensitive area.13. T-PD(2017)1, Guidelines on the protection of individuals with regard to the processing of personal da ta in a world of big data.14. General Data Protection Regulations (DGPS, EU Regulation 2016/679, and Council of Europe C onvention No 108 for the Protection of Individuals with regard to Automatic Processing of P ersonal Data).
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 21Figure 1. Table showing CoE member States having introduced open data on judicial de cisions in 2016  (Q62-4 of the evaluation scheme) 28. O f all the States and observers surveyed, only 5 declared that they had not  implemented an open data policy for judicial decisions in 2016. While this  response rate should be put into perspective, since some answers con-fused  public access to decisions with open data ( Armenia, Belgium, Bos-nia  and Herzegovina , Luxembourg, Russian Federation, Spain, Turkey), it r eveals, on the one hand, a desire for transparency on the part of European judicial  institutions and, on the other hand, a desire on the part of many c ountries to make court decisions public and thus make it possible to deal with  them later using AI tools. This also requires efforts by the institutions c oncerned, since a number of technical measures must be put in place to this end . In France specifically, some administrative case law is already available f or download on the site data.gouv.fr (see below). ISRM ARYesNo N ot a membre of CdED ata not supplied
Page 22 ► European Commission for the Efficiency of Justice (CEPEJ)29.W ith regard to the protection of personal data, 23 countries declared tha t they are pseudonymising15 at least some types of disputes (e.g. per-sonal  status, family status) by erasing data making the parties or witnesses iden tifiable (names, addresses, telephone numbers, identity numbers, bank ac count numbers, tax numbers, health status, etc.). This work appears to be the  responsibility of judicial personnel (e.g. Israel, Republic of Moldova ) or  public officials (e.g. Bosnia and Herzegovina , Spain). Bosnia and Her-z egovina and Hungary on the other hand stated to publish the names of pr ofessionals.30. Ho wever, there is a real difficulty in measuring the impact of open data on  the efficiency and quality of justice. As indicated above, the initiative to r e-use these data is essentially private, targeting a professional clientele (la wyers, legal departments), and an exclusively intergovernmental activity is pr obably not the best means of fully identifying such positive results.31. T he situation in France is representative of the questions raised by this appr oach and reveals a number of the issues at stake. First of all, it is import-an t to underline that France enacted legislation in 2016 imposing a compul-sor y framework for the open data dissemination of decisions on its courts.32. A rticles 20 and 21 of the Law for a Digital Republic16 broke with the pr evious logic17 of selecting which decisions from judicial and administra-tiv e courts and tribunals were to be disseminated if they were “of particular in terest” . Under the new French law, however, the opposite principle that ev erything is publishable has been set, except in specific cases identified by la w (for judicial decisions) and with due regard for the privacy of the per-sons  concerned. Provision is made, however, for judicial and administrative decisions  to be published only after an analysis has been made of the risk of r e-identification of the persons involved.15. As defined by the T-PD in its “Guidelines on the protection of individuals with regard to the  processing of personal data in a world of big data” T-PD(2017)1, pseudonymisation r efers to the processing of personal data “in such a manner that the personal data can no longer  be attributed to a specific data subject without the use of additional information, pr ovided that such additional information is kept separately and is subject to technical and  organisational measures to ensure that the personal data are not attributed to an iden tified or identifiable natural person. ”16. This law was adopted in order to bring French law into line with Directive 2013/37/EU of the E uropean Parliament and of the European Council of 26 June 2013, which in turn amended the  Council Directive of 17 November 2003 on the re-use of public sector information (the “PSI Dir ective”).17. Article R433-3 of the Code of judicial organisation
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 2333.T here are numerous advantages to wide dissemination, promoted in par ticular by the Court of Cassation at two conferences in October 2016 and F ebruary 2018. They include greater awareness of judicial activity and case la w trends, the increased quality of a justice system that knows it is being obser ved and the creation of a completely new factual reference base. 34. T his theoretical desire to “regulate” case-law through digital leverage r aises however a number of general questions that are relevant to all coun-tr ies considering a similar approach. Firstly, it should be placed in the context of  some of the principles set out by the European Court of Human Rights in cases  of differences in domestic case-law. The Court clearly emphasises the need  to balance legal certainty, which makes decisions more predictable, against vitalit y in judicial interpretation.1835. T his desire is also tempered by several aspects, of technical nature in first plac e:a. Collection of all judicial decisions eligible for publication is not nec-essarily  well co-ordinated between all levels of courts: some of the business  applications in European courts have not been designed for this , particularly as regards first instance decisions, and some countries will  have to set up new processes for collating judgments if they want c ollection to be exhaustive;b. Despite some promising developments,  a fully effective automated post -identification mechanism that can prevent any risk of identifi-ca tion or re-identification has not yet been devised ;A nd equally substantive aspects:c. Thought should be given to the transformation of the very logic of the  production of case-law. What is the value of the “standard” result-ing  from the number of decisions given on a specific matter? Does this “ standard” add to the law? If so, is this a new source of law?d. Should judges be asked to provide additional explanations for deci-sions  to deviate from this norm?19 This question is not insignificant and  does not prompt the same reply in all judicial systems. In French civil  law,20 “judges settle disputes in accordance with the rules of law applicable  thereto” . In the French context, if they were asked to justify their  decisions by providing all the reasons for which they deviated 18. Gr eek Catholic parish Lupeni and Others v. Romania  [GC]. No. 76943/11, 29/11/2016, § 116.19. Eloi Buat-Menard and Paolo Giambiasi, «La mémoire numérique des décisions judiciaires», Dallo z Reports, 2017, p. 1483. (French only)20. Article 12 of the French Code of Civil Procedure
Page 24 ► European Commission for the Efficiency of Justice (CEPEJ)from the supposed majority case-law trend on how to resolve the dis-put e (while complying with the relevant rules of law), would this not be  tantamount to removing them from office? Judges would not only settle  disputes in accordance with the rules of law but also in accor-danc e with case law trends derived from statistics compiled by a digital t ool (which could also be biased or developed without external control b y a private operator, see sections 6.1 and 9 below). The question that ar ises goes far beyond a specific national feature, relating more broadly t o the place of AI tools in judges’ decision-making processes. Note that this  analysis is limited to a processing of case-law by algorithms and in no  way refers to the role of case-law in the sources of law or the author-it y of case-law precedents, which, moreover, are well established prin-ciples a t European level.21e. Moreover, would it not be the case that if norms were established ac cording to the majority trend, judicial decisions would be ren-der ed uniform, and no longer be ordered according to the hierarchy of  the courts from which they emanate, disregarding the significance of  the decisions of supreme courts, which are the guarantors of the unif orm interpretation of law in many European States? What would the  relationship be between norms and case-law? Since they derive fr om the majority, would they become a criterion for these courts when det ermining their own case-law, which they would in turn have to jus-tify when they devia te from the majority view?f. Finally, isn’t there a risk of court decisions being written down ac cording to a reproductive logic? While court decisions are likely to ev olve according to the evolution of a normative framework (national, E uropean or international), the case law available for reference (for e xample, from supreme courts and European courts) or the socio-ec onomic context, would not the norm resulting from the majority bec ome a standard to which judges would be encouraged to refer without  question, with an induced effect of the excessive standardisa-tion of judicial decisions? 36. Doubts might also be raised about the consequences for users, who  are supposed to benefit directly from the transparency of activities: will  they really benefit from the publication of all judicial decisions on the 21. The ECJ has stated, on the conditions governing the liability of a Member State for the c ontent of a decision of a supreme national court, that “an infringement of Community la w will be sufficiently serious” and must give rise to compensation for the damage “where the  decision concerned was made in manifest breach of the case-law of the Court in the ma tter” (ECJ, Case C-224/01, Koebler, §56).
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 25Internet or are they not more likely to find themselves overwhelmed by the mass  of decisions, without necessarily deriving qualitative benefits from them,  for lack of legal knowledge and the critical apparatus to analyse them?37. P aradoxically enough, one may wonder of open data be in position of deliv ering meaningful information since it will certainly make it possible to do wnload a considerable set of raw data with a simple click on a link, but the meaning will r emain totally obscure for the majority of citizens. 38. T he first requirement for open data to be transparent and informative is  for third parties to be able to use them for analysis and development. An ec onomic model in which public case law data, which are the subject of work b y judicial authorities in order to make them technically “readable” by the AI  and compliant with the legal requirements for the protection of personal da ta, would be processed free of charge by the private sector and then sold b y to the courts, professionals and citizens, therefore deserves to be ques-tioned and ev en criticised.2.3. P rotection of personal data in open data policies for judicial decisions  2.3.1.  T he names of parties and witnesses39. I n order to strike a fair balance in the digital age between the need to make  judicial decisions public and respect for the fundamental rights of par-ties  or witnesses, their names and addresses must not appear in published decisions , particularly in view of the risk of misappropriation and re-use of such  personal information and the particular sensitivity of the data likely to be  contained in the decisions.22Automated processes can be used to system-a tically to conceal such information.40. O ther identifying information may also be obscured (for example, tele-phone  numbers, e-mail addresses, dates of birth, children’s given names, r are given names, nicknames and place names). In terms of personal data pr otection principles, this concealment amounts to a simple pseudonymi-sa tion of the data, not complete anonymisation. The volume and variety of inf ormation contained in court decisions, combined with the growing ease 22. ECHR, Z. v. Finland No. 22009/93, §§95 et seq. and the European Commission’s Green Paper on  public sector information in the information society (COM(1998)585) (“If special pre-cautions  are not taken, case-law databases, which are legal documentation instruments, can  become information files on individuals if these databases are consulted to obtain a list of c ourt judgments on a specific individual rather than to find out about case-law”).
Page 26 ► European Commission for the Efficiency of Justice (CEPEJ)of cross-referencing with other databases, makes it impossible, in practice, to guar antee that the person concerned cannot be re-identified. In the absence of  such a guarantee, these data cannot be qualified as anonymous and are ther efore subject to personal data protection rules.41. S ome especially sensitive items of personal data warrant particular a ttention, as provided for in Article 6 of Convention 108. This applies to data r evealing ethnic or racial origin, political opinions, trade union membership, r eligious or other beliefs, physical or mental health or sex life, which are con-sider ed intimate details. 42. C ourt decisions may contain other, very varied, types of personal data tha t fall into this category of sensitive data. Courts dealing with criminal mat-t ers are particularly likely to process sensitive data such as those on criminal pr oceedings and convictions. All this sensitive data therefore deserves spe-cial  vigilance. Their mass dissemination would present serious risks of dis-cr imination, profiling23 and violation of human dignity. 2.3.2.  T he names of professionals, including judges43. Ob viously, knowing how a judgment will be arrived at is an essential elemen t for lawyers in predicting the outcome of a case, and they believe tha t knowing one’s judge is sometimes almost as important as knowing the la w. They have long tried to make comparisons between panels of judges, mor e or less empirically, so as to give better advice to clients dealing with a par ticular judge or panel of judges.44. T his method was sufficient when a lawyer was only speaking before a limit ed number of courts, but the gradual loosening of local restrictions on the  bar in many countries and the freedom to move and work within the E uropean Union make it reasonable for any national or even European law-y er to want to know the case-law of each national or European jurisdiction in which he is likely t o plead in full detail. 45. W e cannot therefore exclude the possibility that, in the future, highly useful , and hence very expensive, machine learning applications will be much  more effective than the experience and “good sense” of litigation law-y ers working through cases in the traditional way. The use of such applica-tions  could further accentuate the distortion of competition and inequality 23. Profiling is defined in section 4 of the GDPR. It is processing an individual’s personal data in  order to analyse and predict his/her behavior or his/her situation, such as determining his/her per formance at work, financial situation, health, preferences, lifestyle habits, etc.
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 27of arms between law firms that have or have not used such “predictive” case-la w analysis software.46. T here is a real risk that, for the sake of such a competitive advantage, the  principle of a fair trial established by law will be undermined. The possi-bilit y of judge profiling through cross-referencing of public and private data c ould allow private companies and their lawyers to engage in even more f orum shopping practices. This tactic has already been observed for a long time  in the United States and in France for press offences and violations of pr ivacy in the press, where plaintiffs have already been known to choose the c ourt which appears to award the highest amounts of damages and interest.47. I n addition, much of the discourse on this subject confuses open data with  the need to publish a certain amount of public information. As a result it is  sometimes argued that the names of professionals should appear in open da ta for the sake of publicity and transparency.48. Ho wever, the provision of computerised case-law data is a totally sep-ar ate issue from the principles of publication of original or certified copies of  decisions. The objective of open data is to allow automated processing of case -law under a low-cost operating licence. As stated previously, this pro-vision  is made in the form of a comprehensive computer database, which is opaque and not dir ectly understandable to citizens.49. T his provision obviously does not meet the need to publicise the names  of professionals who have contributed to a specific decision. It should be  noted that, in law, mentioning the names of judges in judicial decisions is  a common obligation for member States, in connection with the principle of  public trial set out in Article 6(1) of the European Convention on Human R ights, in order to ensure the objective impartiality of judges (who must be iden tifiable, and lawfully appointed and assigned to the duties they perform) and c ompliance with procedural rules (e.g. publicity and collegiality).2450. T he answer to the question of the legitimacy or not of publishing the names  of professionals25 in open data therefore has nothing to do with the  obligation to publish the names of professionals in decisions. Rather, it 24. ECHR, Vernes v. France, No. 30183/06 as regards the identification of the judges who r endered the decision, Pretto v. Italy, No. 7984/77 for the publication of the judgment, K ontalexis v. Greece, No. 59000/08, § 38, DMD GROUP , a.s, v. Slovakia , No. 19334/03, § 66, M iracle Europe KFT v. Hungary , No. 57774/13, § 58 for the right to a court established by la w in conjunction with the right to an impartial judge.25. These questions may also apply to Rechtspfleger who make judicial decisions and to clerks, assistan ts to the judge mentioned in the composition of the formation of the court (albeit t o a lesser extent).
Page 28 ► European Commission for the Efficiency of Justice (CEPEJ)seems that the challenge lies in reconciling often conflicting requirements: mak ing public activities transparent by allowing citizens to know and evalu-a te their judges, on one hand, while protecting the privacy of professionals (whose  functions should not limit their fundamental guarantees in this field), on  the other hand. There are rigorous challenges where it comes to guaran-t eeing the impartiality of judges and even judicial institutions as a whole, which  open data policies are actually designed to meet26. What practical measur es can be taken to protect them from potential attempts at destabi-lisa tion which cross-reference judges’ personal data included in databases with  other sources (social networks, commercial sites) to try to identify hypo-thetical political , religious and other biases? 51. T hese questions do not arise in the same form everywhere in Europe and  depend on the specific features of the judicial system concerned (and on the  nature of the judiciary’s career management body), the collegial nature or  not of the judgment and the level of court concerned. In Switzerland, for e xample, where judges are elected, publication is a guarantee of transpar-enc y and social responsibility of judges vis-à-vis citizens and political groups. T his information is already available in online search engines (which are not str ictly speaking open data).2752. Nor do these questions arise in the same form depending on the level of  jurisdiction. The value of characterising the case-law of lower court judges ma y not be the same as for judges of supreme courts or international courts. F or example, the European Court of Human Rights authorises searches for judg ments by the names of the judges members of the decision panel, but does  not allow the calculation of statistics relating to a particular judge.28 On the  other hand, in countries where the judicial bodies are unfamiliar with the  practice of dissenting opinions (existing within this international court), it  may seem unfair to assign a judge personal responsibility for a decision which he v oted against during deliberation in a collegial court.53. T hese debates were well defined by a study mission conducted in F rance by Professor Loïc Cadiet. The conclusions of the mission remain lim-it ed since they do not recommend prohibiting publication but reserving it f or certain types of litigation and ruling it out for others (for example, for 26. See ECHR Previti v. Italy, No. 45291/06, §§ 249 et seq., which recalls the principles of objective impar tiality of the judge.27. See the example of the Swiss Federal Court, whose case-law can be downloaded: h ttps://w ww.bger.ch/fr/index/juridiction/jurisdiction-inherit-template/jurisdiction-recht.htm; or, f or the cantons: h ttp://ge.ch/justice/dans-la-jurisprudence (Canton of Geneva for example).28. The decisions of the European Court of Human Rights are collegial. Publications include an y dissenting opinions.
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 29specialised criminal matters). The possibility of publishing only the names of  supreme court judges was proposed, although it was conceded that this migh t result in a “one-way trip” . 54. A s it stands, a simple precautionary principle could be applied in order t o assess the interest, by type of litigation and degree of jurisdiction, in pub-lishing  the names of professionals in a downloadable database. Likewise, we cannot  rule out the possibility of judicial institutions themselves or autho-r ised third parties exploiting this information outside the open-data context t o find out about case-law.55.  Publication of this sort would in any event still need to be examined in  light of applicable international European rules on the protection of per-sonal  data, such as those of Convention No. 108 and the European Union’s GDPR and Dir ective 680/2016.3. O perating characteristics of artificial intelligence (machine learning) applied t o judicial decisionsN atural language processing and machine learning are the two techniques at the h eart of processing of judicial decisions using artificial intelligence.h eart of processing of judicial decisions using artificial intelligence.I n most occasions, the objective of these systems is not to reproduce legal reason-I n most occasions, the objective of these systems is not to reproduce legal reason-i ng but to identify the correlations between the different parameters of a decision i ng but to identify the correlations between the different parameters of a decision ( for example, in a divorce claim the length of marriage, the income of spouses, the ( for example, in a divorce claim the length of marriage, the income of spouses, the e xistence of adultery, the amount of the benefit pronounced, etc.) andn, through e xistence of adultery, the amount of the benefit pronounced, etc.) andn, through t he use of machine learning, to infer one or more models. Such models would t he use of machine learning, to infer one or more models. Such models would t hen be used to “predict” or “foresee” a future judicial decision.t hen be used to “predict” or “foresee” a future judicial decision.T he purpose of this chapter is to clarify the intrinsic nature of software descr ibed as “predictive” – sometimes “beta” versions, i.e. in a development phase  – both in terms of its potential and its limitations. It is intended to pro-vide  a simple explanation of machine-learning algorithms, which are at the hear t of the automated analysis of case-law.3.1. T he theoretical functionalities of “predictive justice” sof tware5 6. By way of an introduction, we should briefly review the features promi sed by “predictive” software. They propose to establish the probabilities of  t he success (or failure) of a case before a court. These probabilities are establ ished through the statistical modelling of previous decisions using methods  f rom two broad computer science domains: natural language processing  
Page 30 ► European Commission for the Efficiency of Justice (CEPEJ)and machine learning. These modelling approaches are often referred to as AI; in r eality, these are “weak” AI (see the glossary, page 57. I t should be made clear straight away that the term AI is debated by e xperts as it leads to many ambiguities. The term AI has now entered our ev eryday language to describe a diverse range of sciences and technolo-g ies that allow computers to beat the best champions in the game of Go,29t o drive a car, to converse with humans, etc. Researchers prefer to identify the  different applictions through the exact technologies underlying them, including  machine learning, and sometimes refer to all of these highly spe-cialised  AI resources as “weak” (or “moderate”) AIs. These are distinguished fr om an ultimate – still totally theoretical – goal of creating a “strong” AI, i.e. a self-lear ning machine capable of automatically comprehending the world in gener al, in all its complexity.58. I n relation specifically to justice, predictive justice systems are designed f or use by legal departments, insurers (both for their internal needs and for their  policyholders) as well as lawyers for them to anticipate the outcome of litiga tion. Theoretically, they could also assist judges in their decision-making. 59. T hey provide a graphic representation of the probability of success for each  outcome of a dispute based on criteria entered by the user (specific to each  type of dispute). These systems claim to be capable to calculate the likely amoun t of compensation distributed by the courts.3.2. T he practical functioning of artificial intelligence: sta tistical machines constructing models based on the past 60. A distinction must be made from the outset between what is a “predic-tion ” and what is a “forecast” . Prediction is the act of announcing what will happen  (pr ae, before – dictare, say) in advance of future events (by supernat-ur al inspiration, by clairvoyance or premonition). Forecasting, on the other hand , is the result of observing ( aiming, seeing) a set of data in order to envis-age  a future situation. This abuse of language and the spread thereof seems t o be explained by a transfer of the term from the “hard” sciences, where it r efers to a variety of data science techniques derived from mathematics, sta-tistics  and game theory that analyse present and past facts to make hypoth-eses about the c ontent of future events.29. https://www.nature.com/articles/nature16961
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 3161.I t should also be noted that the logic of the operation of predictive jus-tic e software is essentially based on either generative (commonly referred t o as Bayesian) or discriminative methods which eventually try to estimate the  current or future range of values of a variable (e.g. the outcome of a trial) fr om the analysis of past examples.62. G enerally speaking, it is also important to keep in mind the anthro-pomor phic notion that computing machines are intelligent and that their desig ners have managed to slip a mind inside their mechanisms.30 Unfortu-na tely, this idea still permeates many analyses of predictive justice that lend these  devices immediate or future capabilities for the near replication of human  intelligence. This context, fuelled every day by a further series of rev-olutionar y AI advances,31 therefore leads us all to approach these predictive t ools with a certain dose, conscious or not, of mysticism, sometimes stating tha t what is not entirely possible today will inevitably be possible tomorrow.63. T he “strong” AIs of science-fiction literature do not exist. This type of AI,  which would be equipped not only with intelligence but also with con-scienc e, remains purely fictional. The machine learning systems currently being  developed are described as “weak” AIs and are capable of extracting c omplex patterns and learning from large volumes of data efficiently and of ten with high levels of predictive accuracy.64. T o overcome any instinctive or fantasised considerations, these infor-ma tion processing and analysis technologies must be understood on the basis of the f ollowing three concepts.– AI is not a single, homogeneous object : it is actually an assemblage of scienc es and techniques (mathematics, statistics and computer science) capable  of processing data to design very complex computer processing tasks . – AI engines do not produce intelligence per se but proceed using an induc tive approach: the idea is to associate in a nearly automated way a set  of observations (inputs) with a set of possible results (outputs) using v arious preconfigured properties. Specifically for predictive justice, the eng ine builds links between the different lexical groups composing judicial  decisions. These groups are correlated between those identified 30. Dominique Cardon, A quoi rêvent les algorithmes, nos vies à l’heure des big data , La République des idées , Editions du Seuil, 2015, p. 58.31. “AlphaZero : l’IA de DeepMind apprend et maîtrise le jeu d’échecs en moins de 4 heures” , G énération NT, article published on 8 December 2017, [Online], h ttps://www.generation-n t.com/alphazero-ia-deepmind-maitrise-echecs-4-heures-actualite-1948919.html (page ac cessed on 14 December 2017).
Page 32 ► European Commission for the Efficiency of Justice (CEPEJ)at the input stage (facts and reasoning) and those at the output stage (the oper ative part of the decision) then classified.– The reliability of the model (or function) built strongly depends on the qualit y of the data used and the choice of machine learning technique.65. I t is worth returning briefly to the ancestors of these systems – expert sy stems – which for their part relied on processing rules written by a com-put er scientist. Expert systems (ES) developed rapidly in the late 1980s and 1990s , especially in medicine and finance.32 These systems were able to answ er specialised questions and reason using known facts, executing pre-defined  encoding rules in an engine. Despite the success of ESs, such as Deep Blue  against Garry Kasparov in a series of chess games in 1997, these systems ended  up failing, notably because they were unable to interpret “the infinite v ariety of situations and contexts”33 and became ineffective beyond 200 to 300  encoding rules, both in terms of execution performance and mainte-nanc e (the reasoning followed by the system became almost impossible to appr ehend for its designers).F ig. 2: The old expert systems were programmed with rules reproducing the lo gic of legal reasoning32. An example is High Frequency Trading (HFT), which is a type of financial transaction car-r ied out at high speed by software based on algorithms. In 2013 and in Europe, 35 % of tr ansactions were already carried out with HFT. In the USA, HFT represents 70 % of trading v olumes on the equity market. In view of the automation of transactions, the fall is even fast er in the event of crashes, as was the case during the 2007 financial crisis.33. Hubert Dreyfus, What Computers Still Can’t Do. A Critique of Artificial Reason , Cambridge, T he MIT Press, 1992 cited by Dominique Cardon in A quoi rêvent les algorithmes, nos vies à l’ heure des Big Data, p .59.Data Rules/Templates  Resu lts Experts ystem
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 3366.T oday, the idea is no longer to write reasoning rules mirroring human r easoning, as with older expert systems,34 but to let machine learning sys-t ems themselves identify existing statistical models in the data and match them t o specific results. F ig. 3: Machine learning alone produces models by automatically searching for c orrelation results67. I n doing so, these systems do not reproduce or model reasoning (such as  legal reasoning). For example, today’s online translators do not carry out  abstract reasoning. They infer a probable estimate of the best match bet ween groups of lexical structures and translations already done. Users’ ac tions obviously contribute to the improvement of the match search, but the  machine learning algorithm does not actually perform a translation by understanding the meaning of the sen tences processed.3568. T his example shows how the approach of lawyers and researchers can be  different: a lawyer will seek to understand the effectiveness of pre-exist-ing  rules, which the researcher can only provide through the language of his scienc e, and this is sometimes limited to interpreting thousands of parame-t ers from a machine learning system.69. M achine learning is a field of computer science in which computer pr ogrammes learn from experience. Algorithms make a machine perform a 34. Written processing rules could be based on specific code in different programming lan-guages  such as LISP or editors to model rules. See for example Radboud Winkels, “CLIME : Un  projet de développement de serveurs juridiques intelligents” , in Danièle Bourcier, Patricia Hasset  and Christophe Roquilly (eds.), Droit et intelligence artificielle,  2000, Romillat, p. 59.35. Li Gong, “La traduction automatique statistique, comment ça marche ?” , Interstices.info, published  on 29 October 2013, [Online], https://interstices.info/jcms/nn_72253/la-traduction-aut omatique-statistique-comment-ca-marche (page accessed on 14 December 2017).Data Rules/Templates  Resu lts Machinel earning
Page 34 ► European Commission for the Efficiency of Justice (CEPEJ)training process, like a child learning in its environment. In summary, these lear ning techniques may or may not be supervised by a human. The most pr ominent category of machine learning is currently that of reinforcement lear ning: the machine alone reaps virtual “rewards” if action produces the e xpected result in a given environment. Machine learning methods include neur al networks (or their more complex version known as deep learning), which  have been quite widely publicised in view of their autonomy and their quit e striking applications, such as the one succeeding in obtaining high sc ores on old Atari 2600 video games36 (based solely on pixel positioning and  scores). Other practical applications for these technologies are already aff ecting our daily lives and are beginning to appear in the professional w orld of justice.3770. Ho wever, these algorithms remain highly specialised in one particular task  and present discernment problems when faced with chaotic situations or  with insufficient data to allow prediction (such as the actual understand-ing  of natural language). In the social sciences, to which law and justice belong , failure would even appear inevitable in the absence of a convincing model  of cognition. For Pierre Lévy, artificial intelligence is in fact content t o provide a heterogeneous toolbox (logical rules, formal syntaxes, statisti-cal  methods, neural or socio-biological simulations, etc.) that does not offer a  general solution to the problem of mathematical modelling of human c ognition.38 Thus, real predictive learning should in reality be based on a good  systemic representation of the world, which AI researcher Yann LeCun believ es is a fundamental scientific and mathematical issue, not a question of t echnology.3971. M oreover, the uniqueness of current big data processing systems is tha t they do not attempt to reproduce our cognition model but produce c ontext statistics on an unprecedented size of data, without any real guaran-t ee of excluding false correlations.4036. https://www.nature.com/articles/nature1423637. Artificial intelligence by IBM Watson providing a service to the medical field and providing a  search tool called “Ross” , presented as a virtual lawyer – Roos, “Do more than humanly possible ” [Online], h ttp://rossintelligence.com (page ac cessed on 14 December 2017).38. Pierre Lévy, “Intelligence artificielle et sciences humaines” , Pierre Levy’s blog. 8 October 2014.  [Online], h ttps://pierrelevyblog.com/2014/10/08/intelligence-artificielle-et-scienc-es-humaines/  (page ac cessed on 30 December 2017). 39. Yann LeCun, “Qu’est-ce que l’intelligence artificielle” , research published on the Collège de  France website, [Online], h ttps://www.college-de-france.fr/media/yann-lecun/UPL4485925235409209505_I ntelligence_Artificielle__Y._LeCun.pdf (page accessed on 14 D ecember 2017).40. Dominique Cardon, op. cit., p.60.
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 3572.W hile it is commonly accepted that statistical and probabilistic models ar e improved by increasing the data feeding them, some mathematicians ha ve warned of the risks of increasing false correlations (i.e. links between fac tors having absolutely no causal link) in big data. The mathematicians C ristian Sorin Calude and Giuseppe Longo point to the risk of a deluge of false  correlations in big data: the larger a database used for correlations, the  greater the chances of finding recurrent patterns and the greater the chanc es of making errors .41 What may appear as regularities for an AI (recur-r ent links between different data, concepts, contexts or lexical groups) may ac tually be random. Even if the argument of the two mathematicians should not  be generalised too hastily, they note that in certain vast sets of numbers, poin ts or objects, regular random patterns appear and it seems impossible t o distinguish them algorithmically from patterns revealing causalities.4273. Lastly , algorithms, whatever their current level of sophistication, still auto-ma tically boil down to the interactions established in the learning phase and henc e to their past. The content and quality of the data flows used in the compo-sition  of the calculations is therefore fundamental in understanding the results obtained  and in identifying possible analytical biases. There are many challenges her e again since, in an exhaustive approach, the analysis of the greatest possible amoun t of data relating to an activity will produce results whose meaning has t o be clarified with regard to all the factors that have had an influence. In a more r estrictive approach in which incoming data is sampled, risks will also arise from the tr ade-off biases required to select one data over another.4. C an artificial intelligence model legal r easoning in advance?A rtificial intelligence circumvents the difficulties encountered with older expert s ystems: they do not attempt to manually mirror legal reasoning, whose reproduc-s ystems: they do not attempt to manually mirror legal reasoning, whose reproduc-t ion is not in itself an objective for them. Machine learning leads to categorisations t ion is not in itself an objective for them. Machine learning leads to categorisations b etween the different parameters identified by the designers or those discovered b etween the different parameters identified by the designers or those discovered b y the machine.b y the machine.74. C ontinental legal systems are far removed from the ideal of rational-it y that embodied, for example, the 1804 Civil Code in France. There are a multitude  of sources that do not fit together perfectly and which relate to 4 1. Cristian Sorin Calude, Giusseppe Longo, “Le déluge des corrélations fallacieuses dans le  b ig data”, in Bernard Stiegler (dir.) (2017), La toile que nous voulons, FYP editions, p.156. 4 2. Theory of “Ramsey”, Ronald L. Graham, Joel H. Spencer – “Ramsey Theory”, Scientific American,  v ol.263, No.1, July 1990, p112-117 cited by Cristian Sorin Calude, Giusseppe Longo, “Le  d éluge des corrélations fallacieuses dans le big data”, ibid. 
Page 36 ► European Commission for the Efficiency of Justice (CEPEJ)a set of rules whose meaning remains undetermined, which the legal the-or ist Herbert L. A. Hart called the “open texture of law” .43 Common law sys-t ems, although considered more economically efficient because they adapt dynamically  to new legal needs,44 are also evolving and offer no more legal c ertainty. Legal rules therefore do not evolve in a linear fashion, distinguish-ing  them from empirical laws (those of the “exact sciences”), where each new rule  generally complements the previous ones and does not invalidate a c omplete set of reasoning.75. I t should be stressed that, faced with this general complexity of legal sy stems, the old IT expert systems quickly reached their limits once 200 to 300  logical rules were nested. The division of the law into rules of production w as not sufficient to provide a valid representation of the body of knowl-edge and methods tha t guide a lawyer.76. C onsequently, it has proved just as impossible to model the reason-ing  of judges on a computer as it has to model a positive legal system. As the  theory of law has highlighted, judicial reasoning is above all a matter of assessmen t and interpretation, of the proven and relevant facts of a case, of the  applicable rules of law (textual or precedents) – the meaning of which r emains, as has been said, indeterminate –,45 and of the subjective interpreta-tion  by judges of the concept of equity, which should undergo new changes in  Europe with the requirement of a proportionality review encouraged by the E uropean Court of Human Rights.77. X avier Linant de Bellefonds has stressed that the complexity of the law lies  in its teleological and contentious nature: two coherent arguments can lead t o different judgments according to two different priorities.4678. T his is because the famous legal syllogism is more a way of presenting legal  reasoning than its formal translation. It does not reflect the full reason-ing  of the judge, which is in fact made up of a multitude of decision-mak-ing  factors, cannot be formalised a priori, and is sometimes based on his discr etion: what are the relevant facts? Are these facts proven? Which rule applies  to them? What is the meaning of this rule with regard to the case t o be decided? Which source should prevail between a range of conflicting 43. Herbert L. A. Hart (1976), Le concept de droit, Saint-Louis university departments, Brussels. 4 4.https://www.contrepoints.org/2014/08/15/177160-common-law-contre-droit-civil-le xperience-francaise-de-lancien-regime45. On these questions, Michel Troper (2001), La théorie du droit, le droit, l’Etat , PUF, Paris, spec. p . 69-84.46. Xavier Linant de Bellefonds (1994), “L’utilisation des systèmes experts en droit comparé” , Revue int ernationale de droit comparé , Vol. 46, No. 2, p. 703-718.
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 37sources? The overall consistency of judicial decisions is never achieved and is  more a matter of an a posteriori account which judges use in their reason-ing , having more of a function of convincing themselves of the validity of a specific  solution than of describing strictly and objectively all the stages that r esulted in the decision made.79. Ho wever, this work of interpretation is precisely what machine learning t ecnhiques do not do – and do not attempt to do – today, since they carry out , as we have seen, automated processing where the correlation of vast amoun ts of information is supposed to replace the understanding of the real causalities  of a decision. They make not attempt to formalise legal reasoning but  hope that the models captured by them can anticipate the likely deci-sions of a judge in similar situa tions.80. T he results achieved by AIs are in reality unrelated to the question of the legal  conformity of a particular solution and cannot discriminate between legal and illegal ar guments.81. A review of the work of the University College of London (UCL) on the case -law of the European Court of Human Rights confirms this diagnosis. The UCL  study assumed that a simple automatic learning model could predict the out come of a case with 79 % accuracy for that particular court. The machine lear ning model proved to be more accurate with regard to the descriptive par t of the facts of the decisions studied than with regard to the reasoning r elating to the application of the Convention to the case in question.4782. T he examination of the terms correlated with the finding of a violation (with  a positive weighting) and those correlated with a non-violation of the E uropean Convention on Human Rights (with a negative weighting) sheds unambiguous  light on the mechanics at work, which is in no way compara-ble with legal r easoning.47.  Work on a sample of 584 decisions of the European Court of Human Rights: Nikolaos A letras, Dimitrios Tsarapatsanis, Daniel Preoţiuc-Pietro, Vasileios Lampos, “Predicting judicial  decisions of the European Court of Human Rights: a Natural Language Processing perspec tive” , published on 24 October 2016, [Online], https://peerj.com/articles/cs-93/ (page ac cessed on 14 December 2017).
Page 38 ► European Commission for the Efficiency of Justice (CEPEJ)Fig. 4: Illustration of UCL ’s work – Theoretical weight allocated to words or t erms according to their link with findings of violation or non-violation of the E uropean Convention on Human RightsPositive State Obligations: +  13,50 Treatment by  state officials: + 10,20 Detention conditions: +  11,70 Enforcement of domestic  judgmentsand reasonable time: +  11,70 VIOLATION  Ev idence : - 15,20   Sentencing: - 17,40 Prev ious violation of article 2 : - 11,40 Propert y rights and company claims: - 9,08 NON VIO LATION 
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 3983.T he choice of the relevant facts and their interpretation constitute one of  the elements of the judge’s decision. In other words, the UCL study was in r eality only able to produce a probability with lexical material largely derived fr om the reasoning and motivation of the judge and not with that assembled b y the applicant on the basis of frequencies alone. Their AI established thus a high  probability of correspondence between groups of words and a decision tha t had already been formalised and could only give rise to a limited num-ber  of possible outcomes. Under no circumstances can it alone reproduce the  reasoning of European judges nor, above all, predict an outcome, on the basis , for example, of a future applicant’s raw account before the Strasbourg c ourt, whose application will be subject to a very strict admissibility exam-ina tion (nearly 70,356 applications were declared inadmissible or struck from the  list in 2017)48 based largely on the application of standards of assessment (impor tance and seriousness of the complaint, etc.) leaving considerable lee-w ay in decision-making. 5. C an AIs explain judges’ behaviour in retrospect?A n a posteriori explanation of a judge’s behaviour, in particular the revelation of b ias, would require all the potentially causative factors to be identified through an b ias, would require all the potentially causative factors to be identified through an i nterpretative framework and a contextualised analysis. The fact that, statistically, i nterpretative framework and a contextualised analysis. The fact that, statistically, c hildcare is more often entrusted to mothers than fathers does not demonstrate c hildcare is more often entrusted to mothers than fathers does not demonstrate a  bias on the part of judges but reveals the need to mobilise different disciplines a  bias on the part of judges but reveals the need to mobilise different disciplines f rom the social sciences to shed light on this phenomenon.f rom the social sciences to shed light on this phenomenon.84. F rom a scientific point of view, explaining a phenomenon or, as far as w e are concerned, a piece of human behaviour, amounts to determining the causal  mechanisms that led to this behaviour using a certain amount of con-t extual data.85. T his requires, in a very schematic way, the preliminary constitution of an  interpretative framework, itself derived from the repeated observation of this  type of event or behaviour in the presence of certain factors or elements. T he interpretative framework is made up of the hypotheses or points of view adopt ed by the different social science disciplines. This is an additional analyti-cal  step that can be fed into algorithms, but which they cannot perform alone.8 6. Some legal tech companies went further and thought they could ident ify possible personal biases of judges and feed suspicions of bias. The open  d ata of the names of certain presiding judges of administrative courts and  4 8. Source: 2017 Statistical analysis of CEDH, January 2018. 
Page 40 ► European Commission for the Efficiency of Justice (CEPEJ)administrative courts of appeal in France have made it possible to develop an indica tor of the rejection rate of appeals against obligations to leave French t erritory taken by the administrative authorities. Some commentators have ar gued fervently that the alleged impartiality of judges was therefore cast in to doubt by artificial intelligence.87. But can such interpretations really be achieved on the basis of an algo-r ithmic processing of court decisions? For there to be personal bias in judges’ decision-mak ing processes (differing from their personal and public state-men ts in the case concerned) their behaviour, or in this case their decision, needs  to be determined by their personality traits, opinions or religion. How-ev er, as has been said, such a causal explanation cannot simply be deduced fr om the probabilistic result provided by algorithms. On the contrary, it r equires additional analytical work in order to isolate, among the many cor-r elated factors (including the identity of members of the panel of judges), those  that are truly causative. For example, the fact that a family court statis-tically  decides more often that children should live with their mother does not  necessarily reflect the judge’s bias in favour of women, but rather the e xistence of psychosocial, economic and even cultural factors specific to the jur isdiction, such as the working time of each of the parents, their income, the  local availability of collective childcare, whether or not the child is in school , whether or not one of the parents is in a new relationship or even simply the lack of in terest by either parent to take care of a young child.88. Similar ly, decisions on expulsion from a country given by an adminis-tr ative court located near a large detention centre cannot be compared fairly with those of a c ourt that deals with such disputes only occasionally.89. I n addition, regardless of the court’s location, the issue of the case-law of  a single duty judge who only occasionally deals with a certain type of liti-ga tion, but who uses (or ignores) the case law of his colleagues, is particularly in teresting and may legitimately raise the question of equality of citizens in judicial pr oceedings. However, the focus must remain on the remodelling or pr eservation of the collegial nature of the judicial system rather than classifi-ca tion or stigmatisation via machine learning tools.90. W hat can be deduced from the personality of the president of a panel of  judges in a collegial court, when his name is the only nominative informa-tion visible in open administr ative court decisions? 91. F urthermore, how can we account for two distinct philosophical and cultur al approaches to judicial decisions, whereby, in some European coun-tr ies, including France, there is a culture of precedent and a detailed knowl-edge  by judges of the factual databases of all 1st and 2nd instance decisions 
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 41(Ariane database) in the field of administrative justice, while other countries or  systems favour the intellectual independence of each court, along with a desir e to deal with each situation on a case-by-case basis?92. P roviding an accurate explanation of a court decision therefore requires a  much more detailed analysis of the contingent data in each case and the applicable  rules of law, rather than sustaining a vain hope that the mass of links will make sense .6. H ow is AI to be applied in civil, commercial and administr ative justice?T he state of development of machine learning techniques does not allow today to r each reliable results regarding the “prediction” of judicial decisions. On the other r each reliable results regarding the “prediction” of judicial decisions. On the other h and, their application in the field of civil, commercial and administrative justice h and, their application in the field of civil, commercial and administrative justice i s to be considered for the creation of scales or the pre-litigation resolution of dis-i s to be considered for the creation of scales or the pre-litigation resolution of dis-p utes online, when a later appeal to the judge remains possible.p utes online, when a later appeal to the judge remains possible.93.  As we can see, the first question raised by such use of artificial intelli-genc e is not so much whether it is beneficial or harmful, desirable or other-wise , but whether the algorithms proposed can achieve the type of result sough t. The conclusions from the experiments conducted in the Douai and R ennes appeal courts in France clearly demonstrate that in the presence of a though t-provoking discourse promoting a product of AI can hide unaccept-able desig n flaws and totally erroneous analysis results.94. R egardless of the quality of the software tested, anticipating judges’ decisions  in civil, commercial and administrative matters would appear to be a pot entially desirable benefit, albeit sometimes for very different reasons, both f or those responsible for judicial public policy and for private legal professionals.95. W hatever the legal tradition of the country, legal uncertainty, i.e. the r isk of having one’s legal claim validated or rejected, prompts the desire to be  able to quantify these factors using these new technological applications.96. La wyers (or a company’s legal department) see the possibility of using this t echnology to provide their clients with better informed advice by empirically and  systematically assessing the chances of a procedure’s success, as well as e ncouraging the conclusion of transactions that, if necessary, make it possible  t o avoid a long and costly trial. Some insurers already offer the use of predictive  s ystems to their clients to evaluate the merits of their business. 494 9. The Predictive software is offered to Allianz policyholders, for example. 
Page 42 ► European Commission for the Efficiency of Justice (CEPEJ)97.A t the same time, public decision-makers see this as an opportunity to bett er regulate the flow of new proceedings through the courts and provide themselv es with a lever to reduce judicial operating costs. This is thought to enc ourage litigants to use alternative dispute resolution methods (concilia-tion, media tion or arbitration).5098. T he approach that already exists in many judicial systems of harmonising decisions  in many matters by using scales (divorce, dismissal, compensation f or personal injury) could be revitalised through a probabilistic or actuarial appr oach.51 Alternative online dispute resolution services have even been cre-a ted to help assess the amount of compensation for small disputes, inter alia. Ho wever, these interesting approaches are not unbiased and must not deprive citiz ens of access to a judge or call into question the adversarial principle.E xperiments conducted in FranceA t the initiative of the Ministry of Justice, the two courts of appeal in Rennes and A t the initiative of the Ministry of Justice, the two courts of appeal in Rennes and D ouai agreed to test predictive justice software on various litigation appeals in D ouai agreed to test predictive justice software on various litigation appeals in s pring 2017, which in reality was an analysis of civil, social and commercial deci-s pring 2017, which in reality was an analysis of civil, social and commercial deci-s ions of all French courts of appeal. s ions of all French courts of appeal. A lthough these internal and exhaustive case law data had already been available A lthough these internal and exhaustive case law data had already been available t o them free of charge for many years (JURICA database), the Ministry made them t o them free of charge for many years (JURICA database), the Ministry made them s pecially available to the publishing company when they offered to assess the s pecially available to the publishing company when they offered to assess the v alue of a quantified (innovative) analysis of the sums allocated by the two courts, v alue of a quantified (innovative) analysis of the sums allocated by the two courts, i n addition to a geographical classification of the discrepancies noted for similar i n addition to a geographical classification of the discrepancies noted for similar a pplications and trials.a pplications and trials.T he stated objective of the software was therefore to create a decision-making T he stated objective of the software was therefore to create a decision-making t ool in order to reduce, if necessary, excessive variability in court decisions, in the t ool in order to reduce, if necessary, excessive variability in court decisions, in the n ame of the principle of equality of citizens before the law. The result of the exper-n ame of the principle of equality of citizens before the law. The result of the exper-i ment, contradictorily debated between the two courts of appeal, the Ministry i ment, contradictorily debated between the two courts of appeal, the Ministry o f Justice and the legal tech company who designed the product unfortunately o f Justice and the legal tech company who designed the product unfortunately s tated the absence of added value of the tested version of the software for the s tated the absence of added value of the tested version of the software for the w ork of reflection and decision-making of the magistrates. w ork of reflection and decision-making of the magistrates. M ore significantly, software reasoning biases were revealed that led to aberrant or M ore significantly, software reasoning biases were revealed that led to aberrant or i nappropriate results due to confusion between mere lexical occurrences of judi-i nappropriate results due to confusion between mere lexical occurrences of judi-c ial reasoning and the causalities that had been decisive in the judges’ reasoning.c ial reasoning and the causalities that had been decisive in the judges’ reasoning.5 0.In this respect see information report No. 495 (2016-2017) prepared on behalf of the Senate La w Commission, and tabled on 4 April 2017, by Senator Philippe Bas51. With respect to the actuarial approach, Case Law Analytics’ offer announces more of a risk assessmen t than a prediction of a solution to a dispute.
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 436.1. A ne w computing tool to calculate scales99. Th e procedures for calculating scales in various civil matters (for exam-ple , compensation for bodily injury, compensatory allowance and severance pa y) appear to be considerably enhanced when combined with other pro-c essing techniques, subject to numerous design measures and uses (perfor-ma tive effect). 52100.  It is important to highlight wha t Jean-Paul Jean, chair of the CEPEJ ev aluation working group, described as the qualitative challenge during a c onference on open data held in 2016 in France: the procedure carried out b y machine learning or any other method of processing should use certified or iginals, whose integrity has been verified, and which have been enriched t o distinguish the important from the insignificant.53101. T he other risk already encountered by the designers of these tools is tha t of ‘‘data-snooping’’ , namely selecting upstream-only data that is signifi-can t to predetermined analysis grids, for example by excluding from samples decisions  that lend themselves poorly to correlations of linguistic sequences thr ough machine learning or any other method (for example, decisions with-out pr esentation of the litigation or little reasoning).102. But if we calculate a scale, are we not already doing a little prediction? T he borderline may seem relatively sketchy unless we clearly distinguish the aim  of the process: the objective here is not to deliver prescriptive informa-tion but t o provide information about a state of affairs.103. Subjec t to these methodological and operational precautions, the scales  are available for handling certain disputes and are a powerful tool for har monising case-law. Scales were previously calculated on more or less lim-it ed samples of decisions, but AI tools make it possible to survey more deci-sions  and are liable to produce more accurate scales, applying a standard and henc e giving more weight to the outcomes.52. The performative or self-realisation effect is the risk that a system will produce the same output  progressively by influencing the producers of input information; this effect is often men tioned with regard to judicial scales which, when informed by decisions based on these scales , tend to be representative only of themselves.53. J-P . Jean, “Penser les finalités de la nécessaire ouverture des bases de données de juris-prudenc e” , Conference of 14 October 2016 held at the Court of Cassation, h ttps://www.c ourdecassation.fr/IMG///Open%20data,%20par%20Jean-Paul%20Jean.pdf, page accessed on 17 M arch 2018.
Page 44 ► European Commission for the Efficiency of Justice (CEPEJ)6.2. O nline dispute resolution104. A ll European courts face to different degrees repetitive low-value civil litiga tion. The idea of facilitating the procedure through information tech-nology  and/or outsourcing them from the courts is quite widely shared. Gr eat Britain, the Netherlands and Latvia are examples of countries that have alr eady implemented or are about to implement these types of more or less aut omated solution54s. For cross-border claims, the European Union has set up , by means of Regulation n°524/2013, a common framework available on the I nternet (European small claims).105. Ho wever, the scope of these online dispute resolution (ODR) services seems  to have gradually extended. They have gone from restricted online ser vices to alternative dispute resolution measures before the complaint is br ought before the court, and are now being introduced incr easingly into the court process itself, to the point of offering electronic c ourt services’ .55 They do not only concern low-value disputes, but also tax dis-put es or disputes relating to social security services, or divorce proceedings.106. F or those who advocate such solutions, which are of interest to a num-ber  of legal professions and the private sector, access to justice could be sig-nifican tly improved by a broad solution combining ODR and AI (or at least e xpert systems, see section 3 above for the distinction). The idea is to take c omplainants through an automated diagnosis of the dispute by putting a number  of questions, which are then processed by the machine, resulting in pr oposals for a solution. The work of the Cyberjustice de Montréal laboratory, which  assembles the various pre-litigation and litigation phases into a com-put er-based process for low-intensity disputes (for example, small claims c ourts in Quebec), is a good illustration of hybridisation.56 According to the desig ners, there are clear benefits in terms of efficiency and quality.54. See to this end the online dispute resolution available in the UK – h ttps://www.judiciary.go v.uk/wp-content/uploads/2015/02/Online-Dispute-Resolution-Final-Web-Version1.pdf. S ee also the system PAs in the Netherlands, which issues automated decisions based on pr eviously granted permits, and which has given rise to litigation at the national level and bef ore the ECJ: there are two cases (c-293/17 en c-294/17) brought to the Council of State of  the Netherlands (farmers/ nature conservation against the Netherlands) to determine whether  or not a system (Programme Regulating Nitrogen) is allowed to decide whether or  not farmers a.o. are violating the Habitat directive. Recently the Court of Justice of the E uropean Union in Luxembourg has answered on the requests for a preliminary ruling c oncerning these joined cases (ECLI:EU:C:2018:882)55. Darin Thompson, “Creating new pathways to justice using simple artificial intelligence and online disput e resolution” , Osgoode Hall Law School of York University.56. http://www.cyberjustice.ca/projets/odr-plateforme-daide-au-reglement-en-ligne-de-litiges/
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 45107.But on what basis would any compensation proposed by such a system be  calculated? What method? Does the algorithm process information fairly? Is  the proposal intended to be discussed on an adversarial basis with the help  of a trained and certified third party? Is access to a judge always pos-sible?  Some authors even see the widespread use of these dispute resolu-tion  methods as a new manifestation of digital “solutionism” , i.e. the system-a tic use of technologies to try to solve problems that do not necessarily fall within  their scope.57 It should also be noted that in Europe, a more protective r egulatory framework that is binding on the member States has recently been  put in place: Article 22 of the Data Safety Monitoring Plan explicitly pr ovides for persons to be able to refuse to be the subject of a decision based e xclusively on automated processing, with certain exceptions.5857. Evgeny Morozov, “Pour tout résoudre cliquez ici” , FYP editions, cited by David Larrousserie, “ Contre le ‘solutionnisme’ numérique” , Le Monde, 6 October 2014, h ttps://www.lemonde.fr/scienc es/article/2014/10/06/contre-le-solutionnisme-numerique_4501225_1650684.h tml58. Article 22(1) of EU Regulation 2016/679: “The data subject shall have the right not to be subjec t to a decision based solely on automated processing”; exceptions are provided for (such  as the data subject’s consent) but “suitable measures to safeguard the data subject’s r ights and freedoms and legitimate interests “ must be implemented by the controller, including  “the right of the data subject to obtain human intervention on the part of the c ontroller, to express his or her point of view and to contest the decision” .S ee, in the same sense, the Council of Europe Convention for the Protection of Individuals with  regard to Automatic Processing of Personal Data, as amended by the Protocol adopted in  May 2018 when the latter enters into force. Article 9(1)(a) provides the principle that “E veryone has the right not to be subject to a decision affecting him significantly, which shall  be taken solely on the basis of automatic processing of data, without his point of view  being taken into account. Notwithstanding this principle of prohibition, Article 9(2) sta tes that “paragraph 1(a) shall not apply if the decision is authorised by a law to which the  controller is subject and which also provides for appropriate measures to safeguard the r ights, freedoms and legitimate interests of the data subject” . T he explanatory report states (§75): “It is essential that any person likely to be subject to a pur ely automated decision should have the right to challenge that decision by effectively putting  forward his point of view and his arguments. In particular, the data subject must ha ve the possibility to prove the possible inaccuracy of personal data before their use, the inadequac y of the profile to be applied to his particular situation or other factors which will  have an impact on the outcome of the automated decision. This is particularly the case  when the application of an algorithmic reasoning, by leading to the limitation of a r ight, the refusal of a social benefit or the evaluation of their borrowing capacity on the sole  basis of the software, has the effect of stigmatizing individuals. However, the data subjec t may not exercise this right if the automated decision is provided for by the law to which  the controller is subject, which provides for appropriate measures to safeguard the r ights and freedoms and legitimate interests of the data subject” .S ee also Article 9(1)(c) of the modernised Convention 108, which provides for the right of  the data subject “to obtain, at his request, knowledge of the reasoning underlying the pr ocessing of data, when the results of such processing are applied to him” . The explanatory 
Page 46 ► European Commission for the Efficiency of Justice (CEPEJ)108.T he potential benefits of an ODR system, its degree of integration into a  complete judicial process (from pre-litigation to actual litigation) and the almost  decisive role of AI in the execution of the process must therefore be pr operly assessed on a case-by-case basis.109. ODR already offers upstream knowledge of judicial processes. Its role is  clearly to contribute to the implementation of conciliation, mediation and ar bitration services outside the courtroom. These services can also be used dur ing contentious proceedings under the supervision of judges before they decide  on the outcome of disputes based on the merits (for some disputes this phase is c onsidered compulsory).110. On the other hand, the actual contribution of AI should be assessed. Is  it only a question of using machine learning to establish indicative scales or  prescribe a solution? Is it really AI that is being used or an expert system or  just the logical rule chain? In any event, it should be possible to combine these  systems with the requirements of transparency, neutrality and loyalty.59111. F inally, it is also necessary to examine the way in which complainants ar e encouraged to use the system: is there potential confusion in the very name  of what is on offer? If one speaks of a court, it must be the form of or ganisation defined by the European Convention on Human Rights and not  simply a private justice institution with the mere appearance of state justic e60. Is recourse to a judge clearly possible? In the Netherlands, private health  insurance contracts seem to provide automatically for recourse to an ODR bef ore any legal action is brought..112. T he CEPEJ Mediation Working Group (CEPEJ-GT-MED), launched in 2018,  has offered its first thoughts on the contribution of information t echnology towards alternative dispute resolution methods. The CDCJ (c ont’d) dreport of the modernised Convention (§77) states: “Data subjects have the right t o obtain knowledge of the reasoning underlying the data processing, including the c onsequences of this reasoning and the conclusions which may have been drawn from it , in particular when using algorithms for automated decision-making, in particular in the c ontext of profiling. For example, in the case of a credit rating system, borrowers have the r ight to know the logic behind the processing of their data that leads to the decision to g rant or refuse credit, rather than simply being informed. of  the decision itself. Understanding these elements contributes to the effective exercise of  other essential safeguards such as the right of objection and the right of appeal to the c ompetent authority” . This “must obtain useful information concerning the underlying log ic” is also found in the GDR (Article 13(1)(f); Article 14(2)(g); Article 15(1)(h)).5 9.Charlotte Pavillon, “Concerns over a digital judge” , nrc.nl, h ttps://www.nrc.nl/nieuw s/2018/01/19/zorgen-om-populaire-digitale-rechter-a158896360. Scarlett-May Ferrié, Algorithmes tested against fair trial, document consulted on h ttp://le xis360.fr, do wnloaded on 09/07/2018,§ 27-38
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 47is currently doing in-depth work on ODRs to identify the potential of these  tools but also any problem points amounting to possible vio-la tions of Articles 6, 8 and 13 of the European Convention on Human R ights.6.3. T he main guarantees to be reaffirmed in civil, c ommercial and administrative proceedingsR ight of access to a court113. T he provision of online dispute resolution tools should not affect the r ight of access to a court within the meaning of Article 6, even if this right is  not absolute and lends itself to implicit limitations.61 In civil matters, for e xample, every litigant has the right to submit to a court any dispute relating t o his “civil rights and obligations” heard by a court.62 In 2015, the Parliamen-tar y Assembly of the Council of Europe adopted a resolution on “Access to justic e and the Internet: potential and challenges” in which it called to ensure tha t “parties engaging in ODR procedures retain the right to access a judicial appeal  procedure satisfying the requirements of a fair trial pursuant to Arti-cle 6 of the C onvention’’ .63A dversarial principle114. I t seems imperative to make a certain amount of quantitative informa-tion  (for example, the number of decisions processed to obtain the scale) and  qualitative information (origin of decisions, representativeness of selec ted samples, distribution of decisions between different criteria such as the  economic and social context) accessible to citizens and, above all, to the par ties to a trial in order to understand how scales have been constructed, to measur e their possible limits and to be able to debate them before a judge.E quality of arms115. T he use of technological means should not cause imbalances between par ties, since the use of digital means could indeed facilitate proceedings 61. Art.6 §1 « 1. everyone is entitled to a fair and public hearing .. by an independent and impar tial tribunal… who shall decide (...) the merits of any criminal charge against it..; for the limita tions see Deweer c. Belgique, § 49 ; Kart c. Turquie  [GC], § 67.62. CEDH, Golder c. Royaume-Uni, §§ 28-366 3.Resolution 2054 (2015) of the Parliamentary Assembly of the Council of Europe (P ACE), 10 November 2015, h ttp://assembly.coe.int/nw/xml/XRef/Xref-XML2HTML-EN.asp?fileid=22245&lang=en 
Page 48  ►  European Commission for the Efficiency of Justice (CEPEJ) for certain operators (institutions, companies with means, computer liter-a te persons) and, on the contrary, pose difficulties for certain population t ypes that are more uncertain or less familiar with computers. It is important tha t no individuals are left alone in front of their screens, and that they are inf ormed that they can seek legal advice and are assisted where necessary.I mpartiality and independence of judges116. I t has been posited that the norm derived from the majority trend r eferred to above in section 2.2. may have indirect effects on the indepen-denc e and impartiality of the judiciary, particularly in systems where the independenc e of the judiciary is not fully achieved. In these systems, we can-not  rule out the risk that such norms will place indirect pressure on judges when  decisions are taken and prompt their approval, or that the executive will monit or those who depart from the norm.R ight to counselA t the beginning of this chapter, we mentioned the advantages derived from the  application of predictive justice tools for lawyers and, in particular, the possibilit y of providing their clients with better informed advice by empiri-cally  and systematically assessing the chances of a procedure’s success. How-ev er, let us imagine a case where the chances of success for the litigant are e xtremely poor: could this affect the lawyer’s decision to assist his client? P rofessional practice should aim to minimise the risk that persons requiring legal advic e may ultimately be deprived of it. 7. I ssues specific to criminal justice: prevention of offences, risk of r ecidivism and assessment of the level of dangerE ven they are not specifically designed to be discriminatory, the use of statistics a nd AI in criminal proceedings has shown a risk of prompting the resurgence of a nd AI in criminal proceedings has shown a risk of prompting the resurgence of d eterministic doctrines to the detriment of doctrines of individualisation of the d eterministic doctrines to the detriment of doctrines of individualisation of the s anction, which have been widely acquired since 1945 in most European judicial s anction, which have been widely acquired since 1945 in most European judicial s ystems.s ystems.117. T he use of AI science and technology in criminal matters poses specific c hallenges as its application may reflect some current public debates about  t he alleged predictability of offending behaviour. However, this debate  s eemed to have been thoroughly settled for some thirty years in a number  o f European countries. In Italy for example, Article 220, paragraph 2, of the  C ode of Criminal Procedure expressly rules out the use of an expert opinion  
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 49to establish habitual or professional criminal features, the tendency to com-mit  a crime, the character and personality of the accused and, in general, the psy chological qualities of the accused, regardless of the pathological causes. I n France, for example, the doctrine of “new social defence” developed by M arc Ancel was the foundation of criminal law: instead of a merely puni-tiv e and deterministic approach, a system of social rehabilitation was intro-duc ed to prevent the commission of an offence by avoiding the conditions f or crime. This approach is shared by a number of European criminal policy instrumen ts that focus on the objectives of re-educating and reintegrating off enders.64118. C riminal justice tools should therefore be designed in accordance with these  fundamental principles of rehabilitation, 65including the role of the judge  in the individualisation of the sentence, based on objective elements of  personalities (training, employment, regular medicals and social care) without  any other form of analysis than that carried out by specifically trained pr ofessionals, such as probation officers. Big data analytics techniques could be  used by these professionals to centralise and collect information on the person  accused of a crime or misdemeanour, which could then be stored by v arious institutions and agencies and would then need to be examined by a judge , sometimes within a very short time frame (for example in the context of ac celerated trial procedures).7.1. T ools used by investigative authorities before the criminal trial 119. I nstruments described as “predictive policing” (before the judicial pro-c ess or before a court referral) are already growing rapidly and are beginning t o be known by the general public (for example, think of the no fly list, which is  actually a big data analytics application that collects and analyses data on pot ential terrorists in order to prevent the commission of acts, or algorithms used t o detect fraud or money laundering). 120. I n general, a large number of computer tools are commonly used to pre-v ent the commission of criminal acts (by identifying possible places where this  could happen or their authors) or prosecute them more effectively.66 The 64. See European Court of Human Rights, Grand Chamber, Vinter and Others vs. United Kingdom , par as. 114-11865. On the other hand, the use of AI for treatment and rehabilitation purposes (e.g. to collect data on  treatment administered or methods of reintegration in prison) should be encouraged.66. See Ales Zavrsnik, Big Data, crime and social control , page 194 et seq., which lists in detail a ser ies of instruments used by police services in Europe and the United States.
Page 50 ► European Commission for the Efficiency of Justice (CEPEJ)first category includes “predictive policing” instruments that are used to pre-v ent certain types of offences with elements of regularity in their occurrence such  as burglary, street violence, theft from/of vehicles. The designation of these  tools derives from their ability to determine precisely where and when these  offences could be committed and to reproduce this information on a geog raphical map in the form of hot spots that are monitored in real time b y police patrols. This process is called predictive criminal mapping. Most of the  software used in this area is based on historical crime location evidence, such  as police reports, but even more powerful new technologies combining v arious data and from different sources are also being tested.67 These instru-men ts, which have very persuasive rates of effectiveness, are also claimed to ha ve deterrent effects on the commission of offences in areas surrounding hot spots , leading to a positive opinion of public policies.68121. Ho wever, the predictive capabilities of these tools, which show their limita tions with regard to crimes of a less regular nature or targeting different loca tions, such as terrorism, must be put into perspective. In addition, one of their  weaknesses is the effect of “vicious circles” and “self-fulfilling prophe-cies ”: neighbourhoods considered at risk attract more police attention and polic e detect more crime, which leads to excessive police surveillance of the c ommunities living in them.69 Lastly, questions about a possible “tyranny of the  algorithm” that could minimise or even progressively replace human judg ment are not totally absent within the police services themselves, even 67. For example, as part of the project “E-Security – ICT for knowledge-based and predictive ur ban security” (http://www.esecurity.trento.it/), which was conducted in the Italian city of  Trento between November 2012 and May 2015, a database gathering information on cr imes reported to the police, the results of surveys conducted by the city hall on victimis-a tion and real and perceived security by citizens, information on physical and social urban disor der from the police, as well as other variables relating to “SmartCity” (e.g. information r elating to the socio-demographic context, urban setting, night lighting, the presence of sur veillance cameras and public transport). It was created to better equip crime prevention and  urban safety improvement work. The project managers testified to the reliability of the  techniques used, which were said to make it possible to predict criminal acts with a suc cess rate of approximately 60-65 % and which were said to help to better fight crime when  limited resources were available. In addition, tests conducted in the United Kingdom as  part of a pilot project to predict possible burglary, theft, and assault locations show that the  software projections used, called PREDPOL, were accurate in 78 % of cases, compared t o 51 % using traditional techniques.68. Indication of a geographical concentration of the crime would help police forces to better c onsider the environmental factors making the crime more likely in the identified area (ligh ting, presence of shops, etc.) and to plan adequate responses in consultation with other par tners.69. “Predicting crime, LAPD style” , The Guardian, 25 June 2014.
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 51if, for the time being, technology is presented as remaining at the service of human beings in or der to better equip them for decision-making.70122. I n addition, big data analytics are increasingly being applied in the pr osecution of crime. Tools such as Connect, which is used by the UK police t o analyse billions of data generated in financial transactions to find correla-tions  or patterns of operations, or the International Child Sexual Exploita-tion  Database (ICSE DB), managed by Interpol, which helps identify victims and/or  perpetrators through the analysis, for example, of furniture and other objec ts in abusive images, or the analysis of background noise in videos, ha ve proven particularly effective in fighting crime. With Connect, for exam-ple , searches that previously required months of investigation can now be per formed in minutes, with a very high level of complexity and volume of da ta. 123. T he doctrine nevertheless questions the managerial logic of the crime r esponse provided by these predictive tools, in which a thorough analysis of  the reasons for the crime becomes less important than doing something her e and now. This is occurring at a time when available budgets are shrink-ing  and the police must provide the same level of public protection, but with limit ed personnel, equipment and resources.717.2. T ools during the criminal trial124. T he use of predictive tools72 by judges in criminal trials is very rare in E urope.125. HAR T (Harm Assessment Risk Tool) was developed in partnership with C ambridge University and is now being tested in the UK. This technology based  on machine learning was trained using Durham Police archives dating fr om 2008 to 2012. By learning from decisions made by police officers during this  period, and whether or not certain suspects reoffended, the machine is e xpected to be able to assess the risk – low, medium or high – of suspects r eoffending, based on around thirty factors, some of which are not related to the cr ime committed (for example, postal code and gender). 70. “How technology is allowing police to predict where and when crime will happen” , The I ndependent, 7 October 2017.71. Ales Zavrsnik, Big Data, crime and social control , page 196.72. In the literature, these tools are often referred to as “algorithmic justice” or “automated justic e” , or “simulated justice” .
Page 52 ► European Commission for the Efficiency of Justice (CEPEJ)126.I n tests initially conducted in 2013, during which suspect behaviour w as observed over a two-year period after commission of the crime, HART pr edictions were found to be 98 % effective at predicting low risk and 88 % eff ective at high risk of recidivism. In this experimental phase, HART will have a  purely advisory value for the judge. In addition, audits of HART’s function-ing  and the reliability of its conclusions will be regularly conducted by the polic e.127. E ven if it is the only predictive tool identified in Europe to date, it pro-vides  the opportunity to consider the challenges that public decision-mak-ers  could face in the near future if this type of application is tested on a larger scale , particularly in the light of findings in America.128. I n the United States,73 the NGO ProPublica revealed the discriminatory eff ects of the algorithm used in COMPAS software (Correctional Offender M anagement Profiling for Alternative Sanctions), which aims to evaluate the  risk of recidivism when the judge must determine the sentence for an individual .129. T his algorithm, which was developed by a private company and which  must be used by judges in certain American federal states, includes 137  questions answered by either the defendant or information pulled from cr iminal records. The questions are quite varied and include the presence of a  telephone at home, difficulty paying bills, family history, criminal history of  the accused, etc.74 The algorithm rates the person on a scale from 1 (low r isk) to 10 (high risk). It is an aid to judicial decision-making, its conclusions being  only one of the variables considered by the judge when deciding on the sen tence.130. A frican-American populations were assigned a recidivism high-risk rate t wice that of other populations within two years of sentencing – without this eff ect being naturally sought by the designers75. In contrast, the algorithm c onsidered that other populations appeared much less likely to repeat an 73. A 2015 study had identified about sixty predictive tools in the United States.74.  There are other algorithms that have been developed using critical observations expressed b y the doctrine (see next chapter) that are based on smaller variables, more directly related t o the crime committed and less related to race, gender or socio-economic status. An e xample is the Public Safety Assessment Tool used in 30 American jurisdictions.75. This purely discriminatory effect can in fact be explained by the relatively permissive “ calibration” of the algorithm model, which creates many “false positives” .
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 53offence76. It must of course be noted that this type of misleading interpreta-tion  in reality reveals only the social and economic fragility of certain groups of popula tions which are obviously not criminogenic by nature. Researchers a t Dartmouth College have also shown that this type of algorithm does not pr oduce added value since people without any criminal history can repro-duc e exactly the same assessment simply by answering the questionnaire. 131. I n addition, the lack of transparency in the algorithm operation pro-c esses designed by private companies (which claim intellectual property) w as another cause for concern. If we take into account the fact that they take  their source data from the state authorities themselves, their lack of ac countability to citizens poses a major democratic problem. Accounts have sho wn that the public is informed about big data operations accidentally, spor adically and when there are leaks or errors: an example of this is when P roPublica revealed the flaws in the COMPAS algorithm following the owner c ompany’s refusal to share it. The NGO had to appeal to public authorities to ac cess the data and hire its own scientist to examine the algorithm.7.3. T he challenges of “prediction” in criminal matters132. W e have seen in the previous sections that the degree of development of  predictive tools in Europe in the criminal field is very varied. Although instrumen ts described as “predictive policing” are growing rapidly and are ev en beginning to come to the attention of the general public, the situa-tion  is not the same when it comes to the application of this type of tool by 76. Black populations were more frequently classified as high risk, while they did not reoffend within  two years of conviction; white populations, on the other hand, who were more fr equently classified as low risk, committed offences within the next two years. In short, the  algorithm overestimated the risk of recidivism for blacks and underestimated it for whit es (the “false positives” were mostly blacks, while the “false negatives” were mostly whit es). In response to ProPublica’s allegations, NorthPointe (now Equivant since this c ontroversy) replied that white and black populations were equally represented when c onsidering the “true positives” , i.e. those who had actually reoffended . The question of ho w to reconcile both the accuracy of the algorithm in detecting recidivism and the need  to avoid discriminatory effects towards black populations has been the source of in tense debate in the literature; see in particular Chouldechova A (2016), “A fair prediction with  a disparate impact: a study on bias in recidivism prediction instruments” , available at h ttp://arxiv.org/abs/1610.07524; and also “Bias in criminal risks scores is mathematically inevitable , Researchers say” , available at https://www.propublica.org/article/bias-in-crimi-nal-r isk-scores-is-mathematically-inevitable-researchers-say. This debate also reflects the question  of the legitimacy of a private company, without any institutional control, to arbitr ate between two opposing requirements: that of defending society, on the one hand , and that of respecting the rights of individuals, on the other.
Page 54 ► European Commission for the Efficiency of Justice (CEPEJ)judges in criminal trials. As regards the instruments available to prosecution ser vices, thoughts about their advantages and disadvantages have already been e xpressed. Let us now study the tools specific to criminal trials. 133. F irst of all, it is important to rule out arguments based solely on the effi-cienc y or inefficiency of these tools. The examples given above show that there can  be tremendous opportunities but also real risks in the application of new t echnologies that are used without the necessary precautions. Public deci-sion-makers  and judicial stakeholders must be particularly vigilant and play an ac tive role in the development of these technologies; continuous monitoring is  necessary to determine their real effectiveness and efficiency, and to avoid unf oreseen consequences. This is even more important in criminal proceed-ings because of their dir ect impact on the individuals’ personal freedoms.77134. T his implies that both the benefits and the drawbacks of the applica-tion of such t ools in the judicial field should be carefully measured. 135. Suppor ters often argue that they are neutral and that they rely on fac tual and objective methods that help to make justice more accurate and  transparent. Another great asset, it is claimed, is their efficiency, which sometimes  exceeds human capacities and can only be extremely valuable in a gener al context of reduced public funds or even a shortage of resources.136. T he inclusion of algorithmic variables such as criminal history and fam-ily  background means that the past behaviour of a certain group may decide the  fate of an individual, who is, of course, a unique human being with a specific  social background, education, skills, degree of guilt and distinctive motiv ations for committing a crime78. They also argue that human decisions can  be based on values and considerations (e.g. societal) that would not be r etained by the machine. For example, a judge could decide to order the bail of  a female offender who has a risk of recidivism, on the basis of a hierarchy of  values, for example by setting greater store by her role as a mother and pr otector of her children, whereas the algorithm would be able to determine the  risk of reoffending more accurately but would not be able to operate such a hier archy of priorities.77 .An extract from the Wisconsin Supreme Court decision in Wisconsin v. Loomis can also pr ovide inspiration at European level: “It is important to consider that tools such as COMPAS c ontinue to change and evolve. The concerns we address today may very well be allevi-a ted in the future. It is incumbent upon the criminal justice system to recognize that in the  coming months and years, additional research data will become available. Different and  better tools may be developed. As data changes, our use of evidence-based tools will ha ve to change as well. The justice system must keep up with the research and continually assess the use of these t ools. ”78. Aleš Zavrsnik, “Big Data, crime and social control” , page 196.
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 55137.I n criminal matters, there are also potential risks of discrimination when one  considers that these tools, which are constructed and interpreted by humans , can reproduce unjustified and already existing inequalities in the cr iminal justice system concerned; instead of correcting certain problematic policies , technology may end up legitimising them. As already mentioned, the  NGO ProPublica79 clearly revealed the discriminatory effects of the algo-r ithm used in COMPAS,80 which predicted that black populations were twice as  likely to reoffend as white populations within two years of sentencing while  considering that white populations were much less likely to repeat the off ence. They could, however, help reveal the errors in decision-making so tha t they may be corrected.81 Moreover, the lack of transparency in algorithm c onstruction processes by proprietary companies and their accountability to the  public is a cause for concern, all the more so if they are part of steps taken b y state authorities to make data available to the public. 138. I n the light of the foregoing, when algorithms are used in the context of a  criminal trial it seems essential to fully guarantee respect for the principle of equalit y of arms and presumption of innocence established by Article 6 of the ECHR.  The party concerned should have access to and be able to challenge the  scientific validity of an algorithm, the weighting given to its various ele-men ts and any erroneous conclusions it comes to whenever a judge suggests tha t he/she might use it before making his/her decision. Moreover, this right of  access is also covered by the fundamental principle of personal data pro-t ection. All people have the right not to be subject to decisions affecting them sig nificantly made solely on the basis of automated data processing, without their poin t of view having been taken into account beforehand.139. I n this respect, there is a difference between Europe and the United S tates with regard to the right of access to algorithms: while in the United S tates the judicial authorities are still reluctant to recognise this right fully and  weigh private interests (particularly the protection of intellectual prop-er ty) against the rights of defence, in Europe the framework is more protec-tiv e because of the GDPR, which establishes a right to information on the under lying logic of decisions made using algorithms.8279. www.propublica.org/article/technical-response-to-northpointe. 8 0.Other algorithms focus on other elements more directly related to the offence committed.81. Mojca M. Plesnicar and Katja Sugman Stubbs, “Subjectivity, algorithms and the courtroom” .82. Article15, 1. (h) of EU Regulation 2016/679: “The data subject shall have the right to obtain  from the controller” ... “the following information:” ... ” the existence of automated decision-mak ing, including profiling, as referred to in Article 22, paragraphs 1 and 4, and, a t least in those cases, meaningful information about the logic involved, as well as the sig nificance and the envisaged consequences of such processing for the data subject” .
Page 56  ►  European Commission for the Efficiency of Justice (CEPEJ) 140.T he considerations expressed earlier regarding the potentially negative eff ects of these tools on the impartiality of the judge are also valid in criminal ma tters: a judge who decides against the prediction of an algorithm is likely t o take risks as he assumes greater responsibility. It does not seem unrealistic t o imagine that judges would be reluctant to take on this additional burden, par ticularly in systems where their terms of office are not permanent but subjec t to popular vote,83 or in which their personal liability (disciplinary, civil or  even criminal) is likely to be incurred, especially if their statutory guaran-t ees in disciplinary matters are insufficient.8. Sp ecific questions relating to the pr otection of personal dataT he use of algorithms raises the question of the protection of personal data when b eing processed. The precautionary principle should be applied to risk assess-b eing processed. The precautionary principle should be applied to risk assess-m ent policies.m ent policies.141. F or the full potential of algorithms to be exploited while comply-ing  with data protection principles, the precautionary principle should be applied  and preventive policies should be put in place to counter the poten-tial  risks associated with the use of the data processed by these algorithms and the impac t of their use on individuals and society in general.142. T he principle of lawfulness in the processing of personal data and the obliga tion to prevent or minimise the impact of data processing on the rights and  fundamental freedoms of data subjects should induce prior risk assess-men t. This should make it possible to implement the appropriate measures, par ticularly during the design stage (and hence by design) and by default, in or der to mitigate the risks identified.143. Sinc e personal data must be processed for specified and legitimate pur poses, they must not be used in a way that is incompatible with those pur poses and must not be further processed in a way that the data subject ma y consider unexpected, inappropriate or questionable (principle of loy-alt y). The issue of re-using personal data, making them widely accessible, ther efore needs to be handled with the utmost caution.1 44. The design of the data processing methods used by algorithms should  m inimise the presence of redundant or marginal data and avoid any potential  8 3.  Mojca M. Plesnicar and Katja Sugman Stubbs, “Subjectivity, algorithms and the courtroom”. 
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 57hidden bias and any risk of discrimination or negative impact on the funda-men tal rights and freedoms of the data subjects.145. W hen artificial intelligence is used, the rights of data subjects are of especial  importance, and the control that each of us must have over our per-sonal  information implies that it must be possible to exercise the following r ights: the right of data subjects not to be subject to automated decisions sig nificantly affecting them without their point of view being taken into ac count, the right to obtain information about the reasoning underlying the da ta processing carried out by algorithms, the right to oppose such process-ing and the r ight to a legal remedy.9.  The potential and limitations of predictive justice toolsT he term predictive justice should be dismissed because it is ambiguous and mis-l eading. These tools are based on methods of analysis of case-law, using statistical l eading. These tools are based on methods of analysis of case-law, using statistical m ethods that do not in any way reproduce legal reasoning but may attempt to m ethods that do not in any way reproduce legal reasoning but may attempt to d escribe it. Analytical biases, if they cannot be totally eliminated, must be iden-d escribe it. Analytical biases, if they cannot be totally eliminated, must be iden-t ified. The design process and the use of the tool must be embedded in a clear t ified. The design process and the use of the tool must be embedded in a clear e thical framework.e thical framework.146. I n section 3, we already highlighted the ambiguity and fallacy of the c oncept of predictive justice and how it operates a slow shift in the collec-tiv e mind, leading us to believe that machines, devoid of any emotion, will one  day be better able to make the act of judging more reliable. More than ev er, its promises need to be examined in an objective and scientific man-ner , based on solid foundations of fundamental research, in order to identify possible  limitations. In this connection, it should be noted that the risks of dist orted interpretations of the meaning of court decisions are extremely high  when based on statistical modelling alone. This observation is borne out  further by the lack of precise understanding of the links between the da ta and the obvious presence of false correlations that cannot be discerned in lar ge masses of data.147. M oreover, the neutrality of algorithms is a myth, as their creators con-sciously  or unintentionally transfer their own value systems into them. The philosopher , Eric Sadin, noted that, behind their efficient and impersonal f acade, algorithmic systems imperceptibly reflect the intentions of their  d esigners or sponsors, inducing a functioning and asymmetric power over  t he lives of other people. Similarly, researcher Aurélien Grosdidier considers  t hat an algorithm, in itself, is capable of nothing other than allowing us – at  b est – to grasp part of the designer’s intention and extends the questioning  
Page 58 ► European Commission for the Efficiency of Justice (CEPEJ)to the entire information processing chain (designer’s intention, production of  computer code, execution of computer code and context of execution then  maintenance). This observation is also shared by the criminologist Aleš Za vrsnik, who underlines how the stages of construction and interpretation of  algorithms are carried out by man, for man, and cannot escape errors, prej-udic es, values, human interests and a human representation of the world, ho wever they are conceived.148. D espite these significant limitations, should we overlook the contribu-tion  of a technology with unequalled power? Mathematicians C. S. Calude and  G. Longo themselves stress in their study on big data that the restrictive or  negative scope of their results, as often happens, does not destroy the sci-enc e of data but paves the way for greater thought, including the challenge of  a new, more extensive scientific method capable of incorporating both new  algorithmic instruments and classical tools by accompanying process-ing  with a rigorous evaluation of evidence. As highlighted in the introduc-tion,  the use of AI is likely to offer extremely significant support for profes-sionals , including judges and lawyers, but also the general public, especially if  one day they make it possible to construct unparalleled research and docu-men tary analytical tools in legislative, regulatory, jurisprudential and doctri-nal  matters and create dynamic links between all these sources. But this type of  application goes beyond the scope of this article since it is not designed t o predict the outcome of a dispute but to analyse case-law in a given time and spac e.149. A s discussed in section 6, subject to the representativeness of selected and  processed samples, AI has helped to devise much more precise scales of  the average or median amounts of money allocated, mutatis mutandis, in v arious fields (financial support, compensatory benefits, compensation for bodily  injury, severance pay, etc.). These scales, which are based more on a c onsensus than on an average analysis of what already exists, already provide sig nificant support for decision-making and guidance without being able to r eplace the law itself. As mentioned earlier, the risk is that, in the absence of  a statistical representation of reality or of being able to predict anything, the  results of predictive justice software will be set as standards without any v alidation by the legal system and in conflict with it.150. Lastly , let us consider the idea of being able to step back at will from pr edictive systems. Rather than locking users into a probability (or set of pr obabilities), the idea would be to allow them to navigate through the cor-r elations that led the system to propose its assessment and to be able to distanc e themselves by selecting other more relevant concepts or groups of w ords or to exclude false correlations. To use the UCL example, this would 
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 59consist of proposing a graphic representation of the different terms retained b y the system (with their respective weightings) to find that there has been a  violation (or non-violation) and to authorise other paths to be taken by pr oposing the selection of other terms or lexical groups.151. Ho wever bold and seductive this proposal may be, it presupposes that the  professionals themselves (judges, lawyers, universities) take it over col-lec tively to test its feasibility and that they do not allow private operators alone , save for a few unchecked scientists, to design software and abstruse or locked modes of r easoning or calculation. 152. T he ambitious (and unfulfilled) promises of some legal tech companies must  not hide the immense potential of technologies and the need for appli-ca tions adapted and built in directly with scientific and academic research en vironments, as well as with all legal professionals, such as magistrates, cler ks, lawyers, notaries, bailiffs and field experts. A number of measures w ould appear to be able to take full advantage of these new tools through applica tions adapted and devised in direct association with research work-ers  and all legal professionals, including judges, prosecutors, clerks, lawyers, notar ies, bailiffs and experts in the field. 153. I n this dynamic context, it seems essential, firstly, not to make hasty decisions  and to take time to debate in advance the risks and practical appli-ca tions of these instruments for judicial systems, and to test them in the first stage . A judicial system in keeping with its time would be one that is capa-ble  of establishing, administering and guaranteeing genuine cyberethics for both  the public and private sectors, and insisting on total transparency and fair ness in the functioning of algorithms, which may contribute one day to judicial decision-mak ing.10.  The need for an in-depth public debate on these tools prior t o the implementation of public policies for their de velopment. The urgent need for cyberethics to provide a fr amework for the development of artificial intelligence algorithms while r especting fundamental rightsT he challenge of integrating these tools into a judicial decision-making pro-c ess justifies simplifying the concepts for the target group concerned. An ethic ess justifies simplifying the concepts for the target group concerned. An ethic al framework must be set up to promote the rapid development of a form of AI  c al framework must be set up to promote the rapid development of a form of AI  t hat includes mechanisms preventing bias and discrimination in its very design  t hat includes mechanisms preventing bias and discrimination in its very design  p rocesses. p rocesses. 
Page 60 ► European Commission for the Efficiency of Justice (CEPEJ)10.1.T he importance of debating, testing and continually r eviewing the application of these tools prior to the implemen tation of public policies154. T he issues connected with the implementation of predictive jus-tic e tools are so numerous and multifaceted that they call for a balanced appr oach by public decision-makers.155. F irstly, it is essential to hold a public debate around these questions, br inging together both the designers of the tools and legal professionals. Judicial  councils, professional associations of judges and bar associations can  undoubtedly contribute to this and help to identify opportunities and mor e controversial aspects. In addition, judicial training and law schools can pla y a key role in raising awareness among justice professionals on these issues , so that they can better understand and practically contribute to cur-r ent developments.156. I t is also essential to carry out research on the proposed applications and  test them, both to understand their potential and their weaknesses and t o be able to develop them further and adapt them to our needs. A right to e xamine the components and characteristics of the instruments proposed b y the private sector (or those developed by independent and specialised public  institutes, a solution which should be encouraged) seems equally impor tant so that the justice service can effectively carry out its mission. A r igorous evaluation of test results should be conducted before wider deploy-men t and integration into public policy. It also seems strongly advisable to r egularly assess the impact of these tools on the work of justice professionals.10.2. T he establishment of an ethical framework157. F irst of all, merely adopting a legislative or regulatory framework for AI  seems vain in a digital context, which is inherently transnational in its sc ope. On the other hand, scrupulous attention to the nature and quality of  open data is likely to minimise the risks of inadequate cross-referencing and  to reinforce the relevance of the results of automated processing. With r egard to the names of professionals, a simple precaution would be to pro-hibit  their public dissemination in structured raw databases given the risks of  misuse. It would not be a question of limiting access to the information alr eady processed (for example, the composition of a panel of judges) but of filt ering the raw data made freely available. In short, a distinction needs to be made  between access to information and access to databases, which can be manipula ted at will.
Appendix I – In-depth study on the use of AI in judicial systems ►  Page 61158.R esearchers Buttarelli and Marr stressed how big data need to be closely  controlled and protected. Other researchers (Pasquale and Morozov) ha ve stressed the need to establish transparent procedures for the deploy-men t of big data and more generally AI in the judicial field, because the solu-tions pr oposed can never represent life in its complexity.159. T he development of cyberethics rules to guide the activity of stake-holders  in the sector and to promote the above-mentioned principles of tr ansparency, fairness and neutrality of the tool is essential. Regular mon-it oring by independent experts should ensure that the drivers of artificial in telligence used to assist judges in their decisions are not biased. It is not inappr opriate to anticipate the implementation, discreet or not, of paid r eferencing systems (based on Google’s Search Engine Advertising model) allo wing certain operators to give less weight to decisions that are unfavour-able  to them. These rules will play a key role in increasing citizens’ confidence in their judicial sy stems.160. I n this connection, the quality of the best systems could be recognised b y the award of a label or certification. In particular, they must guarantee t otal transparency and perfect fairness in the way information is processed, both  for professionals and for citizens, to prevent the repetition of errors such  as the aforementioned COMPAS algorithm. Justice professionals must be  closely involved to be able to properly assess the risks and the impact of these applica tions on judicial systems.161. No wadays, all the experts involved in the development of AI, includ-ing  researchers, engineers and computer developers, have quite exceptional and  unprecedented responsibilities. Their work could be accompanied by an ev en greater strengthening of the humanities. The example of some innova-tiv e schools of computer developers shows that behind the will to “hack the sy stem” hides, according to some observers, in reality a pragmatism devoid of an y contextualisation of the responsibility that now hangs over technicians with  quasi-demiurge powers. The Hippocratic Oath certainly has its limits in medicine but r itualises responsibility and provides an ethical framework.162. F inally, cyberethics must be accompanied by large-scale training of stakeholders , from algorithm designers and legal tech companies to their users . New transdisciplinary humanities should be made available to all so tha t AI becomes a vector of positive development for humankind. 

► Page 63Appendix IIW hich uses of AI in European judicial sy stems?T his Annex to the Charter reviews different uses of AI in European systems and e ncourages to a different degree their application in the light of the principles and e ncourages to a different degree their application in the light of the principles and v alues set out in the Ethical Charter.v alues set out in the Ethical Charter.T he use of machine learning to constitute search engines for case-law enhance-T he use of machine learning to constitute search engines for case-law enhance-m ent is an opportunity to be taken up for all legal professionals. Additional appli-m ent is an opportunity to be taken up for all legal professionals. Additional appli-c ations (drawing up of scales, support for alternative dispute settlement measures, c ations (drawing up of scales, support for alternative dispute settlement measures, e tc.) should be considered, but due care must be taken (in particular, the quality of e tc.) should be considered, but due care must be taken (in particular, the quality of t he data source and not mass processing of the entire dispute in question). Other t he data source and not mass processing of the entire dispute in question). Other a pplications (“predictive justice”) should be assigned to the field of research and a pplications (“predictive justice”) should be assigned to the field of research and f urther development (in consultation with legal professionals in order to ensure f urther development (in consultation with legal professionals in order to ensure t hat they fully tie in with actual needs) before contemplating use on a significant t hat they fully tie in with actual needs) before contemplating use on a significant s cale in the public sphere.s cale in the public sphere.I n criminal matters, this is a very sensitive issue but it should not be ignored. In the I n criminal matters, this is a very sensitive issue but it should not be ignored. In the l ight of the many existing questions as to their compatibility with a certain num-l ight of the many existing questions as to their compatibility with a certain num-b er of fundamental rights, the use of algorithms to calculate the potential risks of b er of fundamental rights, the use of algorithms to calculate the potential risks of r ecidivism of an individual brought to justice should be considered with the most r ecidivism of an individual brought to justice should be considered with the most e xtreme reservations. On the other hand, the processing of global quantitative e xtreme reservations. On the other hand, the processing of global quantitative d ata for crime prevention is an avenue to be further explored with these new techd ata for crime prevention is an avenue to be further explored with these new techn iques, taking into consideration known biases (performative effects, data quality,  n iques, taking into consideration known biases (performative effects, data quality,  e tc.). Similarly, the use of algorithms to form a better link between the type of  e tc.). Similarly, the use of algorithms to form a better link between the type of  c ommunity service available and an individual’s personality may be a factor in the  c ommunity service available and an individual’s personality may be a factor in the  e ffectiveness of a measure of this kind. e ffectiveness of a measure of this kind. 
Page 64  ►  European Commission for the Efficiency of Justice (CEPEJ) Uses to be encouraged► Case-law enhancement : machine Learning techniques have been incr easingly deployed in the field of natural language processing in the  past years (this includes initial efforts in natural language under-standing)  and are a considerable asset for finding search options to c omplement current keyword or full-text search. These tools could link  various sources (e.g. constitutions and conventions, laws, case la w and legal theory). Data visualisation techniques could illustrate sear ch results.► Access to law: without replacing human intervention, chatbots could be  set up to facilitate access to the various existing sources of informa-tion  using natural language. Document templates (court applications, lease ag reements, etc.) could also be generated online.► Creation of new strategic tools : the use of data science and artificial in telligence techniques on court activity data can help improve the efficienc y of justice by making it possible, for example, to carry out quan titative and qualitative evaluations and to make projections (e.g. futur e human and budgetary resources). Key performance indicators c ould be drawn up on this basis. It is recommended that legal profes-sionals , especially judges, be involved in the implementation of these t ools, in terms of taking ownership of these tools and of analysing the r esults in conjunction with factors relating to the specific features of the  court in question or the quality of justice (for example, the need t o preserve access to justice).P ossible uses, requiring considerable metho dological precautions► Help in the drawing up of scales in certain civil disputes : an analysis of  all judicial decisions is not statistically meaningful if all the causative fac tors (explicit and implicit in the decisions) are not identified. Knowing tha t the average compensation awarded in a certain geographical area is  higher than in another can be explained not due to the behaviour of  judges, but in the light of the characteristics of the area in question. M achine learning can therefore be useful in identifying decisions  ( see Case-law enhancement above), but the automated processing  o f data alone cannot produce meaningful information. An essential  p rerequisite is the compilation of a relevant sample of decisions to  b e processed (for example, via surveys). 
Appendix II – In-depth study on the use of AI in judicial systems ►  Page 65►Support for alternative dispute settlement measures in civil matters: in  some European countries, “predictive justice” tools are used by insur ance companies to evaluate the chances of success of a dispute and  to steer the litigant towards another method of dispute resolu-tion  when it is felt that there is little chance of success. Furthermore, some  systems abroad offer compensation amounts without any real tr ansparency as to the rules of calculation. However, these systems cannot  be considered as impartial and reliable (see the section on machine  learning techniques). Decisions are made about a citizen using  truncated bases. In other cases, a litigant may be advised, by means  of a virtual agent (chatbot), to opt for an alternative dispute settlemen t measure after a preliminary examination of the criteria en tered by the litigant himself or herself, when visiting a court’s w ebsite or searching legal information online. The virtual agent may, wher e appropriate, also recommend that the litigant seek advice from a  mediation service or a lawyer. In all these cases, the presence of a tr ained third party (mediator using not only techniques but maybe scales  as calculated above, or a lawyer) would appear to be the most appr opriate solution at this stage.► Online dispute resolution : when litigants go onto an online dispute r esolution platform, they should be informed in a clear and compre-hensible  manner whether the processing of their dispute is done in an  entirely automated way or with the involvement of a mediator or  arbitrator. In addition, the information given to litigants must be honest  and must avoid giving them the impression that a court is in volved (in this connection, the term “online court” is often used for this  type of platform, whereas technically their purpose is to provide alt ernative dispute resolution services). These are two essential factors in  enabling litigants to make an informed choice, possibly disagreeing with  the advice and deciding to go to a real court within the meaning of  Article 6 of the ECHR. Furthermore, in view of the requirements of A rticles 6 and 13 of the ECHR, forms of review of the online dispute r esolution procedure and its outcome by state courts should always be  considered, especially where the litigant has consented to fully aut omated online dispute resolution.► The use of algorithms in criminal investigation in order to identify wher e criminal offences are being committed : this type of application c ould concern not only the police but also prosecutors in the crime pr evention bodies of which they are a part. Systems have been used in the  United States to guide police patrols in real time towards possible loca tions where offences are being committed. However, this type of 
Page 66  ►  European Commission for the Efficiency of Justice (CEPEJ) quantitative approach can generate a strong “performative effect” (in a g iven location, there is a greater chance of discovering an offence and this  then reinforces the system). Criminal analysis through approaches c ombining geographic information systems (GISs) and large amounts of  data on procedures could be better shared with prosecutors and c ould certainly benefit from a significant machine learning contribu-tion.  Anti-money laundering units already use “predictive” systems t o identify suspicious financial flows, but in the case of quantitative (financial)  information, machines are more able to produce reliable r esults. Researchers should also have better access to these data to pr oduce relevant studies for policy-makers.U ses to be considered following additional scien tific studies► Judge profiling: quantifying a judge’s activity will reveal less about any possible  biases than about any external factors influencing his or her decisions . The judge himself or herself is not the reason why judicial ac tivity in an impoverished area does not produce the same results as in  another territory, whatever his personality may be. When the deci-sion  is rendered in a collegial manner and without the possibility for a judge  to express a divergent opinion, it is pointless to profile each of the  judges of the chamber. On the other hand, offering judges a more detailed  quantitative and qualitative assessment of their activities, thanks  to new tools, but with a purely informative aim of assisting in decision-mak ing and for their exclusive use, could be encouraged.► Anticipating court decisions: statistical processing of lexical groups alone  reveals the frequency of the use of certain groups of words but does  not identify the real reasons for a decision and does not carry out  a legal analysis (see the study carried out on ECHR decisions by the  University College of London which produced better results on the  facts than on the analysis of the law). Hybrid systems, founded on  the construction of mathematical models that are supposed to r epresent the diverse range of judges’ reasoning, are not any more efficien t because they are still limited by bias in the data sample that they  have processed and have to start again from square one if a law is amended or if ther e is a reversal in case law.U ses to be considered with the most extreme reservations ► Use of algorithms in criminal matters in order to profile individu als: experiments in other countries (COMPAS in the United States  
Appendix II – In-depth study on the use of AI in judicial systems ►  Page 67and HART in the United Kingdom) have been criticised by NGOs (see w ork by ProPublica in the United States and Big Brother Watch in the UK ). Because of the limitations of the methodology used, this pure sta tistical approach has led to wrong result: the finding that some A frican-American individuals are more often involved in criminal acts has  led to a higher risk factor for the entire African-American popu-la tion. Thus, even for minor offences, these systems have negatively w eighted African-American defendants, with the result of unfairly incr easing the quantum of their sentences. This approach, which has discr iminatory and deterministic effects, must be replaced by one that is  more respectful of European standards concerning criminal sanctions and  which must offer the individual the possibility of rehabilitation and  reintegration. If algorithmic systems manage to help improve the c ollation of information for probation services, for example, and make it  possible for the relevant information to be collected more quickly f or subsequent human processing, then progress would definitely be  made (particularly in expedited proceedings). Any other use is pr one to biases which will come into conflict with certain national and supr a-national fundamental principles.► Quantity-based norm: it is not only a question of producing scales, which  could be legitimate, but of providing each judge with the con-t ent of the decisions produced by all the other judges and claiming t o lock his future choice into the mass of these “precedents” .  This appr oach should be rejected because this large number cannot add t o or act in place of the law. For the reasons provided above ( Help in the  drawing up of scales ), a quantity-based approach is not the way to go . The CEPEJ study also highlighted the dangers of the crystallisation of  case law and the potentially negative effects on the impartiality and independenc e of judges.

► Page 69Appendix IIIG lossaryT his glossary provides a definition of the terms used by the Ethical Charter and s tudy document. Preference has been given to providing a narrow definition for s tudy document. Preference has been given to providing a narrow definition for a ll the vocabulary used. All the documents must be read and understood in the a ll the vocabulary used. All the documents must be read and understood in the l ight of these definitions.l ight of these definitions.A AL GORITHMF inite sequence of formal rules (logical operations and instruc-tions)  making it possible to obtain a result from the initial input of informa-tion.  This sequence may be part of an automated execution process and dr aw on models designed through machine learning.ANON YMISATION M ethod to process personal data in order to completely and  irreversibly prevent the identification of a natural or legal person. Anony-misa tion therefore implies that there is no longer any possible link between the  information concerned and the person to whom it relates. Identification then  becomes completely impossible.84 As the principles relating to data pr otection apply to all information relating to an identified or identifiable individual , they do not apply to anonymised data. ARTIFICIAL  INTELLIGENCE (AI)  A set of scientific methods, theories and t echniques whose aim is to reproduce, by a machine, the cognitive abilities of  human beings. Current developments seek to have machines perform c omplex tasks previously carried out by humans. 8 4. Working Party on Article 29 Opinion 05/2014 on Anonymisation Techniques. See also  N o. 26 of Regulation (EU) 2016/679 of the European Parliament and of the Council of  2 7 April 2016. 
Page 70 ► European Commission for the Efficiency of Justice (CEPEJ)However, the term artificial intelligence is criticised by experts who distin-guish  between “strong” AIs (yet able to contextualise specialised and varied pr oblems in a completely autonomous manner) and “weak” or “moderate” AIs  (high performance in their field of training). Some experts argue that “ strong” AIs would require significant advances in basic research, and not just simple  improvements in the performance of existing systems, to be able to model the w orld as a whole. T he tools identified in this document are developed using machine learning methods , i. e. “weak” AIs.B BIG  DATA (metadata, large data sets) The term big data refers to large sets of  data from mixed sources (e.g. open data, proprietary data and commer-cially  purchased data). For data derived from judicial activity, big data could be  the combination of statistical data, records of business software connec-tions (applica tion logs), court decisions’ databases, etc.C CHA TBOT (conversational agent ) Conversational agent which converses with  its user (for example, empathy robots used to help those who are ill, or aut omated conversation services in customer relations).85D D ATA Representation of information for automatic processing. When it is said  that algorithms can be “applied” to the most diverse realities in the legal w orld or elsewhere, one presupposes the “digitalizability” of any reality in the f orm of “data” . But it is clear from physics that nothing tells us that physical pr ocesses can be adequately translated in terms of “data” (and integrated in to the input/output cycle of algorithms). If this is already the case in phys-ics , there is no reason why this should not also be the case in social relations. W e must therefore be cautious with the idea of “data” , which always assumes tha t the reality we are trying to describe has a format such that it is naturally algor ithmically processable.85. CNIL Report December 2017: How can humans keep the upper hand? The ethical matters r aised by algorithms and artificial intelligence.
Appendix III – In-depth study on the use of AI in judicial systems ►  Page 71DATABASE A database is a “container” that stores data such as numbers, da tes or words, which can be reprocessed using a computer to produce inf ormation, for example, gathering and sorting numbers and names to form a dir ectory.D ATA MINING Da ta mining makes it possible to analyse a large volume of da ta and highlight models, correlations and trends.D ATA SCIENCE A large field that groups together mathematics, statistics, pr obabilities, data processing and data visualisation in order to gain under-standing  from a mixed set of data (images, sound, text, genome data, links bet ween social networks, physical measurements, etc.). M ethods and tools derived from artificial intelligence fall within this category.DEEP LEARNING  S ee Machine Learning and NeuronsE EXPERT  SYSTEM T his is one of the ways to achieve artificial intelligence. An e xpert system is a tool capable of reproducing the cognitive mechanisms of  an expert in a particular field. More precisely, it is software capable of answ ering questions, by reasoning based on known facts and rules. It con-sists of 3 par ts: ► a fact base;► a rule base;► an inference engine.T he inference engine is able to use facts and rules to produce new facts, until it r eaches the answer to the expert question asked. M ost existing expert systems are based on formal logic mechanisms (Aristo-t elian logic) and use deductive reasoning.L LEGAL  TECH C ompanies using information technology in the field of law in or der to offer innovative legal services. These companies are start-ups spe-cialised  in law. Other terms derived from business sectors have also appeared such  as Fintechs for start-ups offering financial services and Medtechs in the medical field .
Page 72 ► European Commission for the Efficiency of Justice (CEPEJ)MM ACHINE LEARNING M achine learning makes it possible to construct a ma thematical model from data, incorporating a large number of variables tha t are not known in advance. The parameters are configured gradually dur ing the learning phase, which uses training data sets to find and classify links . The different methods of machine learning are chosen by the design-ers  depending on the nature of the tasks to be completed (grouping). These methods  are usually classified into three categories: (human) supervised lear ning, unsupervised learning and reinforcement learning. These three cat-egor ies group together different methods including neural networks, deep lear ning, etc.The gr aph below illustrates the different categories of machine learning:Real-time decisions Robot Navigation Learning Tasks Skill AcquisitionGame AIReinforcement Learning ClusteringRecommenderSystems Targetted Marketing CustomerSegmentation Dimensionalityreduction MeaningfulCompression Big Data Visualisation   FeatureElicitation StructureD iscoveryUnsupervised  Learning Advertising Popularity  Prediction Weather Forecasting Market Forecasting Estimating life expectancy PopulationGrowth Prediction Regression ClassificationDiagnostics CustomerRetention imageClassification Idenity Fraud Detection Supervised Learning  Machine  Learning 
Appendix III – In-depth study on the use of AI in judicial systems ►  Page 73METADATADa ta that make it possible to define, contextualise or describe other  data. In most of its computer uses, the meta prefix means “reference definition or descr iption” .M etadata synthesize basic information about data, they facilitate the search and  manipulation of particular data instances. The author, creation date, modifica tion date and file size are examples. Metadata and its corollary, data filt ering, help to locate a specific document.N NEUR ONS/NEURAL NETWORK  Neural networks are computing systems v aguely inspired by the biological neural networks that constitute animal br ains.[1] Such systems “learn” to perform tasks by considering examples, gener ally without being programmed with any task-specific rules. For exam-ple , in image recognition, they might learn to identify images that contain ca ts by analyzing example images that have been manually labeled as “cat” or  “no cat” and using the results to identify cats in other images. They do this  without any prior knowledge about cats, e.g., that they have fur, tails, whiskers  and cat-like faces. Instead, they automatically generate identifying char acteristics from the learning material that they process. A n ANN is based on a collection of connected units or nodes called artificial neur ons which loosely model the neurons in a biological brain. Each con-nec tion, like the synapses in a biological brain, can transmit a signal from one  artificial neuron to another. An artificial neuron that receives a signal can pr ocess it and then signal additional artificial neurons connected to it. T he original goal of the ANN approach was to solve problems in the same w ay that a human brain would. However, over time, attention moved to per-f orming specific tasks, leading to deviations from biology. Artificial neural net works have been used on a variety of tasks, including computer vision, speech  recognition, machine translation, social network filtering, playing boar d and video games and medical diagnosis.O OPEN  DATA The term refers to making structured databases available for public  download. These data can be inexpensively re-used subject to the t erms of a specific licence, which can, in particular, stipulate or prohibit cer-tain pur poses of re-use.
Page 74 ► European Commission for the Efficiency of Justice (CEPEJ)Open data should not be confused with unitary public information available on  websites, where the entire database cannot be downloaded (for exam-ple , a database of court decisions). Open data do not replace the manda-t ory publication of specific administrative or judicial decisions or measures alr eady laid down by certain laws or regulations.Lastly , there is sometimes confusion between data (strictly speaking open da ta) and their processing methods (machine learning, data science) for dif-f erent purposes (search engines, assistance in drafting documents, analysis of tr ends of decisions, predicting court decisions, etc.).OPEN  SOURCE SOFTWARE  Software for which the source code is avail-able  to everyone. The software can therefore be freely used, modified and r edistributed.P PERSONAL  DATA Any information concerning an identified or identifiable na tural person (the “person concerned”), directly or indirectly.T hese include sensitive data relating to genetic data, biometric data uniquely iden tifying an individual, data relating to offences, criminal proceedings and c onvictions and related security measures, and any data for information they r eveal on racial or ethnic origin, political opinions, trade union membership, r eligious or other beliefs, health or sex life.PREDIC TIVE JUSTICE P redictive justice is the analysis of large amounts of judicial  decisions by artificial intelligence technologies in order to make pre-dic tions for the outcome of certain types of specialised disputes (for exam-ple , redundancy payments or alimentary pensions).T he term “predictive” used by legal tech companies comes from the branches of  science (principally statistics) that make it possible to predict future results thr ough inductive analysis. Judicial decisions are processed with a view to det ecting correlations between input data (criteria set out in legislation, the fac ts of the case and the reasoning) and output data (formal judgment such as the c ompensation amount). C orrelations deemed to be relevant make it possible to create models which, when  used with new input data (new facts or precisions described as a par ameter, such as the duration of the contractual relationship), produce ac cording to their developers a prediction of the decision (for example, the c ompensation range).
Appendix III – In-depth study on the use of AI in judicial systems ►  Page 75Some authors have criticised both the form and substance of this approach.T hey argue that, in general, the mathematical modelling of certain social phenomena  is not a task comparable to other more easily quantifiable activ-ities  (isolating the really causative factors of a court decision is infinitely more c omplex than playing the game of Go or recognising an image for example): her e, there is a much higher risk of false correlations. In addition, in legal the-or y, two contradictory decisions can prove to be valid if the legal reasoning is  sound. Consequently, making predictions would be a purely informative e xercise without any prescriptive claim.PR OFILINGA n automated data processing technique that consists of apply-ing  a “profile” to a natural person, in particular in order to make decisions about  him or her or to analyze or predict personal preferences, behaviours and a ttitudes.PR OCESSING OF PERSONAL DATA A ccording to Article 2 of the revised C onvention 108, “data processing” means any operation or set of operations per formed on personal data, such as the collection, storage, preservation, alt eration, retrieval, disclosure, making available, erasure, or destruction of, or the car rying out of logical and/or arithmetical operations on such data.PSEUDON YMISATION P ursuant to Article 4 of the GDPR, this is the process-ing  of personal data in such a manner that they can no longer be attributed t o a specific data subject without the use of additional information, provided tha t such additional information is kept separately and is subject to tech-nical  and organisational measures to ensure that the personal data are not a ttributed to an identified or identifiable natural person.8686. Article 4 of Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 A pril 2016.
Page 76  ► Appendix IVC hecklist for integrating the C harter’s principles into y our processing methodI n order to assess the compatibility of your processing method with the Char-t er, there is a self-evaluation scale available for each of the principles listed.F or each principle, tick the box that corresponds to your processing methods.T he box furthest to the left indicates complete integration, the box furthest t o the right indicates no integration.A t the bottom of the evaluation sheet (in the line marked “Total”), add up the number  of boxes ticked. The column with the highest score indicates your pr ocessing method’s level of compatibility with the Charter.T his evaluation is of course purely informative and by no means equates to an y certification.grati Pri nciple  i ntegrated Principle  not  i ntegrated 
► Page 77Checklist for evaluating y our processing methodsP rinciple of fundamental rights: Ensur e that the design and implementation of artificial intelligence tools and ser vices are compatible with fundamental rights, including the right to pro-t ection of personal dataP rinciple of non-discrimination: Specifically  prevent the development or intensification of any discrimination bet ween individuals or groups of individuals P rinciple of quality and security: W ith regard to the processing of judicial decisions and data, use certified sour ces and intangible data with models elaborated in a secure technologi-cal en vironmentP rinciple of transparency, impartiality and fairness: M ake data processing methods accessible and understandable, authorise e xternal audits P rinciple “under user control”: Preclude a prescriptive approach and ensure tha t users are informed actors and in control of their choicesC ompatible with the  C harter  Measures to be taken to b e compatible Not compatible with t he CharterTOTAL -+ - Checklist for evaluating your processing methods C hecklist for evaluating your processing methods C hecklist for evaluating your processing methods C hecklist for evaluating your processing methods 
PREMS 005419 EN GT he Council of Europe is the continent’s leading human r ights organisation. It comprises 47 member stat es, including all members of the European Union. All C ouncil of Europe member states have sig ned up to the European Convention on Human R ights, a treaty designed to protect human rights, democrac y and the rule of law. The European Court of Human R ights oversees the implementation of the C onvention in the member states.www.coe.intThe Charter provides a framework of principles that can guide policy makers , legislators and justice professionals when they grapple with the  rapid development of Artificial Intelligence in national judicial pr ocesses.T he CEPEJ’s view is that the application of Artificial Intelligence in the  field of justice can contribute to improve the efficiency and qualit y. It must be implemented in a responsible manner which c omplies with the fundamental rights guaranteed in particular in the E uropean Convention on Human Rights (ECHR) and the Council of E urope Convention on the Protection of Personal Data. It is essential t o ensure that Artificial Intelligence remains a tool in the service of the gener al interest and that its use respects individual rights.T he Charter defines five core principles to be respected in the field of  Artificial Intelligence and justice: respect of fundamental rights; nondiscrimination; quality and security; transparency, impartiality and  fairness;  “under user control” . The Charter is accompanied by a n indepth study on the use of Artificial Intelligence in judicial systems.w ww.coe.int/cepej
