
    
<!DOCTYPE html>




<html lang="en" >
<head >
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Mobile properties -->
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
    <!-- Stylesheets -->
    <link rel="stylesheet" href="/pmc/static/CACHE/css/output.4404d67389b1.css" type="text/css">
  
  <link rel="stylesheet" href="/pmc/static/CACHE/css/output.484f3c634d37.css" type="text/css"><link rel="stylesheet" href="/pmc/static/CACHE/css/output.859ea76d5b06.css" type="text/css"><link rel="stylesheet" href="/pmc/static/CACHE/css/output.3766d7ad0d2d.css" type="text/css"><link rel="stylesheet" href="/pmc/static/CACHE/css/output.e3c3c2c84eb3.css" type="text/css">

  
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"/>
  
<link type="text/css" href="/pmc/static/bundles/base/base.a2ef7ca69e4b20dff539.css" rel="stylesheet" />


    <link rel="stylesheet" href="/corehtml/pmc/css/fonts/stix/stixfonts.css" type="text/css" /><link rel="stylesheet" href="/corehtml/pmc/css/3.18/pmcrefs1.min.css" type="text/css" /><link rel="stylesheet" href="/corehtml/pmc/css/pmc2020_1.1/ncbi_web.min.css?_=a" type="text/css" /><style type="text/css">.pmc-wm {background:transparent repeat-y top left;background-image:url(/corehtml/pmc/pmcgifs/wm-journal.gif);background-size: auto, contain}</style><style type="text/css">.print-view{display:block}</style>

    <link type="text/css" href="/pmc/static/bundles/article/article.3c5d12ff2e4beea20136.css" rel="stylesheet" />
    <link type="text/css" href="/pmc/static/django_pmc_cite_box/lib/cite-box.css" rel="stylesheet" />


    <title>Critiquing the Reasons for Making Artificial Moral Agents - PMC</title>

  
  <!-- Favicons -->
  <link rel="shortcut icon" type="image/ico" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico" />
  <link rel="icon" type="image/png" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.png" />

  <!-- 192x192, as recommended for Android
  http://updates.html5rocks.com/2014/11/Support-for-theme-color-in-Chrome-39-for-Android
  -->
  <link rel="icon" type="image/png" sizes="192x192" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-192.png" />

  <!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
  <link rel="apple-touch-icon-precomposed" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-57.png">
  <!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-72.png">
  <!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-114.png">
  <!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-144.png">



    
        <!-- Logging params: Pinger defaults -->

<meta name="ncbi_app" content="pmc-frontend" />

<meta name="ncbi_db" content="pmc" />

<meta name="ncbi_phid" content="939B8E155A3E87E5000040217BBC208D.1.m_2" />


        <!-- Logging params: Pinger custom -->

<meta name="ncbi_pdid" content="article" />

<meta name="ncbi_op" content="retrieved" />

<meta name="ncbi_app_version" content="0.0" />

<meta name="ncbi_domain" content="springeropen" />

<meta name="ncbi_type" content="fulltext" />

<meta name="ncbi_pcid" content="/articles/PMC6591188/" />


    


    <script>
        
            var ncbiBaseUrl = "https://www.ncbi.nlm.nih.gov";
        
        var useOfficialGovtHeader = true;
    </script>


    <meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE" /><link rel="canonical" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6591188/" /><link rel="schema.DC" href="http://purl.org/DC/elements/1.0/" /><meta name="citation_journal_title" content="Science and Engineering Ethics" /><meta name="citation_title" content="Critiquing the Reasons for Making Artificial Moral Agents" /><meta name="citation_author" content="Aimee van Wynsberghe" /><meta name="citation_author_institution" content="Technical University of Delft, Jaffalaan 5, 2628 BX Delft, Netherlands" /><meta name="citation_author" content="Scott Robbins" /><meta name="citation_author_institution" content="Technical University of Delft, Jaffalaan 5, 2628 BX Delft, Netherlands" /><meta name="citation_publication_date" content="2019" /><meta name="citation_issue" content="3" /><meta name="citation_volume" content="25" /><meta name="citation_firstpage" content="719" /><meta name="citation_doi" content="10.1007/s11948-018-0030-8" /><meta name="citation_abstract_html_url" content="/pmc/articles/PMC6591188/?report=abstract" /><meta name="citation_fulltext_html_url" content="/pmc/articles/PMC6591188/" /><meta name="citation_pmid" content="29460081" /><meta name="DC.Title" content="Critiquing the Reasons for Making Artificial Moral Agents" /><meta name="DC.Type" content="Text" /><meta name="DC.Publisher" content="Springer" /><meta name="DC.Contributor" content="Aimee van Wynsberghe" /><meta name="DC.Contributor" content="Scott Robbins" /><meta name="DC.Date" content="2019" /><meta name="DC.Identifier" content="10.1007/s11948-018-0030-8" /><meta name="DC.Language" content="en" /><meta property="og:title" content="Critiquing the Reasons for Making Artificial Moral Agents" /><meta property="og:type" content="article" /><meta property="og:description" content="Many industry leaders and academics from the field of machine ethics would have us believe that the inevitability of robots coming to have a larger role in our lives demands that robots be endowed with moral reasoning capabilities. Robots endowed in this ..." /><meta property="og:url" content="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6591188/" /><meta property="og:site_name" content="PubMed Central (PMC)" /><meta property="og:image" content="https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0" /><meta name="twitter:card" content="summary_large_image" /><meta name="twitter:site" content="@ncbi" />


</head>
<body >

   
    
        <button
          class="back-to-top back-to-top--bottom"
          data-ga-category="pagination"
          data-ga-action="back_to_top">
          Back to Top
        </button>
    

    <a class="usa-skipnav" href="#main-content">Skip to main content</a>
    <!-- ========== BEGIN HEADER ========== -->
<section class="usa-banner" style="display: none;">
  <div class="usa-accordion">
    <header class="usa-banner-header">
      <div class="usa-grid usa-banner-inner">
        <img src="https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png" alt="U.S. flag" />
        <p>An official website of the United States government</p>
        <button
          class="usa-accordion-button usa-banner-button"
          aria-expanded="false"
          aria-controls="gov-banner-top"
        >
          <span class="usa-banner-button-text">Here's how you know</span>
        </button>
      </div>
    </header>
    <div
      class="usa-banner-content usa-grid usa-accordion-content"
      id="gov-banner-top"
    >
      <div class="usa-banner-guidance-gov usa-width-one-half">
        <img
          class="usa-banner-icon usa-media_block-img"
          src="https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg"
          alt="Dot gov"
        />
        <div class="usa-media_block-body">
          <p>
            <strong>The .gov means it’s official.</strong>
            <br />
            Federal government websites often end in .gov or .mil. Before
            sharing sensitive information, make sure you’re on a federal
            government site.
          </p>
        </div>
      </div>
      <div class="usa-banner-guidance-ssl usa-width-one-half">
        <img
          class="usa-banner-icon usa-media_block-img"
          src="https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg"
          alt="Https"
        />
        <div class="usa-media_block-body">
          <p>
            <strong>The site is secure.</strong>
            <br />
            The <strong>https://</strong> ensures that you are connecting to the
            official website and that any information you provide is encrypted
            and transmitted securely.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<div class="usa-overlay"></div>
<header class="ncbi-header" role="banner" data-section="Header">

	<div class="usa-grid">
		<div class="usa-width-one-whole">

            <div class="ncbi-header__logo">
                <a href="https://www.ncbi.nlm.nih.gov/" class="logo" aria-label="NCBI Logo" data-ga-action="click_image" data-ga-label="NIH NLM Logo">
                  <img src="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg" alt="NIH NLM Logo" />
                </a>
            </div>

			<div class="ncbi-header__account">
				<a id="account_login" href="https://account.ncbi.nlm.nih.gov" class="usa-button header-button" style="display:none" data-ga-action="open_menu" data-ga-label="account_menu">Log in</a>
				<button id="account_info" class="header-button" style="display:none"
						aria-controls="account_popup">
					<span class="fa fa-user" aria-hidden="true"></span>
					<span class="username desktop-only" aria-hidden="true" id="uname_short"></span>
					<span class="sr-only">Show account info</span>
				</button>
			</div>

			<div class="ncbi-popup-anchor">
				<div class="ncbi-popup account-popup" id="account_popup" aria-hidden="true">
					<div class="ncbi-popup-head">
						<button class="ncbi-close-button" data-ga-action="close_menu" data-ga-label="account_menu"><span class="fa fa-times"></span><span class="usa-sr-only">Close</span></button>
						<h4>Account</h4>
					</div>
					<div class="account-user-info">
						Logged in as:<br/>
						<b><span class="username" id="uname_long">username</span></b>
					</div>
					<div class="account-links">
						<ul class="usa-unstyled-list">
							<li><a id="account_myncbi" href="/myncbi/" class="set-base-url" data-ga-action="click_menu_item" data-ga-label="account_myncbi">Dashboard</a></li>
							<li><a id="account_pubs" href="/myncbi/collections/bibliography/" class="set-base-url" data-ga-action="click_menu_item" data-ga-label="account_pubs">Publications</a></li>
							<li><a id="account_settings" href="/account/settings/" class="set-base-url" data-ga-action="click_menu_item" data-ga-label="account_settings">Account settings</a></li>
							<li><a id="account_logout" href="/account/signout/" class="set-base-url" data-ga-action="click_menu_item" data-ga-label="account_logout">Log out</a></li>
						</ul>
					</div>
				</div>
			</div>

		</div>
	</div>
</header>
<div role="navigation" aria-label="access keys">
<a id="nws_header_accesskey_0" href="https://www.ncbi.nlm.nih.gov/guide/browsers/#ncbi_accesskeys" class="usa-sr-only" accesskey="0" tabindex="-1">Access keys</a>
<a id="nws_header_accesskey_1" href="https://www.ncbi.nlm.nih.gov" class="usa-sr-only" accesskey="1" tabindex="-1">NCBI Homepage</a>
<a id="nws_header_accesskey_2" href="/myncbi/" class="set-base-url usa-sr-only" accesskey="2" tabindex="-1">MyNCBI Homepage</a>
<a id="nws_header_accesskey_3" href="#maincontent" class="usa-sr-only" accesskey="3" tabindex="-1">Main Content</a>
<a id="nws_header_accesskey_4" href="#" class="usa-sr-only" accesskey="4" tabindex="-1">Main Navigation</a>
</div>
<section data-section="Alerts">
	<div class="ncbi-alerts-placeholder"></div>
</section>
<!-- ========== END HEADER ========== -->

    
    


    
        

<section class="pmc-alerts">
    
    <div role="alert" class="pmc-alert pmc-alert--info" role="region" aria-label="Alert" aria-hidden="false"  data-key="pmc-alert-welcome">
        <div class="pmc-alert__body pmc-alert--ncbi-icon">
            <div class="pmc-alert__content">
                <p>
                    <strong>
                        The PMC website is updating on October 15, 2024.
                        <a href="https://ncbiinsights.ncbi.nlm.nih.gov/2024/03/14/preview-pmc-improvements/" data-ga-category="cloud_beta_banner" data-ga-label="learn_more" data-ga-action="on_pmc2020">Learn More</a> or
                        <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6591188/" data-ga-category="cloud_beta_banner" data-ga-action="on_pmc2020" data-ga-label="go_to_cloud">Try it out now</a>.
                    </strong>
                </p>
            </div>
        </div>
    </div>
    
</section>
    

    
    
        
        <header class="pmc-header usa-header-extended" role="banner">
    <div class="pmc-header__bar">
        <div class="pmc-header__control usa-accordion">
            
                <button class="usa-menu-btn pmc-header--button pmc-header--left">
                    <i class="fa fa-ellipsis-v" aria-hidden="true"></i>
                </button>
            

            <div class="usa-logo pmc-header__logo pmc-header--stretch
                
               " id="extended-mega-logo">
                <div class="usa-logo-text">
                    <a href="/pmc/" title="Home" aria-label="Home" ></a>
                </div>
            </div>
            <button class="usa-accordion-button pmc-header--right pmc-header--button pmc-header__search--control pmc-header--right" aria-expanded="false" aria-controls="a1">
                <i class="fa fa-search" aria-hidden="true"></i>
                <i class="fa fa-times" aria-hidden="true"></i>
            </button>
        </div>
        <div class="pmc-header__search usa-accordion-content" id="a1">
            <div role="search"  class="pmc-header--stretch"  >
    <form class="usa-search " autocomplete="off">
        <div>
              <label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
              <span class="clearable">
                <input
                  required="required"
                  autocomplete-url="/pmc/autocomplete/pmc/"
                  placeholder="Search PMC Full-Text Archive"
                  id="pmc-search" type="search" name="term"/>
                  <i class="clear-btn" ></i>
              </span>
              <button type="submit" formaction="/pmc/">
                <span class="usa-search-submit-text">Search in PMC</span>
              </button>
        </div>
    </form>
</div>


            <ul class="usa-unstyled-list usa-nav-secondary-links pmc-header--offset-two">
                    <li>
                        <a href="https://www.ncbi.nlm.nih.gov/pmc/advanced" data-ga-action="featured_link" data-ga-label="advanced_search">
                            Advanced Search
                        </a>
                    </li>
                    <li>
                        <a  href="/pmc/about/userguide/" data-ga-action="featured_link"
                        data-ga-label="user guide">
                            User Guide
                        </a>
                    </li>
            </ul>
        </div>
    </div>
     <nav role="navigation" class="usa-nav ">
        <button class="usa-nav-close">
            <i class="fa fa-times" aria-hidden="true"></i>
        </button>
        <div class="usa-breadcrumb usa-breadcrumb--wrap usa-breadcrumb--hack">
             
    <ul class="usa-breadcrumb__list">
            
                <li class="usa-breadcrumb__list-item"
                    >
                    <a href="/pmc/journals/" class="navlink">Journal List</a>
                </li>
            
                <li class="usa-breadcrumb__list-item"
                    >
                    <a href="/pmc/?term=Springer[filter]" class="navlink">Springer</a>
                </li>
            
                <li class="usa-breadcrumb__list-item"
                     aria-current="page" >
                    PMC6591188
                </li>
            
    </ul>
 

        </div>
        
        
            <div class="pmc-sidebar pmc-sidebar-hack">
                

<div class="scroller">

    
        <section>
                <h6>Other Formats</h6>
                <ul class="pmc-sidebar__formats">
                  <li class="pdf-link other_item"><a href="/pmc/articles/PMC6591188/pdf/11948_2018_Article_30.pdf" class="int-view">PDF (677K)</a></li>
                </ul>
        </section>
    
    <section>
        <h6>Actions</h6>
        <ul class="pmc-sidebar__actions">
            <li>
                <button role="button" class="citation-button citation-dialog-trigger ctxp"
                        aria-label="Open dialog with citation text in different styles" data-ga-category="save_share" data-ga-action="cite" data-ga-label="open"
                        data-all-citations-url="/pmc/resources/citations/6591188/"
                        data-citation-style="nlm"
                        data-download-format-link="/pmc/resources/citations/6591188/export/"
                >
                    <span class="button-label">Cite</span>
                </button>
            </li>
            <li>
                
                    

<link type="text/css" href="ncbi-overlay-block/src/overlay-block.css">
<div class="collections-button-container" data-article-id="6591188" data-article-db="pmc">
  <button class="collections-button collections-dialog-trigger"
          aria-label="Save article in MyNCBI collections."
          data-ga-category="collections_button"
          data-ga-action="click"
          data-ga-label="collections_button"
          data-collections-open-dialog-enabled="false"
          data-collections-open-dialog-url="https://www.ncbi.nlm.nih.gov/account?back_url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6591188%2F%23open-collections-dialog"
          data-in-collections="false">
      <span class="button-label">Collections</span>
  </button>
  <div class="overlay" role="dialog">
  <div id="collections-action-dialog"
       class="dialog collections-dialog"
       aria-hidden="true">
    <div class="title">Add to Collections</div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/pmc/list-existing-collections/"
      data-add-to-existing-collection-url="/pmc/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/pmc/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

  <input type="hidden" name="csrfmiddlewaretoken" value="5tFSprKPu5gcS4N43l6QHkkGZ7m6xgcd2McYOXv8DN5iHZaHXwRmiynh6UTaXYix">

  

  <div class="choice-group" role="radiogroup">
    <ul class="radio-group-items">
      <li>
        <input type="radio"
               id="collections-action-dialog-new-header "
               class="collections-new"
               name="collections"
               value="new"
               data-ga-category="collections_button"
               data-ga-action="click"
               data-ga-label="collections_radio_new">
        <label for="collections-action-dialog-new-header ">Create a new collection</label>
      </li>
      <li>
        <input type="radio"
               id="collections-action-dialog-existing-header "
               class="collections-existing"
               name="collections"
               value="existing"
               checked="true"
               data-ga-category="collections_button"
               data-ga-action="click"
               data-ga-label="collections_radio_existing">
        <label for="collections-action-dialog-existing-header ">Add to an existing collection</label>
      </li>
    </ul>
  </div>

  <div class="controls-wrapper">
    <div class="action-panel-control-wrap new-collections-controls">
      <label for="collections-action-dialog-add-to-new" class="action-panel-label required-field-asterisk">
        Name your collection:
      </label>
      <input
        type="text"
        name="add-to-new-collection"
        id="collections-action-dialog-add-to-new"
        class="collections-action-add-to-new"
        pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
        maxlength=""
        data-ga-category="collections_button"
        data-ga-action="create_collection"
        data-ga-label="non_favorties_collection">
      <div class="collections-new-name-too-long usa-input-error-message selection-validation-message">
        Name must be less than  characters
      </div>
    </div>
    <div class="action-panel-control-wrap existing-collections-controls">
      <label for="collections-action-dialog-add-to-existing" class="action-panel-label">
        Choose a collection:
      </label>
      <select id="collections-action-dialog-add-to-existing"
              class="action-panel-selector collections-action-add-to-existing"
              data-ga-category="collections_button"
              data-ga-action="select_collection"
              data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
      </select>
      <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
        Unable to load your collection due to an error<br>
        <a href="#">Please try again</a>
      </div>
    </div>
  </div>

  <div class="action-panel-actions">
    <button class="action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
      Add
    </button>
    <button class="action-panel-cancel"
            aria-label="Close 'Add to Collections' panel"
            ref="linksrc=close_collections_panel"
            aria-controls="collections-action-panel"
            aria-expanded="false"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="cancel">
      Cancel
    </button>
  </div>
</form>
    </div>
  </div>
</div>
</div>
                
            </li>

        </ul>
    </section>
    
        <section class="social-sharing">
            <h6>Share</h6>
            <ul class="pmc-sidebar__share">
                <li><a class="fa-stack fa-lg" target="_blank" rel="noopener noreferrer" role="button" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6591188%2F&amp;text=Critiquing%20the%20Reasons%20for%20Making%20Artificial%20Moral%20Agents" alt="Share on Twitter"><i class="fa fa-twitter fa-stack-1x">&#160;</i></a></li> 
<li><a class="fa-stack fa-lg" target="_blank" rel="noopener noreferrer" role="button" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6591188%2F" alt="Share on Facebook"><i class="fa fa-facebook fa-stack-1x">&#160;</i></a></li>
                <li>
                    <div class="share-permalink">
                        <button class="trigger"  alt="Show article permalink" aria-expanded="false" aria-haspopup="true">
                            <i class="fa-stack fa-lg" >
                                <i class="fa fa-link fa-stack-1x">&nbsp;</i>
                            </i>
                        </button>
                        <div class="dropdown dropdown-container" hidden>
                              <div class="title">
                                Permalink
                              </div>
                              <div class="content">
                                  <input type="text" class="permalink-text" value="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6591188/" aria-label="Article permalink"><button class="permalink-copy-button usa-button-primary" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                                      <span class="button-title">Copy</span>
                                  </button>
                              </div>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
    
    <section>
        <h6>RESOURCES</h6>
        <ul class="pmc-sidebar__resources">
        
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_similar_articles"
                            data-ga-label="/pmc/articles/PMC6591188/"
                            class="usa-accordion-button"
                            aria-controls="similar-articles-accordion-header"
                            aria-expanded="false"
                            data-action-open="open_similar_articles"
                            data-action-close="close_similar_articles"
                    >
                        Similar articles
                    </button>
                    <div
                            
                                data-source-url="/pmc/resources/similar-article-links/29460081/"
                            

                         class="usa-accordion-content pmc-sidebar__resources--citations" id="similar-articles-accordion-header" aria-hidden="true">
                        
                    </div>
                </div>
            </li>
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_cited_by"
                            data-ga-label="/pmc/articles/PMC6591188/"
                            class="usa-accordion-button"
                            aria-controls="cited-by-accordion-header"
                            aria-expanded="false"
                            data-action-open="open_cited_by"
                            data-action-close="close_cited_by"
                    >
                        Cited by other articles
                    </button>
                    <div
                            
                                data-source-url="/pmc/resources/cited-by-links/29460081/"
                            
                            class="usa-accordion-content pmc-sidebar__resources--citations"
                            id="cited-by-accordion-header"
                            aria-hidden="true"
                    >
                        
                    </div>
                </div>
            </li>
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/pmc/articles/PMC6591188/"
                            class="usa-accordion-button"
                            aria-controls="links-accordion-header"
                            aria-expanded="false"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                    >
                        Links to NCBI Databases
                    </button>
                    <div data-source-url="/pmc/resources/db-links/6591188/" class="usa-accordion-content" id="links-accordion-header" aria-hidden="true"></div>
                </div>
            </li>

            
        
        </ul>
    </section>

 </div>
            </div>
        

    </nav>

</header>

    
    

    <div class="usa-overlay"></div>
    
<main id="main-content" class="usa-grid usa-layout-docs pmc-main">
    <article class="usa-width-three-fourths usa-layout-docs-main_content pmc-article">
        <section class="usa-breadcrumb usa-breadcrumb--wrap">
         
    <ul class="usa-breadcrumb__list">
            
                <li class="usa-breadcrumb__list-item"
                    >
                    <a href="/pmc/journals/" class="navlink">Journal List</a>
                </li>
            
                <li class="usa-breadcrumb__list-item"
                    >
                    <a href="/pmc/?term=Springer[filter]" class="navlink">Springer</a>
                </li>
            
                <li class="usa-breadcrumb__list-item"
                     aria-current="page" >
                    PMC6591188
                </li>
            
    </ul>
 

        </section>
        
  <div class="pmc-article__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/pmc/about/disclaimer/">PMC Disclaimer</a>
    |
    <a data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/pmc/about/copyright/">
        PMC Copyright Notice
    </a>
</div>

        <section class="pmc-page-banner" role="banner">
            
                
                    <div><a href="//doi.org/10.1007%2Fs11948-018-0030-8" title="Link to Publisher's site" target="_blank" rel="noopener noreferrer" ref="reftype=publisher&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CBanner&amp;TO=Publisher%7COther%7CN/A"><img src="/corehtml/pmc/pmcgifs/logo-springeropen.png" alt="Logo of springeropen"><!-- --></img></a></div>
                
            
        </section>
        <section  role="document">
            
                <div id="mc" class=" article lit-style content pmc-wm slang-all page-box"><!--main-content--><div class="jig-ncbiinpagenav" data-jigconfig="smoothScroll: false, allHeadingLevels: ['h2'], headingExclude: ':hidden,.nomenu'"><div class="fm-sec half_rhythm no_top_margin"><div class="fm-flexbox"><div class="fm-citation"><div class="citation-default"><div class="part1"><span id="pmcmata">Sci Eng Ethics.</span> 2019; 25(3): 719–735. </div><div class="part2"><span class="fm-vol-iss-date">Published online 2018 Feb 19. </span>  <span class="doi"><span>doi: </span><a href="//doi.org/10.1007%2Fs11948-018-0030-8" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CFront%20Matter&amp;TO=Content%20Provider%7CCrosslink%7CDOI">10.1007/s11948-018-0030-8</a></span></div></div></div><div class="fm-ids"><div class="fm-citation-pmcid"><span class="fm-citation-ids-label">PMCID: </span><span>PMC6591188</span></div><div class="fm-citation-pmid">PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/29460081">29460081</a></div></div></div><h1 class="content-title">Critiquing the Reasons for Making Artificial Moral Agents</h1><div class="half_rhythm"><div class="contrib-group fm-author"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=van%20Wynsberghe%20A%5BAuthor%5D" class="affpopup" co-rid="_co_idm140382828867952" co-class="co-affbox">Aimee van Wynsberghe</a><sup><img src="/corehtml/pmc/pmcgifs/corrauth.gif" alt="corresponding author" /></sup><sup></sup> and  <a href="https://pubmed.ncbi.nlm.nih.gov/?term=Robbins%20S%5BAuthor%5D" class="affpopup" co-rid="_co_idm140382834791952" co-class="co-affbox">Scott Robbins</a><sup></sup></div><div style="display:none" class="contrib-group aff-tip"><div id="_co_idm140382828867952"><h3 class="no_margin">Aimee van Wynsberghe</h3><p>Technical University of Delft, Jaffalaan 5, 2628 BX Delft, Netherlands </p><div>Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=van%20Wynsberghe%20A%5BAuthor%5D">Aimee van Wynsberghe</a></div></div><div id="_co_idm140382834791952"><h3 class="no_margin">Scott Robbins</h3><p>Technical University of Delft, Jaffalaan 5, 2628 BX Delft, Netherlands </p><div>Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=Robbins%20S%5BAuthor%5D">Scott Robbins</a></div></div></div></div><div class="half_rhythm"><div class="togglers fm-copyright-license"><a href="#" class="pmctoggle" rid="idm140382839501168_ai">Author information</a> <a href="#" class="pmctoggle" rid="idm140382839501168_an">Article notes</a> <a href="#" class="pmctoggle" rid="idm140382839501168_cpl">Copyright and License information</a> <a href="/pmc/about/disclaimer/" style="margin-left: 1em">PMC Disclaimer</a></div><div class="fm-authors-info hide half_rhythm" id="idm140382839501168_ai" style="display:none"><div class="fm-affl" id="Aff1">Technical University of Delft, Jaffalaan 5, 2628 BX Delft, Netherlands </div><div><span class="fm-affl">Aimee van Wynsberghe, </span><span class="fm-affl"><span class="email-label">Email: </span><a href="mailto:dev@null" data-email="moc.liamg@tobornaveemia" class="oemail">moc.liamg@tobornaveemia</a></span>.</div><div><a href="#article-aaff-info">Contributor Information</a>.</div><div><sup><img src="/corehtml/pmc/pmcgifs/corrauth.gif" alt="corresponding author" /></sup>Corresponding author.</div></div><div class="fm-article-notes hide half_rhythm" id="idm140382839501168_an" style="display:none"><div class="fm-pubdate half_rhythm">Received 2017 Nov 28; Accepted 2018 Feb 8.</div></div><div class="permissions half_rhythm hide" id="idm140382839501168_cpl" style="display:none"><div class="fm-copyright half_rhythm"><a href="/pmc/about/copyright/">Copyright</a> &#x000a9; The Author(s) 2018</div><div class="license half_rhythm"><strong>Open Access</strong>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<a href="http://creativecommons.org/licenses/by/4.0/" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CFront%20Matter&amp;TO=External%7CLink%7CURI" target="_blank">http://creativecommons.org/licenses/by/4.0/</a>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</div></div></div><div id="pmclinksbox" class="links-box whole_rhythm hidden" role="complementary" aria-label="Related or updated information about this article."></div></div><div class="sec"></div><div id="Abs1" lang="en" class="tsec sec"><h2 class="head no_bottom_margin" id="Abs1title">Abstract</h2><!--article-meta--><div><p id="Par1" class="p p-first-last">Many industry leaders and academics from the field of machine ethics would have us believe that the inevitability of robots coming to have a larger role in our lives demands that robots be endowed with moral reasoning capabilities. Robots endowed in this way may be referred to as artificial moral agents (AMA). Reasons often given for developing AMAs are: the prevention of harm, the necessity for public trust, the prevention of immoral use, such machines are better moral reasoners than humans, and building these machines would lead to a better understanding of human morality. Although some scholars have challenged the very initiative to develop AMAs, what is currently missing from the debate is a closer examination of the reasons offered by machine ethicists to justify the development of AMAs. This closer examination is especially needed because of the amount of funding currently being allocated to the development of AMAs (from funders like Elon Musk) coupled with the amount of attention researchers and industry leaders receive in the media for their efforts in this direction. The stakes in this debate are high because moral robots would make demands on society; answers to a host of pending questions about what counts as an AMA and whether they are morally responsible for their behavior or not. This paper shifts the burden of proof back to the machine ethicists demanding that they give good reasons to build AMAs. The paper argues that until this is done, the development of commercially available AMAs should not proceed further.</p></div><div class="sec"><strong class="kwd-title">Keywords: </strong><span class="kwd-text">Artificial moral agents, Robot ethics, Machine ethics</span></div></div><div id="Sec1" class="tsec sec"><h2 class="head no_bottom_margin" id="Sec1title">Introduction</h2><p id="Par2" class="p p-first">Robots perform exceptionally well at clearly defined tasks like playing chess, assembling a car, classifying images, or vacuuming your floor. Increasingly, however, robots are being assigned more general tasks which require more than one skill. A driverless car, for example is supposed to get you from point A to point B following the rules of the road and reacting to unforeseen situations&#x02014;like a child running out into the middle of the road after a ball. In order for robots to execute their function they require algorithms. These algorithms controlling robots are becoming increasingly autonomous and often require artificial intelligence (AI). As autonomy in robots and AI increases so does the likelihood that they encounter situations which are <em>morally salient</em>. As of 2017 robots are and will continue to be designed, developed and deployed in morally salient contexts; from robots in the hospital lifting and/or bathing patients to robots in the military assisting with bomb disposal or intelligence gathering.</p><p id="Par3">The Executive Summary of the International Federation of Robotics<sup><a href="#Fn1" rid="Fn1" class=" fn">1</a></sup> shows a marked increase in robot sales across every sector from 1 year to the next; including a 25% increase in the total number of service robots sold in 2015 alone. These robots can be used to save lives, to assist in dangerous activities, and/or to enhance the proficiency of human workers. Many industry leaders and academics from the field of machine ethics&#x02014;the study of endowing machines with ethical reasoning&#x02014;would have us believe that robots in these and other morally charged contexts will inevitably demand that these machines be endowed with moral reasoning capabilities. Such robots are often referred to as artificial moral agents (AMAs). In this paper the variety of reasons offered by machine ethicists in favor of AMAs are challenged. This paper asks: are the given reasons adequate justification for the design and development of AMAs?</p><p id="Par5">From the academic domain a variety of scholars in the fields of ethics and technology and/or robot ethics have argued against the development of AMAs (Bryson <a href="#CR13" rid="CR13" class=" bibr popnode">2008</a>; Johnson and Miller <a href="#CR32" rid="CR32" class=" bibr popnode">2008</a>; Sharkey <a href="#CR50" rid="CR50" class=" bibr popnode">2017</a>; Tonkens <a href="#CR58" rid="CR58" class=" bibr popnode">2009</a>). What is currently missing from the debate on AMAs is a closer look at the reasons offered (to society, academics, the media) by machine ethicists to justify the development of AMAs. This closer inspection is particularly compulsory given the amount of funding allocated to the development of AMAs (from funders like Elon Musk) coupled with the amount of attention researchers and industry leaders receive in the media for their efforts in this direction.<sup><a href="#Fn2" rid="Fn2" class=" fn">2</a></sup> Moreover, the stakes are high because the resulting technology could create novel demands on society; questions about what counts as an AMA, whether they are deserving of citizenship,<sup><a href="#Fn3" rid="Fn3" class=" fn">3</a></sup> and/or whether they are morally responsible for their behavior or not. In other words, a machine with moral reasoning capabilities might be thought to deserve moral consideration in the form of rights and/or protections (Coeckelbergh <a href="#CR15" rid="CR15" class=" bibr popnode">2010</a>; Darling <a href="#CR16" rid="CR16" class=" bibr popnode">2012</a>; Gunkel <a href="#CR26" rid="CR26" class=" bibr popnode">2014</a>).</p><p id="Par8" class="p p-last">In order to examine the justifications for AMAs, this paper begins with a description of the field of machine ethics: what it is, the terminology used, and the response to machine ethics found in the literature by robot ethicists and scholars in the field of ethics and technology. In subsequent sections, the reasons offered in favor of developing robots with moral reasoning capabilities are evaluated. It is argued that each of the reasons lack both empirical and intuitive support. The burden of proof is thereby shifted to machine ethicists to justify their pursuits.</p></div><div id="Sec2" class="tsec sec"><h2 class="head no_bottom_margin" id="Sec2title">Machine Ethics</h2><p id="Par9" class="p p-first">Summarized by machine ethicist Susan Anderson, the &#x0201c;ultimate goal of machine ethics is to create autonomous ethical machines&#x0201d; (<a href="#CR6" rid="CR6" class=" bibr popnode">2007</a>, p. 15). The term machine ethics was first used in 1987 by Mitchell Waldrop in the AI Magazine article &#x0201c;A Question of Responsibility&#x0201d; (Waldrop <a href="#CR67" rid="CR67" class=" bibr popnode">1987</a>). In 2005 the AAAI held a symposium on machine ethics which resulted in the edited volume <em>Machine Ethics</em> in 2011 by Susan Leigh and Michael Anderson (Anderson and Anderson <a href="#CR8" rid="CR8" class=" bibr popnode">2011</a>). The field may be referred to by other names, e.g. machine morality, but for the purposes of this paper, machine ethics is a field of study dedicated to the computational entity as a moral entity.<sup><a href="#Fn4" rid="Fn4" class=" fn">4</a></sup></p><p id="Par11">There are several phrases and terms for discussing robots with moral reasoning capabilities (e.g. moral machines, implicit vs explicit ethical agents).<sup><a href="#Fn5" rid="Fn5" class=" fn">5</a></sup> For the purposes of this article, however, the term artificial moral agent (AMA) will be used for consistency and clarity.<sup><a href="#Fn6" rid="Fn6" class=" fn">6</a></sup> This clearly restricts the discussion to robots capable of engaging in autonomous moral reasoning, that is, moral reasoning about a situation without the direct real time input from a human user. This moral reasoning is aimed at going beyond safety and security decisions about a context. How this might be done, and whether or not this can be achieved in practice, are questions that go beyond the scope of this paper (these are the questions underpinning the field of machine ethics itself). Rather, the interest of this paper is in targeting the reasons offered in support of developing such machines.</p><p id="Par14">What a robot or machine would act like if it were to think in an ethical way is a central feature in the 1950 works of science fiction writer Isaac Asimov. Asimov, who coined the term &#x02018;robotics&#x02019; (the study of robots) is best known for his work articulating and exploring the three laws of robotics (Asimov <a href="#CR11" rid="CR11" class=" bibr popnode">1963</a>). In short, these three laws were a kind of principled or deontological approach to embedding ethics into a machine. Through a series of short stories Asimov reveals the difficulty and nuances of robots acting in an ethical manner because each ethical principle conflicts with another to such a degree that experience, wisdom, and intuition are required to come to a solution or resolution of the conflict. His stories highlight the struggle to define ethics in a computational form.</p><p id="Par15">From the academic domain a variety of scholars in the fields of ethics and technology and/or robot ethics have argued against the development of AMAs. On one hand, scholars insist that the technology ought to be designed in such a way that responsibility distribution remains &#x0201c;tethered to humans&#x0201d; (Johnson and Miller <a href="#CR32" rid="CR32" class=" bibr popnode">2008</a>). Similarly, computer scientist Joanna Bryson argues that robots ought to remain in the instrumental service of humans, as slaves if you will, meeting the needs of their human users and intentionally designed not to be a moral agent (Bryson <a href="#CR13" rid="CR13" class=" bibr popnode">2008</a>). This claim is predicated on the assumption that humans will own robots and as such will be responsible for their existence and capacities. On the other hand, philosopher Ryan Tonkens argues that given the impossibility of finding universal agreement concerning the ethical theory used to program a machine, the initiative is moot (Tonkens <a href="#CR58" rid="CR58" class=" bibr popnode">2009</a>).</p><p id="Par16">Outside of these arguments robot ethicist Amanda Sharkey outlines the misappropriation of the use of &#x02018;ethical&#x02019; in the quest to make moral machines and insists on the creation of &#x0201c;safe&#x0201d; machines instead. In the same line of thinking, Miller et al. argue that responsible development requires careful use of terminology and representation in the media (Miller et al. <a href="#CR37" rid="CR37" class=" bibr popnode">2017</a>).</p><p id="Par17" class="p p-last">The above arguments are still waiting to be adequately answered by the machine ethics community. However, the purpose of this paper is to question the positive reasons offered by the machine ethicists <em>for</em> building AMAs. These reasons have not yet been fully evaluated as yet and a closer inspection of them reveals a lack of sufficient justification. Given the high stakes and of the research and development in question coupled with the current speed of (and funding for) machine ethics initiatives these must be addressed now.</p></div><div id="Sec3" class="tsec sec"><h2 class="head no_bottom_margin" id="Sec3title">Reasons for Developing Moral Machines</h2><p id="Par18" class="p p-first">Machine ethicists have offered six reasons (found in the literature) in favor of and/or promoting the development of moral machines. These are not stand alone reasons; rather, they are often intertwined. Part of the reason it sounds so convincing (at first glance) is because of their interdependency rather than the strength of any reason on its own. Disentangling these reasons shows their dubious foundation and allows one to challenge the endeavor of machine ethics.</p><div id="Sec4" class="sec"><h3 id="Sec4title">Inevitability</h3><blockquote class="pullquote"><p>Robots with moral decision making abilities will become a technological necessity (Wallach <a href="#CR68" rid="CR68" class=" bibr popnode">2007</a>).</p></blockquote><blockquote class="pullquote"><p>[Artificial Moral Agents] are necessary and, in a weak sense, inevitable (Colin Allen and Wallach <a href="#CR3" rid="CR3" class=" bibr popnode">2011</a>).</p></blockquote><p>Machine ethicists claim that robots in morally salient contexts will not and cannot be avoided, i.e. their development is inevitable (Anderson and Anderson <a href="#CR7" rid="CR7" class=" bibr popnode">2010</a>; Moor <a href="#CR38" rid="CR38" class=" bibr popnode">2006</a>; Scheutz <a href="#CR47" rid="CR47" class=" bibr popnode">2016</a>; Wallach <a href="#CR69" rid="CR69" class=" bibr popnode">2010</a>).</p><p id="Par22">First, what exactly is meant by morally salient contexts is unclear. For some researchers this would include contexts such as healthcare, elder care, childcare, sex, and or the military&#x02014;where life and death decisions are being made on a daily (or hourly) basis (Arkin <a href="#CR10" rid="CR10" class=" bibr popnode">2009</a>; Lokhorst and van den Hoven <a href="#CR34" rid="CR34" class=" bibr popnode">2011</a>; Sharkey <a href="#CR49" rid="CR49" class=" bibr popnode">2016</a>; Sharkey <a href="#CR51" rid="CR51" class=" bibr popnode">2008</a>; Sharkey and Sharkey <a href="#CR53" rid="CR53" class=" bibr popnode">2011</a>; Sharkey et al. <a href="#CR54" rid="CR54" class=" bibr popnode">2017</a>; van Wynsberghe <a href="#CR62" rid="CR62" class=" bibr popnode">2012</a>). There is no question that robots are entering these service sectors. The International Federation for Robotics Executive Summary of 2016 tells us that &#x0201c;the total number of professional service robots sold in 2015 rose considerably by 25% to 41,060 units up from 32,939 in 2014&#x0201d; and &#x0201c;service robots in defense applications accounted for 27% of the total number of service robots for professional use sold in 2015&#x0201d;. Moreover, sales of medical robots increased by 7% from 2014 to 2015.<sup><a href="#Fn7" rid="Fn7" class=" fn">7</a></sup></p><p id="Par24">For others, morally salient context is much broader than a pre-defined space or institution:</p><blockquote class="pullquote"><p id="Par25">any ordinary decision-making situation from daily life can be turned into a morally charged decision-making situation, where the artificial agent finds itself presented with a moral dilemma where any choice of action (or inaction) can potentially cause harm to other agents. (Scheutz, <a href="#CR47" rid="CR47" class=" bibr popnode">2016</a>, p. 516)</p></blockquote><p>
From the above quote Scheutz is saying that a morally charged situation can arise at any moment in the event that someone could be harmed through (in)action of a robot. This thin description of a morally charged decision making situation adds further ambiguity to the discussion, namely (1) what level of autonomy does the robot have, and (2) what definition of harm is Scheutz talking about? There seems to be an assumption being made in the above quote concerning the robot that the robot must make a choice for action or inaction and thus that the robot must be autonomous. According to Scheutz then any autonomous robot interacting with a human user that has the potential to harm its user should be endowed with moral reasoning capabilities. What would Scheutz have us do with industrial robots that possess divergent levels of autonomy, work with humans in their presence, and for which it has already been shown that the robots can bring serious harm or sometimes death, to humans? Scheutz&#x02019;s position would imply that industrial robots as well ought to be developed into AMAs.</p><p id="Par27">Consider also the definition of &#x02018;harm&#x02019; that ought to be adopted. Is it only physical harm to the corporeal body and mind that is the object of discussion here and if so what about the robot&#x02019;s, or AI algorithm&#x02019;s, ability to collect, store and share information about its users in a home setting? Considering the real possibility that home robots will be connected to the Internet of Things (IoT) which holds the potential for hackers and/or for companies not related to the robotics company to access personal data from users. The harm that can come from the mis-appropriation of one&#x02019;s data has proven to be noteworthy of late: people can be refused mortgage loans, stalked, blackmailed, harassed online, or worse. If harm is to be extended to include the risk of one&#x02019;s digital information, and interaction with a machine that might cause harm demands that it be endowed with ethical reasoning capacities, then one must concede that every device that one interacts with in a day (your tv, phone, fridge, alarm clock, kettle, etc.) ought to have such capabilities. Thus, Scheutz&#x02019;s position leads to the conclusion that any technology that one interacts with and for which there is a potential for harm (physical or otherwise) must be developed as an AMA and this is simply untenable.</p><p id="Par28">Second, a distinction must be made between <em>being in a morally charged situation</em>, on the one hand, and <em>being delegated a moral role</em> on the other. Consider animals used for therapeutic purposes in an elderly care facility; one would never demand that a dog placed in this context would need to reason ethically because of its role in therapy and the potential for harm in this context. Indeed the dog would be trained to ensure a degree of safety and reliability when interacting with it but would the dog be a <em>moral dog</em> in the end?<sup><a href="#Fn8" rid="Fn8" class=" fn">8</a></sup></p><p id="Par30">With this thought in mind let us day the discussion will be limited to an examination of a <em>morally salient context</em> to contexts such as the military and healthcare, which are often thought of as morally salient, and agree that it is inevitable that robots will be placed within these contexts. In this case there is a different, more nuanced problem that can be put into the form of a dilemma: when placed in a morally salient context either machines will be delegated a moral role or they will not. If one chooses the first horn&#x02014;that the machine is delegated a moral role&#x02014;then one must accept that it is inevitable that machines will be delegated a moral role in addition to the inevitability of the machine being in this morally salient context. However, this is simply not the case. There are plenty of machines operating in morally salient contexts which have not been delegated a moral role and are providing a valuable service. Consider for example:</p><p id="Par31">Corti is an AI which listens in on emergency phone calls and makes correlations between the breathing and speech patterns of a caller and the risk of heart attack (Peters <a href="#CR43" rid="CR43" class=" bibr popnode">2018</a>). The information from the AI is presented to the phone operator to assist in their decision making. In short, the AI is a support system for the decision making of the operator. This may be akin to the cancer detection AI or other more traditional technologies in the operating suite (e.g. electrocardiogram, respiratory monitor and so on). Corti is clearly in a morally salient context (life and death), yet the machine is not delegated a moral role. The human being is still in charge and holds all of the responsibility for decision making. If one agrees with machine ethicists then one should accept that it is inevitable that a moral role reserved for the human in this case will be assigned to the machine. While this is probably unnecessary and most likely harmful, the point is that there is simply no reason to believe that this is <em>inevitable</em>.</p><p id="Par32" class="p p-last">If, however, one takes the other horn of the dilemma then the claim is as follows: robots will inevitably be in morally salient contexts without being delegated a morally salient role. The problem with this is that there is little new here. Microwaves and coffee machines exist in the hospital with no need for moral reasoning capabilities; this horn should be of little interest to machine ethicists. In short, there is not any evidence to suggest that it is inevitable that there will be a need for machines with moral reasoning capabilities regardless of whether or not they function in a morally salient context.</p></div><div id="Sec5" class="sec"><h3 id="Sec5title">Artificial Moral Machines to Prevent Harm to Humans</h3><p id="Par33" class="p p-first">For many scholars the development of moral machines is aimed at preventing a robot from hurting human beings. To ensure that humans can overcome the potential for physical harm, a technological solution is presented; namely, to develop AMAs:</p><blockquote class="pullquote"><p id="Par34">the only way to minimize human harm is to build morally competent robots that can detect and resolve morally charged situations in human-like ways (Scheutz <a href="#CR47" rid="CR47" class=" bibr popnode">2016</a>).</p></blockquote><p>
The line of reasoning here is pretty straight forward in that: &#x0201c;it is clear that machines&#x02026;will be capable of causing harm to human beings&#x0201d; (Anderson and Anderson <a href="#CR7" rid="CR7" class=" bibr popnode">2010</a>) and this can be mitigated and/or reduced by endowing the robot with ethical reasoning capabilities. This also speaks to the interconnection of the reasons in favor of AMAs; robots are inevitable, robots could harm us, therefore robots should be made into AMAs.</p><p id="Par36">It is unclear that AMAs are the solution to this problem, however. There are plenty of technologies capable of harming human beings (e.g. lawn mowers, automatic doors, curling irons, blenders); the solution has always been either to design them with safety features or to limit the contexts in which a technology can be used. An elevator door has a sensor so that it does not close on people; lawn mowers have a guard to protect us from their blades; and ovens have lights to warn us when our stovetop is hot. One does not normally use barbeques indoors or chainsaws in daycare centers. Machine ethicists are the first to suggest endowing technology with moral reasoning capabilities as a solution to problems of safety.</p><p id="Par37">Furthermore, machine ethicists may also agree with the pursuit of safe robots and then the real concern for ethicists is that ethics is being reduced to safety. Notions such as values, rights, freedoms, good vs bad, right vs wrong, are central to the study of ethics and form the basis for a discussion of competing conceptions of the good life. One may believe that the values of safety and security are fundamental to achieving the good life; however, ethics cannot be reduced to these issues. So if AMAs are simply a solution to possibly harmful machines, then <em>safety&#x02014;</em>not <em>moral agency&#x02014;</em>is the object of debate.</p><p id="Par38">In this case the world &#x02018;moral&#x02019; is a linguistic &#x02018;trojan horse&#x02019;&#x02014;a word</p><blockquote class="pullquote"><p id="Par39">that smuggles in a rich interconnected web of human concepts that are not part of a computer system or how it operates (Sharkey <a href="#CR52" rid="CR52" class=" bibr popnode">2012</a>, p. 793)</p></blockquote><p>The concept of <em>moral</em> machines or artificial <em>moral</em> agents invites, or more strongly requests that, the user believe the robot may care for him/her, or that the robot can experience feelings. For robot developers this could increase desirability of the robot and therefore profits. However, this is problematic for the public in that it invites a kind of fictive, asymmetric, deceptive relationship between human and robot.</p><p id="Par41" class="p p-last">Thus, machine ethicists must either distinguish what makes their machines &#x0201c;moral&#x0201d; above and beyond &#x0201c;safe&#x0201d; or they must stop using the world &#x0201c;moral&#x0201d; as the word is not appropriate&#x02014;only the most reductionist account of morality would equate it with preventing harm.</p></div><div id="Sec6" class="sec"><h3 id="Sec6title">Complexity</h3><p class="p p-first">
</p><blockquote class="pullquote"><p id="Par42">as systems get more sophisticated and their ability to function autonomously in different contexts and environments expands, it will become more important for them to have &#x02018;ethical subroutines&#x02019; of their own (Allen et al. <a href="#CR4" rid="CR4" class=" bibr popnode">2006</a>, p. 14)</p></blockquote><p>
</p><p id="Par43">The idea behind using complexity as an argument in favor of AMAs is that robots are, and will increasingly become, so complex in terms of their programming that it is no longer possible know what they will do in novel situations. This uncertainty results in the impossibility of the engineer to predict every scenario and as such it will not be possible for the engineer to predict the robot&#x02019;s actions. Consequently, one cannot simply foresee a morally problematic situation and pre-program what the robot should do. Instead, authors who use the complexity argument to promote development of AMAs claim the robot needs to have moral competence in order to govern its unpredictable actions in the inevitably unpredictable and unstructured human environments that the robot will be placed.</p><p id="Par44">First, using complexity as a reason for developing AMAs expects both that there will be complex robots and that such robots ought to be placed in contexts for which this complexity (i.e. unpredictability) could cause problems for human beings.</p><p id="Par45">Next, of importance for this issue is the context within which the robot will be placed. In other words, this problem can be mitigated simply by restricting the context within which these machines are used. For example, designers of Google&#x02019;s complex machine AlphaGo may not have any idea what their machine will do next (which move it will make in the notoriously difficult game of GO); however, this is not an ethical or moral problem because the context (the game of GO) is restricted. Its complexity will not pose a problem for us.</p><p id="Par46" class="p p-last">One may argue that human beings are unpredictable and can cause harm to other human beings. The solution has not been to prevent the delegation of moral roles to human beings. One might ask: why treat machines differently? While it is outside of the scope of this paper to engage in a debate on just how predictable humans are, it can be noted that with regard to serious moral values&#x02014;killing, non-consensual sex, harming innocent people for fun&#x02014;society places restrictions on unpredictable human beings (i.e. imprisonment). Humans may be unpredictable in terms of what they will do next, but most of us assume that a random person will not intentionally cause us harm.</p></div><div id="Sec7" class="sec"><h3 id="Sec7title">Public Trust</h3><p id="Par47" class="p p-first">Other machine ethicists argue that making AMAs will increase public trust: &#x0201c;Constructing artificial moral agents serves at least two purposes: one, better understanding of moral reasoning, and, two, increasing our trust and confidence in creating autonomous agents acting on our behalf&#x0201d; (Wiegel <a href="#CR71" rid="CR71" class=" bibr popnode">2006</a>). There has been talk in the media expressing concerns surrounding AI and robotics&#x02014;voiced by the likes of Elon Musk and Steven Hawking (Cellan-Jones <a href="#CR14" rid="CR14" class=" bibr popnode">2014</a>; Markoff <a href="#CR35" rid="CR35" class=" bibr popnode">2015</a>). Rather than preventing the development of robots that are the source of these fears &#x0201c;machine ethics may offer a viable, more realistic solution&#x0201d; (Anderson and Anderson <a href="#CR6" rid="CR6" class=" bibr popnode">2007</a>).</p><p id="Par48">This line of thinking assumes that if robots are given moral competence then this will put the public at ease and lead to public acceptance. It should be noted here that acceptance differs from acceptability. As an example, the public may <em>accept</em> geo tagging and tracking algorithms on their smartphone devices but this does not meant that such privacy breaching technologies and/or the lack of transparency about their existence are <em>acceptable</em> practices for upholding societal values.</p><p id="Par49">Some important clarifications are needed when discussing trust as a concept. Traditionally speaking, trust is described as an interaction between persons or between a person and an institution and so on. For scholar John Hardwig trust can be placed in people, processes and in knowledge (Hardwig <a href="#CR29" rid="CR29" class=" bibr popnode">1991</a>). In more recent years scholars are discussing a new form of trust; trust in algorithms (Simon <a href="#CR56" rid="CR56" class=" bibr popnode">2010</a>). This new form of trust is most commonly referred to as &#x02018;algorithmic authority&#x02019; and is described as a practice of placing confidence in the decisions made by an algorithm (Shirky <a href="#CR55" rid="CR55" class=" bibr popnode">2009</a>). Wikipedia is an example of this form of trust as it requires trust not in persons but in the algorithms regulating the content on the website.<sup><a href="#Fn9" rid="Fn9" class=" fn">9</a></sup></p><p id="Par51">If trust is broken the result will be feelings of disappointment on the part of the truster. These resulting negative feelings are what relate trust to the concept of reliability: if either one is misplaced the result is oftentimes feelings of disappointment (Simon <a href="#CR56" rid="CR56" class=" bibr popnode">2010</a>). Trust is distinguished from reliability in the intensity of the emotions experienced afterwards; &#x0201c;trust differs from reliance because if we are let down we feel betrayed and not just disappointed&#x0201d; (Baier <a href="#CR12" rid="CR12" class=" bibr popnode">1986</a>; Simon <a href="#CR56" rid="CR56" class=" bibr popnode">2010</a>). Relatedly, Simon claims that one cannot speak of trust for socio-technical systems but rather of reliance: &#x0201c;we usually do not ascribe intentionality to unanimated objects, which is why we do not feel betrayed by them&#x0201d; (p 347). Hence, we do not trust unanimated objects, we rely on them.</p><p id="Par52">With the formulation of Hardwig in mind&#x02014;trust can be placed in people, processes, and in knowledge and when it concerns placing trust in robots one must ask: who, or what, are machine ethicists asking the public to trust: the algorithm directing the robot; the designer; or, the development process?</p><p id="Par53">If the public is being asked to trust the algorithm then one must consider that:</p><blockquote class="pullquote"><p id="Par54">unfortunately we often trust<sup><a href="#Fn10" rid="Fn10" class=" fn">10</a></sup> algorithms blindly. Algorithms are hidden within a system. In most cases we are not aware of how they work and we cannot assess their impact on the information we receive. In other words: algorithms are black-boxed (Simon <a href="#CR56" rid="CR56" class=" bibr popnode">2010</a>).</p></blockquote><p>Consequently, if the public is being asked to trust an algorithm and it is considered a black-box, then, as Simon rightly asserts, it must be <em>opened</em>&#x02014;the way it works, the decisions made in its development, and alternatives&#x02014;must be made transparent and subject to scrutiny.</p><p id="Par57">If, however, the public is being asked to trust the designer then designers and developers ought to develop a code of conduct (perhaps in the form of soft law) to adhere to. Again, transparency of this is required for the public to have the knowledge required for trust.</p><p id="Par58">Last, if the public is being asked to trust the process through which the robot is being developed, a kind of procedural trust, then standards and certifications must be developed to once again provide the user with the knowledge required to place trust in the process through which the robot was developed. Examples of such procedural trust are FairTrade, ISO, GMOs, and so on.</p><p id="Par59" class="p p-last">In any case it is important to point out the inconsistency between the promotion of AMAs for reasons of complexity and for reasons of trust: it is inconsistent to expect unpredictability in a machine and to expect trust in a machine at the same time. While this may not be the case for people&#x02014;one might trust persons who are at the same time unpredictable&#x02014;more clarity is needed in understanding who/what society is being asked to trust and what level of (un)predictability one can assume.</p></div><div id="Sec8" class="sec"><h3 id="Sec8title">Preventing Immoral Use</h3><p id="Par60" class="p p-first">In the 2012 American science fiction comedy-drama movie &#x0201c;Robot &#x00026; Frank&#x0201d; there is a compelling story of how a retired cat burglar convinces his robot to help him enter the business once again. The story raises the question about human&#x02013;robot interaction not in the sense of safe or reliable interactions but rather should the robot be capable of evaluating a human&#x02019;s request for action. Thus, another reason put forward for the development of AMAs can be stated as: preventing humans from misusing, or inappropriately using, a robot requires that the robot be developed as a moral machine and can thus prevent misuse of itself, itself.</p><p id="Par61">The main problem with this reason has to do with the potential to constrain the autonomy of humans. It&#x02019;s not always clear what is the good thing to do and oftentimes context is required for this (Miller et al. <a href="#CR37" rid="CR37" class=" bibr popnode">2017</a>). Consider for example a couple who are at home having a few drinks together, domestic violence ensues after a heated argument, the women tries to get away in her car but the breathalyzer in her car picks up the alcohol in her system and the car won&#x02019;t start. What would be the right thing to do in such an instance? How should the device be programmed? A deontologist would have us believe that one should never get behind the wheel after consuming alcohol, while a utilitarian might suggest that if the woman&#x02019;s life is saved in doing so the overall good is maximized (unless of course the women were to get into a car accident and harm two others). These are not easy problems to solve and require particular details of context and individuals.</p><p id="Par62" class="p p-last">Consider another example where misuse is unclear. If an elderly person at home wants to have a fourth glass of wine and asks his/her robot to fetch it. If the robot fetches the wine, is the robot being misused in so far as it is contributing to poor health choices of the user? Or is the robot &#x02018;good&#x02019; in so far as it fulfilled the request of its user. Presenting scenarios like these is meant to show the difficulty in determining the right or the good thing to do. And yet if one is claiming that robots should be involved in the decision making procedure it must be very clear how a &#x02018;good&#x02019; robot is distinguished from a &#x02018;bad&#x02019; one.</p></div><div id="Sec9" class="sec"><h3 id="Sec9title">Morality: Better with Moral Machines</h3><p id="Par63" class="p p-first">Endowing the robot with the capability to override or edit a human&#x02019;s decisions draws us into the discussion of the robot as a superior moral reasoner to a human. Computer science Professor James Gips suggested back in 1994 that &#x0201c;not many human beings live their lives flawlessly as moral saints. But a robot could (Gips <a href="#CR24" rid="CR24" class=" bibr popnode">1994</a>, p. 250). Also along the same lines, Professor of Philosophy Eric Dietrich has suggested that:</p><blockquote class="pullquote"><p id="Par64">humans are genetically hardwired to be immoral&#x02026;let us &#x02013; the humans &#x02013; exit the stage, leaving behind a planet populated with machines who, although not perfect angels, will nevertheless be a vast improvement over us (Dietrich <a href="#CR18" rid="CR18" class=" bibr popnode">2001</a>)</p></blockquote><p>
The assumption here is that a robot could be better at moral decision making than a human given that it would be impartial, unemotional, consistent, and rational every time it made a decision. Thus, no decisions would be based on bias or emotions, no decision would be the result of an affinity towards one person (or group of people) over another. More importantly, the robot would never tire but would have the energy to be consistent in decision making: to make the same choice time after time.</p><p id="Par66">This line of reasoning to promote AMAs is also often invoked when speaking of robots in military contexts. In particular computer scientist/roboticist Ronald Arkin discusses the power of autonomous military robots for overcoming the shortcomings of humans on the battlefield (Arkin <a href="#CR10" rid="CR10" class=" bibr popnode">2009</a>). These robots would not rape or pillage the villages taken over during wartime and would be programmed as ethical agents according to the Laws of Just war and/or the Rules of Engagement.</p><p id="Par67">There are some general concerns with this reason. First, the underlying programming which will enable machines to reason morally implies that one has an understanding of moral epistemology such that one can program machines to &#x0201c;learn&#x0201d; the correct moral truths&#x02014;or at least know enough to have AMAs learn something that works. This gets complicated as there is no moral epistemology which does not have serious philosophical objections and therefore presents a barrier to being reduced to a programming language.</p><p id="Par68">Machines could only be better if there is some standard of moral truth with which to judge. This implies that there are objective moral truths in a moral realist sense and further that it is possible to know what they are. This is opposed to error theory (the idea that there are no moral truths at all&#x02014;so nothing to know), and moral skepticism (there are moral truths, but it is not possible that we as humans can know them).</p><p id="Par69">Furthermore, based on the above quotes it seems that the moral truths that machines would be better at knowing are truths which are independent of human attitudes. Russ Shafer-Laundau calls these stance-independent moral truths (Shafer-Landau <a href="#CR48" rid="CR48" class=" bibr popnode">1994</a>). If&#x02014;and that is a big if&#x02014;there are stance independent moral truths whereby the truths have no dependence upon human desires, beliefs, needs, etc. then there are objections to how one could come to know such truths (Finlay <a href="#CR20" rid="CR20" class=" bibr popnode">2007</a>). If a machine was built which did somehow discover moral truths that have heretofore yet to be discovered (because morality would be a lot easier if we simply knew the moral truths) then one would have to accept on faith that machines are better than we are.</p><p id="Par70">The moral consistency promised by machine ethicists is only a public good if the moral truths are known in advance&#x02014;the opposite of the situation human beings find themselves in. For, as shown in previous sections, AMAs are argued to be needed because one cannot predict the kind of situations or moral dilemmas they will be faced with. But this is not a chess game where the outcome is a win or a loss. An autonomous car which drives off a cliff&#x02014;killing its one passenger&#x02014;in order to save five passengers in another car would not be a clear cut situation that everyone could agree was the correct decision. Indeed, books are written about that very decision and human disagreement about what should be done (i.e. the trolley problem) (see e.g. Greene <a href="#CR25" rid="CR25" class=" bibr popnode">2013</a>).</p><p id="Par71">Lastly this all presumes that human emotions, human desires, and our evolutionary history are all getting in the way of our moral reasoning&#x02014;causing it to be worse than it could be. There are those who include moral emotions as a necessary part of moral judgment and reasoning (Kristj&#x000e1;nsson <a href="#CR33" rid="CR33" class=" bibr popnode">2006</a>; Pizarro <a href="#CR44" rid="CR44" class=" bibr popnode">2000</a>; Roeser <a href="#CR45" rid="CR45" class=" bibr popnode">2010</a>). If this is so, then AMAs would require emotions&#x02014;something not even on the horizon of AI and robotics.</p><p id="Par72" class="p p-last">Let us say that there are moral principles and that humans can know what they are. So there is a standard with which to judge AMAs. Furthermore, let us also assume they live up to their promise and are better moral reasoners than humans. It might then make sense to outsource our moral decisions to machines. This would assume that being good at moral reasoning is not a necessary part of a human being&#x02019;s good life. Aristotle believed leading a moral life and gaining a moral understanding through practice was necessary to leading a good life (Aristotle et al. <a href="#CR9" rid="CR9" class=" bibr popnode">1998</a>). Many contemporary philosophers agree. Outsourcing our moral reasoning to machines could cause an undesirable moral deskilling in human beings (Vallor <a href="#CR59" rid="CR59" class=" bibr popnode">2015</a>). The point is that it is not clear at all if machines were better moral reasoners than us that this would be a good reason to use them. Added to this, to make such an assumption is to assume we have an understanding of morality and the good life that we may not.</p></div><div id="Sec10" class="sec sec-last"><h3 id="Sec10title">Better Understanding of Morality</h3><p id="Par73" class="p p-first">Finally, machine ethicists sometimes argue that developing robots with moral reasoning capabilities will ultimately lead to a better understanding of human morality:</p><blockquote class="pullquote"><p id="Par74">the hope is that as we try to implement ethical systems on the computer we will learn much more about the knowledge and assumptions built into the ethical theories themselves. That as we build the artificial ethical reasoning systems we will learn how to behave more ethically ourselves (Gips <a href="#CR24" rid="CR24" class=" bibr popnode">1994</a>)</p></blockquote><p> In short, regardless of the resulting machine the very process of attempting to create such a machine would benefit humans in so far as we would learn about ourselves and our moral attributes (Gips <a href="#CR24" rid="CR24" class=" bibr popnode">1994</a>; Moor <a href="#CR38" rid="CR38" class=" bibr popnode">2006</a>; Wiegel <a href="#CR71" rid="CR71" class=" bibr popnode">2006</a>).</p><p id="Par76" class="p p-last">The most important consideration in response to this claim is that ethical theories are not (and have little to do with) how people reason morally so the work doesn&#x02019;t help understand <em>human</em> morality. Experiments in moral psychology show us that human morality is deeply influenced by irrelevant situational factors (Doris <a href="#CR19" rid="CR19" class=" bibr popnode">1998</a>; Merritt <a href="#CR36" rid="CR36" class=" bibr popnode">2000</a>), is driven by emotion (Haidt <a href="#CR27" rid="CR27" class=" bibr popnode">2001</a>; Haidt and Joseph <a href="#CR28" rid="CR28" class=" bibr popnode">2008</a>), and influenced by our evolutionary past (Street <a href="#CR57" rid="CR57" class=" bibr popnode">2006</a>). To be sure, there is an intense debate in the literature with regard to each of these studies. The point is that human morality, in the descriptive sense, is dependent upon many complex factors and building a machine that tries to perfectly emulate human morality must use each of these factors combined rather than rely on ethical theory alone.</p></div></div><div id="Sec11" class="tsec sec"><h2 class="head no_bottom_margin" id="Sec11title">Conclusion</h2><p id="Par77" class="p p-first">In this paper the reasons offered by machine ethicists promoting the development of moral machines are shown to fall short when one takes a closer look at the assumptions underpinning their claims and/or the claims themselves. While autonomous robots and AI can and should be used in morally salient contexts this need not require that the robot be endowed with ethical reasoning capabilities. Merely placing something in an ethical situation, like a heart monitor in an ICU hospital ward, does not also demand the thing to ethically reflect on its course of action. The power of such robots in said contexts can still be harnessed even without making them into so-called moral machines.</p><p id="Par78">This article has shown here that AMAs are promoted for reasons of inevitability, complexity, establishing public trust, preventing immoral use, because they would be better moral reasoners than us, or because there would be a better understanding of human morality with AMAs. None of these reasons&#x02014;as they have been articulated in the literature&#x02014;warrant the development of moral machines nor will they work in practice. This is so because of: inherent bias to learn how to be ethical, the impossibility or difficulty of understanding the complexity of the robot&#x02019;s decision, how to evaluate or trust the superior ethical reasoning of the robot and so on.</p><p id="Par79">There are dangers in the language used for these endeavors. One should not refer to moral machines, artificial moral agents, or ethical agents if the goal is really to create safe, reliable machines. Rather, they should be called what they are: safe robots. The best way to avoid this confusion, considering that no critical or unique operational function appears to be gained through the endowment of ethical reasoning capabilities into robots, is to simply not do it. To that end the authors suggest an implication for policy makers and academics: place a moratorium on the commercialization of robots claiming to have ethical reasoning skills. This would allow academics to study the issues while at the same time protecting users&#x02014;the consumer, the indirect user, and society at large&#x02014;from exposure to this technology which poses an existential challenge.</p><p id="Par80" class="p p-last">In closing, our goal for this paper was to pick apart the reasons in favor of moral machines as a way of shifting the burden of proof back to the machine ethicists. It is not up to ethicists anymore to tell you why they think the pursuit of an AMA is flawed; rather, now that it has been shown that the motivations for developing moral machines do not withstand closer inspection machine ethicists need to provide better reasons. So, to the machine ethicists out there: the ball is in your court.</p></div><div id="ack-a.d.b" class="tsec sec"><h2 class="head no_bottom_margin" id="ack-a.d.btitle">Acknowledgements</h2><div class="sec"><p>This research is supported by the Netherlands Organization for Scientific Research (NWO), Project number 275-20-054. Scott Robbins wishes to acknowledge the European Research Council (ERC) Advanced Grant titled Global Terrorism and Collective Moral Responsibility: Redesigning Military, Police and Intelligence Institutions in Liberal Democracies (GTCMR 670172) which in part made research for this paper possible. We would also like to thank Deborah Johnson for graciously providing such incredibly useful feedback and insights.</p></div></div><div id="fn-group-a.d.a" class="tsec sec"><h2 class="head no_bottom_margin" id="fn-group-a.d.atitle">Footnotes</h2><!--back/fn-group--><div class="fm-sec half_rhythm small"><p class="fn sec" id="Fn1"><sup>1</sup>For more on this see <a href="https://ifr.org/ifr-press-releases/news/world-robotics-report-2016" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CBody&amp;TO=External%7CLink%7CURI" target="_blank">https://ifr.org/ifr-press-releases/news/world-robotics-report-2016</a>.</p><p class="fn sec" id="Fn2"><sup>2</sup>For more on the popular news articles see: (Deng <a href="#CR17" rid="CR17" class=" bibr popnode">2015</a>; &#x0201c;Morals and the machine&#x0201d; <a href="#CR40" rid="CR40" class=" bibr popnode">2012</a>; Rutkin <a href="#CR46" rid="CR46" class=" bibr popnode">2014</a>).</p><p class="fn sec" id="Fn3"><sup>3</sup>Robot Sophie of Hanson Robotics, first robot granted citizenship in Saudi Arabia, see (Gershgorn <a href="#CR23" rid="CR23" class=" bibr popnode">2017</a>; Hatmaker <a href="#CR30" rid="CR30" class=" bibr popnode">2017</a>).</p><p class="fn sec" id="Fn4"><sup>4</sup>For more readings on machine ethics see Wallach and Allen (<a href="#CR70" rid="CR70" class=" bibr popnode">2010</a>), Anderson and Anderson (<a href="#CR6" rid="CR6" class=" bibr popnode">2007</a>, <a href="#CR8" rid="CR8" class=" bibr popnode">2011</a>), Anderson (<a href="#CR5" rid="CR5" class=" bibr popnode">2011</a>), Moor (<a href="#CR39" rid="CR39" class=" bibr popnode">2009</a>, <a href="#CR38" rid="CR38" class=" bibr popnode">2006</a>), Scheutz (<a href="#CR47" rid="CR47" class=" bibr popnode">2016</a>), and Allan et al. (<a href="#CR4" rid="CR4" class=" bibr popnode">2006</a>).</p><p class="fn sec" id="Fn5"><sup>5</sup>For more on this see Wallach and Allen (<a href="#CR70" rid="CR70" class=" bibr popnode">2010</a>), Moor (<a href="#CR39" rid="CR39" class=" bibr popnode">2009</a>, <a href="#CR38" rid="CR38" class=" bibr popnode">2006</a>).</p><p class="fn sec" id="Fn6"><sup>6</sup>The concept and notion of artificial moral agents has built momentum as a thought experiment and/or a possible reality. For a rich and detailed discussion of AMAs the authors recommend the following: (Allen et al. <a href="#CR1" rid="CR1" class=" bibr popnode">2005</a>, <a href="#CR2" rid="CR2" class=" bibr popnode">2000</a>; Floridi and Sanders <a href="#CR21" rid="CR21" class=" bibr popnode">2004</a>; Himma <a href="#CR31" rid="CR31" class=" bibr popnode">2009</a>; Johnson and Miller <a href="#CR32" rid="CR32" class=" bibr popnode">2008</a>; Nagenborg <a href="#CR41" rid="CR41" class=" bibr popnode">2007</a>; Wiegel <a href="#CR72" rid="CR72" class=" bibr popnode">2010</a>).</p><p class="fn sec" id="Fn7"><sup>7</sup>For more on this please refer to: <a href="https://ifr.org/downloads/press/02_2016/Executive_Summary_Service_Robots_2016.pdf" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CBody&amp;TO=External%7CLink%7CURI" target="_blank">https://ifr.org/downloads/press/02_2016/Executive_Summary_Service_Robots_2016.pdf</a>.</p><p class="fn sec" id="Fn8"><sup>8</sup>See also the work of van Wynsberghe illustrating how robots in healthcare need not be delegated roles for which ethical reasoning and/or moral responsibility are required. (van Wynsberghe <a href="#CR62" rid="CR62" class=" bibr popnode">2012</a>, <a href="#CR63" rid="CR63" class=" bibr popnode">2013</a>, <a href="#CR64" rid="CR64" class=" bibr popnode">2015</a>, <a href="#CR65" rid="CR65" class=" bibr popnode">2016</a>). Furthermore there are existing frameworks and applications for realizing ethical values in technological design. See e.g. (Friedman and Nissenbaum <a href="#CR22" rid="CR22" class=" bibr popnode">1996</a>; Nissenbaum <a href="#CR42" rid="CR42" class=" bibr popnode">2001</a>; van de Poel <a href="#CR60" rid="CR60" class=" bibr popnode">2013</a>; van den Hoven <a href="#CR61" rid="CR61" class=" bibr popnode">2007</a>; van Wynsberghe and Robbins <a href="#CR66" rid="CR66" class=" bibr popnode">2013</a>).</p><p class="fn sec" id="Fn9"><sup>9</sup>This form of trust may also be referred to as procedural trust (Simon) as it concerns trust in the process through which knowledge is created rather than in actions of persons.</p><p class="fn sec" id="Fn10"><sup>10</sup>The word trust is used here because it comes from a quotation; however, it should be noted that the authors are inclined to use the work rely instead.</p></div></div><div id="article-aaff-info" class="tsec sec"><h2 class="head no_bottom_margin" id="article-aaff-infotitle">Contributor Information</h2><p><span class="fm-affl">Aimee van Wynsberghe, </span><span class="fm-affl"><span class="email-label">Email: </span><a href="mailto:dev@null" data-email="moc.liamg@tobornaveemia" class="oemail">moc.liamg@tobornaveemia</a></span>.</p><p><span class="fm-affl">Scott Robbins, </span><span class="fm-affl"><span class="email-label">Email: </span><a href="mailto:dev@null" data-email="gro.snibborttocs@ttocs" class="oemail">gro.snibborttocs@ttocs</a></span>.</p></div><div id="Bib1" class="tsec sec"><h2 class="head no_bottom_margin" id="Bib1title">References</h2><div class="ref-list-sec sec" id="reference-list"><ul class="back-ref-list" style="list-style-type:disc;"><li id="CR1"><span class="element-citation">Allen C, Smit I, Wallach W. Artificial morality: Top-down, bottom-up, and hybrid approaches. <span><span class="ref-journal">Ethics and Information Technology. </span>2005;<span class="ref-vol">7</span>(3):149–155.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Ethics+and+Information+Technology&amp;title=Artificial+morality:+Top-down,+bottom-up,+and+hybrid+approaches&amp;author=C+Allen&amp;author=I+Smit&amp;author=W+Wallach&amp;volume=7&amp;issue=3&amp;publication_year=2005&amp;pages=149-155&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR2"><span class="element-citation">Allen C, Varner G, Zinser J. Prolegomena to any future artificial moral agent. <span><span class="ref-journal">Journal of Experimental &#x00026; Theoretical Artificial Intelligence. </span>2000;<span class="ref-vol">12</span>(3):251–261.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Journal+of+Experimental+&#x00026;+Theoretical+Artificial+Intelligence&amp;title=Prolegomena+to+any+future+artificial+moral+agent&amp;author=C+Allen&amp;author=G+Varner&amp;author=J+Zinser&amp;volume=12&amp;issue=3&amp;publication_year=2000&amp;pages=251-261&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR3"><span class="element-citation">Allen C, Wallach W.  Moral machines: Contradition in terms of abdication of human responsibility? In: Lin P, Abney K, Bekey GA, editors. <span class="ref-journal">Robot ethics: The ethical and social implications of robotics.</span> Cambridge: MIT Press; 2011. pp. 55–68. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=Robot+ethics:+The+ethical+and+social+implications+of+robotics&amp;author=C+Allen&amp;author=W+Wallach&amp;publication_year=2011&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR4"><span class="element-citation">Allen C, Wallach W, Smit I. Why machine ethics? <span><span class="ref-journal">IEEE Intelligent Systems. </span>2006;<span class="ref-vol">21</span>(4):12–17.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Intelligent+Systems&amp;title=Why+machine+ethics?&amp;author=C+Allen&amp;author=W+Wallach&amp;author=I+Smit&amp;volume=21&amp;issue=4&amp;publication_year=2006&amp;pages=12-17&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR5"><span class="mixed-citation">Anderson, S. L. (2011). <em>Machine metaethics</em>. In M. Anderson, &#x00026; S. L. Anderson (Eds.), Machine Ethics. New York: Cambridge University Press.</span></li><li id="CR6"><span class="element-citation">Anderson M, Anderson SL. Machine ethics: Creating an ethical intelligent agent. <span><span class="ref-journal">AI Magazine. </span>2007;<span class="ref-vol">28</span>(4):15–26.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=AI+Magazine&amp;title=Machine+ethics:+Creating+an+ethical+intelligent+agent&amp;author=M+Anderson&amp;author=SL+Anderson&amp;volume=28&amp;issue=4&amp;publication_year=2007&amp;pages=15-26&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR7"><span class="element-citation">Anderson M, Anderson SL. Robot be good: A call for ethical autonomous machines. <span><span class="ref-journal">Scientific American. </span>2010;<span class="ref-vol">303</span>(4):15–24.</span> [<a href="https://pubmed.ncbi.nlm.nih.gov/20923132" ref="reftype=pubmed&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Scientific+American&amp;title=Robot+be+good:+A+call+for+ethical+autonomous+machines&amp;author=M+Anderson&amp;author=SL+Anderson&amp;volume=303&amp;issue=4&amp;publication_year=2010&amp;pages=15-24&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR8"><span class="element-citation">Anderson M, Anderson SL.  <span class="ref-journal">Machine ethics.</span> Cambridge: Cambridge University Press; 2011.  <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=Machine+ethics&amp;author=M+Anderson&amp;author=SL+Anderson&amp;publication_year=2011&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR9"><span class="mixed-citation">Aristotle, Ross, W. D., Ackrill, J. L., &#x00026; Urmson, J. O. (1998). <em>The Nicomachean ethics</em>. Oxford University Press. Retrieved from <a href="http://books.google.nl/books?id=Dk2VFlZyiJQC" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://books.google.nl/books?id=Dk2VFlZyiJQC</a>. Accessed 24 Oct 2014.</span></li><li id="CR10"><span class="element-citation">Arkin R.  <span class="ref-journal">Governing lethal behavior in autonomous robots.</span> Boca Raton: CRC Press; 2009.  <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=Governing+lethal+behavior+in+autonomous+robots&amp;author=R+Arkin&amp;publication_year=2009&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR11"><span class="element-citation">Asimov I.  <span class="ref-journal">I, Robot.</span> New York: Spectra; 1963.  <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=I,+Robot&amp;author=I+Asimov&amp;publication_year=1963&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR12"><span class="element-citation">Baier A. Trust and antitrust. <span><span class="ref-journal">Ethics. </span>1986;<span class="ref-vol">96</span>(2):231–260.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Ethics&amp;title=Trust+and+antitrust&amp;author=A+Baier&amp;volume=96&amp;issue=2&amp;publication_year=1986&amp;pages=231-260&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR13"><span class="mixed-citation">Bryson, J. (2008). Robots should be slaves. In Y. Wilks (Ed.), <em>Close Engagements with artificial companions: Key social, psychological, ethical and design issue</em> (pp. 63&#x02013;74). Amsterdam: John Benjamins Publishing. Retrieved from <a href="https://books.google.nl/books?id=EPznZHeG89cC" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://books.google.nl/books?id=EPznZHeG89cC</a>. Accessed 7 Mar 2017.</span></li><li id="CR14"><span class="mixed-citation">Cellan-Jones, R. (2014). Stephen Hawking warns artificial intelligence could end mankind. <em>BBC News</em>. Retrieved from <a href="http://www.bbc.com/news/technology-30290540" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://www.bbc.com/news/technology-30290540</a>. Accessed 29 Aug 2016.</span></li><li id="CR15"><span class="element-citation">Coeckelbergh M. Robot rights? Towards a social-relational justification of moral consideration. <span><span class="ref-journal">Ethics and Information Technology. </span>2010;<span class="ref-vol">12</span>(3):209–221.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Ethics+and+Information+Technology&amp;title=Robot+rights?+Towards+a+social-relational+justification+of+moral+consideration&amp;author=M+Coeckelbergh&amp;volume=12&amp;issue=3&amp;publication_year=2010&amp;pages=209-221&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR16"><span class="mixed-citation">Darling, K. (2012). <em>Extending Legal protection to social robots: The effects of anthropomorphism, empathy, and violent behavior towards robotic objects</em>. Rochester, NY. Retrieved from <a href="https://papers.ssrn.com/abstract=2044797" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://papers.ssrn.com/abstract=2044797</a>.</span></li><li id="CR17"><span class="element-citation">Deng B. Machine ethics: The robot&#x02019;s dilemma. <span><span class="ref-journal">Nature News. </span>2015;<span class="ref-vol">523</span>(7558):24.</span> [<a href="https://pubmed.ncbi.nlm.nih.gov/26135432" ref="reftype=pubmed&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Nature+News&amp;title=Machine+ethics:+The+robot&#x02019;s+dilemma&amp;author=B+Deng&amp;volume=523&amp;issue=7558&amp;publication_year=2015&amp;pages=24&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR18"><span class="element-citation">Dietrich E. Homo sapiens 2.0: Why we should build the better robots of our nature. <span><span class="ref-journal">Journal of Experimental &#x00026; Theoretical Artificial Intelligence. </span>2001;<span class="ref-vol">13</span>(4):323–328.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Journal+of+Experimental+&#x00026;+Theoretical+Artificial+Intelligence&amp;title=Homo+sapiens+2.0:+Why+we+should+build+the+better+robots+of+our+nature&amp;author=E+Dietrich&amp;volume=13&amp;issue=4&amp;publication_year=2001&amp;pages=323-328&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR19"><span class="mixed-citation">Doris, J. M. (1998). Persons, situations, and virtue ethics. <em>Nous</em>, <em>32</em>(4), 504&#x02013;530. Retrieved from <a href="http://www.jstor.org/stable/pdfplus/2671873.pdf?acceptTC=true" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://www.jstor.org/stable/pdfplus/2671873.pdf?acceptTC=true</a>.</span></li><li id="CR20"><span class="element-citation">Finlay S. Four faces of moral realism. <span><span class="ref-journal">Philosophy Compass. </span>2007;<span class="ref-vol">2</span>(6):820–849.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Philosophy+Compass&amp;title=Four+faces+of+moral+realism&amp;author=S+Finlay&amp;volume=2&amp;issue=6&amp;publication_year=2007&amp;pages=820-849&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR21"><span class="element-citation">Floridi L, Sanders JW. On the morality of artificial agents. <span><span class="ref-journal">Minds and Machines. </span>2004;<span class="ref-vol">14</span>(3):349–379.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Minds+and+Machines&amp;title=On+the+morality+of+artificial+agents&amp;author=L+Floridi&amp;author=JW+Sanders&amp;volume=14&amp;issue=3&amp;publication_year=2004&amp;pages=349-379&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR22"><span class="mixed-citation">Friedman, B., &#x00026; Nissenbaum, H. (1996). Bias in computer systems. <em>ACM Transactions on Information Systems</em>, <em>14</em>(3), 330&#x02013;347. 10.1145/230538.230561. Retrieved 10 Feb 2017.</span></li><li id="CR23"><span class="mixed-citation">Gershgorn, D. (2017). Inside the mechanical brain of the world&#x02019;s first robot citizen. <a href="https://qz.com/1121547/how-smart-is-the-first-robot-citizen/" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://qz.com/1121547/how-smart-is-the-first-robot-citizen/</a>. Retrieved 29 Dec 2017.</span></li><li id="CR24"><span class="mixed-citation">Gips, J. (1994). Toward the ethical robot. In K. M. Ford, C. Glymour, &#x00026; P. Hayes (Eds.), <em>Android Epistemology</em>. Cambridge: MIT Press.</span></li><li id="CR25"><span class="element-citation">Greene J.  <span class="ref-journal">Moral tribes: Emotion, reason, and the gap between us and them.</span> 1. New York: Penguin Press; 2013.  <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=Moral+tribes:+Emotion,+reason,+and+the+gap+between+us+and+them&amp;author=J+Greene&amp;publication_year=2013&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR26"><span class="element-citation">Gunkel DJ. A vindication of the rights of machines. <span><span class="ref-journal">Philosophy &#x00026; Technology. </span>2014;<span class="ref-vol">27</span>(1):113–132.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Philosophy+&#x00026;+Technology&amp;title=A+vindication+of+the+rights+of+machines&amp;author=DJ+Gunkel&amp;volume=27&amp;issue=1&amp;publication_year=2014&amp;pages=113-132&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR27"><span class="element-citation">Haidt J. The emotional dog and its rational tail: A social intuitionist approach to moral judgment. <span><span class="ref-journal">Psychological Review. </span>2001;<span class="ref-vol">108</span>(4):814–834.</span> [<a href="https://pubmed.ncbi.nlm.nih.gov/11699120" ref="reftype=pubmed&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Psychological+Review&amp;title=The+emotional+dog+and+its+rational+tail:+A+social+intuitionist+approach+to+moral+judgment&amp;author=J+Haidt&amp;volume=108&amp;issue=4&amp;publication_year=2001&amp;pages=814-834&amp;pmid=11699120&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR28"><span class="element-citation">Haidt J, Joseph C.  The Innate Mind: Volume 3: Foundations and the Future (Evolution and Cognition) In: Carruthers P, Laurence S, Stich S, editors. <span class="ref-journal">The innate mind.</span> New York: Oxford University Press; 2008.  <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=The+innate+mind&amp;author=J+Haidt&amp;author=C+Joseph&amp;publication_year=2008&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR29"><span class="element-citation">Hardwig J. The role of trust in knowledge. <span><span class="ref-journal">The Journal of Philosophy. </span>1991;<span class="ref-vol">88</span>(12):693–708.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=The+Journal+of+Philosophy&amp;title=The+role+of+trust+in+knowledge&amp;author=J+Hardwig&amp;volume=88&amp;issue=12&amp;publication_year=1991&amp;pages=693-708&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR30"><span class="mixed-citation">Hatmaker, T. (2017). Saudi Arabia bestows citizenship on a robot named Sophia. <a href="http://social.techcrunch.com/2017/10/26/saudi-arabia-robot-citizen-sophia/" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://social.techcrunch.com/2017/10/26/saudi-arabia-robot-citizen-sophia/</a>. Retrieved 12 Feb 2018.</span></li><li id="CR31"><span class="element-citation">Himma KE. Artificial agency, consciousness, and the criteria for moral agency: What properties must an artificial agent have to be a moral agent? <span><span class="ref-journal">Ethics and Information Technology. </span>2009;<span class="ref-vol">11</span>(1):19–29.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Ethics+and+Information+Technology&amp;title=Artificial+agency,+consciousness,+and+the+criteria+for+moral+agency:+What+properties+must+an+artificial+agent+have+to+be+a+moral+agent?&amp;author=KE+Himma&amp;volume=11&amp;issue=1&amp;publication_year=2009&amp;pages=19-29&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR32"><span class="element-citation">Johnson DG, Miller KW. Un-making artificial moral agents. <span><span class="ref-journal">Ethics and Information Technology. </span>2008;<span class="ref-vol">10</span>(2&#x02013;3):123–133.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Ethics+and+Information+Technology&amp;title=Un-making+artificial+moral+agents&amp;author=DG+Johnson&amp;author=KW+Miller&amp;volume=10&amp;issue=2&#x02013;3&amp;publication_year=2008&amp;pages=123-133&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR33"><span class="mixed-citation">Kristj&#x000e1;nsson, K. (2006). Emulation and the use of role models in moral education. <em>Journal of Moral Education</em>, <em>35</em>(1), 37&#x02013;49. Retrieved from <a href="http://www.tandfonline.com/doi/abs/10.1080/03057240500495278" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://www.tandfonline.com/doi/abs/10.1080/03057240500495278</a>. Accessed 25 Oct 2014.</span></li><li id="CR34"><span class="element-citation">Lokhorst G-J, van den Hoven J.  Responsibility for Military Robots. In: Lin P, Abney K, Bekey GA, editors. <span class="ref-journal">Robot ethics: The ethical and social implications of robotics.</span> Cambridge: MIT Press; 2011. pp. 145–155. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=Robot+ethics:+The+ethical+and+social+implications+of+robotics&amp;author=G-J+Lokhorst&amp;author=J+van+den+Hoven&amp;publication_year=2011&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR35"><span class="mixed-citation">Markoff, J. (2015). Relax, the terminator is far away. <em>The New York Times</em>. Retrieved from <a href="http://www.nytimes.com/2015/05/26/science/darpa-robotics-challenge-terminator.html" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://www.nytimes.com/2015/05/26/science/darpa-robotics-challenge-terminator.html</a>. Accessed 29 Aug 2016.</span></li><li id="CR36"><span class="element-citation">Merritt M. Virtue ethics and situationist personality psychology. <span><span class="ref-journal">Ethical Theory and Moral Practice. </span>2000;<span class="ref-vol">3</span>(4):365–383.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Ethical+Theory+and+Moral+Practice&amp;title=Virtue+ethics+and+situationist+personality+psychology&amp;author=M+Merritt&amp;volume=3&amp;issue=4&amp;publication_year=2000&amp;pages=365-383&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR37"><span class="element-citation">Miller KW, Wolf MJ, Grodzinsky F. This &#x0201c;ethical trap&#x0201d; is for roboticists, not robots: on the issue of artificial agent ethical decision-making. <span><span class="ref-journal">Science and Engineering Ethics. </span>2017;<span class="ref-vol">23</span>(2):389–401.</span> [<a href="https://pubmed.ncbi.nlm.nih.gov/27116039" ref="reftype=pubmed&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Science+and+Engineering+Ethics&amp;title=This+&#x0201c;ethical+trap&#x0201d;+is+for+roboticists,+not+robots:+on+the+issue+of+artificial+agent+ethical+decision-making&amp;author=KW+Miller&amp;author=MJ+Wolf&amp;author=F+Grodzinsky&amp;volume=23&amp;issue=2&amp;publication_year=2017&amp;pages=389-401&amp;pmid=27116039&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR38"><span class="element-citation">Moor JH. The nature, importance, and difficulty of machine ethics. <span><span class="ref-journal">IEEE Intelligent Systems. </span>2006;<span class="ref-vol">21</span>(4):18–21.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Intelligent+Systems&amp;title=The+nature,+importance,+and+difficulty+of+machine+ethics&amp;author=JH+Moor&amp;volume=21&amp;issue=4&amp;publication_year=2006&amp;pages=18-21&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR39"><span class="mixed-citation">Moor, J. (2009). Four kinds of ethical robots. <em>Philosophy Now</em>, (72), 12&#x02013;14. Retrieved from <a href="https://philosophynow.org/issues/72/Four_Kinds_of_Ethical_Robots" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://philosophynow.org/issues/72/Four_Kinds_of_Ethical_Robots</a>. Accessed 10 Feb 2017.</span></li><li id="CR40"><span class="mixed-citation">Morals and the machine. (2012). <em>The Economist</em>. Retrieved from <a href="http://www.economist.com/node/21556234" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://www.economist.com/node/21556234</a>. Accessed 7 Mar 2017.</span></li><li id="CR41"><span class="mixed-citation">Nagenborg, M. (2007). Artificial moral agents: An intercultural perspective. <em>International Review of Information Ethics,</em><em>7,</em> 129&#x02013;133. <a href="http://www.i-r-i-e.net/inhalt/007/13-nagenborg.pdf" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://www.i-r-i-e.net/inhalt/007/13-nagenborg.pdf</a>. Retrieved 12 Feb 2018.</span></li><li id="CR42"><span class="element-citation">Nissenbaum H. How computer systems embody values. <span><span class="ref-journal">Computer -Los Almalitos- </span>2001;<span class="ref-vol">34</span>:120.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Computer+-Los+Almalitos-&amp;title=How+computer+systems+embody+values&amp;author=H+Nissenbaum&amp;volume=34&amp;publication_year=2001&amp;pages=120&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR43"><span class="mixed-citation">Peters, A. (2018). Having a heart attack? This AI helps emergency dispatchers find out. Retrieved January 16, 2018, from <a href="https://www.fastcompany.com/40515740/having-a-heart-attack-this-ai-helps-emergency-dispatchers-find-out" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://www.fastcompany.com/40515740/having-a-heart-attack-this-ai-helps-emergency-dispatchers-find-out</a>.</span></li><li id="CR44"><span class="element-citation">Pizarro D. Nothing More than Feelings? The Role of Emotions in Moral Judgment. <span><span class="ref-journal">Journal for the Theory of Social Behaviour. </span>2000;<span class="ref-vol">30</span>(4):355–375.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Journal+for+the+Theory+of+Social+Behaviour&amp;title=Nothing+More+than+Feelings?+The+Role+of+Emotions+in+Moral+Judgment&amp;author=D+Pizarro&amp;volume=30&amp;issue=4&amp;publication_year=2000&amp;pages=355-375&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR45"><span class="element-citation">Roeser S.  <span class="ref-journal">Moral emotions and intuitions.</span> Berlin: Springer; 2010.  <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=Moral+emotions+and+intuitions&amp;author=S+Roeser&amp;publication_year=2010&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR46"><span class="mixed-citation">Rutkin, A. (2014). Ethical trap: Robot paralysed by choice of who to save. <a href="https://www.newscientist.com/article/mg22329863-700-ethical-trap-robot-paralysed-by-choice-of-who-to-save/" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://www.newscientist.com/article/mg22329863-700-ethical-trap-robot-paralysed-by-choice-of-who-to-save/</a>. Retrieved 12 Feb 2018.</span></li><li id="CR47"><span class="mixed-citation">Scheutz, M. (2016). The need for moral competency in autonomous agent architectures. In V. C. M&#x000fc;ller (Ed.) (pp. 515&#x02013;525). Springer International Publishing. Retrieved from <a href="http://link.springer.com/chapter/10.1007/978-3-319-26485-1_30" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://link.springer.com/chapter/10.1007/978-3-319-26485-1_30</a>. Accessed 29 Aug 2016.</span></li><li id="CR48"><span class="element-citation">Shafer-Landau R. Ethical disagreement, ethical objectivism and moral indeterminacy. <span><span class="ref-journal">Philosophy and Phenomenological Research. </span>1994;<span class="ref-vol">54</span>(2):331–344.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Philosophy+and+Phenomenological+Research&amp;title=Ethical+disagreement,+ethical+objectivism+and+moral+indeterminacy&amp;author=R+Shafer-Landau&amp;volume=54&amp;issue=2&amp;publication_year=1994&amp;pages=331-344&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR49"><span class="element-citation">Sharkey A. Should we welcome robot teachers? <span><span class="ref-journal">Ethics and Information Technology. </span>2016</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Ethics+and+Information+Technology&amp;title=Should+we+welcome+robot+teachers?&amp;author=A+Sharkey&amp;publication_year=2016&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR50"><span class="element-citation">Sharkey A. Can robots be responsible moral agents? And why should we care? <span><span class="ref-journal">Connection Science. </span>2017;<span class="ref-vol">29</span>(3):210–216.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Connection+Science&amp;title=Can+robots+be+responsible+moral+agents?+And+why+should+we+care?&amp;author=A+Sharkey&amp;volume=29&amp;issue=3&amp;publication_year=2017&amp;pages=210-216&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR51"><span class="element-citation">Sharkey N. The ethical frontiers of robotics. <span><span class="ref-journal">Science. </span>2008;<span class="ref-vol">322</span>(5909):1800–1801.</span> [<a href="https://pubmed.ncbi.nlm.nih.gov/19095930" ref="reftype=pubmed&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Science&amp;title=The+ethical+frontiers+of+robotics&amp;author=N+Sharkey&amp;volume=322&amp;issue=5909&amp;publication_year=2008&amp;pages=1800-1801&amp;pmid=19095930&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR52"><span class="element-citation">Sharkey N. The evitability of autonomous robot warfare. <span><span class="ref-journal">International Review of the Red Cross. </span>2012;<span class="ref-vol">94</span>(886):787–799.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=International+Review+of+the+Red+Cross&amp;title=The+evitability+of+autonomous+robot+warfare&amp;author=N+Sharkey&amp;volume=94&amp;issue=886&amp;publication_year=2012&amp;pages=787-799&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR53"><span class="element-citation">Sharkey N, Sharkey A.  The Rights and Wrongs of Robot Care. In: Lin P, Abney K, Bekey GA, editors. <span class="ref-journal">Robot ethics: The ethical and social implications of robotics.</span> Cambridge: MIT Press; 2011. pp. 267–282. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=Robot+ethics:+The+ethical+and+social+implications+of+robotics&amp;author=N+Sharkey&amp;author=A+Sharkey&amp;publication_year=2011&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR54"><span class="mixed-citation">Sharkey, N., van Wynsberghe, A., Robbins, S., &#x00026; Hancock, E. (2017). <em>Our Sexual Future with Robots</em>. The Hague, Netherlands. Retrieved from <a href="https://responsible-roboticsmyxf6pn3xr.netdna-ssl.com/wp-content/uploads/2017/11/FRR-Consultation-Report-Our-Sexual-Future-with-robots-.pdf" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://responsible-roboticsmyxf6pn3xr.netdna-ssl.com/wp-content/uploads/2017/11/FRR-Consultation-Report-Our-Sexual-Future-with-robots-.pdf</a>. Accessed 1 Feb 2018.</span></li><li id="CR55"><span class="mixed-citation">Shirky, C. (2009). A speculative post on the idea of algorithmic authority. <a href="http://www.shirky.com/weblog/2009/11/a-speculative-post-on-the-idea-of-algorithmic-authority/" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">http://www.shirky.com/weblog/2009/11/a-speculative-post-on-the-idea-of-algorithmic-authority/</a>. Retrieved 12 Feb 2018.</span></li><li id="CR56"><span class="element-citation">Simon J. The entanglement of trust and knowledge on the Web. <span><span class="ref-journal">Ethics and Information Technology. </span>2010;<span class="ref-vol">12</span>(4):343–355.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Ethics+and+Information+Technology&amp;title=The+entanglement+of+trust+and+knowledge+on+the+Web&amp;author=J+Simon&amp;volume=12&amp;issue=4&amp;publication_year=2010&amp;pages=343-355&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR57"><span class="element-citation">Street S. A darwinian dilemma for realist theories of value. <span><span class="ref-journal">Philosophical Studies. </span>2006;<span class="ref-vol">127</span>(1):109–166.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Philosophical+Studies&amp;title=A+darwinian+dilemma+for+realist+theories+of+value&amp;author=S+Street&amp;volume=127&amp;issue=1&amp;publication_year=2006&amp;pages=109-166&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR58"><span class="element-citation">Tonkens R. A challenge for machine ethics. <span><span class="ref-journal">Minds and Machines. </span>2009;<span class="ref-vol">19</span>(3):421–438.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Minds+and+Machines&amp;title=A+challenge+for+machine+ethics&amp;author=R+Tonkens&amp;volume=19&amp;issue=3&amp;publication_year=2009&amp;pages=421-438&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR59"><span class="element-citation">Vallor S. Moral deskilling and upskilling in a new machine age: Reflections on the ambiguous future of character. <span><span class="ref-journal">Philosophy &#x00026; Technology. </span>2015;<span class="ref-vol">28</span>(1):107–124.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Philosophy+&#x00026;+Technology&amp;title=Moral+deskilling+and+upskilling+in+a+new+machine+age:+Reflections+on+the+ambiguous+future+of+character&amp;author=S+Vallor&amp;volume=28&amp;issue=1&amp;publication_year=2015&amp;pages=107-124&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR60"><span class="element-citation">van de Poel I.  Translating Values into design requirements. In: Mitchfelder D, McCarty N, Goldberg DE, editors. <span class="ref-journal">Philosophy and engineering: Reflections on practice, principles, and process.</span> Dordrecht: Springer; 2013.  <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=Philosophy+and+engineering:+Reflections+on+practice,+principles,+and+process&amp;author=I+van+de+Poel&amp;publication_year=2013&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR61"><span class="element-citation">van den Hoven J.  ICT and value sensitive design. In: Goujon P, Lavelle S, Duquenoy P, Kimppa K, editors. <span class="ref-journal">The information society: Innovation, legitimacy, ethics and democracy in honor of professor Jacques Berleur s.j.</span> Boston: Springer; 2007. pp. 67–72. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?title=The+information+society:+Innovation,+legitimacy,+ethics+and+democracy+in+honor+of+professor+Jacques+Berleur+s.j&amp;author=J+van+den+Hoven&amp;publication_year=2007&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR62"><span class="element-citation">van Wynsberghe A. Designing robots for care: Care centered value-sensitive design. <span><span class="ref-journal">Science and Engineering Ethics. </span>2012;<span class="ref-vol">19</span>(2):407–433.</span> <span class="nowrap">[<a class="int-reflink" href="/pmc/articles/PMC3662860/">PMC free article</a>]</span> [<a href="https://pubmed.ncbi.nlm.nih.gov/22212357" ref="reftype=pubmed&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Science+and+Engineering+Ethics&amp;title=Designing+robots+for+care:+Care+centered+value-sensitive+design&amp;author=A+van+Wynsberghe&amp;volume=19&amp;issue=2&amp;publication_year=2012&amp;pages=407-433&amp;pmid=22212357&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR63"><span class="element-citation">van Wynsberghe A. A method for integrating ethics into the design of robots. <span><span class="ref-journal">Industrial Robot: An International Journal. </span>2013;<span class="ref-vol">40</span>(5):433–440.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Industrial+Robot:+An+International+Journal&amp;title=A+method+for+integrating+ethics+into+the+design+of+robots&amp;author=A+van+Wynsberghe&amp;volume=40&amp;issue=5&amp;publication_year=2013&amp;pages=433-440&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR64"><span class="mixed-citation">van Wynsberghe, A. (2015). <em>Healthcare robots: Ethics, design and implementation</em>. <em>Healthcare Robots: Ethics, Design and Implementation</em>. Retrieved from <a href="https://www.scopus.com/inward/record.uri%3feid%3d2-s2.0-84946412196%26partnerID%3d40%26md5%3d5c270c5c2c8d9f4983cbe6c4f2369c97" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946412196&#x00026;partnerID=40&#x00026;md5=5c270c5c2c8d9f4983cbe6c4f2369c97</a>. Accessed 29 Aug 2016.</span></li><li id="CR65"><span class="element-citation">van Wynsberghe A. Service robots, care ethics, and design. <span><span class="ref-journal">Ethics and Information Technology. </span>2016;<span class="ref-vol">18</span>(4):311–321.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Ethics+and+Information+Technology&amp;title=Service+robots,+care+ethics,+and+design&amp;author=A+van+Wynsberghe&amp;volume=18&amp;issue=4&amp;publication_year=2016&amp;pages=311-321&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR66"><span class="mixed-citation">van Wynsberghe, A., &#x00026; Robbins, S. (2014). Ethicist as designer: A pragmatic approach to ethics in the lab. <em>Science and Engineering Ethics</em>, <em>20</em>(4), 947&#x02013;961. 10.1007/s11948-013-9498-4. [<a href="https://pubmed.ncbi.nlm.nih.gov/24254219" ref="reftype=pubmed&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="//doi.org/10.1007%2Fs11948-013-9498-4" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>]</span></li><li id="CR67"><span class="element-citation">Waldrop MM. A question of responsibility. <span><span class="ref-journal">AI Magazine. </span>1987;<span class="ref-vol">8</span>(1):28.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=AI+Magazine&amp;title=A+question+of+responsibility&amp;author=MM+Waldrop&amp;volume=8&amp;issue=1&amp;publication_year=1987&amp;pages=28&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR68"><span class="element-citation">Wallach W. Implementing moral decision making faculties in computers and robots. <span><span class="ref-journal">AI &#x00026; Society. </span>2007;<span class="ref-vol">22</span>(4):463–475.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=AI+&#x00026;+Society&amp;title=Implementing+moral+decision+making+faculties+in+computers+and+robots&amp;author=W+Wallach&amp;volume=22&amp;issue=4&amp;publication_year=2007&amp;pages=463-475&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR69"><span class="element-citation">Wallach W. Robot minds and human ethics: The need for a comprehensive model of moral decision making. <span><span class="ref-journal">Ethics and Information Technology. </span>2010;<span class="ref-vol">12</span>(3):243–250.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Ethics+and+Information+Technology&amp;title=Robot+minds+and+human+ethics:+The+need+for+a+comprehensive+model+of+moral+decision+making&amp;author=W+Wallach&amp;volume=12&amp;issue=3&amp;publication_year=2010&amp;pages=243-250&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li><li id="CR70"><span class="mixed-citation">Wallach, W., &#x00026; Allen, C. (2010). <em>Moral machines: Teaching robots right from wrong</em> (1st ed.). New York: Oxford University Press. Retrieved from <a href="https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975</a>. Accessed 10 Feb 2017.</span></li><li id="CR71"><span class="mixed-citation">Wiegel, V. (2006). <em>Building blocks for artificial moral agents</em>. In <em>Proceedings of EthicalALife06 Workshop</em>. <a href="https://www.researchgate.net/profile/Vincent_Wiegel/publication/228615030_Building_blocks_for_artificial_moral_agents/links/55fabe5708aeafc8ac3fe6f8/Buildingblocks-for-artificial-moral-agents.pdf" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI" target="_blank">https://www.researchgate.net/profile/Vincent_Wiegel/publication/228615030_Building_blocks_for_artificial_moral_agents/links/55fabe5708aeafc8ac3fe6f8/Buildingblocks-for-artificial-moral-agents.pdf</a>. Retrieved 12 Feb 2018.</span></li><li id="CR72"><span class="element-citation">Wiegel V. Wendell Wallach and Colin Allen: Moral machines: Teaching robots right from wrong. <span><span class="ref-journal">Ethics and Information Technology. </span>2010;<span class="ref-vol">12</span>(4):359–361.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Ethics+and+Information+Technology&amp;title=Wendell+Wallach+and+Colin+Allen:+Moral+machines:+Teaching+robots+right+from+wrong&amp;author=V+Wiegel&amp;volume=12&amp;issue=4&amp;publication_year=2010&amp;pages=359-361&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=6591188&amp;issue-id=337042&amp;journal-id=365&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></li></ul></div></div></div><!--post-content--><div class="courtesy-note whole_rhythm small"><hr /><div class="half_rhythm">Articles from <span class="acknowledgment-journal-title">Science and Engineering Ethics</span> are provided here courtesy of <strong>Springer</strong></div><hr /></div><div id="body-link-poppers"><span></span></div></div>
            
        </section>
    </article>
    <aside class="usa-width-one-fourth usa-layout-docs-sidenav pmc-sidebar">
         
  

<div class="scroller">

    
        <section>
                <h6>Other Formats</h6>
                <ul class="pmc-sidebar__formats">
                  <li class="pdf-link other_item"><a href="/pmc/articles/PMC6591188/pdf/11948_2018_Article_30.pdf" class="int-view">PDF (677K)</a></li>
                </ul>
        </section>
    
    <section>
        <h6>Actions</h6>
        <ul class="pmc-sidebar__actions">
            <li>
                <button role="button" class="citation-button citation-dialog-trigger ctxp"
                        aria-label="Open dialog with citation text in different styles" data-ga-category="save_share" data-ga-action="cite" data-ga-label="open"
                        data-all-citations-url="/pmc/resources/citations/6591188/"
                        data-citation-style="nlm"
                        data-download-format-link="/pmc/resources/citations/6591188/export/"
                >
                    <span class="button-label">Cite</span>
                </button>
            </li>
            <li>
                
                    

<link type="text/css" href="ncbi-overlay-block/src/overlay-block.css">
<div class="collections-button-container" data-article-id="6591188" data-article-db="pmc">
  <button class="collections-button collections-dialog-trigger"
          aria-label="Save article in MyNCBI collections."
          data-ga-category="collections_button"
          data-ga-action="click"
          data-ga-label="collections_button"
          data-collections-open-dialog-enabled="false"
          data-collections-open-dialog-url="https://www.ncbi.nlm.nih.gov/account?back_url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6591188%2F%23open-collections-dialog"
          data-in-collections="false">
      <span class="button-label">Collections</span>
  </button>
  <div class="overlay" role="dialog">
  <div id="collections-action-dialog"
       class="dialog collections-dialog"
       aria-hidden="true">
    <div class="title">Add to Collections</div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/pmc/list-existing-collections/"
      data-add-to-existing-collection-url="/pmc/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/pmc/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

  <input type="hidden" name="csrfmiddlewaretoken" value="5tFSprKPu5gcS4N43l6QHkkGZ7m6xgcd2McYOXv8DN5iHZaHXwRmiynh6UTaXYix">

  

  <div class="choice-group" role="radiogroup">
    <ul class="radio-group-items">
      <li>
        <input type="radio"
               id="collections-action-dialog-new-aside "
               class="collections-new"
               name="collections"
               value="new"
               data-ga-category="collections_button"
               data-ga-action="click"
               data-ga-label="collections_radio_new">
        <label for="collections-action-dialog-new-aside ">Create a new collection</label>
      </li>
      <li>
        <input type="radio"
               id="collections-action-dialog-existing-aside "
               class="collections-existing"
               name="collections"
               value="existing"
               checked="true"
               data-ga-category="collections_button"
               data-ga-action="click"
               data-ga-label="collections_radio_existing">
        <label for="collections-action-dialog-existing-aside ">Add to an existing collection</label>
      </li>
    </ul>
  </div>

  <div class="controls-wrapper">
    <div class="action-panel-control-wrap new-collections-controls">
      <label for="collections-action-dialog-add-to-new" class="action-panel-label required-field-asterisk">
        Name your collection:
      </label>
      <input
        type="text"
        name="add-to-new-collection"
        id="collections-action-dialog-add-to-new"
        class="collections-action-add-to-new"
        pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
        maxlength=""
        data-ga-category="collections_button"
        data-ga-action="create_collection"
        data-ga-label="non_favorties_collection">
      <div class="collections-new-name-too-long usa-input-error-message selection-validation-message">
        Name must be less than  characters
      </div>
    </div>
    <div class="action-panel-control-wrap existing-collections-controls">
      <label for="collections-action-dialog-add-to-existing" class="action-panel-label">
        Choose a collection:
      </label>
      <select id="collections-action-dialog-add-to-existing"
              class="action-panel-selector collections-action-add-to-existing"
              data-ga-category="collections_button"
              data-ga-action="select_collection"
              data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
      </select>
      <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
        Unable to load your collection due to an error<br>
        <a href="#">Please try again</a>
      </div>
    </div>
  </div>

  <div class="action-panel-actions">
    <button class="action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
      Add
    </button>
    <button class="action-panel-cancel"
            aria-label="Close 'Add to Collections' panel"
            ref="linksrc=close_collections_panel"
            aria-controls="collections-action-panel"
            aria-expanded="false"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="cancel">
      Cancel
    </button>
  </div>
</form>
    </div>
  </div>
</div>
</div>
                
            </li>

        </ul>
    </section>
    
        <section class="social-sharing">
            <h6>Share</h6>
            <ul class="pmc-sidebar__share">
                <li><a class="fa-stack fa-lg" target="_blank" rel="noopener noreferrer" role="button" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6591188%2F&amp;text=Critiquing%20the%20Reasons%20for%20Making%20Artificial%20Moral%20Agents" alt="Share on Twitter"><i class="fa fa-twitter fa-stack-1x">&#160;</i></a></li> 
<li><a class="fa-stack fa-lg" target="_blank" rel="noopener noreferrer" role="button" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6591188%2F" alt="Share on Facebook"><i class="fa fa-facebook fa-stack-1x">&#160;</i></a></li>
                <li>
                    <div class="share-permalink">
                        <button class="trigger"  alt="Show article permalink" aria-expanded="false" aria-haspopup="true">
                            <i class="fa-stack fa-lg" >
                                <i class="fa fa-link fa-stack-1x">&nbsp;</i>
                            </i>
                        </button>
                        <div class="dropdown dropdown-container" hidden>
                              <div class="title">
                                Permalink
                              </div>
                              <div class="content">
                                  <input type="text" class="permalink-text" value="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6591188/" aria-label="Article permalink"><button class="permalink-copy-button usa-button-primary" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                                      <span class="button-title">Copy</span>
                                  </button>
                              </div>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
    
    <section>
        <h6>RESOURCES</h6>
        <ul class="pmc-sidebar__resources">
        
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_similar_articles"
                            data-ga-label="/pmc/articles/PMC6591188/"
                            class="usa-accordion-button"
                            aria-controls="similar-articles-accordion-aside"
                            aria-expanded="false"
                            data-action-open="open_similar_articles"
                            data-action-close="close_similar_articles"
                    >
                        Similar articles
                    </button>
                    <div
                            
                                data-source-url="/pmc/resources/similar-article-links/29460081/"
                            

                         class="usa-accordion-content pmc-sidebar__resources--citations" id="similar-articles-accordion-aside" aria-hidden="true">
                        
                    </div>
                </div>
            </li>
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_cited_by"
                            data-ga-label="/pmc/articles/PMC6591188/"
                            class="usa-accordion-button"
                            aria-controls="cited-by-accordion-aside"
                            aria-expanded="false"
                            data-action-open="open_cited_by"
                            data-action-close="close_cited_by"
                    >
                        Cited by other articles
                    </button>
                    <div
                            
                                data-source-url="/pmc/resources/cited-by-links/29460081/"
                            
                            class="usa-accordion-content pmc-sidebar__resources--citations"
                            id="cited-by-accordion-aside"
                            aria-hidden="true"
                    >
                        
                    </div>
                </div>
            </li>
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/pmc/articles/PMC6591188/"
                            class="usa-accordion-button"
                            aria-controls="links-accordion-aside"
                            aria-expanded="false"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                    >
                        Links to NCBI Databases
                    </button>
                    <div data-source-url="/pmc/resources/db-links/6591188/" class="usa-accordion-content" id="links-accordion-aside" aria-hidden="true"></div>
                </div>
            </li>

            
        
        </ul>
    </section>

 </div>

    </aside>
</main>

    <div class="overlay" role="dialog" aria-label="Citation Dialog">
  <div class="dialog citation-dialog">
    <button class="close-overlay" tabindex="1">[x]</button>
    <div class="title">Cite</div>
    <div class="citation-text-block">
  <div class="citation-text"></div>
  <div class="citation-actions">
    <button
      class="copy-button dialog-focus"
      data-ga-category="save_share"
      data-ga-action="cite"
      data-ga-label="copy"
      tabindex="2">
      Copy
    </button>

      <a href="#"
              class="export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
          <span class="title">Download .nbib</span>
          <span class="title-mobile">.nbib</span>
      </a>


    

<div class="citation-style-selector-wrapper">
  <label class="selector-label">Format:</label>
  <select aria-label="Format" class="citation-style-selector" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
  </div>
</div>
  </div>
</div>

     <!-- ========== BEGIN FOOTER ========== -->
 <footer>
      <section class="icon-section">
        <div id="icon-section-header" class="icon-section_header">Follow NCBI</div>
        <div class="grid-container container">
          <div class="icon-section_container">
            <a class="footer-icon" id="footer_twitter" href="https://twitter.com/ncbi" aria-label="Twitter"><svg
                data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 300">
                <defs>
                  <style>
                    .cls-11 {
                      fill: #737373;
                    }
                  </style>
                </defs>
                <title>Twitter</title>
                <path class="cls-11"
                  d="M250.11,105.48c-7,3.14-13,3.25-19.27.14,8.12-4.86,8.49-8.27,11.43-17.46a78.8,78.8,0,0,1-25,9.55,39.35,39.35,0,0,0-67,35.85,111.6,111.6,0,0,1-81-41.08A39.37,39.37,0,0,0,81.47,145a39.08,39.08,0,0,1-17.8-4.92c0,.17,0,.33,0,.5a39.32,39.32,0,0,0,31.53,38.54,39.26,39.26,0,0,1-17.75.68,39.37,39.37,0,0,0,36.72,27.3A79.07,79.07,0,0,1,56,223.34,111.31,111.31,0,0,0,116.22,241c72.3,0,111.83-59.9,111.83-111.84,0-1.71,0-3.4-.1-5.09C235.62,118.54,244.84,113.37,250.11,105.48Z">
                </path>
              </svg></a>
            <a class="footer-icon" id="footer_facebook" href="https://www.facebook.com/ncbi.nlm" aria-label="Facebook"><svg
                data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 300">
                <title>Facebook</title>
                <path class="cls-11"
                  d="M210.5,115.12H171.74V97.82c0-8.14,5.39-10,9.19-10h27.14V52l-39.32-.12c-35.66,0-42.42,26.68-42.42,43.77v19.48H99.09v36.32h27.24v109h45.41v-109h35Z">
                </path>
              </svg></a>
            <a class="footer-icon" id="footer_linkedin"
              href="https://www.linkedin.com/company/ncbinlm"
              aria-label="LinkedIn"><svg data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 300">
                <title>LinkedIn</title>
                <path class="cls-11"
                  d="M101.64,243.37H57.79v-114h43.85Zm-22-131.54h-.26c-13.25,0-21.82-10.36-21.82-21.76,0-11.65,8.84-21.15,22.33-21.15S101.7,78.72,102,90.38C102,101.77,93.4,111.83,79.63,111.83Zm100.93,52.61A17.54,17.54,0,0,0,163,182v61.39H119.18s.51-105.23,0-114H163v13a54.33,54.33,0,0,1,34.54-12.66c26,0,44.39,18.8,44.39,55.29v58.35H198.1V182A17.54,17.54,0,0,0,180.56,164.44Z">
                </path>
              </svg></a>
            <a class="footer-icon" id="footer_github" href="https://github.com/ncbi" aria-label="GitHub"><svg
                data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 300">
                <defs>
                  <style>
                    .cls-11,
                    .cls-12 {
                      fill: #737373;
                    }

                    .cls-11 {
                      fill-rule: evenodd;
                    }
                  </style>
                </defs>
                <title>GitHub</title>
                <path class="cls-11"
                  d="M151.36,47.28a105.76,105.76,0,0,0-33.43,206.1c5.28,1,7.22-2.3,7.22-5.09,0-2.52-.09-10.85-.14-19.69-29.42,6.4-35.63-12.48-35.63-12.48-4.81-12.22-11.74-15.47-11.74-15.47-9.59-6.56.73-6.43.73-6.43,10.61.75,16.21,10.9,16.21,10.9,9.43,16.17,24.73,11.49,30.77,8.79,1-6.83,3.69-11.5,6.71-14.14C108.57,197.1,83.88,188,83.88,147.51a40.92,40.92,0,0,1,10.9-28.39c-1.1-2.66-4.72-13.42,1-28,0,0,8.88-2.84,29.09,10.84a100.26,100.26,0,0,1,53,0C198,88.3,206.9,91.14,206.9,91.14c5.76,14.56,2.14,25.32,1,28a40.87,40.87,0,0,1,10.89,28.39c0,40.62-24.74,49.56-48.29,52.18,3.79,3.28,7.17,9.71,7.17,19.58,0,14.15-.12,25.54-.12,29,0,2.82,1.9,6.11,7.26,5.07A105.76,105.76,0,0,0,151.36,47.28Z">
                </path>
                <path class="cls-12"
                  d="M85.66,199.12c-.23.52-1.06.68-1.81.32s-1.2-1.06-.95-1.59,1.06-.69,1.82-.33,1.21,1.07.94,1.6Zm-1.3-1">
                </path>
                <path class="cls-12"
                  d="M90,203.89c-.51.47-1.49.25-2.16-.49a1.61,1.61,0,0,1-.31-2.19c.52-.47,1.47-.25,2.17.49s.82,1.72.3,2.19Zm-1-1.08">
                </path>
                <path class="cls-12"
                  d="M94.12,210c-.65.46-1.71,0-2.37-.91s-.64-2.07,0-2.52,1.7,0,2.36.89.65,2.08,0,2.54Zm0,0"></path>
                <path class="cls-12"
                  d="M99.83,215.87c-.58.64-1.82.47-2.72-.41s-1.18-2.06-.6-2.7,1.83-.46,2.74.41,1.2,2.07.58,2.7Zm0,0">
                </path>
                <path class="cls-12"
                  d="M107.71,219.29c-.26.82-1.45,1.2-2.64.85s-2-1.34-1.74-2.17,1.44-1.23,2.65-.85,2,1.32,1.73,2.17Zm0,0">
                </path>
                <path class="cls-12"
                  d="M116.36,219.92c0,.87-1,1.59-2.24,1.61s-2.29-.68-2.3-1.54,1-1.59,2.26-1.61,2.28.67,2.28,1.54Zm0,0">
                </path>
                <path class="cls-12"
                  d="M124.42,218.55c.15.85-.73,1.72-2,1.95s-2.37-.3-2.52-1.14.73-1.75,2-2,2.37.29,2.53,1.16Zm0,0"></path>
              </svg></a>
            <a class="footer-icon" id="footer_blog" href="https://ncbiinsights.ncbi.nlm.nih.gov/" aria-label="Blog">
              <svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><style>.cls-1{fill:#737373;}</style></defs><path class="cls-1" d="M14,30a4,4,0,1,1-4-4,4,4,0,0,1,4,4Zm11,3A19,19,0,0,0,7.05,15a1,1,0,0,0-1,1v3a1,1,0,0,0,.93,1A14,14,0,0,1,20,33.07,1,1,0,0,0,21,34h3a1,1,0,0,0,1-1Zm9,0A28,28,0,0,0,7,6,1,1,0,0,0,6,7v3a1,1,0,0,0,1,1A23,23,0,0,1,29,33a1,1,0,0,0,1,1h3A1,1,0,0,0,34,33Z"/></svg>
            </a>
          </div>
        </div>
      </section>

      <section class="container-fluid bg-primary">
        <div class="container pt-5">
          <div class="row mt-3">
            <div class="col-lg-3 col-12">
              <p><a class="text-white" href="https://www.nlm.nih.gov/socialmedia/index.html">Connect with NLM</a></p>
              <ul class="list-inline social_media">
                <li class="list-inline-item"><a href="https://twitter.com/NLM_NIH" aria-label="Twitter"
                    target="_blank" rel="noopener noreferrer"><svg version="1.1" xmlns="http://www.w3.org/2000/svg"
                      xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 249 249"
                      style="enable-background:new 0 0 249 249;" xml:space="preserve">
                      <style type="text/css">
                        .st20 {
                          fill: #FFFFFF;
                        }

                        .st30 {
                          fill: none;
                          stroke: #FFFFFF;
                          stroke-width: 8;
                          stroke-miterlimit: 10;
                        }
                      </style>
                      <title>SM-Twitter</title>
                      <g>
                        <g>
                          <g>
                            <path class="st20" d="M192.9,88.1c-5,2.2-9.2,2.3-13.6,0.1c5.7-3.4,6-5.8,8.1-12.3c-5.4,3.2-11.4,5.5-17.6,6.7
                                                c-10.5-11.2-28.1-11.7-39.2-1.2c-7.2,6.8-10.2,16.9-8,26.5c-22.3-1.1-43.1-11.7-57.2-29C58,91.6,61.8,107.9,74,116
                                                c-4.4-0.1-8.7-1.3-12.6-3.4c0,0.1,0,0.2,0,0.4c0,13.2,9.3,24.6,22.3,27.2c-4.1,1.1-8.4,1.3-12.5,0.5c3.6,11.3,14,19,25.9,19.3
                                                c-11.6,9.1-26.4,13.2-41.1,11.5c12.7,8.1,27.4,12.5,42.5,12.5c51,0,78.9-42.2,78.9-78.9c0-1.2,0-2.4-0.1-3.6
                                                C182.7,97.4,189.2,93.7,192.9,88.1z"></path>
                          </g>
                        </g>
                        <circle class="st30" cx="124.4" cy="128.8" r="108.2"></circle>
                      </g>
                    </svg></a></li>
                <li class="list-inline-item"><a href="https://www.facebook.com/nationallibraryofmedicine"
                    aria-label="Facebook" rel="noopener noreferrer" target="_blank">
                    <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px"
                      y="0px" viewBox="0 0 249 249" style="enable-background:new 0 0 249 249;" xml:space="preserve">
                      <style type="text/css">
                        .st10 {
                          fill: #FFFFFF;
                        }

                        .st110 {
                          fill: none;
                          stroke: #FFFFFF;
                          stroke-width: 8;
                          stroke-miterlimit: 10;
                        }
                      </style>
                      <title>SM-Facebook</title>
                      <g>
                        <g>
                          <path class="st10" d="M159,99.1h-24V88.4c0-5,3.3-6.2,5.7-6.2h16.8V60l-24.4-0.1c-22.1,0-26.2,16.5-26.2,27.1v12.1H90v22.5h16.9
                                                      v67.5H135v-67.5h21.7L159,99.1z"></path>
                        </g>
                      </g>
                      <circle class="st110" cx="123.6" cy="123.2" r="108.2"></circle>
                    </svg>
                  </a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/user/NLMNIH" aria-label="Youtube"
                    target="_blank" rel="noopener noreferrer"><svg version="1.1" xmlns="http://www.w3.org/2000/svg"
                      xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 249 249"
                      style="enable-background:new 0 0 249 249;" xml:space="preserve">
                      <title>SM-Youtube</title>
                      <style type="text/css">
                        .st4 {
                          fill: none;
                          stroke: #FFFFFF;
                          stroke-width: 8;
                          stroke-miterlimit: 10;
                        }

                        .st5 {
                          fill: #FFFFFF;
                        }
                      </style>
                      <circle class="st4" cx="124.2" cy="123.4" r="108.2"></circle>
                      <g transform="translate(0,-952.36218)">
                        <path class="st5"
                          d="M88.4,1037.4c-10.4,0-18.7,8.3-18.7,18.7v40.1c0,10.4,8.3,18.7,18.7,18.7h72.1c10.4,0,18.7-8.3,18.7-18.7
                                            v-40.1c0-10.4-8.3-18.7-18.7-18.7H88.4z M115.2,1058.8l29.4,17.4l-29.4,17.4V1058.8z"></path>
                      </g>
                    </svg></a></li>
              </ul>
            </div>
            <div class="col-lg-3 col-12">
              <p class="address_footer text-white">National Library of Medicine<br>
                <a href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/@38.9959508,-77.101021,17z/data=!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb:0x19156f88b27635b8!8m2!3d38.9959508!4d-77.0988323"
                  class="text-white" target="_blank" rel="noopener noreferrer">8600 Rockville Pike<br>
                  Bethesda, MD 20894</a></p>
            </div>
            <div class="col-lg-3 col-12 centered-lg">
              <p><a href="https://www.nlm.nih.gov/web_policies.html" class="text-white">Web Policies</a><br>
                <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office"
                  class="text-white">FOIA</a><br>
                <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="text-white" id="vdp">HHS Vulnerability Disclosure</a></p>
            </div>
            <div class="col-lg-3 col-12 centered-lg">
              <p><a class="supportLink text-white" href="https://support.nlm.nih.gov/">Help</a><br>
                <a href="https://www.nlm.nih.gov/accessibility.html" class="text-white">Accessibility</a><br>
                <a href="https://www.nlm.nih.gov/careers/careers.html" class="text-white">Careers</a></p>
            </div>
          </div>
          <div class="row">
            <div class="col-lg-12 centered-lg">
              <nav class="bottom-links">
                <ul class="mt-3">
                  <li>
                    <a class="text-white" href="//www.nlm.nih.gov/">NLM</a>
                  </li>
                  <li>
                    <a class="text-white"
                  href="https://www.nih.gov/">NIH</a>
                  </li>
                  <li>
                    <a class="text-white" href="https://www.hhs.gov/">HHS</a>
                  </li>
                  <li>
                    <a
                  class="text-white" href="https://www.usa.gov/">USA.gov</a>
                  </li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </section>
    </footer>
 <!-- ========== END FOOTER ========== -->
  <!-- javascript to inject NWDS meta tags. Note: value of nwds_version is updated by "npm version" command -->
 
  <script type="text/javascript">
    var nwds_version = "1.1.9-2";

    var meta_nwds_ver = document.createElement('meta');
    meta_nwds_ver.name = 'ncbi_nwds_ver';
    meta_nwds_ver.content = nwds_version;
    document.getElementsByTagName('head')[0].appendChild(meta_nwds_ver);

    var meta_nwds = document.createElement('meta');
    meta_nwds.name = 'ncbi_nwds';
    meta_nwds.content = 'yes';
    document.getElementsByTagName('head')[0].appendChild(meta_nwds);

	var alertsUrl = "/core/alerts/alerts.js";
	if (typeof ncbiBaseUrl !== 'undefined') {
		alertsUrl = ncbiBaseUrl + alertsUrl;
	}
  </script>



  
    <!-- JavaScript -->
    <script src="/pmc/static/CACHE/js/output.0f72d6a64937.js"></script>
  
  
    <script src="https://code.jquery.com/jquery-3.5.0.min.js"
          integrity="sha256-xNzN2a4ltkB44Mc/Jz3pT4iU1cmeR0FkXs4pru/JxaQ="
          crossorigin="anonymous">
    </script>
    <script>
        var fallbackJquery = "/pmc/static/base/js/jquery-3.5.0.min.js";
        window.jQuery || document.write("<script src=" + fallbackJquery + ">\x3C/script>")
    </script>
  

  <script src="/pmc/static/CACHE/js/output.a212a9fcf845.js"></script>
<script src="/pmc/static/CACHE/js/output.7999321d1aac.js"></script>
<script src="/pmc/static/CACHE/js/output.7ca436b2ea51.js"></script>
<script src="/pmc/static/CACHE/js/output.f8422046fbe0.js"></script>
<script src="/pmc/static/CACHE/js/output.ff40c7d85ff8.js"></script>
<script src="/pmc/static/CACHE/js/output.a6a84a0ad361.js"></script>

<script type="text/javascript" src="/pmc/static/bundles/base/base.038d36e4e31104b40a19.js" ></script>

    <script type="text/javascript">
        if(typeof jQuery !=='undefined') {
            jQuery.migrateMute = true;
        }
    </script>
    <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.4.1.js"></script>
    <script type="text/javascript" src="/core/jig/1.15.2/js/jig.nojquery.min.js">//</script><script type="text/javascript" src="/corehtml/pmc/js/common.min.js?_=3.18">//</script><script type="text/javascript" src="/corehtml/pmc/js/NcbiTagServer.min.js?_=3.18">//</script><script type="text/javascript" src="/corehtml/pmc/js/crb.min.js?_=3.18">//</script><script type="text/javascript" src="/corehtml/pmc/js/jactions.min.js?_=3.18">//</script><script type="text/javascript">window.name="mainwindow";</script>

    <script type="text/javascript">var exports = {};</script>
    <script src="/pmc/static/CACHE/js/output.340a3b9cce7f.js"></script>
<script src="/pmc/static/CACHE/js/output.228f96f3298e.js"></script>
    <script type="text/javascript" src="/pmc/static/bundles/article/article.b7dd20e527283dd68f45.js" ></script>
    <script type="text/javascript">
        window.ncbi.pmc.articlePage.init({ pageURL: '/pmc/articles/PMC6591188/', citeCookieName: 'pmc-cf'});
    </script>


  
  
  <script  type="text/javascript" src="https://www.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


  
      
  

</body>
</html>
