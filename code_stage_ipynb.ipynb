{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import warnings\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.utils.validation import _check_sample_weight, _deprecate_positional_args\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from sklearn.cluster._dbscan_inner import dbscan_inner\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.spatial import distance\n",
    "from hashlib import blake2b\n",
    "import random\n",
    "from nltk.corpus import wordnet  # Import wordnet from the NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "@_deprecate_positional_args\n",
    "def dbscan(X, eps=0.5, *, min_samples=5, metric='minkowski',\n",
    "           metric_params=None, algorithm='auto', leaf_size=30, p=2,\n",
    "           sample_weight=None, n_jobs=None):\n",
    "\n",
    "    est = DBSCAN(eps=eps, min_samples=min_samples, metric=metric,\n",
    "                 metric_params=metric_params, algorithm=algorithm,\n",
    "                 leaf_size=leaf_size, p=p, n_jobs=n_jobs)\n",
    "    est.fit(X, sample_weight=sample_weight)\n",
    "    return est.core_sample_indices_, est.labels_\n",
    "\n",
    "\n",
    "class DBSCAN(ClusterMixin, BaseEstimator):\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, eps=0.5, *, min_samples=5, metric='euclidean',\n",
    "                 metric_params=None, algorithm='auto', leaf_size=30, p=None,\n",
    "                 n_jobs=None, allwords=[], cluster_name_index=\"no_cluster_name\"):\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        self.metric = metric\n",
    "        self.metric_params = metric_params\n",
    "        self.algorithm = algorithm\n",
    "        self.leaf_size = leaf_size\n",
    "        self.p = p\n",
    "        self.n_jobs = n_jobs\n",
    "        self.clusters = []\n",
    "        self.vectors = ft\n",
    "        self.allwords = allwords\n",
    "        self.es = None\n",
    "        self.cluster_name_index = cluster_name_index\n",
    "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "\n",
    "        X = self._validate_data(X, accept_sparse='csr')\n",
    "\n",
    "        if not self.eps > 0.0:\n",
    "            raise ValueError(\"eps must be positive.\")\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = _check_sample_weight(sample_weight, X)\n",
    "\n",
    "        # Calculate neighborhood for all samples. This leaves the original\n",
    "        # point in, which needs to be considered later (i.e. point i is in the\n",
    "        # neighborhood of point i. While True, its useless information)\n",
    "        if self.metric == 'precomputed' and sparse.issparse(X):\n",
    "            # set the diagonal to explicit values, as a point is its own\n",
    "            # neighbor\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)\n",
    "                X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n",
    "\n",
    "        neighbors_model = NearestNeighbors(\n",
    "            radius=self.eps, algorithm=self.algorithm,\n",
    "            leaf_size=self.leaf_size, metric=self.metric,\n",
    "            metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)\n",
    "        neighbors_model.fit(X)\n",
    "        # This has worst case O(n^2) memory complexity\n",
    "        neighborhoods = neighbors_model.radius_neighbors(X,\n",
    "                                                         return_distance=False)\n",
    "        if sample_weight is None:\n",
    "            n_neighbors = np.array([len(neighbors)\n",
    "                                    for neighbors in neighborhoods])\n",
    "        else:\n",
    "            n_neighbors = np.array([np.sum(sample_weight[neighbors])\n",
    "                                    for neighbors in neighborhoods])\n",
    "\n",
    "        # Initially, all samples are noise.\n",
    "        labels = np.full(X.shape[0], -1, dtype=np.intp)\n",
    "\n",
    "        # A list of all core samples found.\n",
    "        core_samples = np.asarray(n_neighbors >= self.min_samples,\n",
    "                                  dtype=np.uint8)\n",
    "        dbscan_inner(core_samples, neighborhoods, labels)\n",
    "\n",
    "        self.core_sample_indices_ = np.where(core_samples)[0]\n",
    "        self.labels_ = labels\n",
    "\n",
    "        if len(self.core_sample_indices_):\n",
    "            # fix for scipy sparse indexing issue\n",
    "            self.components_ = X[self.core_sample_indices_].copy()\n",
    "        else:\n",
    "            # no core samples\n",
    "            self.components_ = np.empty((0, X.shape[1]))\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X, y=None, sample_weight=None):\n",
    "\n",
    "        self.fit(X, sample_weight=sample_weight)\n",
    "        return self.labels_\n",
    "\n",
    "    def get_words(self, rarray, pos):\n",
    "        \"\"\"\n",
    "        Convert a NdArray in List\n",
    "        :param ndrarray:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        l = []\n",
    "        for e in rarray:\n",
    "            self.allwords[e][\"cluster_id\"] = pos\n",
    "            self.allwords[e][\"in_cluster\"] = True\n",
    "            l.append(self.allwords[e][\"word\"])\n",
    "        return l\n",
    "\n",
    "    def build_clusters(self):\n",
    "\n",
    "        for pos in set(self.labels_):\n",
    "            if pos != -1:\n",
    "                pos_list = (np.where(self.labels_ == pos)[0]).tolist()\n",
    "                wordhas = self.allwords[pos_list[0]][\"word\"] + \\\n",
    "                    str(np.array(random.sample(range(0, 500), 8)).sum())\n",
    "                idhas = self.gethash(wordhas)\n",
    "                words = self.get_words(pos_list, idhas)\n",
    "                cluster = {\n",
    "                    'id': idhas,\n",
    "                    'centroid': np.mean(self.vectors.query(words), axis=0),\n",
    "                    'words': words,\n",
    "                }\n",
    "                self.clusters.append(cluster)\n",
    "                self.es.index(index=self.cluster_name_index,\n",
    "                              id=cluster[\"id\"], body=cluster)\n",
    "            else:\n",
    "                for index, e in enumerate(np.where(self.labels_ == pos)[0]):\n",
    "                    words = [self.allwords[e][\"word\"]]\n",
    "                    wordhas = words[0] + \\\n",
    "                        str(np.array(random.sample(range(0, 500), 8)).sum())\n",
    "                    idhas = self.gethash(wordhas)\n",
    "                    self.allwords[e][\"cluster_id\"] = idhas\n",
    "                    self.allwords[e][\"in_cluster\"] = False\n",
    "                    cluster = {\n",
    "                        'id': idhas,\n",
    "                        'centroid': self.vectors.query(words[0]),\n",
    "                        'words': words,\n",
    "                    }\n",
    "                    self.clusters.append(cluster)\n",
    "                    self.es.index(index=self.cluster_name_index,\n",
    "                                  id=cluster[\"id\"], body=cluster)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        point = {\n",
    "            \"word\": word,\n",
    "            \"in_cluster\": False,\n",
    "            \"cluster_id\": None,\n",
    "        }\n",
    "        n_neighbors = self.epsilonVoisinage(point)\n",
    "\n",
    "        if len(n_neighbors) == 0:\n",
    "            wordhas = word+str(np.array(random.sample(range(0, 500), 8)).sum())\n",
    "            idhas = self.gethash(wordhas)\n",
    "            point[\"cluster_id\"] = idhas\n",
    "            self.allwords.append(point)\n",
    "            cluster = {\n",
    "                'id': idhas,\n",
    "                'centroid': self.vectors.query(word),\n",
    "                'words': [word],\n",
    "            }\n",
    "            self.clusters.append(cluster)\n",
    "            self.es.index(index=self.cluster_name_index,\n",
    "                          id=cluster[\"id\"], body=cluster)\n",
    "\n",
    "        else:\n",
    "            on_include = False\n",
    "            for neighbor in n_neighbors:\n",
    "                if neighbor[\"in_cluster\"] == True:\n",
    "                    on_include = True\n",
    "                    break\n",
    "\n",
    "            if on_include == False:\n",
    "                words = []\n",
    "                wordhas = word + \\\n",
    "                    str(np.array(random.sample(range(0, 500), 8)).sum())\n",
    "                idhas = self.gethash(wordhas)\n",
    "                for neighbor in n_neighbors:\n",
    "                    words.append(neighbor[\"word\"])\n",
    "                    self.es.delete(index=self.cluster_name_index,\n",
    "                                   id=neighbor[\"cluster_id\"])\n",
    "                    self.delete_cluster(neighbor[\"cluster_id\"])\n",
    "                    self.allwords[self.allwords.index(\n",
    "                        neighbor)][\"cluster_id\"] = idhas\n",
    "                    self.allwords[self.allwords.index(\n",
    "                        neighbor)][\"in_cluster\"] = True\n",
    "\n",
    "                words.append(word)\n",
    "                point[\"cluster_id\"] = idhas\n",
    "                point[\"in_cluster\"] = True\n",
    "                self.allwords.append(point)\n",
    "                cluster = {\n",
    "                    'id': idhas,\n",
    "                    'centroid': np.mean(self.vectors.query(words), axis=0),\n",
    "                    'words': words,\n",
    "                }\n",
    "                self.clusters.append(cluster)\n",
    "                self.es.index(index=self.cluster_name_index,\n",
    "                              id=cluster[\"id\"], body=cluster)\n",
    "\n",
    "            else:\n",
    "                words = []\n",
    "                idhas = n_neighbors[0][\"cluster_id\"]\n",
    "                for neighbor in n_neighbors:\n",
    "                    cluster = self.get_cluster(neighbor[\"cluster_id\"])\n",
    "                    if cluster != -1:\n",
    "                        words.extend(cluster[\"words\"])\n",
    "                        self.es.delete(\n",
    "                            index=self.cluster_name_index, id=cluster[\"id\"])\n",
    "                        self.delete_cluster(cluster[\"id\"])\n",
    "\n",
    "                self.update_id(words, idhas)\n",
    "                words.append(word)\n",
    "                point[\"cluster_id\"] = idhas\n",
    "                point[\"in_cluster\"] = True\n",
    "                self.allwords.append(point)\n",
    "                cluster = {\n",
    "                    'id': idhas,\n",
    "                    'centroid': np.mean(self.vectors.query(words), axis=0),\n",
    "                    'words': words,\n",
    "                }\n",
    "                self.clusters.append(cluster)\n",
    "                self.es.index(index=self.cluster_name_index,\n",
    "                              id=cluster[\"id\"], body=cluster)\n",
    "\n",
    "    def delete_cluster(self, cluster_id):\n",
    "\n",
    "        for cluster in self.clusters:\n",
    "\n",
    "            if cluster['id'] == cluster_id:\n",
    "                del self.clusters[self.clusters.index(cluster)]\n",
    "                break\n",
    "\n",
    "    def get_cluster(self, cluster_id):\n",
    "\n",
    "        for cluster in self.clusters:\n",
    "            if cluster[\"id\"] == cluster_id:\n",
    "                return cluster\n",
    "        return -1\n",
    "\n",
    "    def update_id(self, words, id):\n",
    "        for i in range(0, len(self.allwords)):\n",
    "            if self.allwords[i][\"word\"] in words:\n",
    "                self.allwords[i][\"cluster_id\"] = id\n",
    "                self.allwords[i][\"in_cluster\"] = True\n",
    "\n",
    "    def merge(self, n_neighbors, word):\n",
    "        #print(\"bonjour\")\n",
    "\n",
    "    def epsilonVoisinage(self, P):\n",
    "        voisins = []\n",
    "        for e in self.allwords:\n",
    "            if e[\"word\"] == P[\"word\"]:\n",
    "                continue\n",
    "            if distance.cosine(self.vectors.query(e[\"word\"]), self.vectors.query(P[\"word\"])) < self.eps:\n",
    "                voisins.append(e)\n",
    "\n",
    "        return voisins\n",
    "\n",
    "    def gethash(self, word):\n",
    "        h = blake2b(digest_size=35)\n",
    "        h.update(str(word).encode('utf-8'))\n",
    "        return h.hexdigest()\n",
    "\n",
    "    def add_cluster(self, word, is_pronom, word_typ):\n",
    "        point = {\n",
    "            \"word\": word,\n",
    "            \"in_cluster\": False,\n",
    "            \"cluster_id\": None,\n",
    "        }\n",
    "\n",
    "        if len(self.clusters) == 0 or is_pronom == True or word_typ == \"NUM\":\n",
    "            wordhas = word+str(np.array(random.sample(range(0, 500), 8)).sum())\n",
    "            idhas = self.gethash(wordhas)\n",
    "            point[\"cluster_id\"] = idhas\n",
    "            self.allwords.append(point)\n",
    "            cluster = {\n",
    "                'id': idhas,\n",
    "                'centroid': self.vectors.query(word),\n",
    "                'words': [word],\n",
    "            }\n",
    "            self.clusters.append(cluster)\n",
    "            # self.es.index(index=self.cluster_name_index, id=cluster[\"id\"], body=cluster)\n",
    "\n",
    "        else:\n",
    "            clustermin_index = None\n",
    "            vector_word = self.vectors.query(word)\n",
    "            val_min = distance.cosine(\n",
    "                self.clusters[0][\"centroid\"], vector_word)\n",
    "            for index, cluster in enumerate(self.clusters):\n",
    "                dis = distance.cosine(cluster[\"centroid\"], vector_word)\n",
    "                if dis < self.eps and val_min >= dis:\n",
    "                    clustermin_index = index\n",
    "                    val_min = dis\n",
    "\n",
    "            if clustermin_index == None:\n",
    "                wordhas = word + \\\n",
    "                    str(np.array(random.sample(range(0, 500), 8)).sum())\n",
    "                idhas = self.gethash(wordhas)\n",
    "                point[\"cluster_id\"] = idhas\n",
    "                self.allwords.append(point)\n",
    "                cluster = {\n",
    "                    'id': idhas,\n",
    "                    'centroid': vector_word,\n",
    "                    'words': [word],\n",
    "                }\n",
    "                self.clusters.append(cluster)\n",
    "               # self.es.index(index=self.cluster_name_index, id=cluster[\"id\"], body=cluster)\n",
    "\n",
    "            else:\n",
    "                if word_typ == \"ADJ\" or word_typ == \"NOUN\" or word_typ == \"VERB\" or word_typ == \"ADV\":\n",
    "                    if self.get_ontonym(self.clusters[clustermin_index][\"words\"][0], word) == True:\n",
    "                        wordhas = word + \\\n",
    "                            str(np.array(random.sample(range(0, 500), 8)).sum())\n",
    "                        idhas = self.gethash(wordhas)\n",
    "                        point[\"cluster_id\"] = idhas\n",
    "                        self.allwords.append(point)\n",
    "                        cluster = {\n",
    "                            'id': idhas,\n",
    "                            'centroid': self.vectors.query(word),\n",
    "                            'words': [word],\n",
    "                        }\n",
    "                        self.clusters.append(cluster)\n",
    "                    else:\n",
    "                        self.clusters[clustermin_index][\"words\"].append(word)\n",
    "                else:\n",
    "                    self.clusters[clustermin_index][\"words\"].append(word)\n",
    "\n",
    "               # self.es.index(index=self.cluster_name_index, id=self.clusters[clustermin_index][\"id\"], body=self.clusters[clustermin_index])\n",
    "    def get_ontonym(self, word1, word2):\n",
    "        ant = list()\n",
    "        wordlema = self.wordnet_lemmatizer.lemmatize(word1, pos=\"v\")\n",
    "        wordlema2 = self.wordnet_lemmatizer.lemmatize(word2, pos=\"v\")\n",
    "        for synset in wordnet.synsets(wordlema):\n",
    "            for lemma in synset.lemmas():\n",
    "                if lemma.antonyms():\n",
    "                    # When antonyms are available, add them into the list\n",
    "                    ant.append(lemma.antonyms()[0].name())\n",
    "        return wordlema2 in ant\n",
    "\n",
    "    def get_mean(self, u, v, longeur):\n",
    "        c = longeur*u\n",
    "        f = (c+v)/(longeur+1)\n",
    "        return f\n",
    "\n",
    "\n",
    "def traitement(text):\n",
    "    import re\n",
    "    from nltk.tokenize import WordPunctTokenizer\n",
    "    from nltk.corpus import stopwords\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    text_trait = text\n",
    "    text_trait = re.sub(r'#\\S+', \"\", text_trait)\n",
    "    text_trait = re.sub(r'@\\S+', \"\", text_trait)\n",
    "    text_trait = re.sub(r'\\S*@\\S*\\s?', \"\", text_trait)\n",
    "    text_trait = re.sub(r'http\\S+', \"\", text_trait)\n",
    "    text_trait = re.sub(r'word01|word02|word03', \"\", text_trait)\n",
    "    text_trait = re.sub(r\"[^A-Za-z0-9]''\", \"\", text_trait)\n",
    "    text_trait = re.sub(f'\\d+', \"\", text_trait)\n",
    "    text_trait = re.sub(r'<[^>]*>', \"\", text_trait)\n",
    "    text_trait = re.sub(\"[^A-Za-z0-9|' ']+\", \"\", text_trait)\n",
    "    doc = nlp(text_trait)\n",
    "    or_per_loc = []\n",
    "    for ent in doc.ents:\n",
    "        # #print('_'.join(ent.text.split(' ')).lower(), ent.label_)\n",
    "        if ent.label_ == \"PERSON\" or ent.label_ == \"GPE\" or ent.label_ == \"ORG\":\n",
    "            or_per_loc.append('_'.join(ent.text.split(' ')).lower())\n",
    "\n",
    "    return doc, or_per_loc\n",
    "\n",
    "\n",
    "def get_ontonym(word1, word2):\n",
    "    ant = list()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    wordlema = wordnet_lemmatizer.lemmatize(word1, pos=\"v\")\n",
    "    wordlema2 = wordnet_lemmatizer.lemmatize(word2, pos=\"v\")\n",
    "    for synset in wordnet.synsets(wordlema):\n",
    "        for lemma in synset.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                # When antonyms are available, add them into the list\n",
    "                ant.append(lemma.antonyms()[0].name())\n",
    "    #print(ant)\n",
    "    return wordlema2 in ant\n",
    "\n",
    "\n",
    "def progressbar(it, prefix=\"\", size=60, file=sys.stdout):\n",
    "    count = len(it)\n",
    "\n",
    "    def show(j):\n",
    "        x = int(size*j/count)\n",
    "        file.write(\"%s[%s%s] %i/%i\\r\" %\n",
    "                   (prefix, \"#\"*x, \".\"*(size-x), j, count))\n",
    "        file.flush()\n",
    "    show(0)\n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        show(i+1)\n",
    "    file.write(\"\\n\")\n",
    "    file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from hashlib import blake2b\n",
    "import json\n",
    "from collections import Counter\n",
    "f = open('train-v2.0.json',)\n",
    "data = json.load(f)\n",
    "\n",
    "\n",
    "def gethash(word):\n",
    "    h = blake2b(digest_size=35)\n",
    "    h.update(str(word).encode('utf-8'))\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "text = \"\"\n",
    "questions = []\n",
    "text_docs = []\n",
    "docs_ids = []\n",
    "tokens = []\n",
    "org_loc_per = []\n",
    "for j in progressbar(range(0, 200), \"Computing: \", 80):\n",
    "    for i in range(len(data[\"data\"])*j//200, len(data[\"data\"])*(j+1)//200):\n",
    "        text += data[\"data\"][i][\"title\"]\n",
    "        for element in data[\"data\"][i]['paragraphs'][0:7]:\n",
    "            wordhas = element[\"context\"][0:15] + \\\n",
    "                str(np.array(random.sample(range(0, 500), 8)).sum())\n",
    "            idhas = gethash(wordhas)\n",
    "            for quest in element[\"qas\"][0:5]:\n",
    "                questions.append({\"ques\": quest[\"question\"], \"res_id\": idhas})\n",
    "\n",
    "            text += element[\"context\"]\n",
    "            text_docs.append(element[\"context\"])\n",
    "            docs_ids.append(idhas)\n",
    "            text += \" \"\n",
    "\n",
    "    tokens1, org_loc_per1 = traitement(text)\n",
    "    tokens.extend(tokens)\n",
    "    org_loc_per.extend(org_loc_per1)\n",
    "    tokens = list(set(tokens))\n",
    "    org_loc_per = list(set(org_loc_per))\n",
    "\n",
    "#print(len(tokens))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
